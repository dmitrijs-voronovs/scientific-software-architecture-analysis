quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability," ```python; sc.pl.spatial(adata, color=""leiden"", groups=[""0""]); ```; <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102686727-cc8e5f00-41e9-11eb-8d61-5c53700b39d7.png). </details>. Finally, all the image processing part is removed from embedding and only present in spatial. --------------------. > No behaviour changes in embedding if the basis is called ""spatial"" vs anything else, this should be triggered by calling the spatial function. this is addressed, embedding changes behaviour only if img is passed, but has nothing to do with spatial, there is a small trick, and has to do with `ax.invert_yaxis()`. See following point. --------------------. > When spatial is called, it’s always shapes being drawn on an image. If there isn’t an image passed, an empty image would be generated. There would be no scatter plot case here. I played around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:2755,avail,available,2755,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,1,['avail'],['available']
Availability," `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1212,error,error,1212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['error'],['error']
Availability," as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4686,error,error,4686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['error'],['error']
Availability," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:1732,down,downstream,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,1,['down'],['downstream']
Availability," setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that makes it easier to reason about how our test suite works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:2254,down,down,2254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,1,['down'],['down']
Availability," y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:4482,error,errors,4482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['error'],['errors']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2201?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@2c55a14`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2201 +/- ##; ========================================; Coverage ? 71.95% ; ========================================; Files ? 98 ; Lines ? 11538 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3236 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2201#issuecomment-1086301791:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2201#issuecomment-1086301791,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2202?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@2c55a14`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2202 +/- ##; ========================================; Coverage ? 71.95% ; ========================================; Files ? 98 ; Lines ? 11538 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3236 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2202#issuecomment-1086316533:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2202#issuecomment-1086316533,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2213?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@3ac9169`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 9c0054f differs from pull request most recent head 02123f2. Consider uploading reports for the commit 02123f2 to get more accurate results. ```diff; @@ Coverage Diff @@; ## 1.9.x #2213 +/- ##; ========================================; Coverage ? 71.94% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3237 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2213#issuecomment-1088659791:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2213#issuecomment-1088659791,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2221?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@6cc7541`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2221 +/- ##; ========================================; Coverage ? 71.94% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3237 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2221#issuecomment-1088855272:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2221#issuecomment-1088855272,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2226?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a08c155`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2226 +/- ##; ========================================; Coverage ? 71.82% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8288 ; Misses ? 3251 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2226#issuecomment-1090215514:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2226#issuecomment-1090215514,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2241?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`master@cab9f78`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2241 +/- ##; =========================================; Coverage ? 71.74% ; =========================================; Files ? 99 ; Lines ? 11560 ; Branches ? 0 ; =========================================; Hits ? 8294 ; Misses ? 3266 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2241#issuecomment-1104534706:327,error,error-reference,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2241#issuecomment-1104534706,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2275?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@5bcb539`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2275 +/- ##; ========================================; Coverage ? 71.82% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8288 ; Misses ? 3251 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2275#issuecomment-1156972689:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2275#issuecomment-1156972689,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2279?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@4d0d8be`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2279 +/- ##; ========================================; Coverage ? 71.82% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8288 ; Misses ? 3251 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2279#issuecomment-1157013971:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2279#issuecomment-1157013971,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2348?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@6b5f786`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 6fb5f26 differs from pull request most recent head fb557a1. Consider uploading reports for the commit fb557a1 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2348 +/- ##; ========================================; Coverage ? 71.77% ; ========================================; Files ? 97 ; Lines ? 11519 ; Branches ? 0 ; ========================================; Hits ? 8268 ; Misses ? 3251 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2348#issuecomment-1273857794:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2348#issuecomment-1273857794,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2350?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@cf6d820`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2350 +/- ##; ========================================; Coverage ? 71.77% ; ========================================; Files ? 97 ; Lines ? 11519 ; Branches ? 0 ; ========================================; Hits ? 8268 ; Misses ? 3251 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2350#issuecomment-1274792439:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2350#issuecomment-1274792439,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2419?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@97c2617`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 72ea692 differs from pull request most recent head 8fb038a. Consider uploading reports for the commit 8fb038a to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2419 +/- ##; ========================================; Coverage ? 71.83% ; ========================================; Files ? 98 ; Lines ? 11543 ; Branches ? 0 ; ========================================; Hits ? 8292 ; Misses ? 3251 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2419#issuecomment-1433063184:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2419#issuecomment-1433063184,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2435?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1fbbfcd`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2435 +/- ##; ========================================; Coverage ? 71.88% ; ========================================; Files ? 98 ; Lines ? 11546 ; Branches ? 0 ; ========================================; Hits ? 8300 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2435#issuecomment-1451099582:326,error,error-reference,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2435#issuecomment-1451099582,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`master@62bb643`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `74.48%`. ```diff; @@ Coverage Diff @@; ## master #1413 +/- ##; =========================================; Coverage ? 71.34% ; =========================================; Files ? 92 ; Lines ? 11186 ; Branches ? 0 ; =========================================; Hits ? 7981 ; Misses ? 3205 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `70.62% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462:329,error,error-reference,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@c943b93`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `71.86%`. [![Impacted file tree graph](https://codecov.io/gh/theislab/scanpy/pull/1693/graphs/tree.svg?width=650&height=150&src=pr&token=UsIEoV0aqg)](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #1693 +/- ##; =========================================; Coverage ? 71.32% ; =========================================; Files ? 89 ; Lines ? 10969 ; Branches ? 0 ; =========================================; Hits ? 7824 ; Misses ? 3145 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_dotplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19kb3RwbG90LnB5) | `86.79% <ø> (ø)` | |; | [scanpy/plotting/\_matrixplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:228,error,error-reference,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1920?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8750212`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1920 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1923?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@d0f851f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1923 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1924?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@1605f63`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1924 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8053 ; Misses ? 3193 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1935?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8c51f19`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1935 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1938?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@2883269`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1938 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1947?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@9360422`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1947 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1952?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@c9d319e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1952 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1961?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@aeaa07b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1961 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1962?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@370d1c6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1962 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669,1,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1966?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@b9cd8c8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1966 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703:328,error,error-reference,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703,1,['error'],['error-reference']
Availability,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1828,error,errors,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,1,['error'],['errors']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2516?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@2979f99`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2516 +/- ##; ========================================; Coverage ? 71.89% ; ========================================; Files ? 98 ; Lines ? 11488 ; Branches ? 0 ; ========================================; Hits ? 8259 ; Misses ? 3229 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2516#issuecomment-1596813650:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2516#issuecomment-1596813650,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2523?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@ec78ca9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 249ef59 differs from pull request most recent head c69bc0f. Consider uploading reports for the commit c69bc0f to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2523 +/- ##; ========================================; Coverage ? 71.89% ; ========================================; Files ? 98 ; Lines ? 11488 ; Branches ? 0 ; ========================================; Hits ? 8259 ; Misses ? 3229 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2523#issuecomment-1599208625:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2523#issuecomment-1599208625,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2528?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@af11c8f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2528 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2528#issuecomment-1604254568:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528#issuecomment-1604254568,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2538?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@120dcd0`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2538 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2538#issuecomment-1609797792:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2538#issuecomment-1609797792,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2549?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@c40d2ee`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2549 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2549#issuecomment-1625396556:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549#issuecomment-1625396556,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2567?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@759960d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2567 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2567#issuecomment-1645436066:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2567#issuecomment-1645436066,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2568?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@759960d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2568 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2568#issuecomment-1645437518:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2568#issuecomment-1645437518,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2574?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@053f47e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2574 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2574#issuecomment-1649346773:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574#issuecomment-1649346773,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2597?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@64fab42`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2597 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2597#issuecomment-1665680324:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2597#issuecomment-1665680324,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2606?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@b5506e1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2606 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8404 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2606#issuecomment-1670939429:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2606#issuecomment-1670939429,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2613?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a8b931a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2613 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2613#issuecomment-1677226444:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2613#issuecomment-1677226444,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2616?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a8b931a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2616 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8404 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2616#issuecomment-1677462171:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2616#issuecomment-1677462171,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2619?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@fbd73ac`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2619 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2619#issuecomment-1678888447:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2619#issuecomment-1678888447,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2620?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a6f6c6d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2620 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2620#issuecomment-1681915003:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2620#issuecomment-1681915003,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2638?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@9ba0251`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2638 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2638#issuecomment-1691735842:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2638#issuecomment-1691735842,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2641?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@8aa93e2`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2641 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2641#issuecomment-1691930772:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2641#issuecomment-1691930772,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2643?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@6e8a8b4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2643 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2643#issuecomment-1696930445:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2643#issuecomment-1696930445,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2652?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@16e7e9f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2652 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2652#issuecomment-1706132242:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2652#issuecomment-1706132242,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2659?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@efce8f8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head eea03bb differs from pull request most recent head 256ce44. Consider uploading reports for the commit 256ce44 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2659 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2659#issuecomment-1712163821:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2659#issuecomment-1712163821,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2676?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@fc498c3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2676 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 104 ; Lines ? 11640 ; Branches ? 0 ; ========================================; Hits ? 8381 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2676#issuecomment-1753090073:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2676#issuecomment-1753090073,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2677?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@46969b4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2677 +/- ##; ========================================; Coverage ? 71.69% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8347 ; Misses ? 3296 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2677#issuecomment-1753261271:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2677#issuecomment-1753261271,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2686?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@418baff`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 503876d differs from pull request most recent head 5c315d4. Consider uploading reports for the commit 5c315d4 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2686 +/- ##; ========================================; Coverage ? 71.98% ; ========================================; Files ? 104 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8381 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2686#issuecomment-1764071999:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2686#issuecomment-1764071999,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2687?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`master@8353e45`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 601888e differs from pull request most recent head f257b7f. Consider uploading reports for the commit f257b7f to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #2687 +/- ##; =========================================; Coverage ? 71.97% ; =========================================; Files ? 108 ; Lines ? 11907 ; Branches ? 0 ; =========================================; Hits ? 8570 ; Misses ? 3337 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2687#issuecomment-1764131250:332,error,error-reference,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2687#issuecomment-1764131250,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2690?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@3c15b99`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2690 +/- ##; ========================================; Coverage ? 71.99% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8382 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2690#issuecomment-1764724800:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2690#issuecomment-1764724800,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2697?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@d71a4a9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2697 +/- ##; ========================================; Coverage ? 72.01% ; ========================================; Files ? 104 ; Lines ? 11656 ; Branches ? 0 ; ========================================; Hits ? 8394 ; Misses ? 3262 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2697#issuecomment-1766637428:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2697#issuecomment-1766637428,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2721?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@05405f1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2721 +/- ##; ========================================; Coverage ? 72.03% ; ========================================; Files ? 104 ; Lines ? 11659 ; Branches ? 0 ; ========================================; Hits ? 8398 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2721#issuecomment-1785251436:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2721#issuecomment-1785251436,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2722?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@4936b7e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2722 +/- ##; ========================================; Coverage ? 72.03% ; ========================================; Files ? 104 ; Lines ? 11659 ; Branches ? 0 ; ========================================; Hits ? 8398 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2722#issuecomment-1787308099:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2722#issuecomment-1787308099,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2727?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@0b624b0`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2727 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 103 ; Lines ? 11641 ; Branches ? 0 ; ========================================; Hits ? 8382 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2727#issuecomment-1787571772:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2727#issuecomment-1787571772,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2728?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1083b36`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2728 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 103 ; Lines ? 11641 ; Branches ? 0 ; ========================================; Hits ? 8382 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2728#issuecomment-1787613419:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2728#issuecomment-1787613419,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2738?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@d1fe8da`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2738 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 103 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8383 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2738#issuecomment-1801509928:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2738#issuecomment-1801509928,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2751?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@ec4d79f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2751 +/- ##; ========================================; Coverage ? 71.87% ; ========================================; Files ? 103 ; Lines ? 11635 ; Branches ? 0 ; ========================================; Hits ? 8363 ; Misses ? 3272 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2751#issuecomment-1808250177:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2751#issuecomment-1808250177,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2752?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@295d889`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2752 +/- ##; ========================================; Coverage ? 72.10% ; ========================================; Files ? 103 ; Lines ? 11635 ; Branches ? 0 ; ========================================; Hits ? 8389 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2752#issuecomment-1808445091:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2752#issuecomment-1808445091,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2774?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@70d55d5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2774 +/- ##; ========================================; Coverage ? 72.11% ; ========================================; Files ? 103 ; Lines ? 11640 ; Branches ? 0 ; ========================================; Hits ? 8394 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2774#issuecomment-1838034698:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2774#issuecomment-1838034698,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2783?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@4058e36`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 09a432a differs from pull request most recent head 3b44d11. Consider uploading reports for the commit 3b44d11 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2783 +/- ##; ========================================; Coverage ? 17.51% ; ========================================; Files ? 103 ; Lines ? 11645 ; Branches ? 0 ; ========================================; Hits ? 2040 ; Misses ? 9605 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2783#issuecomment-1860972298:331,error,error-reference,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2783#issuecomment-1860972298,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2796?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1daae5b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2796 +/- ##; ========================================; Coverage ? 71.35% ; ========================================; Files ? 103 ; Lines ? 11645 ; Branches ? 0 ; ========================================; Hits ? 8309 ; Misses ? 3336 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2796#issuecomment-1870329572:405,error,error-reference,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2796#issuecomment-1870329572,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2812?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`1.9.x@518e76a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2812 +/- ##; ========================================; Coverage ? 71.34% ; ========================================; Files ? 103 ; Lines ? 11632 ; Branches ? 0 ; ========================================; Hits ? 8299 ; Misses ? 3333 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2812#issuecomment-1893321168:405,error,error-reference,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2812#issuecomment-1893321168,1,['error'],['error-reference']
Availability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2972?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`main@3ceb740`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #2972 +/- ##; =======================================; Coverage ? 75.49% ; =======================================; Files ? 116 ; Lines ? 12911 ; Branches ? 0 ; =======================================; Hits ? 9747 ; Misses ? 3164 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2972#issuecomment-2029918769:422,error,error-reference,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2972#issuecomment-2029918769,1,['error'],['error-reference']
Availability,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:690,error,error,690,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['error'],['error']
Availability,"-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; pr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:2686,error,error,2686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['error'],['error']
Availability,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7690,error,errors,7690,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['error'],['errors']
Availability,". ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3324,error,errors,3324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['error'],['errors']
Availability,"6f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:2835,error,error,2835,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['error'],['error']
Availability,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:163,down,down,163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478,1,['down'],['down']
Availability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:134,down,download,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,4,['down'],"['download', 'downloaded']"
Availability,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:402,down,down,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677,4,"['down', 'mainten']","['down', 'maintenance']"
Availability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:10,error,error,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,1,['error'],['error']
Availability,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754:92,error,errors,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754,2,['error'],"['error', 'errors']"
Availability,"> Hi, please provide the data you use, otherwise this is not reproducible:; > ; > ```; > FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); > ```. Hey, the data is publicly available under this link: https://www.10xgenomics.com/resources/datasets/human-lung-cancer-ffpe-2-standard. I simple copied the `curl` bash script to download all the files and then unzipped the file corresponding to the images to get the ""spatial"" folder",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906:271,error,error,271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906,3,"['avail', 'down', 'error']","['available', 'download', 'error']"
Availability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1167,error,errors,1167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,2,"['error', 'fault']","['errors', 'fault']"
Availability,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:45,down,downloading,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241,2,"['down', 'error']","['downloading', 'error']"
Availability,"> I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. I would definitely like this to be the case. I'm not sure I see . > Why? `sc.pp.neighbors` already has `method='gauss'`. To me, it’s largely of a maintenance and documentation issue. Most bugs I fix (here, and in upstream libraries) come from argument handling. The more features you lump into a function, the more complicated argument handling gets. There are questions of default values and fallbacks for different backends, and being sure users understand which arguments are valid for each backend. The use of the `Neighbors` class ends up making the `neighbors` function much more complicated than it needs to be. I think skipping out on that here can make this implementation much more simple. From an API stand point, I would like the ""blessed"" `tsne` workflow to be dead obvious. I'm thinking:. ```python; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```. How many arguments is it going to take to make this work if this functionality is in `sc.pp.neighbors`? At a minimum, `k=30, method=tsne_affinity, nn_method=""annoy""`, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450:268,mainten,maintenance,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450,1,['mainten'],['maintenance']
Availability,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:272,error,error,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652,1,['error'],['error']
Availability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:169,avail,available,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,2,"['avail', 'down']","['available', 'downstream']"
Availability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:1216,mainten,maintenance,1216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,1,['mainten'],['maintenance']
Availability,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1440,avail,available,1440,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['avail'],['available']
Availability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:914,recover,recover,914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['recover'],['recover']
Availability,"> Should the reference object where you learn the transformation always be a subset of the data you're going to apply the transformation to? If so, instead of passing a separate object, could there be a mask of which samples to train on?; > ; > If not, what do you think about making this a separate function? Maybe `combat_by_reference`?. Thank you for your great suggestions. I think it's easier to add a mask for train/evaluate instead of splitting into 2 objects. ; I don't think it should be a separate `combat_by_reference` function, though, because the chance in the function is small and I preserved the original functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703:203,mask,mask,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703,2,['mask'],['mask']
Availability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:325,down,download,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,2,['down'],['download']
Availability,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:1406,down,downstream,1406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128,1,['down'],['downstream']
Availability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:579,avail,available,579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,1,['avail'],['available']
Availability,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:533,down,downstream,533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729,1,['down'],['downstream']
Availability,"> This is the standard way of writing a numpydoc returns section. […] This solution is dropping support for them. It certainly shouldn’t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I don’t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I don’t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons?. I’ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the “optional” is redundant. What would it even mean to have “a parameter that isn’t optional but has a default value”?. I’m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417:829,redundant,redundant,829,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417,2,['redundant'],['redundant']
Availability,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:187,avail,available,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026,1,['avail'],['available']
Availability,"> To be able to reproduce and help, it is a big aid for us if you can supply a code sample that we can run: that is, with some dummy data (the datasets scanpy readily supplies are great for that), and the error/unexpected behaviour you get. Can you show such an example, with data? It is not immediately clear to me what specific you are trying to add or construct; I'm not sure whether basically the dataframe gets destroyed by the operation you intend to perform, or whether it is the violin plot failing (if the dataframe is crooked, it would be this to be fixed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546:205,error,error,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546,1,['error'],['error']
Availability,"> hi @yotamcons ,; > ; > thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,; > ; > Thank you!. Would love to starting November, ping me if thats still relevant",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531:262,ping,ping,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531,1,['ping'],['ping']
Availability,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:1025,down,down,1025,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013,1,['down'],['down']
Availability,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:890,error,errors,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681,1,['error'],['errors']
Availability,"> thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. No worries!. I think communicating about the ideas we have for these tools can be fraught. > in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot). I don't think this is the case. . First, I believe there are non-visium grid based spatial methods (I remember seeing a product page for one, but can't find it atm). Second, I think you don't need segmentation info to use this function. You just need coordinates (probably derived from segmentation) and possibly an image. Like this:. > unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units. But I think a user already having done the segmentation, then coming to scanpy is a reasonable workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703:686,mask,mask,686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703,1,['mask'],['mask']
Availability,"@PGmajev, I would recommend setting up pre-commit for the repo (as described in the contributing guide [here](https://scanpy.readthedocs.io/en/latest/dev/getting-set-up.html#pre-commit)). It should save you from dealing with these formatting errors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1074431745:242,error,errors,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1074431745,1,['error'],['errors']
Availability,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:149,error,error,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154,1,['error'],['error']
Availability,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:82,error,error,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,1,['error'],['error']
Availability,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436:261,mainten,maintenance,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436,1,['mainten'],['maintenance']
Availability,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`?. Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437:93,down,down,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437,2,"['down', 'error']","['down', 'errors']"
Availability,"@giovp I haven't been able to work around the issue. When I rebuild a container with different versions of scanorama, scanpy, numpy, scikit-learn I end up with errors. Most of the time it's this ValueError about the wrong shape. The only different error I noticed is when I tried scanorama 1.6 and there was an error about `concatenate()` not being an available function. . Whenever you find time to update the tutorial, that will be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633:160,error,errors,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633,4,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:619,mask,masking,619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,1,['mask'],['masking']
Availability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:218,down,download,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,3,"['avail', 'down', 'error']","['available', 'download', 'error']"
Availability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:353,avail,available,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,1,['avail'],['available']
Availability,"@ivirshup ; Hello ivirshup, these error comes from the below environments. When I upgrade to py3.8 and use scanpy 1.8.2, everything works well. Thanks a lot!. <html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-visibility:none;}; col; 	{mso-width-source:auto;; 	mso-ruby-visibility:none;}; br; 	{mso-data-placement:same-cell;}; td; 	{padding-top:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; ble",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:34,error,error,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['error'],['error']
Availability,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417:105,error,error,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417,1,['error'],['error']
Availability,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:265,avail,available,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714,2,"['avail', 'error']","['available', 'error']"
Availability,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:157,error,error,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604,2,['error'],['error']
Availability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:506,avail,available,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,1,['avail'],['available']
Availability,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>; <summary> Basic PCA projection </summary>. ```python; def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings (just whether to center?) from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt; ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated.; * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up?; * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:; * Does the syntax still work for cases other than 1-to-1 transfer? ; * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`.; * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842:1538,mask,masking,1538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842,1,['mask'],['masking']
Availability,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](; https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460:293,error,error,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460,1,['error'],['error']
Availability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:857,mask,mask,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,3,"['avail', 'mask']","['available', 'mask']"
Availability,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:197,avail,availible,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844,1,['avail'],['availible']
Availability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:456,mainten,maintenance,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,1,['mainten'],['maintenance']
Availability,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```; conda create -n scanpy_test1 python; pip install scanpy leidenalg scvi-tools; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118; ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; absl-py 1.4.0 pypi_0 pypi; adjusttext 0.8 pypi_0 pypi; aiohttp 3.8.5 pypi_0 pypi; aiosignal 1.3.1 pypi_0 pypi; airr 1.4.1 pypi_0 pypi; anndata 0.9.1 pypi_0 pypi; anyio 3.7.1 pypi_0 pypi; arrow 1.2.3 pypi_0 pypi; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; async-timeout 4.0.2 pypi_0 pypi; attrs 23.1.0 pypi_0 pypi; awkward 2.3.1 pypi_0 pypi; awkward-cpp 21 pypi_0 pypi; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backoff 2.2.1 pypi_0 pypi; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pypi_0 pypi; blessed 1.20.0 pypi_0 pypi; brotli-python 1.0.9 py311ha362b79_9 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2022.12.7 pypi_0 pypi; charset-normalizer 2.1.1 pypi_0 pypi; chex 0.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:874,down,download,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['down'],['download']
Availability,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575:82,error,error,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575,1,['error'],['error']
Availability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:161,mask,masking,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['mask'],['masking']
Availability,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:227,failure,failure,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843,1,['failure'],['failure']
Availability,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496837522:60,error,error,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496837522,1,['error'],['error']
Availability,"Encountered this same error, not clear what is causing it. . ``` ; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata.raw = adata; sc.pp.scale(adata, max_value=10); sc.pp.filter_genes(adata, min_cells=2); sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=2000); ; ```; With the same error:. ```; File ""/opt/venv/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 400, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n""; ValueError: Bin edges must be unique: array([ -inf, -4.47034836e-08, -2.98023224e-08, -2.98023224e-08,; -1.49011612e-08, -8.38190317e-09, 1.00000000e-12, 1.00000000e-12,; 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 4.09781933e-09,; 1.49011612e-08, 1.49011612e-08, 1.49011612e-08, 1.49011612e-08,; 2.98023224e-08, 4.47034836e-08, 4.47034836e-08, 5.96046448e-08,; inf]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. Very possibly an input error, but the output doesn't point to anything useful as a starting point to debug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/509#issuecomment-1147252922:22,error,error,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509#issuecomment-1147252922,3,['error'],['error']
Availability,Exactly same error here.; info:; scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371:13,error,error,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371,1,['error'],['error']
Availability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:231,down,downgrading,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,2,"['down', 'error']","['downgrading', 'errors']"
Availability,"Found the same error in our internal workflows. Saved the data to h5py files, but could not open them anymore for some reason. Error:. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:15,error,error,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['error'],['error']
Availability,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:45,error,error,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745,1,['error'],['error']
Availability,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:270,down,downstream,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845,1,['down'],['downstream']
Availability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:688,avail,available,688,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,1,['avail'],['available']
Availability,"Great, thanks for the feedback. Hopefully this merge and commit fix everything. I wasn't able to see what errors were causing the readthedocs build fail as the ""Details"" link just took me to a page that said ""SORRY / This page does not exist yet."", so let me know if there are any other issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338:106,error,errors,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338,1,['error'],['errors']
Availability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:1055,avail,available,1055,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,1,['avail'],['available']
Availability,"Hello,; I am now facing a problem of failure in computing neighbours when using scanpy or scvelo; when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`; or; `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`; it will always reports that. ```pytb; `computing neighbors; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:37,failure,failure,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,"['error', 'failure']","['errors', 'failure']"
Availability,"Hello,I am having the same problem. ; When I run this:; ```python; from umap import UMAP; ```; It occurs this error. ```python; TypeError Traceback (most recent call last); File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:823, in new_error_context(fmt_, *args, **kwargs); [822](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=821) try:; --> [823](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=822) yield; [824](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=823) except NumbaError as e:. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\lowering.py:293, in BaseLower.lower_block(self, block); [291](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=290) with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; [292](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=291) loc=self.loc, errcls_=defaulterrcls):; --> [293](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=292) self.lower_inst(inst); [294](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=293) self.post_block(block). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\lowering.py:438, in Lower.lower_inst(self, inst); [437](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=436) ty = self.typeof(inst.target.name); --> [438](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=437) val = self.lower_assign(ty, inst); [439](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=438) argidx = None. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:110,error,error,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,5,['error'],"['error', 'errors']"
Availability,"Here is the summary:; * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`; * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`; * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150:562,error,error,562,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150,1,['error'],['error']
Availability,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:1133,error,error,1133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317,1,['error'],['error']
Availability,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:407,reliab,reliable,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,2,"['error', 'reliab']","['error', 'reliable']"
Availability,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:283,ping,ping,283,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133,1,['ping'],['ping']
Availability,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:532,reliab,reliably,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,2,"['avail', 'reliab']","['available', 'reliably']"
Availability,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:; `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]; `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,; `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""); `; there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip.; I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124:620,error,error,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:1535,avail,available,1535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404,1,['avail'],['available']
Availability,"Hi @vitkl . thanks a lot for the feedback, all noted, we'll work toward enabling large tissue image available for storing+plotting. Will keep this open for reference!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782:100,avail,available,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782,1,['avail'],['available']
Availability,"Hi Alex!. Sorry for this long delay, I just forgot completely.; Maybe I wasn't clear enough in my original post, here is where the issue lies:; ; When I run . ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'); ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'); ```. then I get the following error. ```pytb; 100 groups_order = [str(n) for n in groups_order]; 101 if reference != 'rest' and reference not in set(groups_order):; --> 102 groups_order += [reference]; 103 if (reference != 'rest'; 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']); ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624:473,error,error,473,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624,1,['error'],['error']
Availability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:125,error,error,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['error'],['error']
Availability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:187,robust,robust,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,2,['robust'],['robust']
Availability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1695,avail,available,1695,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,2,['avail'],['available']
Availability,"Hi! Sorry, my bad. That parameter was undocumented, and I added it to the wrong docstring building block. It’s available in all scatterplots for dimension reductions, like `pca`, `tsne` and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/545#issuecomment-475158480:111,avail,available,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545#issuecomment-475158480,1,['avail'],['available']
Availability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1346,down,down,1346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,1,['down'],['down']
Availability,"Hi,. I have modified version 1.4, but i think git only allow latest version to; fork. Is there any other simple way, so that i can share my code for Scanpy; version 1.4. Thanks,; Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc; > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>); > reverts a lot of changes we made since.; >; > i assume you just copied all your code over the current master branch, and; > not the version of the master branch as it was when you made the changes.; >; > you need to find the version of scanpy that you downloaded before you made; > your changes and modify that one to have just the changes you want to; > commit. otherwise we have no idea what your actual changes are.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492076397:676,down,downloaded,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492076397,1,['down'],['downloaded']
Availability,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:258,error,error,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821,1,['error'],['error']
Availability,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are; ```; scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216:136,avail,available,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216,1,['avail'],['available']
Availability,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think?. But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run?. The latest version of the tutorial says; ```; scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495:446,error,error,446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495,2,['error'],['error']
Availability,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:421,error,error,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350,1,['error'],['error']
Availability,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:90,error,errors,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542,3,['error'],['errors']
Availability,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:22,error,error,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309,1,['error'],['error']
Availability,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:18,error,error,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994,1,['error'],['error']
Availability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:1040,error,error,1040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,1,['error'],['error']
Availability,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555:6,recover,recover,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555,2,"['down', 'recover']","['downgrading', 'recover']"
Availability,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:629,error,error,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,1,['error'],['error']
Availability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2,down,downloaded,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,2,['down'],"['download', 'downloaded']"
Availability,"I encounter the same error as well when i tried sc.pp.normalize_total(adata, target_sum=5e4). . Environment:; scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. [conda list]; https://github.com/phamidko/codesnippets/blob/master/scanpy-conda-list.txt; [ipynb]; https://github.com/phamidko/codesnippets/blob/master/Tissue-Tcell-activation.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477:21,error,error,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477,1,['error'],['error']
Availability,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:43,down,downloading,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116,1,['down'],['downloading']
Availability,I got the same error with scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.2 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1; But I am so glad to find answer here and thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485:15,error,error,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485,1,['error'],['error']
Availability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1346,avail,available,1346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,1,['avail'],['available']
Availability,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:347,down,downstream,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884,1,['down'],['downstream']
Availability,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:26,error,error,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559,1,['error'],['error']
Availability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:552,error,errors,552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,1,['error'],['errors']
Availability,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674:875,error,error,875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674,1,['error'],['error']
Availability,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:289,avail,available,289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,1,['avail'],['available']
Availability,"I think this looks good and simple enough. Could you please fix the CI errors?. Also there’s 3 added optional deps: cuml, cudf, and cugraph. I assume they’re all different CUDA packages. Could you add them into an `extra` in setup.py?. The RAPIDS umap branch doesn’t use a metric argument. Does it support metrics other than euclidean?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279:71,error,errors,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279,1,['error'],['errors']
Availability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:58,error,error,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['error'],['error']
Availability,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114:8,down,downgrading,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114,1,['down'],['downgrading']
Availability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:1344,robust,robust,1344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,1,['robust'],['robust']
Availability,"I would agree the results of `sc.pp.filter_genes(..., inplace=False)` are not the most intuitive. Instead of returning a filtered anndata, it returns which cells would have been filtered and the stats which were used to make this decision. This is documented under the `Returns` section for these functions. What you might want is. ```python; mask, _ = sc.pp.filter_cells(adata, min_genes=200, inplace=False); a = adata[mask].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137:343,mask,mask,343,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137,2,['mask'],['mask']
Availability,"I'd like to bump the version requirement down a bit, since it seems like it's not that uncommon to pin `map-learn` lower than 0.5.5: https://github.com/search?q=%2F%5B%22%27%5D%3Fumap-learn%5B%22%27%5D%3F%5B%3D%3E%3C%5D%2B%2F&type=code. The cellxgene one is worrying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2870#issuecomment-1957629969:41,down,down,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2870#issuecomment-1957629969,1,['down'],['down']
Availability,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:15,error,error,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782,1,['error'],['error']
Availability,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:21,error,error,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,"['avail', 'error']","['available', 'error']"
Availability,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:818,error,error,818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['error'],['error']
Availability,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:47,mask,masked,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421,1,['mask'],['masked']
Availability,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:354,down,downstream,354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206,1,['down'],['downstream']
Availability,"In my experience, this happens if batch key is not None and one or more batches have low number of cells. Does it make sense to catch this error and simply skip the problematic batch or inform the user that batch doesn't have enough cells?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060:139,error,error,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060,1,['error'],['error']
Availability,"It looks like your adata object is corrupted. You should be able to type; `adata.X` to get the matrix. How are you generating the adata object?. On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,; >; > When running sc.pp.highly_variable_genes(adata.X) I get the following; > error:; >; > AttributeError: X not found; >; > I then ran sc.pp.highly_variable_genes(adata) and got the following:; >; > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,; > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]).; > You can drop duplicate edges by setting the duplicates kwarg; >; > The older sc.pp.filter_genes_dispersion(adata.X) works fine.; >; > Do you know how to fix this?; >; > Thank you!; >; > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0; > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > louvain==0.6.1; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/391>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-444950693:309,error,error,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-444950693,1,['error'],['error']
Availability,"It seems that upgrading from 1.8.1 to 1.8.2 introduce an error on umap version checking, mentioned by #1978 (and a potential solution); > Why are we using umap.__version__ instead of importlib.metadata.version('umap-learn')?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681:57,error,error,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681,1,['error'],['error']
Availability,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:328,down,download,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,5,"['down', 'echo', 'error']","['download', 'echo', 'error']"
Availability,It turned out that this is definitely something wrong with my system setup. After I circumvented the bug above by clearing `README.rst` I found another package that spits out the same error (`louvain`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102:184,error,error,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102,1,['error'],['error']
Availability,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:152,fault,faulty,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026,1,['fault'],['faulty']
Availability,"Just added a test and changed the behaviour of scale a little more.; The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/cus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:449,error,errors,449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['error'],['errors']
Availability,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:1225,avail,available,1225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698,1,['avail'],['available']
Availability,"Miniconda3/envs/py48/lib/contextlib.py?line=129) try:; --> [131](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=130) self.gen.throw(type, value, traceback); [132](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:32631,error,errors,32631,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['error'],['errors']
Availability,"Mmh no, it does work as you expect, just need to turn the dendrogram off.; ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""CD79A"", ""MS4A1""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=False,; swap_axes=True,; ); ```. or just pass the list of markers (list, not mapping); ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_list = [""GNLY"", ""NKG7""]. sc.pl.heatmap(; pbmc,; marker_genes_list,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. If you pass a dict with incomplete annotation and request dendrogram, then it fails, and the warning says this clearly:; ```; WARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.; categories: 0, 1, 2, etc.; var_group_labels: 1, 0, 2, etc.; ```; and it still produces a plot (yet unordered). The `var_group_labels` is also described in `help(sc.pl.heatmap)` as you pointed out. I think the misunderstanding is that passing a mapping or a list the behaviour is different, although potentially expected since a mapping and a list are different things. This could probably be explained clearer in the `var_names` argument yes. Just to go back to your original problem, in your case you were using as mapping categories that were not present in your `groupby` key altogether. This is a different issue, and probably the function should have thrown an error saying `var_group_labels` are not present in `categories`. . If you feel like opening a PR for this, we would really appreciate!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024:1700,error,error,1700,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024,1,['error'],['error']
Availability,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python; dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])); # and instead could just do:; adata.uns[key + ""_colors""]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480739679:258,down,down,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480739679,1,['down'],['down']
Availability,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:55,mask,masking,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,3,['mask'],"['masked', 'masking']"
Availability,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091:60,down,downgraded,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091,1,['down'],['downgraded']
Availability,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:215,reliab,reliable,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565,1,['reliab'],['reliable']
Availability,"OK, I got rid of a few genes that were not expressed in the dataset (its a subset of cells from the full dataset) with ```sc.pp.filter_genes(adata, min_counts=1)``` and now the zero variance genes are gone but still same warning, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:249,error,error,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,2,"['avail', 'error']","['available', 'error']"
Availability,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:117,error,error,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962,1,['error'],['error']
Availability,"OK; now I have more time. The error thrown at ; ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```; astonishes me. The line has been working for me on pandas 0.19.2 and 0.20.3 and for others for other versions for many months already. Do you have an old pandas version? The line should work as `disp_mean_bin` has been computed from `disp_grouped = df.groupby('mean_bin')['dispersion']` [here](https://github.com/theislab/scanpy/blob/65503d34d6b9d0a1d23e831d6daeba86856b3eee/scanpy/preprocessing/simple.py#L215); i.e., the Series 'mean_bin' was used to initialize the index of `disp_mean_bin`. Hence, you should be able to index with 'mean_bin'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115:30,error,error,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115,1,['error'],['error']
Availability,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:33,failure,failures,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463,2,['failure'],['failures']
Availability,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:179,error,error,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924,1,['error'],['error']
Availability,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:46,error,error,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937,1,['error'],['error']
Availability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:229,error,error,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,1,['error'],['error']
Availability,Seems like this is still an issue. I am getting this error below randomly. It disappears after trying sometime later with the exact code and files... scanpy==1.9.4 anndata==0.9.2 umap==0.5.3 numpy==1.23.4 scipy==1.11.2 pandas==2.1.4 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 louvain==0.8.1 pynndescent==0.5.10. `adata=sc.read_h5ad('foo.h5ad')`; Error:; `AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.group.Group'> from /.`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1890353810:53,error,error,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1890353810,2,['error'],['error']
Availability,"Should the reference object where you learn the transformation (currently `adata`) always be a subset of the data you're going to apply the transformation to (`adata2`)? If so, instead of passing a separate object, could there be a mask of which samples to train on?. If not, what do you think about making this a separate function? Maybe `combat_by_reference`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047:232,mask,mask,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047,1,['mask'],['mask']
Availability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:1135,error,error,1135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,1,['error'],['error']
Availability,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:104,down,download,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,4,"['down', 'error']","['download', 'downloading', 'error']"
Availability,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:100,error,error,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,2,['error'],['error']
Availability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:866,down,downstream,866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,1,['down'],['downstream']
Availability,"Tbh, I found out about `groups` after writing the function and looking for a way to put the dots in front. Maybe there is a simpler way to do this... But then the command you suggest gives an error on my own data if I don't also specify `color='bulk_labels'` (works for the pbmc68k, but doesn't colour anything in), and then it just puts all the labels on the same plot and doesn't create small multiples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331:192,error,error,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331,1,['error'],['error']
Availability,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:670,avail,available,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,1,['avail'],['available']
Availability,"Thank you for these thoughts!. I guess the high documentation quality in R stems from the Bioconductor project, which really set some standards. Nothing like this exists in Python - everyone just does what he or she wants. There are few people thinking about setting up something similar to Bioconductor for Python - but this will likely take some time... With Scanpy, we try to provide documentation at the Standards of the big packages: numpy, scipy, statsmodels, seaborn, scikit-learn, h5py, pytables, etc. There are many more and all of them have great docs. I think, with Scanpy, one can still do a lot better. Tuturials tend to be too short. Also, there should be a properly rendered html output of the notebooks - with a button where you can simply download it and then run it yourself to start playing around with it. Hope we will have this in a couple of weeks. And yes, other packages maybe just need to take time. But I'd guess that this will get much better soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498:756,down,download,756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498,1,['down'],['download']
Availability,"Thank you so much for the feedback!; I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using ; conda create -n scanpy python=3.6 scanpy; conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341:422,error,error,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341,1,['error'],['error']
Availability,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:95,error,error,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094,1,['error'],['error']
Availability,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:; ```; WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)); Reason for being yanked: License Violation; ```; Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615:196,down,download,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615,1,['down'],['download']
Availability,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:173,avail,available,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256,1,['avail'],['available']
Availability,"Thanks for the feedback and sorry for the delay. The code snippet using `sklearn.decomposition.FastICA` gave me quite similar results to PCA for my data in terms of the downstream UMAP visualisations (when I simply embedded 50 components in the neighbourhood graph). One difference is that the ICA was slower to compute. I am not confident to create a vignette, as I'm unclear what the 'correct' results should look like. I have not tried the `picard` implementation yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004:169,down,downstream,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004,1,['down'],['downstream']
Availability,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:495,error,error,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,1,['error'],['error']
Availability,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317:48,error,error,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317,1,['error'],['error']
Availability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:262,down,downloading,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,3,"['down', 'mainten']","['downloading', 'maintenance']"
Availability,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed.; ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png); ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354:66,error,error,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354,1,['error'],['error']
Availability,"Thanks! A little bit of context. We needed this aggregation for one of the projects using pseudobulks of the data. We could use scanpy aggregation methods for simple averaging, but to test the outlier-robust median aggregation, we had to write our code. scanpy didn't have it for some reason, so @farhadmd7 kindly agreed to contribute here. Perhaps someone else will find it helpful, too. @eroell, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730:201,robust,robust,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730,1,['robust'],['robust']
Availability,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:5,error,error,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,3,['error'],['error']
Availability,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:185,down,down,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480,1,['down'],['down']
Availability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:294,error,error,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['error'],['error']
Availability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:87,down,downgrade,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,2,"['down', 'error']","['downgrade', 'error']"
Availability,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:20,error,error,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744,1,['error'],['error']
Availability,"The problem is that it does not install at all. ; When I run; ```; conda create -n test; conda activate test; conda install python=3.11; conda install -c conda-forge scanpy; ```; I get an error output for the last line, which is:; ```; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:188,error,error,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,3,['error'],['error']
Availability,"The same error happens to me, @Blohrer . **Versions:**. > scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557:9,error,error,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557,1,['error'],['error']
Availability,"This error is certainly caused by ""scikit-learn"". I abandoned this conda environment and created a new one by `conda create -n Scanpy -c conda-forge scikit-learn`[https://scikit-learn.org/stable/install.html](url).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729:5,error,error,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729,1,['error'],['error']
Availability,"This is identical with #2339. The simplest way is to downgrade `python-igraph` to `0.9.9`, https://github.com/scverse/scanpy/issues/2339#issuecomment-1261132252",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341#issuecomment-1271333881:53,down,downgrade,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341#issuecomment-1271333881,1,['down'],['downgrade']
Availability,This is the output of `sc.logging.print_versions()`:; ```; scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520:235,down,downgrading,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520,1,['down'],['downgrading']
Availability,"To answer the following question:. > 2. what you said: The original approach samples from the full list of genes in each bin, then restricts the sample to valid ones. Your approach samples from the valid genes in each bin.; >; > So if a bin e.g. contains mostly invalid genes, the original code adds only a few genes for that bin, while yours adds the maximum possible number.; >; > So the questions is: is the sampling bias introduced in the original code wanted? If not, you not only made the code more resilient, but also more objective. After going through the original [code from Seurat](https://github.com/satijalab/seurat/blob/c54e57d3423b3f711ccd463e14965cc8de86c31b/R/utilities.R#L280C3-L303), it seems to me that there's not equivalent to removing genes to be scored from the control gene set.; From what I can tell, if one of the genes to be scored happens to be chosen as the background, it will be included in the calculation.; But please correct me if that's not the case. So if the original implementation does not remove score genes from the control gene set, we would simply need to remove the following line: https://github.com/scverse/scanpy/blob/ec4457470618efd85da3c7b29f951cab01a49e3a/scanpy/tools/_score_genes.py#L169. (Note: if we want to keep the current behaviour, we should still remove the line above, since it would be redundant)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358:505,resilien,resilient,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358,2,"['redundant', 'resilien']","['redundant', 'resilient']"
Availability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:372,mainten,maintenance,372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['mainten'],['maintenance']
Availability,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:474,avail,available,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,1,['avail'],['available']
Availability,"When I read. > like Union[OrderedDict, pd.Series], this is a fine substitute for Intersect[Mapping, Sequence]. in the context of. > An example I could think of is needing key value lookup which is also ordered could be thought of as the intersection of Mapping and Sequence types. then `Intersect[Mapping, Sequence]` expects a new ""intersection object"" (here just an `OrderedDict`), which is to me an ""intersection way"" of subclassing - and the first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:973,error,error,973,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884,1,['error'],['error']
Availability,"With respect to the heatmap, indeed it is possible to transpose the matrix.; Currently, this option is only available for `stacked_violin`. I thought; about adding this option to other plots like heatmap, matrixplot and; dotplot but I have not find the time and it is always possible to save the; figure and rotate it so it has low priority for me. The changes are not as; trivial as simply rotating the matrix as all other elements need to be; adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this...; > 😄; >; > —; > You are receiving this because you were mentioned.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839:108,avail,available,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839,1,['avail'],['available']
Availability,"Yes, we already have a good mask for sparse scaling. Boolean arrays are very effective for indicating where computations should be performed, as they eliminate the need for copying and reintegration. One clear example is the `tl.score_genes` function. masks there as booleans for the nanmean is a lot more efficent but less pythonic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711:28,mask,mask,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711,2,['mask'],"['mask', 'masks']"
Availability,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:62,ping,pinging,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,2,"['down', 'ping']","['down', 'pinging']"
Availability,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:1714,down,down,1714,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,1,['down'],['down']
Availability,"b.py?line=130) self.gen.throw(type, value, traceback); [132](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:32744,error,errors,32744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['error'],['errors']
Availability,"by calling the spatial function. this is addressed, embedding changes behaviour only if img is passed, but has nothing to do with spatial, there is a small trick, and has to do with `ax.invert_yaxis()`. See following point. --------------------. > When spatial is called, it’s always shapes being drawn on an image. If there isn’t an image passed, an empty image would be generated. There would be no scatter plot case here. I played around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. -------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3012,down,down,3012,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,1,['down'],['down']
Availability,"changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a lon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1402,down,download,1402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['down'],['download']
Availability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1886,avail,available,1886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['avail'],['available']
Availability,"conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5866,avail,available,5866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['avail'],['available']
Availability,"e 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2144,down,down,2144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['down'],['down']
Availability,"e of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I wa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1145,down,downloaded,1145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['down'],['downloaded']
Availability,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2209,down,down,2209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['down'],['down']
Availability,"e; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\_typing.py in <module>; 82 # array-like; 83 ; ---> 84 ArrayLike = Union[""ExtensionArray"", np.ndarray]; 85 AnyArrayLike = Union[ArrayLike, ""Index"", ""Series""]; 86 . AttributeError: module 'numpy' has no attribute 'ndarray'; ```; It looks like an endless error. What's wrong with the `!pip install scanpy[leiden]`? It installed so many incompatible packages which never happened before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:8491,error,error,8491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['error'],['error']
Availability,"e; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/minic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4630,error,error,4630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['error'],['error']
Availability,"e_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip installed by pip? False; Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:10564,error,error,10564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['error'],['error']
Availability,"ed graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1915,avail,available,1915,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['avail'],['available']
Availability,"envs/py48/lib/site-packages/numba/core/dispatcher.py?line=151) cres = compiler.compile_extra(self.targetdescr.typing_context,; [153](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=152) self.targetdescr.target_context,; [154](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=153) impl,; [155](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=154) args=args, return_type=return_type,; [156](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=155) flags=flags, locals=self.locals,; [157](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=156) pipeline_class=self.pipeline_class); [158](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=157) # Check typing error if object mode is used; [159](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=158) if cres.typing_error is not None and not flags.enable_pyobject:. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:693, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); [669](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Min",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:17829,error,error,17829,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['error'],['error']
Availability,"es/numba/core/decorators.py?line=218) disp.compile(sig); [220](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=219) disp.disable_compile(); [221](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=220) return disp. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:965, in Dispatcher.compile(self, sig); [963](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=962) with ev.trigger_event(""numba:compile"", data=ev_details):; [964](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=963) try:; --> [965](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=964) cres = self._compiler.compile(args, return_type); [966](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=965) except errors.ForceLiteralArg as e:; [967](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=966) def folded(args, kws):. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:125, in _FunctionCompiler.compile(self, args, return_type); [124](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=123) def compile(self, args, return_type):; --> [125](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=124) status, retval = self._compile_cached(args, return_type); [126](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=125) if status:; [127](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=126) return retval. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:139, in _FunctionCompile",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:14657,error,errors,14657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['error'],['errors']
Availability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:251,down,downgraded,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,3,"['down', 'error']","['downgrade', 'downgraded', 'error']"
Availability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2327,redundant,redundant,2327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['redundant'],['redundant']
Availability,"ggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3924,error,error,3924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['error'],['error']
Availability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1042,down,downloads,1042,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['down'],['downloads']
Availability,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py; import numba; import warnings. with warnings.catch_warnings():; warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):; do_thing(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/688#issuecomment-504451555:191,error,errors,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688#issuecomment-504451555,1,['error'],['errors']
Availability,"in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:2794,error,errors,2794,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['error'],['errors']
Availability,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1223,error,error,1223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['error'],['error']
Availability,"iniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py (53); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:32865,error,errors,32865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,3,['error'],['errors']
Availability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9559,error,errors,9559,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['error'],['errors']
Availability,"it's not clear what the problem here sorry, can you copy the error and report a reproducible example? thank you!; I'll close this for now, feel free to reopen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437:61,error,error,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437,1,['error'],['error']
Availability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1375,avail,available,1375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['avail'],['available']
Availability,"m__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1734,error,error,1734,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['error'],['error']
Availability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3275,error,error,3275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['error'],['error']
Availability,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1841,avail,available,1841,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,2,['avail'],['available']
Availability,"nDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1857,error,error,1857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['error'],['error']
Availability,"odepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1515,error,errors,1515,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['error'],['errors']
Availability,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:17,error,error,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006,1,['error'],['error']
Availability,onda-forge; libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; lightning 2.0.5 pypi_0 pypi; lightning-cloud 0.5.37 pypi_0 pypi; lightning-utilities 0.9.0 pypi_0 pypi; lit 15.0.7 pypi_0 pypi; llvmlite 0.40.1 pypi_0 pypi; markdown-it-py 3.0.0 pypi_0 pypi; markupsafe 2.1.2 pypi_0 pypi; matplotlib 3.7.2 pypi_0 pypi; matplotlib-inline 0.1.6 pyhd8ed1ab_0 conda-forge; mdurl 0.1.2 pypi_0 pypi; ml-collections 0.1.1 pypi_0 pypi; ml-dtypes 0.2.0 pypi_0 pypi; mpmath 1.2.1 pypi_0 pypi; msgpack 1.0.5 pypi_0 pypi; mudata 0.2.3 pypi_0 pypi; multidict 6.0.4 pypi_0 pypi; multipledispatch 1.0.0 pypi_0 pypi; muon 0.1.5 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4 hcb278e6_0 conda-forge; nest-asyncio 1.5.6 pyhd8ed1ab_0 conda-forge; networkx 3.1 pypi_0 pypi; numba 0.57.1 pypi_0 pypi; numpy 1.24.4 pypi_0 pypi; numpyro 0.12.1 pypi_0 pypi; openssl 3.1.1 hd590300_1 conda-forge; opt-einsum 3.3.0 pypi_0 pypi; optax 0.1.5 pypi_0 pypi; orbax-checkpoint 0.2.7 pypi_0 pypi; ordered-set 4.1.0 pypi_0 pypi; packaging 23.1 pyhd8ed1ab_0 conda-forge; pandas 2.0.3 pypi_0 pypi; parasail 1.3.4 pypi_0 pypi; parso 0.8.3 pyhd8ed1ab_0 conda-forge; patsy 0.5.3 pypi_0 pypi; pexpect 4.8.0 pyh1a96a4e_2 conda-forge; pickleshare 0.7.5 py_1003 conda-forge; pillow 10.0.0 pypi_0 pypi; pip 23.2.1 pyhd8ed1ab_0 conda-forge; platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge; pooch 1.7.0 pyha770c72_3 conda-forge; prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; protobuf 4.23.4 pypi_0 pypi; psutil 5.9.5 py311h2582759_0 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pydantic 1.10.11 pypi_0 pypi; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pypi_0 pypi; pynndescent 0.5.10 pypi_0 pypi; pyparsing 3.0.9 pypi_0 pypi; pyro-api 0.1.2 pypi_0 pypi; pyro-ppl 1.8.5 pypi_0 pypi; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.11.4 hab00c5b_0_cpython conda-forge; p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:5147,checkpoint,checkpoint,5147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['checkpoint'],['checkpoint']
Availability,"rror: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3702,error,error,3702,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['error'],['error']
Availability,"rs/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5230,error,error,5230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['error'],['error']
Availability,"rs/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=128) value = type(); [130](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=129) try:; --> [131](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=130) self.gen.throw(type, value, traceback); [132](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:32485,error,errors,32485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['error'],['errors']
Availability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:291,avail,available,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,2,['avail'],['available']
Availability,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:815,reboot,reboot,815,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,2,['reboot'],['reboot']
Availability,"t I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1070,down,downloaded,1070,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['down'],['downloaded']
Availability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:2510,down,down,2510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['down'],['down']
Availability,"te the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:1098,mask,masks,1098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,2,['mask'],"['mask', 'masks']"
Availability,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:768,mask,mask,768,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756,1,['mask'],['mask']
Availability,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:924,down,downstream,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['down'],['downstream']
Availability,"that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them ha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1165,down,downloaded,1165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['down'],['downloaded']
Availability,"umba/core/dispatcher.py?line=124) status, retval = self._compile_cached(args, return_type); [126](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=125) if status:; [127](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=126) return retval. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:139, in _FunctionCompiler._compile_cached(self, args, return_type); [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=135) pass; [138](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=137) try:; --> [139](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=138) retval = self._compile_core(args, return_type); [140](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=139) except errors.TypingError as e:; [141](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=140) self._failed_cache[key] = e. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:152, in _FunctionCompiler._compile_core(self, args, return_type); [149](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=148) flags = self._customize_flags(flags); [151](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=150) impl = self._get_implementation(args, {}); --> [152](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=151) cres = compiler.compile_extra(self.targetdescr.typing_context,; [153](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=152) self.targetdescr.target_context,; [154](file:///d%3A/Users/xiangrong1/Miniconda3/envs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:16206,error,errors,16206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['error'],['errors']
Availability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1072,down,downloads,1072,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['down'],['downloads']
Availability,"version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1082,error,error,1082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['error'],['error']
Availability,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k_processed(); sc.pp.neighbors(adata); ```. <details>; <summary>Details</summary>. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import nump",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:61,error,error,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['error'],['error']
Availability,"xit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3777,avail,available,3777,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['avail'],['available']
Availability,"y in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_typ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3974,error,errors,3974,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['error'],['errors']
Availability,"y48/lib/site-packages/numba/core/compiler.py?line=495) assert self.state.func_ir is None; --> [497](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=496) return self._compile_core(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:476, in CompilerBase._compile_core(self); [474](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=473) self.state.status.fail_reason = e; [475](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=474) if is_final_pipeline:; --> [476](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=475) raise e; [477](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=476) else:; [478](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=477) raise CompilerError(""All available pipelines exhausted""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:21418,avail,available,21418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['avail'],['available']
Availability,"yes, I know, that's non-ideal... the sparseness issue is circumvented by only returning top-scoring genes... I see that you make suggestions for how the user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:1003,down,down,1003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458,1,['down'],['down']
Availability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:2088,down,down,2088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,1,['down'],['down']
Availability,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4504,error,error,4504,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,3,['error'],['error']
Availability,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4105,mask,masked,4105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['mask'],['masked']
Availability,"⠈⢀⠌⠚⠀⠀⠃; ⠁⠂⡃⠈⠀⢀⠀⠙⢀⠥⠀⠀⠄⡁⠀⠠⠈⠀⠈⠃⠂⠠⣀⠀⠈⣁⠁⠆; ⡀⠐⠐⠠⠀⠐⢐⡄⣂⠀⠀⠘⠀⠀⠀⠠⠂⠀⡀⠨⠁⠀⠀⠀⠁⠁⠣⠤; ⠀⡐⢀⢢⠀⠁⠔⠀⠁⠀⠃⠀⢀⢀⠐⠃⠄⠀⡇⠊⠄⠀⡈⢀⠀⠀⣀⠆; ⠀⢐⣤⡄⠠⠂⠃⡈⠘⠀⠀⠀⡂⠰⢄⠊⡂⠀⠐⠂⠀⠄⠀⠀⢱⠩⠈⢀; ⢁⠀⠑⠚⠁⠂⠂⠐⠁⠀⠀⢀⠠⠀⠐⠈⠈⡨⠀⠂⠀⡈⠈⠁⡐⣀⢁⠂; ⠀⠀⠀⠁⠀⠠⠅⠁⡠⠇⢐⠀⠀⠖⢉⣀⠀⢀⠀⠠⡀⠀⡀⢰⠁⠂⢉⠂; ⠀⠀⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3427,mask,masks,3427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,2,['mask'],"['mask', 'masks']"
Deployability," 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-forge/linux-64::python-igraph==0.7.1.post7=py38h516909a_0; - r/linux-64::r==3.6.0=r36_0; - r/linux-64::r-base==3.6.1=haffb61f_2; - r/noarch::r-boot==1.3_20=r36h6115d3f_0; - r/linux-64::r-class==7.3_15=r36h96ca727_0; - r/linux-64::r-cluster==2.0.8=r36ha65eedd_0; - r/noarch::r-codetools==0.2_16=r36h6115d3f_0; - r/linux-64::r-foreign==0.8_71=r36h96ca727_0; - r/linux-64::r-kernsmooth==2.23_15=r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2699,update,updated,2699,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['updated']
Deployability," `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1707,install,install,1707,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['install'],['install']
Deployability," error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for outpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:1822,install,install-,1822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability," implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1117,update,update,1117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['update'],['update']
Deployability," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5580,update,updated,5580,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['update'],['updated']
Deployability," no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:1473,pipeline,pipeline,1473,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,2,['pipeline'],['pipeline']
Deployability," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:2179,patch,patching,2179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,1,['patch'],['patching']
Deployability," pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=492) """"""; [494](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=493) Populate and run pipeline for bytecode input; [495](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=494) """"""; [496](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=495) assert self.state.func_ir is None; --> [497](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=496) return self._compile_core(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:476, in CompilerBase._compile_core(self); [474](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=473) self.state.status.fail_reason = e; [475](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=474) if is_final_pipeline:; --> [476](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=475) raise e; [477](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:20220,pipeline,pipeline,20220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability," remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`?. ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these?. ```python; sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns; sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]); ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD?. Basically, I'd say do something more like:. ```python; from collections.abc import Iterable; import numpy as np; import pandas as pd; from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):; if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):; orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]; sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering; ; new_clustering = adata.obs[orig_key].copy(); # Make n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:3448,continuous,continuous,3448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['continuous'],['continuous']
Deployability," the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3585,update,updates,3585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['update'],['updates']
Deployability," the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4538,install,install-,4538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2108,pipeline,pipelines,2108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,2,"['continuous', 'pipeline']","['continuous', 'pipelines']"
Deployability,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1922,integrat,integrating,1922,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,1,['integrat'],['integrating']
Deployability,"(= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3-5),; libarchive-zip-perl (= 1.68-1),; libargon2-1 (= 0~20171227-0.2),; libarpack2 (= 3.8.0-1),; libasan6 (= 11.2.0-10),; libatk-bridge2.0-0 (= 2.38.0-2),; libatk1.0-0 (= 2.36.0-2),; libatk1.0-data (= 2.36.0-2),; libatlas3-base (= 3.10.3-11),; libatomic1 (= 11.2.0-10),; libatspi2.0-0 (= 2.42.0-2),; libattr1 (= 1:2.5.1-1),; libaudit-common (= 1:3.0.6-1),; libaudit1 (= 1:3.0.6-1),; libavahi-client3 (= 0.8-5),; libavahi-common-data (= 0.8-5),; libavahi-common3 (= 0.8-5),; libbinutils (= 2.37-8),; libblas3 (= 3.10.0-1),; libblkid1 (= 2.37.2-4),; libblosc1 (= 1.21.1+ds1-1),; libbr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2851,update,update-icon-cache,2851,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['update'],['update-icon-cache']
Deployability,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4,install,installed,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['install'],"['install', 'installed']"
Deployability,"-----------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/_utils.py in <module>; 16 from numpy import random; 17 from scipy import sparse; ---> 18 from anndata import AnnData, __version__ as anndata_version; 19 from textwrap import dedent; 20 from packaging import version. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/_core/anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py/h5.pyx in init h5py.h5(). ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3; ```. </Details>. Before, when I was trying to update `h5py` and getting lots of conflicts, I let conda try to figure out all of them, printing out a huge list that reached the output limit of my console (~6000 lines). Let me know if I should post the list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:20564,update,update,20564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['update']
Deployability,"-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1364,install,installed,1364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['install'],['installed']
Deployability,.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat 2.4.1 h2531618_2 ; fa2 0.3.5 pypi_0 pypi; fastcache 1.1.0 py38h7b6447c_0 ; fbpca 1.0 pypi_0 pypi; fcsparser 0.2.1 pypi_0 pypi; filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:6569,patch,patch,6569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['patch'],['patch']
Deployability,"0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 3: As you recommend, I do `!pip uninstall tables` and `conda install -c conda-forge pytables`, then; ```python; import tables # pass. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_15024/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 19 from numpy import random; 20 from scipy import sparse; ---> 21 from anndata import AnnData, __version__ as anndata_version; 22 from textwrap import dedent; 23 f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:3591,install,install,3591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,"48/lib/site-packages/numba/core/dispatcher.py?line=157) # Check typing error if object mode is used; [159](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=158) if cres.typing_error is not None and not flags.enable_pyobject:. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:693, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); [669](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:18717,pipeline,pipeline,18717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"4f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:2610,install,install,2610,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install']
Deployability,"6.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:2266,install,install-,2266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability,"99 (= 3.5-2),; libxau6 (= 1:1.0.9-1),; libxcb-render0 (= 1.14-3),; libxcb-shm0 (= 1.14-3),; libxcb1 (= 1.14-3),; libxcomposite1 (= 1:0.4.5-1),; libxcursor1 (= 1:1.2.0-2),; libxdamage1 (= 1:1.1.5-2),; libxdmcp6 (= 1:1.1.2-3),; libxext6 (= 2:1.3.4-1),; libxfixes3 (= 1:5.0.3-2),; libxft2 (= 2.3.2-2),; libxi6 (= 2:1.8-1),; libxinerama1 (= 2:1.1.4-2),; libxkbcommon0 (= 1.3.1-1),; libxml2 (= 2.9.12+dfsg-5),; libxrandr2 (= 2:1.5.2-1),; libxrender1 (= 1:0.9.10-1),; libxss1 (= 1:1.2.3-1),; libz3-4 (= 4.8.12-1+b1),; libzstd1 (= 1.4.8+dfsg-3),; linux-libc-dev (= 5.14.16-1),; llvm-11 (= 1:11.1.0-4),; llvm-11-linker-tools (= 1:11.1.0-4),; llvm-11-runtime (= 1:11.1.0-4),; login (= 1:4.8.1-2),; lsb-base (= 11.1.0),; m4 (= 1.4.18-5),; mailcap (= 3.70),; make (= 4.3-4.1),; man-db (= 2.9.4-2),; mawk (= 1.3.4.20200120-2),; media-types (= 4.0.0),; mime-support (= 3.66),; mount (= 2.37.2-4),; ncurses-base (= 6.2+20210905-1),; ncurses-bin (= 6.2+20210905-1),; openssl (= 1.1.1l-1),; passwd (= 1:4.8.1-2),; patch (= 2.7.6-7),; perl (= 5.32.1-6),; perl-base (= 5.32.1-6),; perl-modules-5.32 (= 5.32.1-6),; po-debconf (= 1.0.21+nmu1),; procps (= 2:3.3.17-5),; python-matplotlib-data (= 3.3.4-2),; python-tables-data (= 3.6.1-5),; python3 (= 3.9.7-1),; python3-all (= 3.9.7-1),; python3-anndata (= 0.7.5+ds-3),; python3-asciitree (= 0.3.3-3),; python3-attr (= 20.3.0-1),; python3-certifi (= 2020.6.20-1),; python3-cffi-backend (= 1.15.0-1),; python3-chardet (= 4.0.0-1),; python3-cov-core (= 1.15.0-3),; python3-coverage (= 5.1+dfsg.1-2+b2),; python3-cryptography (= 3.3.2-1),; python3-cycler (= 0.11.0-1),; python3-dateutil (= 2.8.1-6),; python3-decorator (= 4.4.2-2),; python3-distutils (= 3.9.8-1),; python3-docutils (= 0.17.1+dfsg-2),; python3-fasteners (= 0.14.1-2),; python3-gi (= 3.42.0-1+b1),; python3-h5py (= 3.3.0-5),; python3-h5py-serial (= 3.3.0-5),; python3-harmony (= 0.7.1-1),; python3-idna (= 2.10-1),; python3-igraph (= 0.9.6-1),; python3-importlib-metadata (= 4.6.4-1),; python3-iniconfig (= 1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:10418,patch,patch,10418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['patch'],['patch']
Deployability,"; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(reticulate); repl_python(); ```. ```; import pandas as pd; import scanpy as sc; ```. ```; ModuleNotFoundError: No module named 'scanpy'; ```. Check the `PATH`:. ```; import sys; sys.path; ```. ```; ['', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/bin', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python36.zip', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/lib-dynload', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages', '/home/tsundoku/R/x86_64-pc-linux-gnu-library/3.6/reticulate/python']; ```. Okay so `scanpy` is installed but the `PATH` are different. Not sure why `py_install()` doesn't work. I guess the alternative is including the those paths for reticulate but not sure at the moment how to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:15253,install,installed,15253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['installed']
Deployability,"; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2198,install,installing,2198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['install'],['installing']
Deployability,<details>; <summary>Installed scanpy on jupyter notebook/ anaconda: </summary>. ```; pip install scanpy. Requirement already satisfied: scanpy in c:\users\charles\anaconda3\lib\site-packages (1.7.2); Requirement already satisfied: numba>=0.41.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\an,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:89,install,install,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['install'],['install']
Deployability,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:512,install,install,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,1,['install'],['install']
Deployability,"> 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:2439,install,install-,2439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:253,install,installed,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478,1,['install'],['installed']
Deployability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:169,install,installed,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,1,['install'],['installed']
Deployability,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:392,release,release,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515,1,['release'],['release']
Deployability,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:618,install,install,618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,1,['install'],['install']
Deployability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:68,integrat,integrate,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,2,['integrat'],"['integrate', 'integrated']"
Deployability,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450877926:28,update,update,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450877926,1,['update'],['update']
Deployability,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:558,update,updates,558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677,1,['update'],['updates']
Deployability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:344,install,install,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,1,['install'],['install']
Deployability,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). ; What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy; >; > Could does this relate to @davidsebfischer and diffxpy?. Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226:779,release,released,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226,1,['release'],['released']
Deployability,"> Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with; > ; > ```; > pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; > ```; > ; > Seems to work now for me. Thank you. It just worked for me, in July 2024.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559:137,install,installed,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559,3,"['install', 'patch']","['install', 'installed', 'patch']"
Deployability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1053,integrat,integration,1053,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,2,['integrat'],['integration']
Deployability,"> Hi @JackieMium, I remember you said something similar in another issue.; > ; > If there’s things bugging you, how about making a PR that fixes it?. Not sure what you're referring to but I don't think I ever reported color pallette issue before. ; I hope I could help fix things but I am familiar with R/Seurat and Python/scanpy is a whole new universe to me. I am starting to learning the scanpy pipeline. How things work under the hood with scanpy or basically Python plotting are really beyond my capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040:398,pipeline,pipeline,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040,1,['pipeline'],['pipeline']
Deployability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920,1,['integrat'],['integration']
Deployability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457,1,['integrat'],['integration']
Deployability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:176,upgrade,upgrade,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,11,"['install', 'upgrade']","['install', 'installation', 'installations', 'upgrade']"
Deployability,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:130,install,install,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241,9,['install'],"['install', 'installed']"
Deployability,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesn’t seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that I’m now member of the team on rtd.com (which I wasn’t before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450818246:476,update,update,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450818246,1,['update'],['update']
Deployability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1465,update,update,1465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,1,['update'],['update']
Deployability,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:532,integrat,integration,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,1,['integrat'],['integration']
Deployability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I can’t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! That’s a concrete thing I can do. I’ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:627,update,update,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,1,['update'],['update']
Deployability,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:1458,install,installation,1458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064,3,['install'],"['install', 'installation', 'installations']"
Deployability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:257,install,installed,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['install'],['installed']
Deployability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:489,release,release,489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,3,"['install', 'release']","['install', 'release']"
Deployability,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap.; Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304:589,update,updated,589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304,1,['update'],['updated']
Deployability,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:782,continuous,continuous,782,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['continuous'],['continuous']
Deployability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:945,pipeline,pipelines,945,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['pipeline'],['pipelines']
Deployability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,integrat,integrate,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,2,['integrat'],['integrate']
Deployability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,1,['integrat'],['integration']
Deployability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:550,integrat,integrate,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['integrat'],['integrate']
Deployability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:943,install,installing,943,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['install'],['installing']
Deployability,"> Thank you! With “tests” I mean “functions named `test_*` with `assert ...` statements inside”; ; Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, ; Khalid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243:230,update,updated,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243,1,['update'],['updated']
Deployability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,1,['update'],['update']
Deployability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:127,release,release,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,1,['release'],['release']
Deployability,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:33,install,install,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220,2,['install'],['install']
Deployability,"> hi @yotamcons ,; > ; > thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,; > ; > Thank you!. Would love to starting November, ping me if thats still relevant",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531:162,update,updated,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531,1,['update'],['updated']
Deployability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:1475,integrat,integrated,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,1,['integrat'],['integrated']
Deployability,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:3160,update,update,3160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['update'],['update']
Deployability,"@Zethson Ready to merge. Thanks for your feedback; I added to the release notes, and rebased on master.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184:66,release,release,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184,1,['release'],['release']
Deployability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,integrat,integrate,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,1,['integrat'],['integrate']
Deployability,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095:65,rolling,rolling,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095,1,['rolling'],['rolling']
Deployability,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:431,patch,patching,431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874,1,['patch'],['patching']
Deployability,"@giovp I haven't been able to work around the issue. When I rebuild a container with different versions of scanorama, scanpy, numpy, scikit-learn I end up with errors. Most of the time it's this ValueError about the wrong shape. The only different error I noticed is when I tried scanorama 1.6 and there was an error about `concatenate()` not being an available function. . Whenever you find time to update the tutorial, that will be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633:400,update,update,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633,1,['update'],['update']
Deployability,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:253,release,releases,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512,1,['release'],['releases']
Deployability,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:568,install,installed,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,2,"['install', 'update']","['installed', 'updated']"
Deployability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:493,install,installing,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,1,['install'],['installing']
Deployability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:41,release,release,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,9,"['install', 'release', 'update', 'upgrade']","['install', 'installation', 'release', 'released', 'updated', 'upgrade']"
Deployability,"@ivirshup ; Hello ivirshup, these error comes from the below environments. When I upgrade to py3.8 and use scanpy 1.8.2, everything works well. Thanks a lot!. <html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-visibility:none;}; col; 	{mso-width-source:auto;; 	mso-ruby-visibility:none;}; br; 	{mso-data-placement:same-cell;}; td; 	{padding-top:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; ble",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:82,upgrade,upgrade,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['upgrade'],['upgrade']
Deployability,"@ivirshup Thank you for the feedback. I will add a release note soon. I also thought about the naming of the parameter. However, if we assume that also in the future it is mostly used to subset the number of PCs in PCA arrays stored under different names, it should be fine?. I can not comment further on what these changes may break, at least ideally they should make the use of n_pcs more consistent and as expected. Would you suggest, I implement some further, more comprehensive tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928:51,release,release,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928,1,['release'],['release']
Deployability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:157,patch,patch,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,2,['patch'],['patch']
Deployability,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:491,release,release,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714,1,['release'],['release']
Deployability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:229,configurat,configurations,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,"['configurat', 'release']","['configurations', 'release']"
Deployability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:713,integrat,integrate,713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,2,['integrat'],"['integrate', 'integrated']"
Deployability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:167,install,installed,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,1,['install'],['installed']
Deployability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,integrat,integration,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,1,['integrat'],['integration']
Deployability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:35,integrat,integrate,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,1,['integrat'],['integrate']
Deployability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,integrat,integrate,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,1,['integrat'],['integrate']
Deployability,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:61,release,release,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,1,['release'],['release']
Deployability,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```; conda create -n scanpy_test1 python; pip install scanpy leidenalg scvi-tools; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118; ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; absl-py 1.4.0 pypi_0 pypi; adjusttext 0.8 pypi_0 pypi; aiohttp 3.8.5 pypi_0 pypi; aiosignal 1.3.1 pypi_0 pypi; airr 1.4.1 pypi_0 pypi; anndata 0.9.1 pypi_0 pypi; anyio 3.7.1 pypi_0 pypi; arrow 1.2.3 pypi_0 pypi; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; async-timeout 4.0.2 pypi_0 pypi; attrs 23.1.0 pypi_0 pypi; awkward 2.3.1 pypi_0 pypi; awkward-cpp 21 pypi_0 pypi; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backoff 2.2.1 pypi_0 pypi; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pypi_0 pypi; blessed 1.20.0 pypi_0 pypi; brotli-python 1.0.9 py311ha362b79_9 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2022.12.7 pypi_0 pypi; charset-normalizer 2.1.1 pypi_0 pypi; chex 0.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:198,install,install,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,7,['install'],"['install', 'installed']"
Deployability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,integrat,integrate,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,1,['integrat'],['integrate']
Deployability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:888,release,releases,888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,2,['release'],['releases']
Deployability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:658,update,update,658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,1,['update'],['update']
Deployability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:72,install,install,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,1,['install'],['install']
Deployability,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745,4,"['install', 'update']","['install', 'updated']"
Deployability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:148,patch,patches,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['patch'],['patches']
Deployability,"Great! :smile:. Sure, next Wednesday is fine. Also, we can always simply make 1.4.2. I just need to check one tiny thing before we actually make the release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355:149,release,release,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355,1,['release'],['release']
Deployability,"Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with ; ```; pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; ```. Seems to work now for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996:135,install,installed,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996,3,"['install', 'patch']","['install', 'installed', 'patch']"
Deployability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,1,['integrat'],['integration']
Deployability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,integrat,integration,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,1,['integrat'],['integration']
Deployability,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:73,install,install,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,3,['install'],"['install', 'installed']"
Deployability,"Hello @Koncopd ,; Thanks for the response!. Step 1: I created a fresh new environment (py3.8.12); ```python; !pip install scanpy[leiden]. Successfully installed anndata-0.7.8 cycler-0.11.0 fonttools-4.28.5 h5py-3.6.0 igraph-0.9.9 joblib-1.1.0 kiwisolver-1.3.2 leidenalg-0.8.8 llvmlite-0.38.0 matplotlib-3.5.1 natsort-8.0.2 networkx-2.6.3 numba-0.55.0 numexpr-2.8.1 numpy-1.21.5 pandas-1.3.5 patsy-0.5.2 pillow-9.0.0 pynndescent-0.5.5 python-igraph-0.9.9 scanpy-1.8.2 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.13.1 stdlib-list-0.8.0 tables-3.7.0 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:114,install,install,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],"['install', 'installed']"
Deployability,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:14,update,updates,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,2,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up!; Cheers,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007:56,update,updates,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007,1,['update'],['updates']
Deployability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,integrat,integration,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,2,['integrat'],['integration']
Deployability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:1472,release,release,1472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,1,['release'],['release']
Deployability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:173,integrat,integration,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,1,['integrat'],['integration']
Deployability,"Hey! Sorry for the late reply:; 1. Yes, a separate file, please.; 2. Put both in the same function. I wouldn't call it coexpression though. Something along the lines of `cell_selection_by_genes()` or just `cell_selection()`.; 3. There is a `sort_order` keyword for plotting which works for continuous covariates. I imagine that should work.; 4. That may be overkill... but it would definitely be interesting. I think MAGIC is in `sc.external` and DCA is also easily usable in this framework. They are not part of the core package though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328:290,continuous,continuous,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328,1,['continuous'],['continuous']
Deployability,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:155,continuous,continuous,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527,2,['continuous'],['continuous']
Deployability,"Hey,. indeed there seems to be an [issue](https://github.com/mwaskom/seaborn/issues/3522) with our current usage of `seaborn`, not working with `seaborn 0.13.0`.; This has been fixed on the main branch [here](https://github.com/scverse/scanpy/pull/2661), and we'll eventually take over the newest `seaborn` version once this is cleared. For users running into this issue now ; - first check if you indeed have `seaborn 0.13.0`. If yes, then do; - `pip install seaborn==0.12.2` if using pip or; - `conda install seaborn=0.12.2` if using conda. this makes sure you are using the working version of seaborn. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215:452,install,install,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215,2,['install'],['install']
Deployability,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:328,update,update,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773,1,['update'],['update']
Deployability,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:111,patch,patch,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,2,['patch'],"['patch', 'patched']"
Deployability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,integrat,integration,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,1,['integrat'],['integration']
Deployability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,integrat,integrated,912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['integrat'],['integrated']
Deployability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:15,update,updated,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,1,['update'],['updated']
Deployability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:293,pipeline,pipeline,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['pipeline'],['pipeline']
Deployability,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i; analysed most of them are from previous code. Regards,; Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great!; >; > The only duplicated code left is that _prepare_weighted_dataframe is very; > similar to _prepare_dataframe. I think you can delete; > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return; > categories, obs_tidy, categorical. Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:20,update,updated,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,1,['update'],['updated']
Deployability,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:63,install,install,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498,3,"['install', 'release']","['install', 'release']"
Deployability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:276,release,release,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['release'],['release']
Deployability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:624,install,install,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,2,"['install', 'integrat']","['install', 'integrate']"
Deployability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:128,integrat,integrating,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,2,['integrat'],"['integrating', 'integration']"
Deployability,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help?. This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058,1,['install'],['install']
Deployability,"Hi, thanks for the contribution!. The converting to dense is quite iffy, we should probably add real support for sparse here. [We could use this as a base](https://github.com/scikit-learn/scikit-learn/blob/45cf8ec555a026c4263e8bef12850755a83df10e/sklearn/utils/sparsefuncs.py#L685). Do you think you can do that or should we help?. This is also missing release notes in 1.11.0.md",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449:353,release,release,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449,1,['release'],['release']
Deployability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:24,install,install,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,1,['install'],['install']
Deployability,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:102,install,installation,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463,1,['install'],['installation']
Deployability,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:12,install,installed,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350,3,['install'],"['install', 'installed']"
Deployability,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:475,install,installing,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['install'],['installing']
Deployability,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:163,install,install,163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309,2,['install'],['install']
Deployability,I am facing the same issue. I have recently updated my scanpy to the latest version.; I think that it was working before that. Here is rest of my software versions; scanpy==1.4.6 anndata==0.7.1 umap==0.3.9 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992:44,update,updated,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992,1,['update'],['updated']
Deployability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:356,pipeline,pipeline,356,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,1,['pipeline'],['pipeline']
Deployability,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:537,update,updated,537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994,2,['update'],"['update', 'updated']"
Deployability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:347,update,updated,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,1,['update'],['updated']
Deployability,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555:248,install,installed,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555,1,['install'],['installed']
Deployability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,integrat,integration,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,2,['integrat'],['integration']
Deployability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:184,update,updated,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['update'],['updated']
Deployability,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:167,upgrade,upgrade,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500,11,"['install', 'upgrade']","['install', 'installation', 'installations', 'upgrade']"
Deployability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:89,patch,patches,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,3,"['install', 'patch']","['installed', 'patches']"
Deployability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,1,['integrat'],['integrate']
Deployability,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:130,install,install,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116,5,['install'],['install']
Deployability,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:68,install,install,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,1,['install'],['install']
Deployability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1401,install,install,1401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,5,"['install', 'update', 'upgrade']","['install', 'updated', 'upgrade']"
Deployability,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:465,install,install,465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559,3,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:618,install,installed,618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,2,['install'],['installed']
Deployability,"I see, [densmap](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html). Hmm, I think that `method='densmap'` and `method_kwds={...}` would be a better API for us (which would then be translated into `densmap=True, densmap_kwds=method_kwds`). This also needs tests and a release note. Also we probably should just remove the umap 0.4 compatibility code, what do you think @ivirshup?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449:279,release,release,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449,1,['release'],['release']
Deployability,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:45,pipeline,pipeline,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,"I think maybe i found a solution to solve this problem.; Maybe this problem is caused by the version of scikit—misc，when you use pip install --user scikit-misc or pip install scikit-misc，the system will install scikit-misc==0.1.4.; so,i try to install another verion of scikit-misc,you can use install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1.; In addition, this line of command needs to be used when python is greater than or equal to 3.8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497:133,install,install,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497,5,['install'],['install']
Deployability,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:416,deploy,deploys,416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,3,"['deploy', 'pipeline']","['deployed', 'deploys', 'pipelines']"
Deployability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:763,install,installing,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['install'],['installing']
Deployability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,integrat,integrated,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,1,['integrat'],['integrated']
Deployability,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:689,continuous,continuous,689,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251,1,['continuous'],['continuous']
Deployability,"I think this is now ready for review. . ## Legends. I've decided to leave showing the null value in continuous legends for another PR, since I don't have an obvious solution now. I have added an argument for specifying whether the na value should show up in the legend, `na_in_legend`. It defaults to `True`. Here's an example:. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""]); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855448-fd3cc400-e3c2-11ea-9e01-6e8266ab6d10.png); ![image](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:100,continuous,continuous,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,1,['continuous'],['continuous']
Deployability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:48,install,installed,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,1,['install'],['installed']
Deployability,"I was eventually able to contact the maintainer and he's looking into making a new release. Will see what happens. Nevertheless, it might not be a bad idea to simplify the code and to only support `igraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980:83,release,release,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980,1,['release'],['release']
Deployability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:149,continuous,continuous,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,2,['continuous'],['continuous']
Deployability,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['integrat'],['integration']
Deployability,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027:839,pipeline,pipeline,839,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027,1,['pipeline'],['pipeline']
Deployability,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:251,install,installed,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['installed']
Deployability,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246:259,install,installed,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246,1,['install'],['installed']
Deployability,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:338,install,installed,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181,1,['install'],['installed']
Deployability,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129:5,update,updated,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129,1,['update'],['updated']
Deployability,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825:115,release,releases,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825,2,"['release', 'update']","['releases', 'update']"
Deployability,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:61,install,install,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206,1,['install'],['install']
Deployability,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:96,install,install,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,8,['install'],"['install', 'installed']"
Deployability,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041:48,install,install,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041,2,['install'],"['install', 'installed']"
Deployability,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:202,install,installation,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026,1,['install'],['installation']
Deployability,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:8,update,update,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434,1,['update'],['update']
Deployability,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:71,install,installed,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994,2,['install'],"['install', 'installed']"
Deployability,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950:31,install,installed,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950,2,['install'],['installed']
Deployability,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:64,release,release,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,2,['release'],['release']
Deployability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:255,install,installs,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,4,"['install', 'patch']","['installs', 'patch']"
Deployability,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:1546,update,update,1546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['update'],['update']
Deployability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:156,toggle,toggleswitch,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,1,['toggle'],['toggleswitch']
Deployability,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:121,install,install,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:96,upgrade,upgraded,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,2,['upgrade'],['upgraded']
Deployability,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672:352,upgrade,upgraded,352,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672,1,['upgrade'],['upgraded']
Deployability,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:491,install,installed,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962,1,['install'],['installed']
Deployability,"Okay, I solved the issue. In my environment, scanpy==1.7.2 does not work with umap==0.5.2. I pip uninstall scanpy and umap-learn. Next I installed umap-learn through conda which was umap==0.5.1. When installing scanpy again it works as expected. . Not sure if its worth looking further into it but pip install scanpy also installs umap==0.5.2 (which does not work at least for me). . Many thanks! ; ![umap_0 5 1](https://user-images.githubusercontent.com/20926246/148250255-5ac00a46-cbc9-4608-a893-0472e32f5fb5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077:137,install,installed,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077,4,['install'],"['install', 'installed', 'installing', 'installs']"
Deployability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:62,install,install,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,3,"['install', 'integrat']","['install', 'installation', 'integration']"
Deployability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,integrat,integrated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,1,['integrat'],['integrated']
Deployability,"Out of experience, I don't really feel like conda-forge is much more complicated to maintain - at least didn't feel too bad for me when I added a few recipes there in the past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:901,release,release,901,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,2,['release'],['release']
Deployability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:55,install,installing,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['install'],['installing']
Deployability,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:145,pipeline,pipeline,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924,1,['pipeline'],['pipeline']
Deployability,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:733,release,releases,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['release'],['releases']
Deployability,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:110,continuous,continuous,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932,1,['continuous'],['continuous']
Deployability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:277,install,installed,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,1,['install'],['installed']
Deployability,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:57,install,install,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518,2,['install'],"['install', 'installing']"
Deployability,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014:58,install,install,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014,1,['install'],['install']
Deployability,"Same! Seems like -maxiter gets set/clobbered to 1. I'm seeing it on one machine (which I have limited access too, its a galaxy installation using scanpy scripts) but not another (my local), both of which are apparently running scanpy 1.8.1. . Im wondering if there's a umap-learn version issue? In order to set the umap n_epochs(aka maxiter) default , it looks like older versions of umap-learn expected 0 (e.g. https://github.com/lmcinnes/umap/blob/0.5.0/umap/umap_.py), whereas the newer expect None. My working installation has umap-learn 0.5.2 (which seems to expect None), and I'm not sure about the one on the other server. Might be barking up the wrong tree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544:127,install,installation,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544,2,['install'],['installation']
Deployability,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:144,release,released,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294,1,['release'],['released']
Deployability,"Sergei (@Koncopd) tested it out and will get back to you. He also found a peak memory usage of 121 GB. I have to admit that I never made checks with that degree of detail and I fear that for now, I'll simply update the documentation stating that peak memory usage can go up to ~120 GB. I'm still puzzled by that, and maybe some efficiency found it's way into the code which wasn't there (simple guess: is everything in `float32`?). But we'll need some time to work it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466:208,update,update,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466,1,['update'],['update']
Deployability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:19,update,updated,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,1,['update'],['updated']
Deployability,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:21,install,installed,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310,1,['install'],['installed']
Deployability,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:155,release,release,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,2,['release'],['release']
Deployability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:571,integrat,integration,571,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,1,['integrat'],['integration']
Deployability,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:306,update,update,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225,1,['update'],['update']
Deployability,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:31,upgrade,upgraded,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575,2,['upgrade'],['upgraded']
Deployability,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:212,install,installation,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620,1,['install'],['installation']
Deployability,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:133,install,installed,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572,1,['install'],['installed']
Deployability,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:108,update,updated,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549,1,['update'],['updated']
Deployability,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:1487,pipeline,pipelines,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,1,['pipeline'],['pipelines']
Deployability,"Thank you so much for the feedback!; I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using ; conda create -n scanpy python=3.6 scanpy; conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341:99,update,update,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341,1,['update'],['update']
Deployability,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:388,release,release,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969,1,['release'],['release']
Deployability,"Thanks Philipp, I have updated and push the code. I hope you will accept; this pull request now. To support PCA and scanpy for weighted sampling, you; can just set a parameter , observations/samples weights at the time user; input matrix and then we can modify PCA and remaining this code is fine. I; am asking for weights because user may extracted those weights either with; sampling technique or may be sometime user can give weights of his own; desired e.g. he want to focus one cell type etc. So we should support; weights generally rather specifically. Thanks,; Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,; >; > But i will suggest to just support weights instead of coreset, may be user; > want to sample data with some other weighting technique. So we should ask; > them to just put the weights for observations, then we need to modify PCA; > as well and i think my code will support most of plots and marker genes,; > but not PCA, because my input is PCA matrix with weights for each; > observations.; >; > Thanks,; > Khalid; >; > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>; > wrote:; >; >> Long-term, we should think about the design here: Specifying weights all; >> the time is possible, but not very nice for users. So a few questions come; >> to mind:; >>; >> Should we add scanpy.pp.coreset, which would create a sampling and add; >> adata.obs['coreset_weights'] or simply adata.obs['weights']?; >>; >> If we do that or plan to in the future, how should the added weights; >> parameter to all these functions work?; >>; >> I think it might default to 'coreset_weights', and the functions would; >> automatically use that .obs column if it exists. Users should also still; >> be able to specify weights manually as in this PR.; >>; >> So the type of the parameter would be Union[str, pd.DataFrame,; >> Sequence[Union[float, int]]].; >> ------------------------------; >>; >> All of that doesn’t really affect th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004:23,update,updated,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004,1,['update'],['updated']
Deployability,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and; push when it passed tests for all 5 plots. Thanks,; Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,; >; > I have removed issue from the pull request by the testing tool, now the; > tools showed me duplications, which are mostly from other code and 1-2 from; > my code. Please have a look into it. It's my first pull request and its; > taking too much time :(; >; > Thanks; > Khalid; >; > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:; >; >> Ok , thanks for letting me know. Please check the pull request. I have; >> verified my code by keeping weights 1 and it has same values when; >> observations has no weights or all weights equal to 1.; >>; >> I also suggest to update PCA for weighted sampled data.; >>; >> Thanks,; >> Khalid Usman; >>; >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; >> wrote:; >>; >>> You can just open a new one, I’ll close this one then 🙂; >>>; >>> —; >>> You are receiving this because you authored the thread.; >>> Reply to this email directly, view it on GitHub; >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >>> or mute the thread; >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >>> .; >>>; >>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520:883,update,update,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520,1,['update'],['update']
Deployability,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:; ```; WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)); Reason for being yanked: License Violation; ```; Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615:208,install,install,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615,2,['install'],"['install', 'installation']"
Deployability,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:499,update,updated,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256,1,['update'],['updated']
Deployability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:782,release,releases,782,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,1,['release'],['releases']
Deployability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,1,['update'],['update']
Deployability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:1365,release,releases,1365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,1,['release'],['releases']
Deployability,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:39,release,released,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435,3,"['patch', 'release']","['patch', 'release', 'released']"
Deployability,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:121,update,updated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,4,"['install', 'update']","['install', 'installing', 'update', 'updated']"
Deployability,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:112,install,installed,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092,1,['install'],['installed']
Deployability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:469,update,updated,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['update'],['updated']
Deployability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:113,update,updated,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,3,['update'],"['update', 'updated']"
Deployability,"The problem is that it does not install at all. ; When I run; ```; conda create -n test; conda activate test; conda install python=3.11; conda install -c conda-forge scanpy; ```; I get an error output for the last line, which is:; ```; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:32,install,install,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,5,['install'],"['install', 'installed']"
Deployability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:290,install,install,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['install'],['install']
Deployability,"This error is certainly caused by ""scikit-learn"". I abandoned this conda environment and created a new one by `conda create -n Scanpy -c conda-forge scikit-learn`[https://scikit-learn.org/stable/install.html](url).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729:195,install,install,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729,1,['install'],['install']
Deployability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,configurat,configuration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,5,"['configurat', 'install', 'update']","['configuration', 'install', 'installation', 'installed', 'updated']"
Deployability,This is great! 👍 . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924:32,update,update,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924,1,['update'],['update']
Deployability,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811:463,update,update,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811,1,['update'],['update']
Deployability,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:31,install,install,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129,2,['install'],['install']
Deployability,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:308,install,installs,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984,3,['install'],"['installation', 'installations', 'installs']"
Deployability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:28,update,update,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['update'],['update']
Deployability,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:31,install,installation,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825,10,['install'],"['install', 'installation', 'installations', 'installing']"
Deployability,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,1,['update'],['update']
Deployability,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:589,install,installed,589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316,1,['install'],['installed']
Deployability,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:529,install,install,529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:51,release,release,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,2,"['release', 'update']","['release', 'updates']"
Deployability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:187,install,installation,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,2,['install'],"['installation', 'installed']"
Deployability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,integrat,integrate,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['integrat'],['integrate']
Deployability,"\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:2413,install,install,2413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],['install']
Deployability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:103,install,install,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,5,['install'],"['install', 'installation', 'installing']"
Deployability,"able_pyobject:. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:693, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); [669](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:18948,pipeline,pipeline,18948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"aging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ---->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5555,install,install,5555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,"an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1259,update,update,1259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['update'],['update']
Deployability,"anpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2258,update,update,2258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['update'],['update']
Deployability,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1573,pipeline,pipeline,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipeline']
Deployability,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:40,release,released,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896,2,['release'],"['release', 'released']"
Deployability,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,integrat,integrate,1047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['integrat'],['integrate']
Deployability,bd54_1 ; multicoretsne 0.1 pypi_0 pypi; multidict 5.1.0 pypi_0 pypi; multipledispatch 0.6.0 py38_0 ; mypy_extensions 0.4.3 py38_0 ; natsort 7.1.1 pyhd3eb1b0_0 ; nbclassic 0.2.6 pyhd3eb1b0_0 ; nbclient 0.5.3 pyhd3eb1b0_0 ; nbconvert 6.0.7 py38_0 ; nbformat 5.1.3 pyhd3eb1b0_0 ; ncurses 6.2 he6710b0_1 ; nest-asyncio 1.5.1 pyhd3eb1b0_0 ; networkx 2.5 py_0 ; nltk 3.6.2 pyhd3eb1b0_0 ; nose 1.3.7 pyhd3eb1b0_1006 ; notebook 6.4.0 py38h06a4308_0 ; numba 0.53.1 py38ha9443f7_0 ; numexpr 2.7.3 py38h22e1b3c_1 ; numpy 1.20.2 py38h2d18471_0 ; numpy-base 1.20.2 py38hfae3a4d_0 ; numpydoc 1.1.0 pyhd3eb1b0_1 ; nvidia-ml-py3 7.352.0 pypi_0 pypi; olefile 0.46 py_0 ; opencensus 0.7.13 pypi_0 pypi; opencensus-context 0.1.2 pypi_0 pypi; openpyxl 3.0.7 pyhd3eb1b0_0 ; openssl 1.1.1k h27cfd23_0 ; packaging 20.9 pyhd3eb1b0_0 ; palantir 1.0.0 pypi_0 pypi; pandas 1.2.4 py38h2531618_0 ; pandoc 2.12 h06a4308_0 ; pandocfilters 1.4.3 py38h06a4308_1 ; pango 1.42.4 h049681c_0 ; parso 0.7.0 py_0 ; partd 1.2.0 pyhd3eb1b0_0 ; patchelf 0.12 h2531618_1 ; path 15.1.2 py38h06a4308_0 ; path.py 12.5.0 0 ; pathlib2 2.3.5 py38h06a4308_2 ; pathspec 0.7.0 py_0 ; pathtools 0.1.2 py_1 ; patsy 0.5.1 py38_0 ; pcre 8.44 he6710b0_0 ; pep8 1.7.1 py38_0 ; pexpect 4.8.0 pyhd3eb1b0_3 ; phenograph 1.5.7 pypi_0 pypi; pickleshare 0.7.5 pyhd3eb1b0_1003 ; pillow 8.2.0 py38he98fc37_0 ; pip 21.1.1 py38h06a4308_0 ; pixman 0.40.0 h7b6447c_0 ; pkginfo 1.7.0 py38h06a4308_0 ; pluggy 0.13.1 py38h06a4308_0 ; ply 3.11 py38_0 ; progeny-py 1.0.3 pypi_0 pypi; progressbar2 3.37.1 py38h06a4308_0 ; prometheus_client 0.10.1 pyhd3eb1b0_0 ; prompt-toolkit 3.0.17 pyh06a4308_0 ; prompt_toolkit 3.0.17 hd3eb1b0_0 ; protobuf 3.17.0 pypi_0 pypi; psutil 5.8.0 py38h27cfd23_1 ; ptyprocess 0.7.0 pyhd3eb1b0_2 ; py 1.10.0 pyhd3eb1b0_0 ; py-lief 0.10.1 py38h403a769_0 ; py-spy 0.3.7 pypi_0 pypi; pyasn1 0.4.8 pypi_0 pypi; pyasn1-modules 0.2.8 pypi_0 pypi; pycairo 1.19.1 py38h708ec4a_0 ; pycodestyle 2.6.0 pyhd3eb1b0_0 ; pycosat 0.6.3 py38h7b6447c_1 ; pycparser 2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:12434,patch,patchelf,12434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['patch'],['patchelf']
Deployability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,integrat,integrating,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['integrat'],['integrating']
Deployability,"cksSM.connect('changed', cb.update_normal); 1734 mappable.colorbar = cb. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3618,update,updated,3618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['update'],['updated']
Deployability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1362,patch,patch,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,4,['patch'],['patch']
Deployability,"conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5876,pipeline,pipelines,5876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipelines']
Deployability,"d, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1732,install,install,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['install'],['install']
Deployability,"e 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2032,update,update,2032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['update']
Deployability,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2450,pipeline,pipeline,2450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['pipeline'],['pipeline']
Deployability,"e; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\_typing.py in <module>; 82 # array-like; 83 ; ---> 84 ArrayLike = Union[""ExtensionArray"", np.ndarray]; 85 AnyArrayLike = Union[ArrayLike, ""Index"", ""Series""]; 86 . AttributeError: module 'numpy' has no attribute 'ndarray'; ```; It looks like an endless error. What's wrong with the `!pip install scanpy[leiden]`? It installed so many incompatible packages which never happened before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:8526,install,install,8526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],"['install', 'installed']"
Deployability,"e\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=349) msg = ""Failed in %s mode pipeline (step: %s)"" % \; [351](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=350) (self.pipeline_name, pass_desc); [352](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=351) patched_exception = self._patch_error(msg, e); --> [353](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=352) raise patched_exception. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:341, in PassManager.run(self, state); [339](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=338) pass_inst = _pass_registry.get(pss).pass_inst; [340](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=339) if isinstance(pass_inst, CompilerPass):; --> [341](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:22478,pipeline,pipeline,22478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"e_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip installed by pip? False; Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:10719,install,installed,10719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['installed']
Deployability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5170,install,installation,5170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['install'],"['installation', 'installed']"
Deployability,"ent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5910,install,install,5910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install']
Deployability,"equirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); tsundoku@tsundoku-OptiPlex-7070:~/networkanalyst/src/main/webapp/resources/data$ pip install scanpy; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:6079,install,install,6079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6771,update,updates,6771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['update'],['updates']
Deployability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:415,pipeline,pipeline,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5006,pipeline,pipeline,5006,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,['pipeline'],['pipeline']
Deployability,"for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:1469,release,release,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['release'],['release']
Deployability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2297,pipeline,pipelines,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipelines']
Deployability,"format 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:1971,update,updated,1971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['updated']
Deployability,"ghts the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this param",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1195,install,install,1195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['install'],['install']
Deployability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1098,integrat,integrate,1098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,2,['integrat'],"['integrate', 'integration']"
Deployability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,integrat,integration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['integrat'],['integration']
Deployability,"hi @yotamcons ,. thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561:154,update,updated,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561,1,['update'],['updated']
Deployability,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1214,install,install,1214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"iniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py (53); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:33208,pipeline,pipeline,33208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9850,pipeline,pipeline,9850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipeline']
Deployability,"ler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self); 134 if self.generator_info is None:; 135 self.genlower = None; --> 136 self.lower_normal_function(self.fndesc); 137 else:; 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7735,pipeline,pipeline,7735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipeline']
Deployability,"lib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.1; matplotlib 3.3.2; matplotlib_inline NA; ml_collections NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.1; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pyro 1.8.2; pytorch_lightning 1.7.7; pytz 2021.3; requests 2.27.1; rich NA; scipy 1.8.0; scrublet NA; scvi 0.19.0; seaborn 0.11.2; session_info 1.0.0; setuptools 60.9.3; setuptools_scm NA; six 1.16.0; sklearn 0.23.2; socks 1.7.1; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.13.2; tensorboard 2.10.1; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; torch 1.11.0; torchmetrics 0.10.2; torchvision 0.12.0; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.7; typing_extensions NA; umap 0.5.2; urllib3 1.26.8; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 8.1.1; jupyter_client 7.1.2; jupyter_core 4.9.2; notebook 6.4.9; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.4.0-139-generic-x86_64-with-glibc2.10; -----; Session information updated at 2023-02-26 19:13; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:6085,update,updated,6085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['update'],['updated']
Deployability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,integrat,integration,1716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integration']
Deployability,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:5394,install,installation,5394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,3,"['install', 'update']","['installation', 'installed', 'updated']"
Deployability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1735,install,installs,1735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,2,['install'],['installs']
Deployability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3312,install,installed,3312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['installed']
Deployability,"n them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_k",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3372,install,installed,3372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['install'],['installed']
Deployability,"n3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3879,pipeline,pipeline,3879,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,3,['pipeline'],['pipeline']
Deployability,"naconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10787,install,install,10787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"nd we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1247,pipeline,pipeline,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipeline']
Deployability,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:188,release,release,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112,2,['release'],['release']
Deployability,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:48,install,installing,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006,1,['install'],['installing']
Deployability,onda-forge; atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge; attrs 20.2.0 pyh9f0ad1d_0 conda-forge; autopep8 1.5.4 pyh9f0ad1d_0 conda-forge; babel 2.8.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 py_2 conda-forge; backports.functools_lru_cache 1.6.1 py_0 conda-forge; bleach 3.2.1 pyh9f0ad1d_0 conda-forge; blosc 1.20.1 hb1e8313_0 conda-forge; brotlipy 0.7.0 py38h94c058a_1001 conda-forge; bzip2 1.0.8 haf1e3a3_3 conda-forge; c-ares 1.16.1 haf1e3a3_3 conda-forge; ca-certificates 2020.10.14 0 ; cairo 1.16.0 h360c52f_1006 conda-forge; certifi 2020.6.20 py38h5347e94_2 conda-forge; cffi 1.14.3 py38h9edaa1b_1 conda-forge; chardet 3.0.4 py38h5347e94_1008 conda-forge; click 7.1.2 pyh9f0ad1d_0 conda-forge; cloudpickle 1.6.0 py_0 conda-forge; colorama 0.4.4 pyh9f0ad1d_0 conda-forge; cryptography 3.2 py38hf6767f5_0 conda-forge; cycler 0.10.0 py_2 conda-forge; dbus 1.13.18 h18a8e69_0 ; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge; docutils 0.16 py38h5347e94_2 conda-forge; entrypoints 0.3 py38h32f6830_1002 conda-forge; expat 2.2.10 hb1e8313_2 ; fa2 0.3.5 py38h4d0b108_0 conda-forge; flake8 3.8.4 py_0 conda-forge; fontconfig 2.13.1 h79c0d67_1002 conda-forge; freetype 2.10.4 ha233b18_0 conda-forge; future 0.18.2 py38h32f6830_2 conda-forge; get_version 2.1 py_1 conda-forge; gettext 0.19.8.1 haf92f58_1004 conda-forge; glib 2.66.2 hb1e8313_0 conda-forge; gmp 6.2.0 hb1e8313_3 conda-forge; h5py 2.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:1573,patch,patch,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['patch'],['patch']
Deployability,"ong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=492) """"""; [494](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=493) Populate and run pipeline for bytecode input; [495](file:///",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:19268,pipeline,pipeline,19268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:1715,update,update,1715,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['update'],['update']
Deployability,"ows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1254,patch,patches,1254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['patch'],['patches']
Deployability,prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-editor 1.0.4 py_0 conda-forge; python-igraph 0.10.6 py310h33b8572_0 conda-forge; python-installer 0.7.0 pyhd8ed1ab_0 conda-forge; python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.10 3_cp310 conda-forge; pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch; pytorch-cuda 11.8 h7e8668a_5 pytorch; pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge; pytorch-mutex 1.0 cuda pytorch; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyyaml 6.0 py310h5764c6d_5 conda-forge; pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge; rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge; re2 2023.03.02 h8c504da_0 conda-forge; readchar 4.0.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; referencing 0.30.0 pyhd8ed1ab_0 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge; rich 13.4.2 pyhd8ed1ab_0 conda-forge; rpds-py 0.9.2 py310hcb5633a_0 conda-forge; scanpy 1.9.3 pyhd8ed1ab_0 conda-forge; scikit-learn 1.3.0 py310hf7d194e_0 conda-forge; scipy 1.11.1 py310ha4c1d20_0 conda-forge; scvi-tools,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:18493,install,installer,18493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['install'],['installer']
Deployability,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2771,install,installed,2771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['install'],['installed']
Deployability,"sk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:2948,pipeline,pipeline,2948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['pipeline'],['pipeline']
Deployability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:2207,configurat,configuration,2207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,2,['configurat'],['configuration']
Deployability,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:147,install,install,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671,2,['install'],"['install', 'installed']"
Deployability,"to make formatting easier, you should do `pre-commit install` like mentioned in the contributor guide: https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#pre-commit. Please also fill out the checklist:. - if there is an issue closed by this, please link it; - you can check the box for tests, since this function already has tests. I added the `benchmark` label so we see that it actually makes things faster; - what’s missing is a release note entry: just edit `1.10.2.md` and add a line there please",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949,2,"['install', 'release']","['install', 'release']"
Deployability,"ts that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4258,install,install-,4258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:39,install,install,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609,1,['install'],['install']
Deployability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1128,integrat,integrate,1128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integrate']
Deployability,"ustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2789,release,release,2789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['release'],['release']
Deployability,"utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:7215,install,install,7215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,"working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1313,install,install,1313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"y48/lib/site-packages/numba/core/compiler.py?line=495) assert self.state.func_ir is None; --> [497](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=496) return self._compile_core(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:476, in CompilerBase._compile_core(self); [474](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=473) self.state.status.fail_reason = e; [475](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=474) if is_final_pipeline:; --> [476](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=475) raise e; [477](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=476) else:; [478](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=477) raise CompilerError(""All available pipelines exhausted""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:21428,pipeline,pipelines,21428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipelines']
Deployability,"ypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; triton 2.0.0 pypi_0 pypi; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023.3 pypi_0 pypi; umap-learn 0.5.3 pypi_0 pypi; urllib3 1.26.13 pypi_0 pypi; uvicorn 0.23.1 pypi_0 pypi; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; websocket-client 1.6.1 pypi_0 pypi; websockets 11.0.3 pypi_0 pypi; wheel 0.41.0 pyhd8ed1ab_0 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pypi_0 pypi; xz 5.2.6 h166bdaf_0 conda-forge; yamlordereddictloader 0.4.0 pypi_0 pypi; yarl 1.9.2 pypi_0 pypi; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; bro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:8418,install,install,8418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,3,['install'],['install']
Deployability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1930,release,release,1930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,"['release', 'update']","['release', 'update']"
Deployability,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4886,install,install,4886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['install'],['install']
Deployability,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3796,pipeline,pipeline,3796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['pipeline'],['pipeline']
Energy Efficiency," :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution?. > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1180,efficient,efficient,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['efficient'],['efficient']
Energy Efficiency," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,adapt,adaptive,6385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaptive']
Energy Efficiency," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,adapt,adaption,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaption']
Energy Efficiency," cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1203,power,power,1203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['power'],['power']
Energy Efficiency," get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3639,efficient,efficient,3639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['efficient'],['efficient']
Energy Efficiency,"=========; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1205,reduce,reducer,1205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"========================= test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1107,reduce,reducer,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"=================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1168,reduce,reducer,1168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:147,efficient,efficiently,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,1,['efficient'],['efficiently']
Energy Efficiency,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:796,reduce,reduce,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,1,['reduce'],['reduce']
Energy Efficiency,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:507,reduce,reduced,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599,1,['reduce'],['reduced']
Energy Efficiency,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:102,reduce,reduce,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['reduce'],['reduce']
Energy Efficiency,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,adapt,adapt,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,1,['adapt'],['adapt']
Energy Efficiency,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:305,power,powerful,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,1,['power'],['powerful']
Energy Efficiency,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:164,sustainab,sustainable,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,2,"['adapt', 'sustainab']","['adapting', 'sustainable']"
Energy Efficiency,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:160,reduce,reduce,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,1,['reduce'],['reduce']
Energy Efficiency,"Ah sorry, I should have told you that. All of the preprocessing functions can be migrated to only work with AnnData; there is no important setting in which you want to pass an array or a sparse matrix. That's also remniscient from the early days when I thought people might not like to use AnnData. But that's of course stupid, they wouldn't use Scanpy in that case, anyway. I'm merging this for now so that we have something working for 1.4.1, but if you want to simplify and reduce this to AnnData-only, happy to merge a PR on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016:477,reduce,reduce,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016,1,['reduce'],['reduce']
Energy Efficiency,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:90,efficient,efficient,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,2,"['adapt', 'efficient']","['adapted', 'efficient']"
Energy Efficiency,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:150,efficient,efficient,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207,1,['efficient'],['efficient']
Energy Efficiency,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,adapt,adapt,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['adapt'],['adapt']
Energy Efficiency,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:241,efficient,efficient,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,1,['efficient'],['efficient']
Energy Efficiency,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,adapt,adapting,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['adapt'],['adapting']
Energy Efficiency,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,adapt,adapt,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['adapt'],['adapt']
Energy Efficiency,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:1156,power,powershell,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['power'],['powershell']
Energy Efficiency,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,adapt,adapt,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['adapt'],"['adapt', 'adaption']"
Energy Efficiency,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,adapt,adapted,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['adapt'],['adapted']
Energy Efficiency,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:145,efficient,efficiently,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['efficient'],['efficiently']
Energy Efficiency,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:117,power,powers,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140,2,['power'],['powers']
Energy Efficiency,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:474,schedul,scheduled,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,1,['schedul'],['scheduled']
Energy Efficiency,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:1054,power,powerful,1054,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,1,['power'],['powerful']
Energy Efficiency,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,adapt,adapt,379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,1,['adapt'],['adapt']
Energy Efficiency,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:397,monitor,monitor,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937,1,['monitor'],['monitor']
Energy Efficiency,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:415,adapt,adapting-to-real-world-data,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['adapt'],['adapting-to-real-world-data']
Energy Efficiency,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:653,power,powerful,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['power'],['powerful']
Energy Efficiency,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['adapt'],['adapted']
Energy Efficiency,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:127,green,green,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387,1,['green'],['green']
Energy Efficiency,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:35,reduce,reduce,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,3,['reduce'],['reduce']
Energy Efficiency,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:109,power,power,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702,3,['power'],['power']
Energy Efficiency,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:404,sustainab,sustainable,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['sustainab'],['sustainable']
Energy Efficiency,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:161,power,powerful,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['power'],['powerful']
Energy Efficiency,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:292,drain,draining,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,1,['drain'],['draining']
Energy Efficiency,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:701,efficient,efficient,701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,1,['efficient'],['efficient']
Energy Efficiency,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```; def sparse_pca(X,npcs,mu = None):; # X -- scipy sparse data matrix; # npcs -- number of principal components; # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features; if mu is None: ; mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means; mmat = mdot = mu.dot ; # dot product operator for the transposed means; mhmat = mhdot = mu.T.dot ; # dot product operator for the data; Xmat = Xdot = X.dot ; # dot product operator for the transposed data; XHmat = XHdot = X.T.conj().dot ; # dot product operator for a vector of ones; ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means; def matvec(x): ; return Xdot(x) - mdot(x); def matmat(x): ; return Xmat(x) - mmat(x); def rmatvec(x): ; return XHdot(x) - mhdot(ones(x)); def rmatmat(x): ; return XHmat(x) - mhmat(ones(x)); ; # construct the LinearOperator; XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,; matmat = matmat,; shape = X.shape,; rmatvec = rmatvec, rmatmat = rmatmat); ; u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs); ; # i like my eigenvalues sorted in decreasing order; idx = np.argsort(-s); S = np.diag(s[idx]); # principal components; pcs = u[:,idx].dot(S) ; # equivalent to PCA.components_ in sklearn ; components_ = v[idx,:] ; return pcs,components_; ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727:1634,efficient,efficient,1634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727,1,['efficient'],['efficient']
Energy Efficiency,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:295,power,powerful,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,1,['power'],['powerful']
Energy Efficiency,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:2009,efficient,efficient,2009,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178,1,['efficient'],['efficient']
Energy Efficiency,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2827,schedul,scheduler,2827,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['schedul'],['scheduler']
Energy Efficiency,fastcache 1.1.0 py38h7b6447c_0 ; fbpca 1.0 pypi_0 pypi; fcsparser 0.2.1 pypi_0 pypi; filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7841,green,greenlet,7841,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['green'],['greenlet']
Integrability," Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10416,wrap,wrap,10416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability," `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:2219,message,messages,2219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['message'],['messages']
Integrability," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1991,wrap,wrap,1991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['wrap'],['wrap']
Integrability," is much more complicated to maintain - at least didn't feel too bad for me when I added a few recipes there in the past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1024,depend,depend,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['depend'],['depend']
Integrability," past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspective",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1142,depend,dependents,1142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['depend'],['dependents']
Integrability," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1091,wrap,wrap,1091,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021,1,['wrap'],['wrap']
Integrability,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2178,inject,injecting,2178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['inject'],['injecting']
Integrability,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:840,interface,interface,840,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,2,"['integrat', 'interface']","['integrating', 'interface']"
Integrability,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:354,wrap,wrapping,354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['wrap'],['wrapping']
Integrability,"*kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1367,wrap,wrapper,1367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['wrap'],['wrapper']
Integrability,"-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1339,message,message,1339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['message'],['message']
Integrability,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7900,depend,depending,7900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['depend'],['depending']
Integrability,.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.17.0 py_0 conda-forge; jpeg 9d h0b31af3_0 conda-forge; jsonschema 3.2.0 py38h32f6830_1 conda-forge; jupyter_client 6.1.7 py_0 conda-forge; jupyter_core 4.6.3 py38h32f6830_2 conda-forge; jupyterlab_pygments 0.1.2 pyh9f0ad1d_0 conda-forge; keyring 21.4.0 py38h32f6830_2 conda-forge; kiwisolver 1.3.0 py38h02bb52f_0 conda-forge; krb5 1.17.1 h75d18d8_3 conda-forge; lazy-object-proxy 1.4.3 py38h64e0658_2 conda-forge; lcms2 2.11 h174193d_0 conda-forge; legacy-api-wrap 1.2 py_0 conda-forge; libblas 3.9.0 2_openblas conda-forge; libcblas 3.9.0 2_openblas conda-forge; libclang 10.0.1 default_hf57f61e_1 conda-forge; libcurl 7.71.1 h9bf37e3_8 conda-forge; libcxx 11.0.0 h439d374_0 conda-forge; libedit 3.1.20191231 hed1e85f_2 conda-forge; libev 4.33 haf1e3a3_1 conda-forge; libffi 3.2.1 1 bioconda; libgfortran 4.0.0 h50e675f_13 conda-forge; libgfortran4 7.5.0 h50e675f_13 conda-forge; libglib 2.66.2 hdb5fb44_0 conda-forge; libiconv 1.16 haf1e3a3_0 conda-forge; liblapack 3.9.0 2_openblas conda-forge; libllvm10 10.0.1 h009f743_3 conda-forge; libnghttp2 1.41.0 h7580e61_2 conda-forge; libopenblas 0.3.12 openmp_h63d9170_1 conda-forge; libpng 1.6.37 hb0a8c7a_2 conda-forge; libpq 12.3 haa216e0_2 conda-forge; libsodium 1.0.18 haf1e3a3_1 conda-forge; libspatialindex 1.9.3 h4a8c4bd_3 conda-forge; libssh2 1.9.0 h39bdce6_5 conda-forge; libtiff 4.1.0 h2ae36a8_6 conda-forge; libwebp-base 1.1.0 h0b31af3_3 conda-forge; libxml2 2.9.10 h7fdee97_2 conda-for,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:3095,wrap,wrap,3095,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['wrap'],['wrap']
Integrability,1 ; jeepney 0.6.0 pyhd3eb1b0_0 ; jinja2 3.0.0 pyhd3eb1b0_0 ; joblib 1.0.1 pyhd3eb1b0_0 ; jpeg 9b h024ee3a_2 ; json5 0.9.5 py_0 ; jsonschema 3.2.0 py_2 ; jupyter 1.0.0 py38_7 ; jupyter-packaging 0.7.12 pyhd3eb1b0_0 ; jupyter_client 6.1.12 pyhd3eb1b0_0 ; jupyter_console 6.4.0 pyhd3eb1b0_0 ; jupyter_contrib_core 0.3.3 py_2 conda-forge; jupyter_contrib_nbextensions 0.5.1 pyhd8ed1ab_2 conda-forge; jupyter_core 4.7.1 py38h06a4308_0 ; jupyter_highlight_selected_word 0.2.0 py38h578d9bd_1002 conda-forge; jupyter_latex_envs 1.4.6 pyhd8ed1ab_1002 conda-forge; jupyter_nbextensions_configurator 0.4.1 py38h578d9bd_2 conda-forge; jupyter_server 1.4.1 py38h06a4308_0 ; jupyterlab 3.0.14 pyhd3eb1b0_1 ; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.4.0 pyhd3eb1b0_0 ; jupyterlab_widgets 1.0.0 pyhd3eb1b0_1 ; keyring 23.0.1 py38h06a4308_0 ; kiwisolver 1.3.1 py38h2531618_0 ; krb5 1.17.1 h173b8e3_0 ; lazy-object-proxy 1.6.0 py38h27cfd23_0 ; lcms2 2.12 h3be6417_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 py_0 conda-forge; leidenalg 0.8.2 py38habedc41_0 conda-forge; libarchive 3.4.2 h62408e4_0 ; libcurl 7.69.1 h20c2e04_0 ; libedit 3.1.20210216 h27cfd23_1 ; libev 4.33 h7b6447c_0 ; libffi 3.2.1 hf484d3e_1007 ; libgcc-ng 9.3.0 h2828fa1_19 conda-forge; libgfortran-ng 7.3.0 hdf63c60_0 ; libgomp 9.3.0 h2828fa1_19 conda-forge; libiconv 1.15 h63c8f33_5 ; liblief 0.10.1 he6710b0_0 ; libllvm10 10.0.1 hbcb73fb_5 ; libllvm9 9.0.1 h4a3c616_1 ; libpng 1.6.37 hbc83047_0 ; libsodium 1.0.18 h7b6447c_0 ; libspatialindex 1.9.3 h2531618_0 ; libssh2 1.9.0 h1ba5d50_1 ; libstdcxx-ng 9.1.0 hdf63c60_0 ; libtiff 4.2.0 h85742a9_0 ; libtool 2.4.6 h7b6447c_1005 ; libuuid 1.0.3 h1bed415_2 ; libuv 1.40.0 h7b6447c_0 ; libwebp-base 1.2.0 h27cfd23_0 ; libxcb 1.14 h7b6447c_0 ; libxml2 2.9.10 hb55368b_3 ; libxslt 1.1.34 hc22bd24_0 ; llvmlite 0.36.0 py38h612dafd_4 ; locket 0.2.1 py38h06a4308_1 ; loompy 2.0.16 py_0 bioconda; lxml 4.6.3 py38h9120a33_0 ; lz4-c 1.9.3 h2531618_0 ; lzo 2.10 h7b6447c_2 ; magic-i,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:9905,wrap,wrap,9905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['wrap'],['wrap']
Integrability,"3 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2345 self.sca(current_ax); 2346 self.stale = True. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1731, in colorbar_factory(cax, mappable, **kwargs); 1729 cb = ColorbarPatch(cax, mappable, **kwargs); 1730 else:; -> 1731 cb = Colorbar(cax, mappable, **kwargs); 1733 cid = mappable.callbacksSM.connect('changed', cb.update_normal); 1734 mappable.colorbar = cb. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defuse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3121,wrap,wrapper,3121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,2,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"39 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2889,wrap,wrapper,2889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,; cycler=0.10.0=pypi_0; decorator=4.4.2=py_0; defusedxml=0.6.0=py_0; entrypoints=0.3=py37hc8dfbb8_1001; fontconfig=2.13.1=h86ecdb6_1001; freetype=2.10.1=he06d7ca_0; get-version=2.1=pypi_0; gettext=0.19.8.1=hc5be6a0_1002; glib=2.64.2=h6f030ca_0; gmp=6.2.0=he1b5a44_2; h5py=2.10.0=pypi_0; icu=64.2=he1b5a44_1; idna=2.9=py_1; importlib-metadata=1.6.0=py37hc8dfbb8_0; importlib_metadata=1.6.0=0; ipykernel=5.2.1=py37h43977f1_0; ipython=7.13.0=py37hc8dfbb8_2; ipython_genutils=0.2.0=py_1; jedi=0.17.0=py37hc8dfbb8_0; jinja2=2.11.2=pyh9f0ad1d_0; joblib=0.14.1=pypi_0; json5=0.9.0=py_0; jsonschema=3.2.0=py37hc8dfbb8_1; jupyter_client=6.1.3=py_0; jupyter_contrib_core=0.3.3=py_2; jupyter_contrib_nbextensions=0.5.1=py37_0; jupyter_core=4.6.3=py37hc8dfbb8_1; jupyter_highlight_selected_word=0.2.0=py37_1000; jupyter_latex_envs=1.4.6=py37_1000; jupyter_nbextensions_configurator=0.4.1=py37_0; jupyterlab=2.1.1=py_0; jupyterlab_server=1.1.1=py_0; kiwisolver=1.2.0=pypi_0; ld_impl_linux-64=2.33.1=h53a641e_7; legacy-api-wrap=1.2=pypi_0; leidenalg=0.8.0=py37h43df1e8_0; libedit=3.1.20181209=hc058e9b_0; libffi=3.2.1=hd88cf55_4; libgcc-ng=9.1.0=hdf63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; libstdcxx-ng=9.1.0=hdf63c60_0; libuuid=2.32.1=h14c3975_1000; libxcb=1.13=h14c3975_1002; libxml2=2.9.10=hee79883_0; libxslt=1.1.33=h31b3aaa_0; llvmlite=0.32.0=pypi_0; lxml=4.5.0=py37he3881c9_1; markupsafe=1.1.1=py37h8f50634_1; matplotlib=3.2.1=pypi_0; mistune=0.8.4=py37h8f50634_1001; natsort=7.0.1=pypi_0; nbconvert=5.6.1=py37hc8dfbb8_1; nbformat=5.0.6=py_0; ncurses=6.2=he6710b0_0; networkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:1747,wrap,wrap,1747,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['wrap'],['wrap']
Integrability,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8765,wrap,wrapt,8765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['wrap'],['wrapt']
Integrability,<details>; <summary>Installed scanpy on jupyter notebook/ anaconda: </summary>. ```; pip install scanpy. Requirement already satisfied: scanpy in c:\users\charles\anaconda3\lib\site-packages (1.7.2); Requirement already satisfied: numba>=0.41.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\an,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:585,wrap,wrap,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['wrap'],['wrap']
Integrability,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:570,wrap,wrap,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,1,['wrap'],['wrap']
Integrability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:403,interface,interface,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,2,['interface'],['interface']
Integrability,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:331,wrap,wrapper,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,1,['wrap'],['wrapper']
Integrability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:1141,interface,interfaces,1141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,2,['interface'],"['interface', 'interfaces']"
Integrability,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:684,wrap,wrapper,684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953,1,['wrap'],['wrapper']
Integrability,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:40,wrap,wrapper,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515,1,['wrap'],['wrapper']
Integrability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:68,integrat,integrate,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,2,['integrat'],"['integrate', 'integrated']"
Integrability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:49,depend,dependency,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,3,['depend'],['dependency']
Integrability,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). ; What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy; >; > Could does this relate to @davidsebfischer and diffxpy?. Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226:916,interface,interface,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226,1,['interface'],['interface']
Integrability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1053,integrat,integration,1053,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,2,['integrat'],['integration']
Integrability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920,1,['integrat'],['integration']
Integrability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457,1,['integrat'],['integration']
Integrability,"> Hi, please provide the data you use, otherwise this is not reproducible:; > ; > ```; > FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); > ```. Hey, the data is publicly available under this link: https://www.10xgenomics.com/resources/datasets/human-lung-cancer-ffpe-2-standard. I simple copied the `curl` bash script to download all the files and then unzipped the file corresponding to the images to get the ""spatial"" folder",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906:277,message,message,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906,1,['message'],['message']
Integrability,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:660,depend,depends,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,1,['depend'],['depends']
Integrability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1238,message,messagestream,1238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,1,['message'],['messagestream']
Integrability,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:1336,interface,interfaces,1336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140,1,['interface'],['interfaces']
Integrability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1692,depend,dependencies,1692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,1,['depend'],['dependencies']
Integrability,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:532,integrat,integration,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,1,['integrat'],['integration']
Integrability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:215,depend,depend,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,2,"['depend', 'wrap']","['depend', 'wrapped']"
Integrability,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:729,depend,depending,729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['depend'],['depending']
Integrability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:213,depend,dependencies,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,4,['depend'],"['depend', 'dependencies', 'dependents']"
Integrability,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:106,depend,dependency,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,1,['depend'],['dependency']
Integrability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:877,depend,dependencies,877,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['depend'],['dependencies']
Integrability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,integrat,integrate,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,2,['integrat'],['integrate']
Integrability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,1,['integrat'],['integration']
Integrability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:109,depend,depends,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,2,"['depend', 'interface']","['depends', 'interface']"
Integrability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:415,depend,depends,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,2,"['depend', 'integrat']","['depends', 'integrate']"
Integrability,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:521,interface,interface,521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128,1,['interface'],['interface']
Integrability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:208,depend,depend,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,1,['depend'],['depend']
Integrability,"> To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. I disagree very much :) First of all I didn't say that these 10 dotplots are for ""looking at data,"" most of them are indeed for a paper (we can count the number of dotplots in sc papers, if we want to have a more accurate estimate of the time cost) and all of them need to be edited in a pdf editing software. Once the preprint is out, I can write in this post which dotplots I am talking about, how many there are in reality and how much time it took us to manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:864,depend,dependency,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,2,"['depend', 'interface']","['dependency', 'interface']"
Integrability,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:776,depend,dependent,776,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973,1,['depend'],['dependent']
Integrability,"> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=40) ; [42](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=41) Parameters; (...); [49](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=48) The squared euclidean distance between x and y; [50](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=49) """"""; [51](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=50) result = 0.0. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\decorators.py:219, in _jit.<locals>.wrapper(func); [217](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=216) with typeinfer.register_dispatcher(disp):; [218](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=217) for sig in sigs:; --> [219](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=218) disp.compile(sig); [220](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=219) disp.disable_compile(); [221](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=220) return disp. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:965, in Dispatcher.compile(self, sig); [963](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=962) with ev.trigger_event(""numba:compile"", data=ev_details):; [964](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:13278,wrap,wrapper,13278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['wrap'],['wrapper']
Integrability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:1475,integrat,integrated,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,1,['integrat'],['integrated']
Integrability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:508,wrap,wrapper,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,1,['wrap'],['wrapper']
Integrability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,integrat,integrate,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,2,"['integrat', 'wrap']","['integrate', 'wrapper']"
Integrability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:298,wrap,wrap,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['wrap'],['wrap']
Integrability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:466,interface,interfaces,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,2,['interface'],['interfaces']
Integrability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:634,depend,depend,634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,1,['depend'],['depend']
Integrability,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:195,wrap,wrapper,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106,1,['wrap'],['wrapper']
Integrability,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:403,wrap,wrapper,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293,1,['wrap'],['wrapper']
Integrability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:76,depend,dependency,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,1,['depend'],['dependency']
Integrability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:722,depend,dependencies,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['depend'],['dependencies']
Integrability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:165,depend,depend,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,3,"['depend', 'integrat']","['depend', 'integrate', 'integrated']"
Integrability,"Agreed. I don’t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:110,wrap,wrapper,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247,1,['wrap'],['wrapper']
Integrability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:394,interface,interface,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,2,['interface'],['interface']
Integrability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:334,depend,depends,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,1,['depend'],['depends']
Integrability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,integrat,integration,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,1,['integrat'],['integration']
Integrability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:35,integrat,integrate,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,1,['integrat'],['integrate']
Integrability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,integrat,integrate,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,1,['integrat'],['integrate']
Integrability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,integrat,integrate,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,1,['integrat'],['integrate']
Integrability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:898,depend,depending,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,1,['depend'],['depending']
Integrability,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:52,interface,interface,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485,2,"['interface', 'wrap']","['interface', 'wrap']"
Integrability,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:183,message,message,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843,2,['message'],['message']
Integrability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:251,depend,dependencies,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,1,['depend'],['dependencies']
Integrability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:154,rout,route,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,1,['rout'],['route']
Integrability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:178,depend,dependency,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,4,"['depend', 'wrap']","['dependency', 'depends', 'wrapper']"
Integrability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,1,['integrat'],['integration']
Integrability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,integrat,integration,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,1,['integrat'],['integration']
Integrability,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:381,wrap,wrap,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133,1,['wrap'],['wrap']
Integrability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,integrat,integration,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,2,['integrat'],['integration']
Integrability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:722,wrap,wrapper,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,1,['wrap'],['wrapper']
Integrability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:173,integrat,integration,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,1,['integrat'],['integration']
Integrability,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:220,interface,interface,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,1,['interface'],['interface']
Integrability,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:712,interface,interface,712,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172,1,['interface'],['interface']
Integrability,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:482,message,messages,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,1,['message'],['messages']
Integrability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,integrat,integration,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,1,['integrat'],['integration']
Integrability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,integrat,integrated,912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['integrat'],['integrated']
Integrability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:131,message,message,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['message'],['message']
Integrability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:962,integrat,integrate,962,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,1,['integrat'],['integrate']
Integrability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:128,integrat,integrating,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,2,['integrat'],"['integrating', 'integration']"
Integrability,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:38,interface,interface,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113,1,['interface'],['interface']
Integrability,"Hi, thanks for the report!. Note that the plots in the documentation are generated on the fly when building the documentation. The plot you currently see on https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.dotplot.html has therefore been created with scanpy 1.10.1. Must be a dependency issue, I’ll try to reproduce with the environment you provided. /edit: I can reproduce it with that environment:. <details><summary>environment.yml</summary>. ```yaml; name: scanpy-3062; channels:; - conda-forge; dependencies:; - ipykernel. - python==3.10.10; - anndata==0.10.7; - scanpy==1.10.1; - IPython==8.13.2; - pillow==10.0.0; - astunparse==1.6.3; - backcall==0.2.0; - cffi==1.15.1; - cloudpickle==2.2.1; - colorama==0.4.4; - cycler==0.10.0; - cytoolz==0.12.0; - dask==2023.10.1; #- dateutil==2.8.2; - decorator==5.1.1; - defusedxml==0.7.1; - dill==0.3.6; - entrypoints==0.4; - exceptiongroup==1.1.1; - executing==1.2.0; - fasteners==0.17.3; - gmpy2==2.1.2; - h5py==3.8.0; #- icu==2.11; - python-igraph==0.11.2; - jedi==0.19.1; - jinja2==3.1.2; - joblib==1.2.0; - kiwisolver==1.4.4; - leidenalg==0.10.2; - llvmlite==0.42.0; - lz4==4.3.2; - markupsafe==2.1.2; - matplotlib==3.8.3; - mpmath==1.3.0; #- msgpack==1.0.5; - natsort==8.3.1; - numba==0.59.1; - numcodecs==0.11.0; - numexpr==2.7.3; - numpy==1.26.4; - packaging==23.1; - pandas==1.5.3; - parso==0.8.3; - pexpect==4.8.0; - pickleshare==0.7.5; - plotly==5.14.1; - prompt_toolkit==3.0.38; - psutil==5.9.5; - ptyprocess==0.7.0; - pure_eval==0.2.2; - pyarrow==10.0.1; - pydot==1.4.2; - pygments==2.15.1; - pyparsing==3.0.9; - pytz==2023.3.post1; - scipy==1.13.0; #- session_info==1.0.0; #- setuptools==67.7.2; - six==1.16.0; - scikit-learn==1.2.2; - stack_data==0.6.2; - sympy==1.11.1; - tblib==1.7.0; - texttable==1.6.7; - threadpoolctl==3.1.0; #- tlz==0.12.0; - toolz==0.11.2; #- pytorch==2.1.1; - tqdm==4.65.0; - traitlets==5.9.0; - wcwidth==0.2.6; #- yaml==5.4.1; - zarr==2.14.2; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516:287,depend,dependency,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:252,depend,dependencies,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,1,['depend'],['dependencies']
Integrability,"Hi,; Your question is actually quite difficult to answer. It depends on how you define cell identity. For example, depending on how you set the resolution parameter for your clustering you can get a very different number of clusters. As clusters often have super- and sub-structure, you cannot easily say when a sub- or superstructure is not meaningful (e.g., T-cells vs CD4+ and CD8+ T-cells). Unfortunately many clustering techniques do not incorporate an assessment of uncertainty to tell you when you are fitting noise. And even when they do, this is based on a model of what a cell cluster should look like, which does not have to conform to the biological reality. The heuristic that tends to be used is that if you can biologically interpret your clusters based on marker genes and other signatures, then they are meaningful. That does however not mean that sub- or superstructures involving these clusters are not also meaningful. Sorry, I can't give a clearer answer than that. In terms of assessing differences between clusters, this is a somewhat related problem. An approach that you could use is to look at the distribution of Euclidean distances in gene- or PCA-space. However, that comes with the caveat that Euclidean distance does not take into account the biological manifold on which the cells lie (and on which distances will be more informative). You can try to use pseudotime as a measure for distance over this manifold, however that would require you to have sampled enough of the manifold to fit it in your data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874:61,depend,depends,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874,2,['depend'],"['depending', 'depends']"
Integrability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,integrat,integration,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,2,['integrat'],['integration']
Integrability,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:131,interface,interface,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,2,"['interface', 'wrap']","['interface', 'wrappers']"
Integrability,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:194,message,message,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671,1,['message'],['message']
Integrability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:676,depend,dependencies,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,2,['depend'],['dependencies']
Integrability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,1,['integrat'],['integrate']
Integrability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1100,wrap,wrap,1100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,1,['wrap'],['wrap']
Integrability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:199,depend,dependency,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,1,['depend'],['dependency']
Integrability,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:743,protocol,protocol,743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,2,['protocol'],['protocol']
Integrability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['message'],['message']
Integrability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:45,interface,interface,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,2,['interface'],['interface']
Integrability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,integrat,integrated,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,1,['integrat'],['integrated']
Integrability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:64,message,message,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['message'],['message']
Integrability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,1,['depend'],['dependency']
Integrability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:1417,depend,dependent,1417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,2,['depend'],"['dependence', 'dependent']"
Integrability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:448,depend,dependent,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,1,['depend'],['dependent']
Integrability,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:286,depend,depend,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973,1,['depend'],['depend']
Integrability,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['integrat'],['integration']
Integrability,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051:131,depend,depending,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051,1,['depend'],['depending']
Integrability,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800,1,['depend'],['dependency']
Integrability,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:124,depend,depend,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146,1,['depend'],['depend']
Integrability,"My main reasoning was that:. * Use dask array/ delayed is more simple; * One can go from dask array/ dask delayed -> `Future` with `client.compute`, but the other way around isn't really possible since `Future`s kick off computation immediately. So I'd like to use the higher level interface as much as possible, and only use lower level when necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332:282,interface,interface,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332,1,['interface'],['interface']
Integrability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:62,depend,dependencies,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,4,['depend'],"['dependencies', 'dependency']"
Integrability,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:898,depend,depending,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179,1,['depend'],['depending']
Integrability,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:698,interface,interface,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315,1,['interface'],['interface']
Integrability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:74,depend,dependencies,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,4,['depend'],"['dependencies', 'dependency']"
Integrability,"OK, issues like this are almost always either memory or dependency problems: Something’s miscompiled or compiled for the wrong architecture (e.g. a newer CPU than you have) or simply buggy. We have no native code in Scanpy, so we don’t cause segfaults. If there’s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108:56,depend,dependency,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108,2,['depend'],"['dependencies', 'dependency']"
Integrability,"OK, so now the question is: should this become part of legacy-api-wrap?. I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py; @renamed_args(new=""old""); ```. feels more natural than. ```py; @deprecated_arg_names({""old"": ""new""}); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422:66,wrap,wrap,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422,1,['wrap'],['wrap']
Integrability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:137,depend,dependencies,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,3,"['bridg', 'depend', 'integrat']","['bridge', 'dependencies', 'integration']"
Integrability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,integrat,integrated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,2,"['integrat', 'wrap']","['integrated', 'wrapper']"
Integrability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:120,interface,interface,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['interface'],['interface']
Integrability,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:796,depend,dependency,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,2,['depend'],['dependency']
Integrability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:54,interface,interface,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,4,"['depend', 'interface', 'message']","['dependencies', 'interface', 'messages']"
Integrability,"Same thing: If the line is under-indented, the first line summary can’t be properly extracted. I’ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:109,message,message,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728,1,['message'],['message']
Integrability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:64,interface,interface,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,1,['interface'],['interface']
Integrability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:1141,message,message,1141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,1,['message'],['message']
Integrability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:155,wrap,wrapping,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,2,"['integrat', 'wrap']","['integration', 'wrapping']"
Integrability,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:240,message,message,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['message'],['message']
Integrability,"Thanks for bearing with me Isaac 😅 🙏 took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:334,wrap,wrap,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,1,['wrap'],['wrap']
Integrability,"Thanks for the quick response, @flying-sheep!. I can confirm that updating _pandas-2.2.2_ does fix this. I totally missed this possibility; it's not clear to me why the dots would change ordering, but the totals wouldn't (maybe _scanpy_ relies on default _pandas_ behaviour that changed between 1.x and 2.x?). That said, _pandas-2.x_ unfortunately breaks some dependencies in our environment, so I'll either pin _scanpy_ or use your workaround. Regarding the ordering and issue title change. Maybe a nit, but it's my understanding that the default ordering is alphabetical (which makese perfect sense as a default!). If this is correct, then I'd suggest that the wrong ordering is not the totals, but the categories themselves. Given this, the workaround that gives me the expected behaviour would be `dp.categories_order = dp.dot_color_df.index.sort_values()`:; <img width=""439"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/5192495/6f7622f5-14b5-4ea5-a44f-288c4507c4f0"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629:360,depend,dependencies,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629,1,['depend'],['dependencies']
Integrability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:106,wrap,wrapper,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,5,"['interface', 'wrap']","['interfaces', 'wrapper']"
Integrability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:765,wrap,wrapper,765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,2,['wrap'],['wrapper']
Integrability,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:201,depend,depend,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550,1,['depend'],['depend']
Integrability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:300,message,message,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['message'],['message']
Integrability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:125,depend,dependencies,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,2,['depend'],['dependencies']
Integrability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:56,interface,interfaces,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,2,['interface'],['interfaces']
Integrability,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:304,wrap,wrapping,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,2,['wrap'],['wrapping']
Integrability,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:405,depend,dependency,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,1,['depend'],['dependency']
Integrability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:243,depend,dependencies,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,2,['depend'],['dependencies']
Integrability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:482,wrap,wrapper,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,2,['wrap'],['wrapper']
Integrability,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:576,wrap,wrapper,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,3,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846:199,depend,dependent,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846,1,['depend'],['dependent']
Integrability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,integrat,integrate,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,2,"['integrat', 'wrap']","['integrate', 'wrapper']"
Integrability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:429,message,message,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,1,['message'],['message']
Integrability,"ar-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2717,wrap,wrap,2717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['wrap'],['wrap']
Integrability,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,integrat,integrate,1047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['integrat'],['integrate']
Integrability,"ass_inst = _pass_registry.get(pss).pass_inst; [340](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=339) if isinstance(pass_inst, CompilerPass):; --> [341](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=340) self._runPass(idx, pass_inst, state); [342](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=341) else:; [343](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=342) raise BaseException(""Legacy pass in use""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); [32](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=31) @functools.wraps(func); [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=32) def _acquire_compile_lock(*args, **kwargs):; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=33) with self:; ---> [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=34) return func(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); [294](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=293) mutated |= check(pss.run_initialization, internal_state); [295](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=294) with SimpleTimer() as pass_time:; --> [296](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=295) mutated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:24165,wrap,wraps,24165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['wrap'],['wraps']
Integrability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,integrat,integrating,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['integrat'],['integrating']
Integrability,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4969,depend,dependency,4969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['depend'],['dependency']
Integrability,"e of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:999,wrap,wrap,999,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,1,['wrap'],['wrap']
Integrability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5088,wrap,wrap,5088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['wrap'],"['wrap', 'wrap-']"
Integrability,"fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspectives on the general topic here 👍🏻",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1640,depend,dependencies,1640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,2,['depend'],['dependencies']
Integrability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1098,integrat,integrate,1098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,2,['integrat'],"['integrate', 'integration']"
Integrability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,integrat,integration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['integrat'],['integration']
Integrability,"hon3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4912,wrap,wrap,4912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", lin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5744,wrap,wrapper,5744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['wrap'],['wrapper']
Integrability,"ipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10280,wrap,wrap,10280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ite-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:7652,wrap,wrap,7652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1257,depend,dependencies,1257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['depend'],['dependencies']
Integrability,"lling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1206,wrap,wrapper,1206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,integrat,integration,1716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integration']
Integrability,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4909,wrap,wrap,4909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['wrap'],['wrap']
Integrability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3281,message,message,3281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['message'],['message']
Integrability,"methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1441,rout,route,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['rout'],['route']
Integrability,"mple_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; url",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1790,wrap,wrap,1790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['wrap'],['wrap']
Integrability,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1543,inject,injects,1543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['inject'],['injects']
Integrability,"ng and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:1560,message,message,1560,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['message'],['message']
Integrability,"ode 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:2231,wrap,wrap,2231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ome/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4752,wrap,wrap,4752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1139,synchroniz,synchronization,1139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['synchroniz'],['synchronization']
Integrability,"py?line=1129) res = impl(self.builder, argvals, self.loc); [1131](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=1130) return res. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\base.py:1201, in _wrap_impl.__call__(self, builder, args, loc); [1200](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1199) def __call__(self, builder, args, loc=None):; -> [1201](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1200) res = self._imp(self._context, builder, self._sig, args, loc=loc); [1202](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1201) self._context.add_linking_libs(getattr(self, 'libs', ())). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\base.py:1231, in _wrap_missing_loc.__call__.<locals>.wrapper(*args, **kwargs); [1230](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1229) kwargs.pop('loc') # drop unused loc; -> [1231](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1230) return fn(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\cpython\rangeobj.py:40, in make_range_impl.<locals>.range1_impl(context, builder, sig, args); [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=38) state.start = context.get_constant(int_type, 0); ---> [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=39) state.stop = stop; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=40) state.step = context.get_constant(int_type, 1). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\cgutils.py:164, in _StructProxy.__setattr__(self, field, value); [",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:5259,wrap,wrapper,5259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['wrap'],['wrapper']
Integrability,rotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.35.0; loompy 3.0.7; louvain 0.7.0; Markdown 3.3.7; MarkupSafe 2.1.1; matplotlib 3.4.1; matplotlib-inline 0.1.2; mistune 0.8.4; msgpack 1.0.4; multidict 6.0.2; multipledispatch 0.6.0; multiprocess 0.70.11.1; natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; pyth,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:3424,wrap,wrap,3424,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['wrap'],['wrap']
Integrability,"round. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1820,depend,depend,1820,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['depend'],['depend']
Integrability,"s 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/_utils.py in <module>; 16 from numpy import random; 17 from scipy import sparse; ---> 18 from anndata import AnnData, __version__ as anndata_version; 19 from textwrap import dedent; 20 from packaging import version. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnDat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18608,interface,interface,18608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['interface'],['interface']
Integrability,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2533,depend,depending,2533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['depend'],['depending']
Integrability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:340,wrap,wrapper,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,1,['wrap'],['wrapper']
Integrability,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2746,message,message,2746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['message'],['message']
Integrability,"sed that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:1042,message,message,1042,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['message'],['message']
Integrability,seems like the `--deps` flag can be used to select dependencies. Default is all dependencies. ```bash; $ beni --deps production pyproject.toml; channels:; - conda-forge; dependencies:; - pip:; - flit; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; name: scanpy; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811:51,depend,dependencies,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811,3,['depend'],['dependencies']
Integrability,"t fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:2392,wrap,wrapper,2392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['wrap'],['wrapper']
Integrability,"t_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1347,wrap,wrapper,1347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1753,interface,interface,1753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['interface'],['interface']
Integrability,"tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18220,wrap,wrapt,18220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['wrap'],['wrapt']
Integrability,"ties_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3583,wrap,wrapper,3583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['wrap'],['wrapper']
Integrability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1128,integrat,integrate,1128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integrate']
Integrability,"venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2748,wrap,wrapper,2748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Modifiability," ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)); > ; > ValueError: no field of name X_pca. ```; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I do the pca on the original AnnData object for the highly variable genes, it works:; ```; sc.tl.pca(adata, use_highly_variable=True, svd_solver='arpack'); print(adata); ```. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; > uns: 'pca'; > obsm: 'X_pca'; > varm: 'PCs'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:2555,variab,variable,2555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['variab'],['variable']
Modifiability," Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your conce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1481,config,configuring,1481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['config'],['configuring']
Modifiability," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,adapt,adaptive,6385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaptive']
Modifiability," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,adapt,adaption,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaption']
Modifiability, conda-forge; numpy 1.24.4 py310ha4c1d20_0 conda-forge; numpyro 0.12.1 pyhd8ed1ab_0 conda-forge; openh264 2.1.1 h780b84a_0 conda-forge; openjpeg 2.5.0 hfec8fc6_2 conda-forge; openpyxl 3.1.2 py310h2372a71_0 conda-forge; openssl 3.1.1 hd590300_1 conda-forge; opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge; optax 0.1.5 pyhd8ed1ab_0 conda-forge; ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge; orjson 3.9.2 py310h1e2579a_0 conda-forge; packaging 23.1 pyhd8ed1ab_0 conda-forge; pandas 2.0.3 py310h7cbd5c2_1 conda-forge; parso 0.8.3 pyhd8ed1ab_0 conda-forge; patsy 0.5.3 pyhd8ed1ab_0 conda-forge; pcre2 10.40 hc3806b6_0 conda-forge; pexpect 4.8.0 pyh1a96a4e_2 conda-forge; pickleshare 0.7.5 py_1003 conda-forge; pillow 9.4.0 py310h023d228_1 conda-forge; pip 23.2 pyhd8ed1ab_0 conda-forge; pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge; pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge; platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge; poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge; poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge; poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge; pooch 1.7.0 pyha770c72_3 conda-forge; prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 co,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:17396,plugin,plugin-export,17396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['plugin'],['plugin-export']
Modifiability, filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 hdba287a_0 ; jdcal 1.4.1 py_0 ; jedi 0.17.2 py38h06a4308_1 ; jeepney 0.6.0 pyhd3eb1b0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7925,plugin,plugins-base,7925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['plugin'],['plugins-base']
Modifiability," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5138,plugin,plugin-wit,5138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['plugin'],['plugin-wit']
Modifiability," of either cargo culting it or becaue they know that makes setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1362,config,configured,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,1,['config'],['configured']
Modifiability," setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that makes it easier to reason about how our test suite works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1493,config,config,1493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,2,"['config', 'plugin']","['config', 'plugins']"
Modifiability,""", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 242 in <lambda>; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 341 in from_call; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 241 in call_and_report; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 132 in runtestprotocol; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 113 in pytest_runtest_protocol; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 362 in pytest_runtestloop; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 337 in _main; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 283 in wrap_session; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 330 in pytest_cmdline_main; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""<venv>/lib/python3.12/site-packages/_pytest/config/__init__.py"", line 201 in console_main; File ""<venv>/bin/pytest"", line 10 in <module>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:7291,config,config,7291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['config'],['config']
Modifiability,"; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2277,config,config,2277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['config'],['config']
Modifiability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:188,config,config,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,1,['config'],['config']
Modifiability,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:1383,refactor,refactored,1383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777,1,['refactor'],['refactored']
Modifiability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:1227,config,config,1227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,2,['config'],"['config', 'configs']"
Modifiability,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:657,plugin,plugin,657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:211,plugin,plugins,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,2,['plugin'],['plugins']
Modifiability,"> Can you point to a package whose test organization you would like our tests to emulate?. - pytest: https://github.com/pytest-dev/pytest/tree/main/testing; - loompy: https://github.com/linnarsson-lab/loompy/tree/master/tests. The others have their tests in the package, and just have that useless `__init__.py` in the tests directory because of either cargo culting it or becaue they know that makes setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:728,variab,variables,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,1,['variab'],['variables']
Modifiability,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:871,rewrite,rewrite,871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,2,['rewrite'],['rewrite']
Modifiability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1368,flexible,flexible,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,1,['flexible'],['flexible']
Modifiability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I can’t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! That’s a concrete thing I can do. I’ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:73,refactor,refactor,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,2,"['parameteriz', 'refactor']","['parameterization', 'refactor']"
Modifiability,"> I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. You can see some quick comparisons between Pynndescent and Annoy here: https://github.com/pavlin-policar/openTSNE/issues/101#issuecomment-597178379. But I have not investigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:581,flexible,flexible,581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833,1,['flexible'],['flexible']
Modifiability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:846,flexible,flexible,846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['flexible'],['flexible']
Modifiability,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:806,extend,extends,806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['extend'],['extends']
Modifiability,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:100,variab,variables,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,1,['variab'],['variables']
Modifiability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:110,refactor,refactor,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['refactor'],['refactor']
Modifiability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:264,variab,variables,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,2,"['refactor', 'variab']","['refactoring', 'variables']"
Modifiability,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:779,config,configs,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,3,['config'],"['config', 'configs']"
Modifiability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:629,config,configured,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['config'],['configured']
Modifiability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:312,variab,variable,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,1,['variab'],['variable']
Modifiability,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,adapt,adapt,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,1,['adapt'],['adapt']
Modifiability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:107,refactor,refactoring,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,4,"['maintainab', 'refactor']","['maintainable', 'refactor', 'refactoring']"
Modifiability,">=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11199,flexible,flexible,11199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['flexible'],['flexible']
Modifiability,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:361,plugin,plugins,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['plugin'],['plugins']
Modifiability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:23,refactor,refactoring,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,1,['refactor'],['refactoring']
Modifiability,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:295,flexible,flexible,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259,1,['flexible'],['flexible']
Modifiability,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1188,variab,variable,1188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240,4,['variab'],['variable']
Modifiability,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:1019,variab,variability,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,1,['variab'],['variability']
Modifiability,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730:227,extend,extend,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730,1,['extend'],['extend']
Modifiability,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:116,variab,variable,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154,1,['variab'],['variable']
Modifiability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:101,plugin,plugins,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['plugin'],['plugins']
Modifiability,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:512,adapt,adapting,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,1,['adapt'],['adapting']
Modifiability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:204,polymorphi,polymorphism,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,1,['polymorphi'],['polymorphism']
Modifiability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:832,variab,variable,832,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,1,['variab'],['variable']
Modifiability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:285,variab,variables,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,2,['variab'],['variables']
Modifiability,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:168,variab,variable,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735,1,['variab'],['variable']
Modifiability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:185,config,configuring,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,['config'],"['configurations', 'configuring']"
Modifiability,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:453,variab,variable,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301,1,['variab'],['variable']
Modifiability,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:17,variab,variable,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604,3,['variab'],"['variable', 'variables']"
Modifiability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:186,config,config,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,1,['config'],['config']
Modifiability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:127,variab,variable,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,2,['variab'],['variable']
Modifiability,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:10,layers,layers,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952,3,['layers'],['layers']
Modifiability,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673:234,inherit,inherits,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673,1,['inherit'],['inherits']
Modifiability,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:162,variab,variables,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844,1,['variab'],['variables']
Modifiability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:202,plugin,plugins,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,2,['plugin'],['plugins']
Modifiability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:397,maintainab,maintainability,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['maintainab'],['maintainability']
Modifiability,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:295,rewrite,rewrite,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109,2,['rewrite'],['rewrite']
Modifiability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:44,refactor,refactor,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['refactor'],['refactor']
Modifiability,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:642,adapt,adapted,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,1,['adapt'],['adapted']
Modifiability,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:471,flexible,flexible,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,2,['flexible'],['flexible']
Modifiability,"Hey Phil!. Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:193,variab,variable,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['variab'],['variable']
Modifiability,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:119,variab,variables,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347,1,['variab'],['variables']
Modifiability,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:113,polymorphi,polymorphic,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790,1,['polymorphi'],['polymorphic']
Modifiability,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:338,rewrite,rewrite,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211,1,['rewrite'],['rewrite']
Modifiability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:481,variab,variable,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,1,['variab'],['variable']
Modifiability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1638,variab,variable,1638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['variab'],['variable']
Modifiability,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:272,variab,variables,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889,1,['variab'],['variables']
Modifiability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1110,variab,variable,1110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,1,['variab'],['variable']
Modifiability,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,1,['variab'],['variable']
Modifiability,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,adapt,adapt,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['adapt'],['adapt']
Modifiability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:29,variab,variable,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,1,['variab'],['variable']
Modifiability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:436,config,config,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,2,"['config', 'layers']","['config', 'layers']"
Modifiability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:703,variab,variable,703,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['variab'],['variable']
Modifiability,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:166,variab,variable,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,1,['variab'],['variable']
Modifiability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:864,variab,variables,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['variab'],['variables']
Modifiability,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:224,flexible,flexible,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,2,['flexible'],['flexible']
Modifiability,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:506,variab,variable,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884,2,['variab'],['variable']
Modifiability,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:357,enhance,enhance,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,3,"['enhance', 'inherit']","['enhance', 'inherit']"
Modifiability,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:643,variab,variable,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054,1,['variab'],['variable']
Modifiability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,adapt,adapting,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['adapt'],['adapting']
Modifiability,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py; def _plot_legend(self, legend_ax, return_ax_dict, normalize): ; self._plot_colorbar(legend_ax, normalize) ; return_ax_dict['color_legend_ax'] = color_legend_ax; ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829:456,flexible,flexible,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829,1,['flexible'],['flexible']
Modifiability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,adapt,adapt,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['adapt'],['adapt']
Modifiability,"I think the problem is the option `sort_order` which is True by default for; numerical data. This changes the ordering of the dots and thus it messes; up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot; > with the following code; >; > import scanpy.api as sc; > import numpy as np; > sc.settings.figdir = ""testdir""; > sc.settings.file_format_figs = ""png""; > sc.logging.print_versions(); >; > With these libraries; > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4; > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Running the following code bit. I use some dummy variable for size.; >; > somedata = sc.datasets.paul15(); > sc.pp.pca(somedata); > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); > sc.tl.leiden(somedata, resolution=0.5, random_state=42); > z = np.abs(somedata.obsm['X_pca'][:,0])**1; > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); >; > I get the following two figure as output; > [image: umapcontinuous_expr]; > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>; > [image: umapgroup_value]; > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>; >; > I would expect to see a similar size allocation/distribution but they are; > very different. I Could not really find a cause for this looking at the; > scatter plot function so it might be somewhere deeper.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/478>, or ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152:795,variab,variable,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152,1,['variab'],['variable']
Modifiability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:24,enhance,enhancement,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['enhance'],['enhancement']
Modifiability,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:187,variab,variable,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,1,['variab'],['variable']
Modifiability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:109,variab,variable,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,7,['variab'],"['variable', 'variable-and-a-categorical-variab', 'variables']"
Modifiability,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:418,flexible,flexible,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['flexible'],['flexible']
Modifiability,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,adapt,adapt,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['adapt'],"['adapt', 'adaption']"
Modifiability,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,adapt,adapted,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['adapt'],['adapted']
Modifiability,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:98,variab,variables,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['variab'],['variables']
Modifiability,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:1080,flexible,flexible,1080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['flexible'],['flexible']
Modifiability,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:140,config,config,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453,1,['config'],['config']
Modifiability,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:366,extend,extending,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085,2,['extend'],"['extended', 'extending']"
Modifiability,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:1311,refactor,refactoring,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738,1,['refactor'],['refactoring']
Modifiability,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:350,variab,variables,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483,2,['variab'],['variables']
Modifiability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,adapt,adapt,379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,2,"['adapt', 'variab']","['adapt', 'variabale']"
Modifiability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:325,maintainab,maintainability,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,1,['maintainab'],['maintainability']
Modifiability,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:253,variab,variable,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637,1,['variab'],['variable']
Modifiability,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:159,config,configure,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,3,['config'],"['configure', 'configured']"
Modifiability,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:72,refactor,refactoring,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813,1,['refactor'],['refactoring']
Modifiability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:133,extend,extend,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"Sorry for the late response! This seems to have come just after I went through the issues last weekend...; ; It looks great! :smile:. Some small notes:; * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...); * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916:469,variab,variables,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916,3,"['extend', 'rewrite', 'variab']","['extend', 'rewrite', 'variables']"
Modifiability,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:415,adapt,adapting-to-real-world-data,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['adapt'],['adapting-to-real-world-data']
Modifiability,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:275,variab,variables,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:131,plugin,plugins,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:198,variab,variables,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,1,['variab'],['variables']
Modifiability,"Thanks for the PR! I've just renamed the variable to be a bit more clear. I do think this test could be a bit better (e.g. check that the structure of the object is correct), but also this is an improvement so LGTM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692,1,['variab'],['variable']
Modifiability,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['adapt'],['adapted']
Modifiability,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:277,variab,variables,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398,2,['variab'],['variables']
Modifiability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:296,variab,variable,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,1,['variab'],['variable']
Modifiability,"Thanks for your comment Jason!; Don't you think the sentence before the one you quoted:; ""If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""].""; in combination with the default being ""None"" would make this clear?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845:162,variab,variable,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845,1,['variab'],['variable']
Modifiability,That could also work. But this would require a bit of a rewrite. I think the current solution is simpler and also really fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797:56,rewrite,rewrite,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797,1,['rewrite'],['rewrite']
Modifiability,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:423,variab,variability,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639,1,['variab'],['variability']
Modifiability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,config,configuration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['config'],['configuration']
Modifiability,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:728,extend,extend,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596,1,['extend'],['extend']
Modifiability,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:166,plugin,plugins,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:40,config,configurable,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,1,['config'],['configurable']
Modifiability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:359,enhance,enhance,359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['enhance'],['enhance']
Modifiability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:305,extend,extending,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['extend'],['extending']
Modifiability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:2150,refactor,refactored,2150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['refactor'],['refactored']
Modifiability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1959,variab,variable,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['variab'],['variable']
Modifiability,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1951,parameteriz,parameterized,1951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,1,['parameteriz'],['parameterized']
Modifiability,"ing, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variabl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1228,variab,variables,1228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['variab'],['variables']
Modifiability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9724,config,config,9724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['config'],['config']
Modifiability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:2167,config,configure,2167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['config'],['configure']
Modifiability,"manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2206,config,config,2206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,1,['config'],['config']
Modifiability,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1271,refactor,refactor,1271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,1,['refactor'],['refactor']
Modifiability,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4212,config,config,4212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['config'],['config']
Modifiability,"nDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1890,layers,layers,1890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['layers'],['layers']
Modifiability,"ng: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1756,variab,variable,1756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['variab'],['variable']
Modifiability,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2115,layers,layers,2115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,2,['layers'],['layers']
Modifiability,"ould I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious to know. Ultimately, the decision is up to you guys, since this is more of a desi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2888,refactor,refactoring,2888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['refactor'],['refactoring']
Modifiability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1159,enhance,enhanced,1159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['enhance'],['enhanced']
Modifiability,"py log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1457,config,configured,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['config'],['configured']
Modifiability,"s this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1075,variab,variable,1075,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['variab'],['variable']
Modifiability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1772,config,configs,1772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,3,['config'],"['configs', 'configuration']"
Modifiability,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1230,variab,variables,1230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119,2,['variab'],['variables']
Modifiability,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:989,variab,variable,989,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['variab'],['variable']
Modifiability,"that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```; ⡠⠄⠀⠨⡯⢀⠀⠐⢡⠀⠠⢀⠠⢂⠀⠐⠀⠐⠐⣢⠀⢂⠀⠈⠒⣂⠂⠀; ⠆⢌⠁⡁⠈⠀⡀⠖⠂⠀⠁⠂⠀⠉⠐⡀⠀⠀⠈⠠⠄⠉⢀⡀⠀⠀⠀⠂; ⠑⠀⠠⠀⠃⠀⠀⠅⢀⠠⠄⡀⠅⠂⢀⠪⠀⠦⢀⠀⢃⠈⢀⠌⠚⠀⠀⠃; ⠁⠂⡃⠈⠀⢀⠀⠙⢀⠥⠀⠀⠄⡁⠀⠠⠈⠀⠈⠃⠂⠠⣀⠀⠈⣁⠁⠆; ⡀⠐⠐⠠⠀⠐⢐⡄⣂⠀⠀⠘⠀⠀⠀⠠⠂⠀⡀⠨⠁⠀⠀⠀⠁⠁⠣⠤; ⠀⡐⢀⢢⠀⠁⠔⠀⠁⠀⠃⠀⢀⢀⠐⠃⠄⠀⡇⠊⠄⠀⡈⢀⠀⠀⣀⠆; ⠀⢐⣤⡄⠠⠂⠃⡈⠘⠀⠀⠀⡂⠰⢄⠊⡂⠀⠐⠂⠀⠄⠀⠀⢱⠩⠈⢀; ⢁⠀⠑⠚⠁⠂⠂⠐⠁⠀⠀⢀⠠⠀⠐⠈⠈⡨⠀⠂⠀⡈⠈⠁⡐⣀⢁⠂; ⠀⠀⠀⠁⠀⠠⠅⠁⡠⠇⢐⠀⠀⠖⢉⣀⠀⢀⠀⠠⡀⠀⡀⢰⠁⠂⢉⠂; ⠀⠀⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:2007,variab,variables,2007,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['variab'],['variables']
Modifiability,"tion with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3864,rewrite,rewrite,3864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['rewrite'],['rewrite']
Modifiability,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:1968,inherit,inherits,1968,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,1,['inherit'],['inherits']
Modifiability,"ython3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10953,flexible,flexible,10953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['flexible'],['flexible']
Modifiability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1766,variab,variables,1766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,['variab'],['variables']
Modifiability,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4201,layers,layers,4201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,3,"['layers', 'variab']","['layers', 'variables']"
Performance," (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2084,cache,cached,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance," File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/python3.12/site-packages/dask/threaded.py"", line 90 in get; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 662 in compute; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 376 in compute; File ""~/Dev/scanpy/tests/test_utils.py"", line 243 in test_is_constant_dask; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 159 in pytest_pyfunc_call; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 1627 in runtest; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 17",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:4389,queue,queue,4389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['queue'],['queue']
Performance," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2003,cache,cached,2003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance," parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneTy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3198,optimiz,optimization,3198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance, py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5795,cache,cached-property,5795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['cache'],['cached-property']
Performance," the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1323,perform,perform,1323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['perform'],['perform']
Performance,"(= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3-5),; libarchive-zip-perl (= 1.68-1),; libargon2-1 (= 0~20171227-0.2),; libarpack2 (= 3.8.0-1),; libasan6 (= 11.2.0-10),; libatk-bridge2.0-0 (= 2.38.0-2),; libatk1.0-0 (= 2.36.0-2),; libatk1.0-data (= 2.36.0-2),; libatlas3-base (= 3.10.3-11),; libatomic1 (= 11.2.0-10),; libatspi2.0-0 (= 2.42.0-2),; libattr1 (= 1:2.5.1-1),; libaudit-common (= 1:3.0.6-1),; libaudit1 (= 1:3.0.6-1),; libavahi-client3 (= 0.8-5),; libavahi-common-data (= 0.8-5),; libavahi-common3 (= 0.8-5),; libbinutils (= 2.37-8),; libblas3 (= 3.10.0-1),; libblkid1 (= 2.37.2-4),; libblosc1 (= 1.21.1+ds1-1),; libbr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2863,cache,cache,2863,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['cache'],['cache']
Performance,"(most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1345,cache,cached-property,1345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['cache'],['cached-property']
Performance,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:164,cache,cached,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,10,['cache'],['cached']
Performance,*; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.w,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1122,cache,cached,1122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4197,cache,cached,4197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1749,cache,cached,1749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (7.1.1); Requirement already satisfied: networkx>=2.3 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.6.3); Requirement already satisfied: importlib-metadata>=0.7 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.8.2); Requirement already satisfied: joblib in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.1.0); Requirement already satisfied: sinfo in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.3.4); Requirement already satisfied: patsy in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.2); Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Requirement already satisfied: six in c:\users\charles\anaconda3\lib\site-packages (from h5py>=2.10.0->scanpy) (1.16.0); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\charles\anaconda3\lib\site-packages (from importlib-metadata>=0.7->scanpy) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from importlib-metadata>=0.7->scanpy) (3.7.0); Requirement already satisfied: cycler>=0.10 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (0.11.0); Requirement already satisfied: pyparsing>=2.2.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (3.0.4); Requirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:2644,cache,cached,2644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,".1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2306,cache,cached-property,2306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['cache'],"['cached', 'cached-property']"
Performance,"0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 3: As you recommend, I do `!pip uninstall tables` and `conda install -c conda-forge pytables`, then; ```python; import tables # pass. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_15024/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 19 from numpy import random; 20 from scipy import sparse; ---> 21 from anndata import AnnData, __version__ as anndata_version; 22 from textwrap import dedent; 23 f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:3476,load,load,3476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1379,cache,cached,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1846,cache,cached,1846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2939,cache,cached,2939,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from import,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1528,cache,cached,1528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9295,cache,cached-property,9295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cached-property']
Performance,"; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(reticulate); repl_python(); ```. ```; import pandas as pd; import scanpy as sc; ```. ```; ModuleNotFoundError: No module named 'scanpy'; ```. Check the `PATH`:. ```; import sys; sys.path; ```. ```; ['', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/bin', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python36.zip', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/lib-dynload', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages', '/home/tsundoku/R/x86_64-pc-linux-gnu-library/3.6/reticulate/python']; ```. Okay so `scanpy` is installed but the `PATH` are different. Not sure why `py_install()` doesn't work. I guess the alternative is including the those paths for reticulate but not sure at the moment how to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:14523,load,load,14523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['load'],['load']
Performance,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:125,cache,cached-property,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,1,['cache'],['cached-property']
Performance,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:109,cache,cached,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,2,['cache'],"['cache', 'cached']"
Performance,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:68,cache,cache,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,4,['cache'],"['cache', 'cached']"
Performance,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:719,load,loaded,719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,1,['load'],['loaded']
Performance,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:671,optimiz,optimize,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,3,"['optimiz', 'scalab']","['optimization', 'optimize', 'scalability']"
Performance,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:561,load,load,561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,1,['load'],['load']
Performance,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:970,perform,performance,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['perform'],['performance']
Performance,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:305,perform,perform,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,1,['perform'],['perform']
Performance,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:370,perform,performs,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924,1,['perform'],['performs']
Performance,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1456,perform,performance,1456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['perform'],['performance']
Performance,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:332,load,loading,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,2,['load'],['loading']
Performance,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:87,perform,performance,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,4,['perform'],"['performance', 'performing']"
Performance,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:29,cache,cache,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,6,['cache'],"['cache', 'cached']"
Performance,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:431,cache,cached,431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,['cache'],"['cache', 'cached']"
Performance,"> To be able to reproduce and help, it is a big aid for us if you can supply a code sample that we can run: that is, with some dummy data (the datasets scanpy readily supplies are great for that), and the error/unexpected behaviour you get. Can you show such an example, with data? It is not immediately clear to me what specific you are trying to add or construct; I'm not sure whether basically the dataframe gets destroyed by the operation you intend to perform, or whether it is the violin plot failing (if the dataframe is crooked, it would be this to be fixed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546:457,perform,perform,457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546,1,['perform'],['perform']
Performance,"> absolute numbers of cells expressing a gene is similar between clusters as a use case. I am aware that this is a bit of a niche problem, and I am not particularly happy with domino plots as a solution either. I have no better vehicle to discuss this than opening an issue :( Hopefully this inspires the next person who deals with this problem. As to the question, maybe sticking to this example will help me explain:. I am looking at the expression of Hb9/Mnx in my whole-body dataset. I notice from the feature scatter that it seems to be somewhat expressed in clusters 0, 2, and 18. Wanting to be sure, I look at the dotplot. The dotplot tells me that there is a greater proportion of cells in cluster 18 that express it, compared to 0 and 2. The dotplot might make me believe that Hb9 is a marker for cluster 18, and if I do an in-situ hybridisation, these are the cells I would be staining. However, the truth is that the vast majority of cells that express Hb9 are actually in clusters 0 and 2, different cell types than 18. The number of cells in each cluster correlates with the number of cells in the organism, so if I performed the in-situ I would get lots of cells that I could mistakenly all identify as cluster 18. Does this make more sense?. EDIT: I am not advocating for domino plots to be part of ScanPy. I am simply trying to start a discussion, and trying to see if there was an easy fix that I missed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889:1129,perform,performed,1129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889,1,['perform'],['performed']
Performance,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:602,perform,perform,602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161,2,['perform'],['perform']
Performance,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:731,perform,performant,731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215,1,['perform'],['performant']
Performance,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:216,perform,perform,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,3,['perform'],"['perform', 'performs']"
Performance,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:338,perform,performs,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['perform'],['performs']
Performance,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:1812,perform,performance,1812,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,1,['perform'],['performance']
Performance,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:130,optimiz,optimization,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:1302,cache,cache,1302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,1,['cache'],['cache']
Performance,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417:46,cache,cache,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417,1,['cache'],['cache']
Performance,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:312,load,load,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['load'],['load']
Performance,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:608,perform,performance,608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:883,perform,performance,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,1,['perform'],['performance']
Performance,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:315,perform,performance,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732,1,['perform'],['performance']
Performance,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:393,perform,performed,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:656,optimiz,optimize,656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,3,"['optimiz', 'scalab']","['optimization', 'optimize', 'scalability']"
Performance,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:419,cache,cache,419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467,1,['cache'],['cache']
Performance,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:164,perform,performance,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216,1,['perform'],['performance']
Performance,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:577,scalab,scalability,577,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,1,['scalab'],['scalability']
Performance,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:542,optimiz,optimized,542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,1,['optimiz'],['optimized']
Performance,"Hi James!. Thank you for the remark! And you're right... several repetitions of the following are consistent:; ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. Thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:857,bottleneck,bottleneck,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['bottleneck'],['bottleneck']
Performance,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:969,load,load,969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['load'],['load']
Performance,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:307,perform,performed,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,2,['perform'],['performed']
Performance,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:475,load,load,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,2,"['load', 'perform']","['load', 'perform']"
Performance,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:286,perform,performance,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542,1,['perform'],['performance']
Performance,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:728,concurren,concurrently,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrently']
Performance,"I created a new environment (see below for package details) and there everything works as it should. . <Details>; <summary>Versions in the new working environment</summary>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.6; anyio NA; argon2 20.1.0; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.05.0; dateutil 2.8.1; decorator 5.0.9; fsspec 2021.05.0; get_version 2.1; h5py 3.2.1; idna 2.10; igraph 0.9.1; ipykernel 5.5.5; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.8.0; jupyterlab_server 2.5.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:323,bottleneck,bottleneck,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['bottleneck'],['bottleneck']
Performance,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:144,load,loading,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,2,['load'],"['load', 'loading']"
Performance,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:268,optimiz,optimization,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342,1,['optimiz'],['optimization']
Performance,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>; <summary> Setup (loading, simulating, and projecting) </summary>. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from scipy import sparse; from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):; adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""); sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True); sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000); sc.pp.log1p(adata); return adata. def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt. def simulate_doublets(adata, frac=.5):; """"""Simulate doublets from count data.; ; Params; ------; adata; The anndata object to sample from. Must have count data.; frac; Fraction of total cells to simulate.; """"""; m, n = adata.X.shape; n_doublets = int(np.round(m * frac)); pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))); combos = np.random.randint(0, m, (n_doublets * 2)); pos = sparse.csr_matrix(; (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), ; shape=(n_doublets, m);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:670,load,loading,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['load'],['loading']
Performance,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:33,bottleneck,bottleneck,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,5,"['bottleneck', 'perform']","['bottleneck', 'bottlenecks', 'perform', 'performance']"
Performance,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699:150,perform,perform,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699,1,['perform'],['perform']
Performance,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:167,load,loadings,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,2,['load'],['loadings']
Performance,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue?. This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914:244,load,loading,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914,1,['load'],['loading']
Performance,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:21,load,loading,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782,2,['load'],['loading']
Performance,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:1056,load,loads,1056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['load'],['loads']
Performance,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:689,perform,performs,689,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['perform'],['performs']
Performance,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:338,load,load,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096,1,['load'],['load']
Performance,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:330,perform,perform,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['perform'],['perform']
Performance,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:1270,cache,cache-dir,1270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497,1,['cache'],['cache-dir']
Performance,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:312,cache,cache,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,1,['cache'],['cache']
Performance,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:350,perform,performs,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807,1,['perform'],['performs']
Performance,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:468,optimiz,optimized,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698,1,['optimiz'],['optimized']
Performance,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:149,perform,performance,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['perform'],['performance']
Performance,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:916,cache,cache,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,1,['cache'],['cache']
Performance,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:219,load,loaded,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672,5,"['cache', 'load']","['cache', 'load', 'loaded']"
Performance,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:307,optimiz,optimization,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688,5,['optimiz'],"['optimization', 'optimized', 'optimizing']"
Performance,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:173,cache,cache,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,5,"['cache', 'load']","['cache', 'cachedir', 'loaded']"
Performance,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:152,load,load,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361,1,['load'],['load']
Performance,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:478,perform,performs,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['perform'],['performs']
Performance,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:383,load,loaded,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,1,['load'],['loaded']
Performance,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:696,load,loading,696,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,1,['load'],['loading']
Performance,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:103,load,load,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609,1,['load'],['load']
Performance,"What about comparing communities between for example CPM and RBERVertexPartition at the same resolution, using modularity score? This way we are not comparing scores obtained by optimization functions, just simple ""external"" measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127:178,optimiz,optimization,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127,1,['optimiz'],['optimization']
Performance,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:693,bottleneck,bottleneck,693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,1,['bottleneck'],['bottleneck']
Performance,"Yes , the sampling is done with weights and I used the coreset technique; for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>; wrote:. > I understand the benefits of sampling regarding computational speed up.; > What I'm not clear on is how you choose your weights for the calculations; > you perform here. You mentioned that you get wrong marker gene results when; > you sample and don't use weights. That makes sense if you get a; > non-representative set of cells in your sample. I wonder how you select the; > weights to fix this. I guess you don't just try a lot of different values; > until one works, right?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494:322,perform,perform,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494,1,['perform'],['perform']
Performance,"Yes, we already have a good mask for sparse scaling. Boolean arrays are very effective for indicating where computations should be performed, as they eliminate the need for copying and reintegration. One clear example is the `tl.score_genes` function. masks there as booleans for the nanmean is a lot more efficent but less pythonic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711:131,perform,performed,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711,1,['perform'],['performed']
Performance,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:401,tune,tune,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['tune'],['tune']
Performance,"\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:2349,load,load,2349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:874,perform,performance,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,1,['perform'],['performance']
Performance,"ading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3585,concurren,concurrent,3585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"aging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ---->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5470,load,load,5470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1613,load,load,1613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['load'],['load']
Performance,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2046,load,loading,2046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475,1,['load'],['loading']
Performance,"ateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4029,cache,cached,4029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4368,cache,cached,4368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:735,optimiz,optimized,735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['optimiz'],['optimized']
Performance,"canpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4535,cache,cached,4535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a lon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1423,cache,cache,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['cache'],['cache']
Performance,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4818,scalab,scalable,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['scalab'],['scalable']
Performance,da-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 py_0 conda-forge; dbus 1.13.6 h5008d03_3 conda-forge; debugpy 1.6.7 py310heca2aa9_0 conda-forge; decorator 5.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9878,cache,cached-property,9878,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cached-property']
Performance,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2420,perform,performance,2420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"eading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3510,concurren,concurrent,3510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"earn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:2638,perform,perform,2638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,1,['perform'],['perform']
Performance,ecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.wh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1213,cache,cached,1213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4865,cache,cached-property,4865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['cache'],"['cached-property', 'cached-property-']"
Performance,"equirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.3.2); Requirement already satisfied: fonttools>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4377,cache,cached,4377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,"et things a bit into order here, as at the moment some wrong impressions are around I think:. **1. Incorrect comparisons done here**; To my current knowledge,; - `sc.pp.highly_variable_genes(…, flavor=“seurat”)` mimics `FindVariableFeatures(…, method=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:1106,perform,perform,1106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['perform'],['perform']
Performance,"fied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4451,cache,cached,4451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"g that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1190,perform,performance,1190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['perform'],['performance']
Performance,"h-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3190,cache,cached,3190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Curr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1640,optimiz,optimization,1640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['optimiz'],['optimization']
Performance,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1015,load,loaders,1015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['load'],['loaders']
Performance,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:570,tune,tune,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['tune'],['tune']
Performance,"ib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4117,cache,cached,4117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2378,concurren,concurrent,2378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,ing tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecti,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1300,cache,cached,1300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2575,optimiz,optimization,2575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance,"ixbuf2.0-common (= 2.42.6+dfsg-2),; libgfortran5 (= 11.2.0-10),; libgirepository-1.0-1 (= 1.70.0-2),; libglib2.0-0 (= 2.70.1-1),; libglpk40 (= 5.0-1),; libgmp10 (= 2:6.2.1+dfsg-2),; libgnutls30 (= 3.7.2-2),; libgomp1 (= 11.2.0-10),; libgpg-error0 (= 1.42-3),; libgraphite2-3 (= 1.3.14-1),; libgssapi-krb5-2 (= 1.18.3-7),; libgtk-3-0 (= 3.24.30-3),; libgtk-3-common (= 3.24.30-3),; libharfbuzz0b (= 2.7.4-1),; libhdf5-103-1 (= 1.10.7+repack-4),; libhdf5-hl-100 (= 1.10.7+repack-4),; libheif1 (= 1.12.0-2+b3),; libhogweed6 (= 3.7.3-1),; libicu67 (= 67.1-7),; libidn2-0 (= 2.3.2-2),; libigraph1 (= 0.8.5+ds1-1),; libimagequant0 (= 2.12.2-1.1),; libip4tc2 (= 1.8.7-1),; libisl23 (= 0.24-2),; libitm1 (= 11.2.0-10),; libjbig0 (= 2.1-3.1+b2),; libjpeg62-turbo (= 1:2.1.1-1),; libjs-jquery (= 3.5.1+dfsg+~3.5.5-8),; libjs-jquery-hotkeys (= 0~20130707+git2d51e3a9+dfsg-2.1),; libjs-jquery-isonscreen (= 1.2.0-1.1),; libjs-jquery-metadata (= 12-3),; libjs-jquery-tablesorter (= 1:2.31.3+dfsg1-2),; libjs-jquery-throttle-debounce (= 1.1+dfsg.1-1.1),; libjs-jquery-ui (= 1.13.0+dfsg-1),; libjs-sphinxdoc (= 4.2.0-5),; libjs-underscore (= 1.9.1~dfsg-4),; libjson-c5 (= 0.15-2),; libk5crypto3 (= 1.18.3-7),; libkeyutils1 (= 1.6.1-2),; libkmod2 (= 29-1),; libkrb5-3 (= 1.18.3-7),; libkrb5support0 (= 1.18.3-7),; liblapack3 (= 3.10.0-1),; liblbfgsb0 (= 3.0+dfsg.3-9),; liblcms2-2 (= 2.12~rc1-2),; libldap-2.4-2 (= 2.4.59+dfsg-1),; libllvm11 (= 1:11.1.0-4),; liblqr-1-0 (= 0.4.2-2.1),; liblsan0 (= 11.2.0-10),; libltdl7 (= 2.4.6-15),; liblz4-1 (= 1.9.3-2),; liblzf1 (= 3.6-3),; liblzma5 (= 5.2.5-2),; liblzo2-2 (= 2.10-2),; libmagic-mgc (= 1:5.39-3),; libmagic1 (= 1:5.39-3),; libmagickcore-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmagickwand-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmd0 (= 1.0.4-1),; libmount1 (= 2.37.2-4),; libmpc3 (= 1.2.1-1),; libmpdec3 (= 2.5.1-2),; libmpfr6 (= 4.1.0-3),; libncurses6 (= 6.2+20210905-1),; libncursesw6 (= 6.2+20210905-1),; libnettle8 (= 3.7.3-1),; libnghttp2-14 (= 1.43.0-1),; libnsl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:6253,throttle,throttle-debounce,6253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['throttle'],['throttle-debounce']
Performance,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:1744,load,loading,1744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297,1,['load'],['loading']
Performance,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4735,cache,cached,4735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1291,cache,cache,1291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,['cache'],['cache']
Performance,"md64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2246,cache,cached,2246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4726,load,load,4726,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['load'],['load']
Performance,"n3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3182,optimiz,optimization,3182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['optimiz'],['optimization']
Performance,n_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1455,cache,cached,1455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"n_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Coll",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2174,cache,cached,2174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ne-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1921,cache,cached,1921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ng-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2043,concurren,concurrent,2043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3361,cache,cached,3361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2901,optimiz,optimization,2901,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance,onda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 py_0 conda-forge; dbus 1.13.6 h5008d03_3 conda-forge; d,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9816,cache,cachecontrol-with-filecache,9816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cachecontrol-with-filecache']
Performance,"one-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3270,cache,cached,3270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"op:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2032,cache,cached-property,2032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['cache'],['cached-property']
Performance,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2549,perform,performance,2549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['perform'],['performance']
Performance,"ow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4278,cache,cached,4278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"p us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1469,load,loading,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['load'],['loading']
Performance,p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9769,cache,cachecontrol,9769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cachecontrol']
Performance,pyhd3eb1b0_0 ; anaconda custom py38_1 ; anaconda-client 1.7.2 py38_0 ; anaconda-project 0.10.0 pyhd3eb1b0_0 ; anndata 0.7.6 pypi_0 pypi; anyio 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5616,bottleneck,bottleneck,5616,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['bottleneck'],['bottleneck']
Performance,rgon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat 2.4.1 h2531618_2 ; fa2 0.3.5 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5824,cache,cachetools,5824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['cache'],['cachetools']
Performance,"s__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2458,cache,cachetools,2458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['cache'],['cachetools']
Performance,"self, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; Fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1968,concurren,concurrent,1968,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"spatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2435,cache,cached-property,2435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['cache'],['cached-property']
Performance,"ss CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:1212,cache,cache,1212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,4,['cache'],"['cache', 'cache-control', 'cached']"
Performance,"ssion"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1651,scalab,scalable,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['scalab'],['scalable']
Performance,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:244,cache,cache,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,9,['cache'],['cache']
Performance,"t I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1096,cache,cached,1096,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['cache'],['cached']
Performance,"t import NNDescent; [48](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=47) from pynndescent.distances import named_distances as pynn_named_distances. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py:39, in <module>; [25](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=24) else:; [26](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=25) return val; [29](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=28) @numba.njit(; [30](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=29) ""f4(f4[::1],f4[::1])"",; [31](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=30) fastmath=True,; [32](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=31) cache=True,; [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=32) locals={; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=33) ""result"": numba.types.float32,; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=34) ""diff"": numba.types.float32,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=35) ""dim"": numba.types.int32,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=36) },; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=37) ); ---> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:11618,cache,cache,11618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['cache'],['cache']
Performance,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1941,perform,performs,1941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['perform'],['performs']
Performance,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1045,load,loaders,1045,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['load'],['loaders']
Performance,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:2177,perform,performance,2177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,1,['perform'],['performance']
Performance,"users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3608,cache,cached,3608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Safety," line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3465,avoid,avoids,3465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['avoid'],['avoids']
Safety," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1527,detect,detection,1527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,2,['detect'],"['detecting', 'detection']"
Safety," we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1319,detect,detected,1319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['detect'],['detected']
Safety,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7602,redund,redundency,7602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['redund'],['redundency']
Safety,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). 👍 . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:329,avoid,avoid,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877,1,['avoid'],['avoid']
Safety,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:914,recover,recover,914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['recover'],['recover']
Safety,"> This is the standard way of writing a numpydoc returns section. […] This solution is dropping support for them. It certainly shouldn’t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I don’t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I don’t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons?. I’ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the “optional” is redundant. What would it even mean to have “a parameter that isn’t optional but has a default value”?. I’m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417:829,redund,redundant,829,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417,2,['redund'],['redundant']
Safety,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:1383,detect,detection,1383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,1,['detect'],['detection']
Safety,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:244,detect,detected,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,1,['detect'],['detected']
Safety,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:454,avoid,avoid,454,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,1,['avoid'],['avoid']
Safety,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:406,detect,detection,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['detect'],['detection']
Safety,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:451,avoid,avoid,451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,1,['avoid'],['avoid']
Safety,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:53,avoid,avoid,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293,1,['avoid'],['avoid']
Safety,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>; <summary> Basic PCA projection </summary>. ```python; def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings (just whether to center?) from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt; ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated.; * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up?; * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:; * Does the syntax still work for cases other than 1-to-1 transfer? ; * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`.; * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842:161,predict,prediction,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842,1,['predict'],['prediction']
Safety,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:599,safe,safely,599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,1,['safe'],['safely']
Safety,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:79,detect,detection,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845,1,['detect'],['detection']
Safety,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:858,abort,abort,858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,1,['abort'],['abort']
Safety,"Hey Isaac - thanks for the tip, I did not know about `add_totals`! Violin plots are, in principle, exactly what I would like to have, but they are very hard to read for genes with lots of zeros or for clusters with a lot of cells. I am still worried that this doesn't make the relative numbers clear. For instance, something like 40% of CD14+ monocytes express LDHB, while maybe 80% of dendritic cells do. This looks like it might be more ""relevant"", but in reality, the number of monocytes that express the gene is three times the number of total dendritic cells. Does this make sense?. I guess that in the end the onus is on me to avoid or highlight such ambiguities in the analysis, and remain cognisant of them while looking at the data. I will keep thinking about this and post here if I have an epiphany.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016453592:633,avoid,avoid,633,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016453592,1,['avoid'],['avoid']
Safety,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:783,predict,predict,783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347,1,['predict'],['predict']
Safety,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:891,avoid,avoid,891,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,1,['avoid'],['avoid']
Safety,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:991,detect,detection,991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404,3,['detect'],"['detect', 'detection']"
Safety,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:392,predict,prediction,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['predict'],['prediction']
Safety,"Hi David, . thanks for your reply and your interest!. > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or cust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:234,predict,prediction,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['predict'],['prediction']
Safety,"Hi all,. thanks for the nice discussion!. Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:134,detect,detection,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['detect'],['detection']
Safety,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:448,detect,detected,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350,1,['detect'],['detected']
Safety,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:19,detect,detected,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['detect'],['detected']
Safety,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555:6,recover,recover,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555,1,['recover'],['recover']
Safety,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-488610378:127,detect,detect,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-488610378,1,['detect'],['detect']
Safety,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:612,abort,abort,612,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,1,['abort'],['abort']
Safety,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>; <summary> Setup (loading, simulating, and projecting) </summary>. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from scipy import sparse; from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):; adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""); sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True); sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000); sc.pp.log1p(adata); return adata. def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt. def simulate_doublets(adata, frac=.5):; """"""Simulate doublets from count data.; ; Params; ------; adata; The anndata object to sample from. Must have count data.; frac; Fraction of total cells to simulate.; """"""; m, n = adata.X.shape; n_doublets = int(np.round(m * frac)); pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))); combos = np.random.randint(0, m, (n_doublets * 2)); pos = sparse.csr_matrix(; (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), ; shape=(n_doublets, m);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:131,detect,detection,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['detect'],['detection']
Safety,"It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of `sc.pl.umap(adata, color='clusters')` -> `sc.pl.umap(adata, 'clusters')`. About the changes that you suggest: I have concerns with breaking previous functionality and I wonder what is your position with respect to this. The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage from the very firsts versions of scanpy but maybe you have some good ideas for transitioning this from a string to a tuple. . The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. . For the plots being the product of `color` and `components`: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. . For your question about replacing `components` by `dimensions`. We need to be careful here because in many places the use of components is in the context of PCA as in `sc.pp.neighbors` with the parameter `n_pcs`. I think that the replacement of `components` by `dimensions` should only be done for the embedding functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392:471,avoid,avoid,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392,1,['avoid'],['avoid']
Safety,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:288,risk,risk,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,1,['risk'],['risk']
Safety,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:845,avoid,avoid,845,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['avoid'],['avoid']
Safety,"Regarding your other bug: `scanpy.plotting` used to have the attribute and `scanpy.api.plotting` would simply import the module. To make the [overview of the API](https://scanpy.readthedocs.io/en/latest/api.html) work, I had to introduce a [dummy module](https://github.com/theislab/scanpy/blob/master/scanpy/api/pl.py). In order to avoid duplication, I removed all exports from `scanpy.plotting.__init__`. I readded it to fix the bug on the development branch, but I need to think of a better solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094:333,avoid,avoid,333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094,1,['avoid'],['avoid']
Safety,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:866,detect,detected,866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518,1,['detect'],['detected']
Safety,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png); ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png); ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411:1664,detect,detected,1664,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411,1,['detect'],['detected']
Safety,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:484,safe,safeguard,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,1,['safe'],['safeguard']
Safety,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:1034,detect,detection,1034,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['detect'],['detection']
Safety,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:709,detect,detective,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808,1,['detect'],['detective']
Safety,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:632,redund,redundancy,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,1,['redund'],['redundancy']
Safety,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:1288,detect,detected,1288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,1,['detect'],['detected']
Safety,"The problem is that it does not install at all. ; When I run; ```; conda create -n test; conda activate test; conda install python=3.11; conda install -c conda-forge scanpy; ```; I get an error output for the last line, which is:; ```; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:336,abort,abort,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['abort'],['abort']
Safety,"To answer the following question:. > 2. what you said: The original approach samples from the full list of genes in each bin, then restricts the sample to valid ones. Your approach samples from the valid genes in each bin.; >; > So if a bin e.g. contains mostly invalid genes, the original code adds only a few genes for that bin, while yours adds the maximum possible number.; >; > So the questions is: is the sampling bias introduced in the original code wanted? If not, you not only made the code more resilient, but also more objective. After going through the original [code from Seurat](https://github.com/satijalab/seurat/blob/c54e57d3423b3f711ccd463e14965cc8de86c31b/R/utilities.R#L280C3-L303), it seems to me that there's not equivalent to removing genes to be scored from the control gene set.; From what I can tell, if one of the genes to be scored happens to be chosen as the background, it will be included in the calculation.; But please correct me if that's not the case. So if the original implementation does not remove score genes from the control gene set, we would simply need to remove the following line: https://github.com/scverse/scanpy/blob/ec4457470618efd85da3c7b29f951cab01a49e3a/scanpy/tools/_score_genes.py#L169. (Note: if we want to keep the current behaviour, we should still remove the line above, since it would be redundant)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358:1348,redund,redundant,1348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358,1,['redund'],['redundant']
Safety,"We should make dynamic 3D plots ;-) . If I remember correctly, in the past we have the issue that the categorical colors were given by the adata.obs order and we change them such that they follow the order of the categories. Yet, I agree that a good mix of categorical colors is good sometimes. To address this issue I think that we can simply randomize the order if `sort_order=False` to avoid adding any new parameters. . Isaac's solution looks great for dealing with of lots of cells, something that I imagine will become more frequent. I think we should have a 'cookbook' where we can keep this and other information. I find this better than adding more and more functionality to the scatter plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279:389,avoid,avoid,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279,1,['avoid'],['avoid']
Safety,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:914,avoid,avoid,914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['avoid'],['avoid']
Safety,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:77,redund,redundancy,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['redund'],['redundancy']
Safety,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2892,avoid,avoid,2892,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['avoid'],['avoid']
Safety,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2327,redund,redundant,2327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['redund'],['redundant']
Safety,"ge interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```; conda create -n scanpy_test1 python; pip install scanpy leidenalg scvi-tools; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118; ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; absl-py 1.4.0 pypi_0 pypi; adjusttext 0.8 pypi_0 pypi; aiohttp 3.8.5 pypi_0 pypi; aiosignal 1.3.1 pypi_0 pypi; airr 1.4.1 pypi_0 pypi; anndata 0.9.1 pypi_0 pypi; anyio 3.7.1 pypi_0 pypi; arrow 1.2.3 pypi_0 pypi; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; async-timeout 4.0.2 pypi_0 pypi; attrs 23.1.0 pypi_0 pypi; awkward 2.3.1 pypi_0 pypi; awkward-cpp 21 pypi_0 pypi; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backoff 2.2.1 pypi_0 pypi; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pypi_0 pypi; blessed 1.20.0 pypi_0 pypi; brotli-python 1.0.9 py311ha362b79_9 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2022.12.7 pypi_0 pypi; charset-normalizer 2.1.1 pypi_0 pypi; chex 0.1.7 pypi_0 pypi; click 8.1.6 pypi_0 pypi; cmake 3.25.0 pypi_0 pypi; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pypi_0 pypi; contourpy 1.1.0 pypi_0 pypi; croniter 1.4.1 pypi_0 pypi; cycler 0.11.0 pypi_0 pypi; dateutils 0.6.12 pypi_0 pypi; debugpy 1.6.7 py311hcafe171_0 conda-forge; decorator 5.1.1 pyhd8ed1ab_0 conda-forge; deepdiff 6.3.1 pypi_0 pypi; dm-tree 0.1.8 pypi_0 pypi; docr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:1447,timeout,timeout,1447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['timeout'],['timeout']
Safety,h96ca727_0; - r/linux-64::r-recommended==3.6.0=r36_0; - r/linux-64::r-rpart==4.1_15=r36h96ca727_0; - r/linux-64::r-spatial==7.3_11=r36h96ca727_4; - r/linux-64::r-survival==2.44_1.1=r36h96ca727_0; ```. </Details>. <Details>; <Summary>Package versions in old environment</Summary>. ```# packages in environment at /home/karl/anaconda3/envs/scanpy1_7:; #; # Name Version Build Channel; _anaconda_depends 2020.07 py38_0 ; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 1_gnu conda-forge; _r-mutex 1.0.0 anacondar_1 ; adjusttext 0.7.3.1 py_1 conda-forge; aiohttp 3.7.4.post0 pypi_0 pypi; aiohttp-cors 0.7.0 pypi_0 pypi; aioredis 1.3.1 pypi_0 pypi; alabaster 0.7.12 pyhd3eb1b0_0 ; anaconda custom py38_1 ; anaconda-client 1.7.2 py38_0 ; anaconda-project 0.10.0 pyhd3eb1b0_0 ; anndata 0.7.6 pypi_0 pypi; anyio 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:4951,timeout,timeout,4951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['timeout'],['timeout']
Safety,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2333,predict,predict,2333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['predict'],['predict']
Safety,"raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2286,timeout,timeout,2286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['timeout'],['timeout']
Safety,"reate a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1026,detect,detection,1026,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,1,['detect'],['detection']
Safety,"st debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:1024,avoid,avoid,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,1,['avoid'],['avoid']
Safety,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:1591,safe,safely,1591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,1,['safe'],['safely']
Safety,"tools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11343,abort,abort,11343,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['abort'],['abort']
Security, 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5759,certificate,certificates,5759,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['certificate'],['certificates']
Security," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2211,access,accessible,2211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,1,['access'],['accessible']
Security,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2178,inject,injecting,2178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,2,"['inject', 'validat']","['injecting', 'validated']"
Security,",0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tool",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1412,access,access,1412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,1,['access'],['access']
Security,". The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:1475,certificate,certificates,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['certificate'],['certificates']
Security,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487343093:205,access,access,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487343093,1,['access'],['access']
Security,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553:548,access,access,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553,1,['access'],['access']
Security,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:479,access,access,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,1,['access'],['access']
Security,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:189,validat,validated,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['validat'],['validated']
Security,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:50,access,access,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026,2,['access'],"['access', 'accessible']"
Security,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:525,access,accessible,525,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,1,['access'],['accessible']
Security,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:34,expose,exposed,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013,1,['expose'],['exposed']
Security,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730:152,expose,expose,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730,1,['expose'],['expose']
Security,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`?. Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437:200,access,access,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437,1,['access'],['access']
Security,"@phamidko I've modified your comment, and placed the output in the collapsed section below so I don't have to scroll so much on this issue. Unfortunately replies sent from email don't support markdown :(. <details>; <summary> phamidko's environment </summary>. ```; # This file may be used to create an environment using:; # $ conda create --name <env> --file <this file>; # platform: linux-64; _libgcc_mutex=0.1=main; anndata=0.7.1=pypi_0; attrs=19.3.0=py_0; backcall=0.1.0=py_0; bleach=3.1.4=pyh9f0ad1d_0; brotlipy=0.7.0=py37h8f50634_1000; ca-certificates=2020.4.5.1=hecc5488_0; cairo=1.16.0=hcf35c78_1003; certifi=2020.4.5.1=py37hc8dfbb8_0; cffi=1.14.0=py37hd463f26_0; chardet=3.0.4=py37hc8dfbb8_1006; cryptography=2.9.2=py37hb09aad4_0; cycler=0.10.0=pypi_0; decorator=4.4.2=py_0; defusedxml=0.6.0=py_0; entrypoints=0.3=py37hc8dfbb8_1001; fontconfig=2.13.1=h86ecdb6_1001; freetype=2.10.1=he06d7ca_0; get-version=2.1=pypi_0; gettext=0.19.8.1=hc5be6a0_1002; glib=2.64.2=h6f030ca_0; gmp=6.2.0=he1b5a44_2; h5py=2.10.0=pypi_0; icu=64.2=he1b5a44_1; idna=2.9=py_1; importlib-metadata=1.6.0=py37hc8dfbb8_0; importlib_metadata=1.6.0=0; ipykernel=5.2.1=py37h43977f1_0; ipython=7.13.0=py37hc8dfbb8_2; ipython_genutils=0.2.0=py_1; jedi=0.17.0=py37hc8dfbb8_0; jinja2=2.11.2=pyh9f0ad1d_0; joblib=0.14.1=pypi_0; json5=0.9.0=py_0; jsonschema=3.2.0=py37hc8dfbb8_1; jupyter_client=6.1.3=py_0; jupyter_contrib_core=0.3.3=py_2; jupyter_contrib_nbextensions=0.5.1=py37_0; jupyter_core=4.6.3=py37hc8dfbb8_1; jupyter_highlight_selected_word=0.2.0=py37_1000; jupyter_latex_envs=1.4.6=py37_1000; jupyter_nbextensions_configurator=0.4.1=py37_0; jupyterlab=2.1.1=py_0; jupyterlab_server=1.1.1=py_0; kiwisolver=1.2.0=pypi_0; ld_impl_linux-64=2.33.1=h53a641e_7; legacy-api-wrap=1.2=pypi_0; leidenalg=0.8.0=py37h43df1e8_0; libedit=3.1.20181209=hc058e9b_0; libffi=3.2.1=hd88cf55_4; libgcc-ng=9.1.0=hdf63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:545,certificate,certificates,545,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['certificate'],['certificates']
Security,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:243,access,access,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,1,['access'],['access']
Security,"Do you guys think this PR makes sense or is it too much to add R packages to the travis setup? @ivirshup @flying-sheep . There are bunch of useful R packages out there that will most likely not be reimplemented in Python (limma-voom pseudobulk DE, Liger, MAST etc.). I think it'd be cool to revive rtools and add access to such packages. I'm not sure if this is the right way but, any guidance is appreciated :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854:313,access,access,313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854,1,['access'],['access']
Security,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:431,attack,attack,431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,1,['attack'],['attack']
Security,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:412,access,accessible,412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,2,['access'],['accessible']
Security,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:77,expose,exposed,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,1,['expose'],['exposed']
Security,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:103,access,access,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,1,['access'],['access']
Security,"Hi Joshua, can you upload an example dataset somewhere? So that I can reproduce the figure above? I'm confident that I can speed this up...; PS: Still consolidating all the gene correlation stuff... Everything works, but we do not want to expose things to the user that have not been checked 3 times... in particular the conventions need to be intuitive etc.; PPS: The new cell cycle example could interest you:; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes_cell_cycle.html; Both link to the notebook in the ""Examples"" section; https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899:239,expose,expose,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899,1,['expose'],['expose']
Security,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:392,hash,hashem,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821,1,['hash'],['hashem']
Security,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:177,expose,expose,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['expose'],['expose']
Security,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:628,access,access,628,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,3,['access'],"['access', 'accessed']"
Security,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:353,access,access,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['access'],['access']
Security,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825:213,secur,security,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825,1,['secur'],['security']
Security,"Same! Seems like -maxiter gets set/clobbered to 1. I'm seeing it on one machine (which I have limited access too, its a galaxy installation using scanpy scripts) but not another (my local), both of which are apparently running scanpy 1.8.1. . Im wondering if there's a umap-learn version issue? In order to set the umap n_epochs(aka maxiter) default , it looks like older versions of umap-learn expected 0 (e.g. https://github.com/lmcinnes/umap/blob/0.5.0/umap/umap_.py), whereas the newer expect None. My working installation has umap-learn 0.5.2 (which seems to expect None), and I'm not sure about the one on the other server. Might be barking up the wrong tree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544:102,access,access,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544,1,['access'],['access']
Security,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708:277,access,access,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708,1,['access'],['access']
Security,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:582,access,access,582,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,2,['access'],['access']
Security,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:261,access,access,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,1,['access'],['access']
Security,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:280,access,accessor,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480,1,['access'],['accessor']
Security,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:133,access,accessible,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['access'],['accessible']
Security,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst; parameter : some.type; Description; ```. Later I’ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then I’ll check if there’s a section like. ```rst; one_identifier; Desc; second_identifier; Desc; ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value; 2. The above for returning a tuple; 3. “this function adds some AnnData.obs/var fields”. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``; 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```; 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484124854:100,access,access,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484124854,1,['access'],['access']
Security,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846:15,access,access,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846,1,['access'],['access']
Security,"aga_path(adata, nodes = [0, 5, 1], keys = ['IFNG', 'GZMB'], show = True)`. > TypeError: Image data of dtype object cannot be converted to float. What is the current solution to this issue?. Many thanks,; Lucy. ```; # Name Version Build Channel; alabaster 0.7.12 py_0 conda-forge; anndata 0.7.4 py38h32f6830_0 conda-forge; applaunchservices 0.2.1 py_0 conda-forge; appnope 0.1.0 py38h32f6830_1002 conda-forge; argh 0.26.2 py38_1001 conda-forge; astroid 2.4.2 py38h32f6830_1 conda-forge; async_generator 1.10 py_0 conda-forge; atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge; attrs 20.2.0 pyh9f0ad1d_0 conda-forge; autopep8 1.5.4 pyh9f0ad1d_0 conda-forge; babel 2.8.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 py_2 conda-forge; backports.functools_lru_cache 1.6.1 py_0 conda-forge; bleach 3.2.1 pyh9f0ad1d_0 conda-forge; blosc 1.20.1 hb1e8313_0 conda-forge; brotlipy 0.7.0 py38h94c058a_1001 conda-forge; bzip2 1.0.8 haf1e3a3_3 conda-forge; c-ares 1.16.1 haf1e3a3_3 conda-forge; ca-certificates 2020.10.14 0 ; cairo 1.16.0 h360c52f_1006 conda-forge; certifi 2020.6.20 py38h5347e94_2 conda-forge; cffi 1.14.3 py38h9edaa1b_1 conda-forge; chardet 3.0.4 py38h5347e94_1008 conda-forge; click 7.1.2 pyh9f0ad1d_0 conda-forge; cloudpickle 1.6.0 py_0 conda-forge; colorama 0.4.4 pyh9f0ad1d_0 conda-forge; cryptography 3.2 py38hf6767f5_0 conda-forge; cycler 0.10.0 py_2 conda-forge; dbus 1.13.18 h18a8e69_0 ; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge; docutils 0.16 py38h5347e94_2 conda-forge; entrypoints 0.3 py38h32f6830_1002 conda-forge; expat 2.2.10 hb1e8313_2 ; fa2 0.3.5 py38h4d0b108_0 conda-forge; flake8 3.8.4 py_0 conda-forge; fontconfig 2.13.1 h79c0d67_1002 conda-forge; freetype 2.10.4 ha233b18_0 conda-forge; future 0.18.2 py38h32f6830_2 conda-forge; get_version 2.1 py_1 conda-forge; gettext 0.19.8.1 haf92f58_1004 conda-forge; glib 2.66.2 hb1e8313_0 conda-forge; gmp 6.2.0 hb1e8313_3 conda-forge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:1076,certificate,certificates,1076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['certificate'],['certificates']
Security,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973:331,access,accessibility,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973,1,['access'],['accessibility']
Security,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:2350,access,accessible,2350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,1,['access'],['accessible']
Security,"earn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:3107,access,access,3107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,1,['access'],['access']
Security,"like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means of aggr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:1028,access,accessing,1028,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,1,['access'],['accessing']
Security,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1543,inject,injects,1543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,"['inject', 'validat']","['injects', 'validation']"
Security,nment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9722,certificate,certificates,9722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['certificate'],['certificates']
Security,"round. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1696,access,access,1696,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['access'],['access']
Security,s://download.pytorch.org/whl/cu118; ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; absl-py 1.4.0 pypi_0 pypi; adjusttext 0.8 pypi_0 pypi; aiohttp 3.8.5 pypi_0 pypi; aiosignal 1.3.1 pypi_0 pypi; airr 1.4.1 pypi_0 pypi; anndata 0.9.1 pypi_0 pypi; anyio 3.7.1 pypi_0 pypi; arrow 1.2.3 pypi_0 pypi; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; async-timeout 4.0.2 pypi_0 pypi; attrs 23.1.0 pypi_0 pypi; awkward 2.3.1 pypi_0 pypi; awkward-cpp 21 pypi_0 pypi; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backoff 2.2.1 pypi_0 pypi; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pypi_0 pypi; blessed 1.20.0 pypi_0 pypi; brotli-python 1.0.9 py311ha362b79_9 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2022.12.7 pypi_0 pypi; charset-normalizer 2.1.1 pypi_0 pypi; chex 0.1.7 pypi_0 pypi; click 8.1.6 pypi_0 pypi; cmake 3.25.0 pypi_0 pypi; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pypi_0 pypi; contourpy 1.1.0 pypi_0 pypi; croniter 1.4.1 pypi_0 pypi; cycler 0.11.0 pypi_0 pypi; dateutils 0.6.12 pypi_0 pypi; debugpy 1.6.7 py311hcafe171_0 conda-forge; decorator 5.1.1 pyhd8ed1ab_0 conda-forge; deepdiff 6.3.1 pypi_0 pypi; dm-tree 0.1.8 pypi_0 pypi; docrep 0.3.2 pypi_0 pypi; etils 1.3.0 pypi_0 pypi; executing 1.2.0 pyhd8ed1ab_0 conda-forge; fa2 0.3.5 py311hd4cff14_2 conda-forge; fastapi 0.100.0 pypi_0 pypi; filelock 3.9.0 pypi_0 pypi; flax 0.7.0 pypi_0 pypi; fonttools 4.41.1 pypi_0 pypi; frozenlist 1.4.0 pypi_0 pypi; fsspec 2023.6.0 pypi_0 pypi; h11 0.14.0 pypi_0 pypi; h5py 3.9.0 pypi_0 pypi; idna 3.4 pyhd8ed1ab_0 conda-forge; igraph 0.10.6 pypi_0 pypi; importlib-metadata 6.8.0 pyh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:1876,certificate,certificates,1876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['certificate'],['certificates']
Security,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:2617,access,access,2617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['access'],['access']
Testability," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1294,assert,assert,1294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,3,"['assert', 'log', 'test']","['assert', 'logging', 'tests']"
Testability, ; liblief 0.10.1 he6710b0_0 ; libllvm10 10.0.1 hbcb73fb_5 ; libllvm9 9.0.1 h4a3c616_1 ; libpng 1.6.37 hbc83047_0 ; libsodium 1.0.18 h7b6447c_0 ; libspatialindex 1.9.3 h2531618_0 ; libssh2 1.9.0 h1ba5d50_1 ; libstdcxx-ng 9.1.0 hdf63c60_0 ; libtiff 4.2.0 h85742a9_0 ; libtool 2.4.6 h7b6447c_1005 ; libuuid 1.0.3 h1bed415_2 ; libuv 1.40.0 h7b6447c_0 ; libwebp-base 1.2.0 h27cfd23_0 ; libxcb 1.14 h7b6447c_0 ; libxml2 2.9.10 hb55368b_3 ; libxslt 1.1.34 hc22bd24_0 ; llvmlite 0.36.0 py38h612dafd_4 ; locket 0.2.1 py38h06a4308_1 ; loompy 2.0.16 py_0 bioconda; lxml 4.6.3 py38h9120a33_0 ; lz4-c 1.9.3 h2531618_0 ; lzo 2.10 h7b6447c_2 ; magic-impute 2.0.4 pypi_0 pypi; make 4.2.1 h1bed415_1 ; markupsafe 2.0.1 py38h27cfd23_0 ; matplotlib 3.3.4 py38h06a4308_0 ; matplotlib-base 3.3.4 py38h62a2d02_0 ; mccabe 0.6.1 py38_1 ; mistune 0.8.4 py38h7b6447c_1000 ; mkl 2021.2.0 h06a4308_296 ; mkl-service 2.3.0 py38h27cfd23_1 ; mkl_fft 1.3.0 py38h42c9631_2 ; mkl_random 1.2.1 py38ha9443f7_2 ; mnnpy 0.1.9.5 pypi_0 pypi; mock 4.0.3 pyhd3eb1b0_0 ; more-itertools 8.7.0 pyhd3eb1b0_0 ; mpc 1.1.0 h10f8cd9_1 ; mpfr 4.0.2 hb69a4c5_1 ; mpmath 1.2.1 py38h06a4308_0 ; msgpack-python 1.0.2 py38hff7bd54_1 ; multicoretsne 0.1 pypi_0 pypi; multidict 5.1.0 pypi_0 pypi; multipledispatch 0.6.0 py38_0 ; mypy_extensions 0.4.3 py38_0 ; natsort 7.1.1 pyhd3eb1b0_0 ; nbclassic 0.2.6 pyhd3eb1b0_0 ; nbclient 0.5.3 pyhd3eb1b0_0 ; nbconvert 6.0.7 py38_0 ; nbformat 5.1.3 pyhd3eb1b0_0 ; ncurses 6.2 he6710b0_1 ; nest-asyncio 1.5.1 pyhd3eb1b0_0 ; networkx 2.5 py_0 ; nltk 3.6.2 pyhd3eb1b0_0 ; nose 1.3.7 pyhd3eb1b0_1006 ; notebook 6.4.0 py38h06a4308_0 ; numba 0.53.1 py38ha9443f7_0 ; numexpr 2.7.3 py38h22e1b3c_1 ; numpy 1.20.2 py38h2d18471_0 ; numpy-base 1.20.2 py38hfae3a4d_0 ; numpydoc 1.1.0 pyhd3eb1b0_1 ; nvidia-ml-py3 7.352.0 pypi_0 pypi; olefile 0.46 py_0 ; opencensus 0.7.13 pypi_0 pypi; opencensus-context 0.1.2 pypi_0 pypi; openpyxl 3.0.7 pyhd3eb1b0_0 ; openssl 1.1.1k h27cfd23_0 ; packaging 20.9 pyhd3eb1b0_0 ; palantir 1.0.0 py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:11263,mock,mock,11263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['mock'],['mock']
Testability," all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1318,test,tests,1318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['test'],['tests']
Testability," arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1097,test,test,1097,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['test'],['test']
Testability," claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found.; 2. Should people be using these values for visualization?; 3. What's the workflow if people need both log normalized and pearson residuals?; 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:2550,log,log,2550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693,1,['log'],['log']
Testability," cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1115,log,log,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['log'],['log']
Testability," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:2118,benchmark,benchmark,2118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['benchmark'],['benchmark']
Testability," not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:2726,log,logic,2726,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['log'],['logic']
Testability," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:2280,log,logical,2280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,1,['log'],['logical']
Testability," of either cargo culting it or becaue they know that makes setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1327,test,tests,1327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,1,['test'],['tests']
Testability," setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that makes it easier to reason about how our test suite works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:2021,test,test,2021,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,6,['test'],"['test', 'tests']"
Testability," the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the sca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:1533,test,tested,1533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['test'],['tested']
Testability," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2627,test,test,2627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,1,['test'],['test']
Testability," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1861,test,testpath,1861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021,1,['test'],['testpath']
Testability,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1959,log,log,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['log'],['log']
Testability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2796?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1daae5b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2796 +/- ##; ========================================; Coverage ? 71.35% ; ========================================; Files ? 103 ; Lines ? 11645 ; Branches ? 0 ; ========================================; Hits ? 8309 ; Misses ? 3336 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2796#issuecomment-1870329572:234,test,tests,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2796#issuecomment-1870329572,1,['test'],['tests']
Testability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2812?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`1.9.x@518e76a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2812 +/- ##; ========================================; Coverage ? 71.34% ; ========================================; Files ? 103 ; Lines ? 11632 ; Branches ? 0 ; ========================================; Hits ? 8299 ; Misses ? 3333 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2812#issuecomment-1893321168:234,test,tests,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2812#issuecomment-1893321168,1,['test'],['tests']
Testability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2972?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`main@3ceb740`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #2972 +/- ##; =======================================; Coverage ? 75.49% ; =======================================; Files ? 116 ; Lines ? 12911 ; Branches ? 0 ; =======================================; Hits ? 9747 ; Misses ? 3164 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2972#issuecomment-2029918769:252,test,tests,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2972#issuecomment-2029918769,1,['test'],['tests']
Testability,"## `X`. So, the first and third result have the same values of `X`. . ```python; assert np.array_equal(pc1.X, pc3.X); ```. The second is only slightly different:. ```python; diff = pc2.X - pc1.X; diff[diff != 0]; ```. ```; array([7.450581e-09], dtype=float32); ```. This might be adjustable by setting ""regress out"" to a fixed number of jobs. ## PCA. The results of the PCA differ more significantly, but here you should just be calling scikit-learn's implementation. Could you try calling that directly and letting us know the results?. E.g. ```python; from sklearn.decomposition import PCA. pca = PCA(n_components=50, solver=""arpack"", random_state=0); result = pca.fit_transform(adata.X); ```. If this doesn't give consistent results on your machines, the issue is upstream in scikit-learn. If you need reproducibility now, I would suggest switching out the solver for the PCA and/ or using 64 bit values for `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114#issuecomment-1021048765:81,assert,assert,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114#issuecomment-1021048765,1,['assert'],['assert']
Testability,"'m not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to hav",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1521,benchmark,benchmarks,1521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['benchmark'],['benchmarks']
Testability,"); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 3: As you recommend, I do `!pip uninstall tables` and `conda install -c conda-forge pytables`, then; ```python; import tables # pass. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_15024/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 19 from numpy import random; 20 from scipy import sparse; ---> 21 from anndata import AnnData, __version__ as anndata_version; 22 from textwrap import dedent; 23 from packaging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge impo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:3801,log,logging,3801,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['log'],['logging']
Testability,"-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2164,test,tested,2164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['test'],['tested']
Testability,-forge; libffi 3.2.1 1 bioconda; libgfortran 4.0.0 h50e675f_13 conda-forge; libgfortran4 7.5.0 h50e675f_13 conda-forge; libglib 2.66.2 hdb5fb44_0 conda-forge; libiconv 1.16 haf1e3a3_0 conda-forge; liblapack 3.9.0 2_openblas conda-forge; libllvm10 10.0.1 h009f743_3 conda-forge; libnghttp2 1.41.0 h7580e61_2 conda-forge; libopenblas 0.3.12 openmp_h63d9170_1 conda-forge; libpng 1.6.37 hb0a8c7a_2 conda-forge; libpq 12.3 haa216e0_2 conda-forge; libsodium 1.0.18 haf1e3a3_1 conda-forge; libspatialindex 1.9.3 h4a8c4bd_3 conda-forge; libssh2 1.9.0 h39bdce6_5 conda-forge; libtiff 4.1.0 h2ae36a8_6 conda-forge; libwebp-base 1.1.0 h0b31af3_3 conda-forge; libxml2 2.9.10 h7fdee97_2 conda-forge; llvm-openmp 11.0.0 h73239a0_1 conda-forge; llvmlite 0.34.0 py38h3707e27_2 conda-forge; loompy 3.0.6 py_0 conda-forge; lz4-c 1.9.2 hb1e8313_3 conda-forge; markupsafe 1.1.1 py38h94c058a_2 conda-forge; matplotlib-base 3.3.2 py38had0acaf_1 conda-forge; mccabe 0.6.1 py_1 conda-forge; mistune 0.8.4 py38h4d0b108_1002 conda-forge; mock 4.0.2 py38h32f6830_1 conda-forge; mysql-common 8.0.21 2 conda-forge; mysql-libs 8.0.21 hfb8f7af_2 conda-forge; natsort 7.0.1 py_0 conda-forge; nbclient 0.5.1 py_0 conda-forge; nbconvert 6.0.7 py38h32f6830_2 conda-forge; nbformat 5.0.8 py_0 conda-forge; ncurses 6.2 hb1e8313_2 conda-forge; nest-asyncio 1.4.1 py_0 conda-forge; networkx 2.5 py_0 conda-forge; nspr 4.29 hb1e8313_1 conda-forge; nss 3.47 hcec2283_0 conda-forge; numba 0.51.2 py38h6be0db6_0 conda-forge; numexpr 2.7.1 py38h6be0db6_3 conda-forge; numpy 1.19.2 py38ha98150c_1 conda-forge; numpy_groupies 0.9.13 pyh9f0ad1d_1 conda-forge; numpydoc 1.1.0 py_1 conda-forge; olefile 0.46 pyh9f0ad1d_1 conda-forge; openssl 1.1.1h haf1e3a3_0 conda-forge; packaging 20.4 pyh9f0ad1d_0 conda-forge; pandas 1.1.3 py38h48ddb8e_2 conda-forge; pandoc 2.11.0.4 h22f3db7_0 conda-forge; pandocfilters 1.4.2 py_1 conda-forge; parso 0.7.0 pyh9f0ad1d_0 conda-forge; pathtools 0.1.2 py_1 conda-forge; patsy 0.5.1 py_0 conda-forge; pcre 8.44 hb1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:4409,mock,mock,4409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['mock'],['mock']
Testability,". in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding practices here that will come in handy in; > the future, I promise!; >; > Thank you for your contribution 🎉; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:1795,test,test,1795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,2,['test'],['test']
Testability,"/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:7529,test,test,7529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-serializinghtml 1.1.4 pyhd3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi; tbb 2020.3 hfd86e86_0 ; tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:17270,test,testpath,17270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['test'],['testpath']
Testability,"4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3685,test,testpath,3685,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['test'],['testpath']
Testability,63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; libstdcxx-ng=9.1.0=hdf63c60_0; libuuid=2.32.1=h14c3975_1000; libxcb=1.13=h14c3975_1002; libxml2=2.9.10=hee79883_0; libxslt=1.1.33=h31b3aaa_0; llvmlite=0.32.0=pypi_0; lxml=4.5.0=py37he3881c9_1; markupsafe=1.1.1=py37h8f50634_1; matplotlib=3.2.1=pypi_0; mistune=0.8.4=py37h8f50634_1001; natsort=7.0.1=pypi_0; nbconvert=5.6.1=py37hc8dfbb8_1; nbformat=5.0.6=py_0; ncurses=6.2=he6710b0_0; networkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:2882,stub,stubs,2882,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['stub'],['stubs']
Testability,"97, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:9568,test,test,9568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,":; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:6497,test,test,6497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8179,test,testpath,8179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['test'],['testpath']
Testability,"<notifications@github.com> wrote:. > Hi, looks great!; >; > The only duplicated code left is that _prepare_weighted_dataframe is very; > similar to _prepare_dataframe. I think you can delete; > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return; > categories, obs_tidy, categorical. Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding pra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:1116,test,test,1116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,1,['test'],['test']
Testability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:910,log,logic,910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,1,['log'],['logic']
Testability,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:129,test,testing,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,2,['test'],"['test', 'testing']"
Testability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:266,log,logging,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,2,"['log', 'test']","['logging', 'test']"
Testability,"> Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Fully agree. > ; > Also: If trying to call a t-test with non-logarithmized data, a warning should be written.; > . Also agree. > The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?. Oh true, it wasn't log transformed, true.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325:214,test,test,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325,4,"['log', 'test']","['log', 'logarithmized', 'test']"
Testability,"> Can you point to a package whose test organization you would like our tests to emulate?. - pytest: https://github.com/pytest-dev/pytest/tree/main/testing; - loompy: https://github.com/linnarsson-lab/loompy/tree/master/tests. The others have their tests in the package, and just have that useless `__init__.py` in the tests directory because of either cargo culting it or becaue they know that makes setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:35,test,test,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,8,['test'],"['test', 'testing', 'tests']"
Testability,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:32,test,tests,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677,11,"['assert', 'test']","['asserting', 'test', 'tested', 'testing', 'tests']"
Testability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:16,log,log,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,1,['log'],['log']
Testability,"> Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with; > ; > ```; > pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; > ```; > ; > Seems to work now for me. Thank you. It just worked for me, in July 2024.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559:211,test,test,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559,1,['test'],['test']
Testability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1130,test,tested,1130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,1,['test'],['tested']
Testability,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:50,log,logic,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140,3,['log'],['logic']
Testability,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:747,log,logical,747,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106,2,['log'],"['logic', 'logical']"
Testability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I can’t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! That’s a concrete thing I can do. I’ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:340,test,tests,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,4,['test'],"['test', 'tests']"
Testability,"> I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. You can see some quick comparisons between Pynndescent and Annoy here: https://github.com/pavlin-policar/openTSNE/issues/101#issuecomment-597178379. But I have not investigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:548,benchmark,benchmarks,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833,1,['benchmark'],['benchmarks']
Testability,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:262,assert,assertion,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652,1,['assert'],['assertion']
Testability,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:609,log,logical,609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308,1,['log'],['logical']
Testability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:214,log,logically,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['log'],['logically']
Testability,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:127,benchmark,benchmarks,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,2,['benchmark'],['benchmarks']
Testability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:366,test,test,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,1,['test'],['test']
Testability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:867,benchmark,benchmarking,867,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,1,['benchmark'],['benchmarking']
Testability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:509,test,testing,509,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['test'],['testing']
Testability,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:336,log,logging,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,3,['log'],"['log', 'logging']"
Testability,"> Thank you! With “tests” I mean “functions named `test_*` with `assert ...` statements inside”; ; Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, ; Khalid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243:19,test,tests,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243,2,"['assert', 'test']","['assert', 'tests']"
Testability,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:11,test,test,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837,4,['test'],"['test', 'tests']"
Testability,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:73,log,log,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664,3,"['log', 'test']","['log', 'test']"
Testability,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:155,test,tests,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729,1,['test'],['tests']
Testability,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:24,log,logic,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973,4,['log'],['logic']
Testability,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:273,test,test,273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,2,"['log', 'test']","['logic', 'test']"
Testability,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:133,log,logic,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013,1,['log'],['logic']
Testability,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:240,test,tests,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681,8,['test'],"['test', 'tested', 'tests']"
Testability,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:28,test,test,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,5,['test'],"['test', 'testpaths', 'tests']"
Testability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:401,log,logfc,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,1,['log'],['logfc']
Testability,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances; * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191:115,log,logic,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191,1,['log'],['logic']
Testability,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487410333:542,test,tests,542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487410333,2,['test'],['tests']
Testability,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:65,test,tested,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259,2,['test'],['tested']
Testability,"@LuckyMD Yeah, we can definitely do the differences in mean expression on the log-scale as a quick fix. However, I think it's a little bit more intuitive to express fold-changes with a base of 2, as opposed to a base of e, which is the transformation used for the data matrix. This also mimics what other packages usually report, but I'm happy either way!. For the differential testing itself, I completely agree with you about the assumption of normality with t-tests, so we definitely shouldn't change that. For the wilcoxon test, however, normality isn't assumed, so do you think it would be ok to run on raw counts?. Either way, we should definitely fix the double-log for the fold-change reporting. Is there a preference on whether to keep it was loge(base e) difference or log2 difference? For simplicity, this change in scale would only affect the fold-change reporting, all other tests I think we should keep in loge to keep it consistent. One final thought I wanted to ask your opinion on: log-mean vs mean-log. (e.g. log(mean(count)) vs mean(log(count)). For t-tests I think it makes sense to use mean-log, however, when calculating fold-change differences in mean expression, I'm not entirely sure what's preferred (particularly for methods like wilcoxon that don't use the mean expression to calculate scores). Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-470250609:78,log,log-scale,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-470250609,14,"['log', 'test']","['log', 'log-mean', 'log-scale', 'loge', 'test', 'testing', 'tests']"
Testability,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:188,log,log,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854,3,"['log', 'test']","['log', 'tests']"
Testability,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:13,benchmark,benchmarks,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215,1,['benchmark'],['benchmarks']
Testability,"@dawe a cell cycle scoring function would be great! everything that's a bit more extensive and non-standard should go into [sc.tl](https://github.com/theislab/scanpy/tree/master/scanpy/tools), everything that's really just simple preprocessing and stats with a few lines can go to [sc.pp](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/simple.py). usually, there should be a plotting function in sc.pl that presents a canonical visualization of the annotation added in with the tool... writing a test for your function would also be great ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398:517,test,test,517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398,1,['test'],['test']
Testability,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:123,test,tests,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,10,"['log', 'test']","['log', 'test', 'testing', 'tests']"
Testability,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642:103,test,tests,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642,1,['test'],['tests']
Testability,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:193,benchmark,benchmark,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135,1,['benchmark'],['benchmark']
Testability,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:203,test,test,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973,3,['test'],['test']
Testability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:35,log,logical,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,1,['log'],['logical']
Testability,"@giovp thanks for your reply, I agree on all points :) Have a good vacation!; @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG).; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459:198,test,tests,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459,1,['test'],['tests']
Testability,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:29,test,tests,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,6,['test'],"['test', 'tests']"
Testability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:852,log,log,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,1,['log'],['log']
Testability,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831:143,log,logic,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831,1,['log'],['logic']
Testability,"@ivirshup I'd actually be interest in hearing those. My packages also have the test folder outside the package, but I am happy to learn why many major packages have theirs in the actual package and why that might be a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545:79,test,test,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545,1,['test'],['test']
Testability,"@ivirshup Thank you for the feedback. I will add a release note soon. I also thought about the naming of the parameter. However, if we assume that also in the future it is mostly used to subset the number of PCs in PCA arrays stored under different names, it should be fine?. I can not comment further on what these changes may break, at least ideally they should make the use of n_pcs more consistent and as expected. Would you suggest, I implement some further, more comprehensive tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928:483,test,tests,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928,1,['test'],['tests']
Testability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:515,stub,stubs,515,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['stub'],['stubs']
Testability,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ?. Yes!. > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with reall",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:1006,log,log,1006,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693,1,['log'],['log']
Testability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:847,log,logic,847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,1,['log'],['logic']
Testability,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didn’t do it yet. I didn’t have time to review the whole thing, but if y’all want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-462858876:55,log,logging,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-462858876,1,['log'],['logging']
Testability,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point.; ```; adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing; ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629:242,test,testing,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629,1,['test'],['testing']
Testability,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>; <summary> Basic PCA projection </summary>. ```python; def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings (just whether to center?) from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt; ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated.; * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up?; * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:; * Does the syntax still work for cases other than 1-to-1 transfer? ; * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`.; * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842:68,test,test,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842,1,['test'],['test']
Testability,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138:122,test,tests,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138,1,['test'],['tests']
Testability,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:203,benchmark,benchmarking,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,2,"['benchmark', 'test']","['benchmarking', 'tests']"
Testability,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:340,log,logic,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732,1,['log'],['logic']
Testability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:33,test,test,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,4,['test'],"['test', 'testing', 'tests']"
Testability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:501,test,test,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['test'],['test']
Testability,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks!. Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405:140,benchmark,benchmarks,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405,1,['benchmark'],['benchmarks']
Testability,"Even something simple doesn't work anymore, without going through h5ad:. ```; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; Traceback (most recent call last):; File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>; cellbrowser.cbScanpyCli(); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli; adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices; obs = _normalize_index(obs, self._adata.obs_names); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index; positions = positions[index]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__; return self._get_with(key); File ""/cluster/home/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138:480,log,logFname,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138,1,['log'],['logFname']
Testability,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:319,log,logging,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745,2,['log'],"['logging', 'logical']"
Testability,Gotcha! I'll prioritize getting the benchmarks up and then I'll need some guidance on how to organize it to fit in scanpy's codebase. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655:36,benchmark,benchmarks,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655,1,['benchmark'],['benchmarks']
Testability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:815,benchmark,benchmarked,815,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,5,"['benchmark', 'test']","['benchmarked', 'test', 'tests']"
Testability,"Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with ; ```; pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; ```. Seems to work now for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996:202,test,test,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996,1,['test'],['test']
Testability,"Hello @Koncopd ,; Thanks for the response!. Step 1: I created a fresh new environment (py3.8.12); ```python; !pip install scanpy[leiden]. Successfully installed anndata-0.7.8 cycler-0.11.0 fonttools-4.28.5 h5py-3.6.0 igraph-0.9.9 joblib-1.1.0 kiwisolver-1.3.2 leidenalg-0.8.8 llvmlite-0.38.0 matplotlib-3.5.1 natsort-8.0.2 networkx-2.6.3 numba-0.55.0 numexpr-2.8.1 numpy-1.21.5 pandas-1.3.5 patsy-0.5.2 pillow-9.0.0 pynndescent-0.5.5 python-igraph-0.9.9 scanpy-1.8.2 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.13.1 stdlib-list-0.8.0 tables-3.7.0 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:789,log,logging,789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['log'],['logging']
Testability,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:711,test,tested,711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,1,['test'],['tested']
Testability,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:8,test,test,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317,1,['test'],['test']
Testability,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:858,log,log,858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['log'],['log']
Testability,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:86,test,tests,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292,8,['test'],"['test', 'tests']"
Testability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1115,test,tested,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,1,['test'],['tested']
Testability,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:453,test,testing,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467,1,['test'],['testing']
Testability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:195,test,tests,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,1,['test'],['tests']
Testability,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:77,test,tests,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207,5,['test'],['tests']
Testability,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:683,log,logs,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172,1,['log'],['logs']
Testability,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:; `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]; `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,; `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""); `; there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip.; I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124:531,test,test,531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124,3,['test'],['test']
Testability,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:353,log,logreg,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773,1,['log'],['logreg']
Testability,"Hi @fidelram,; One way in which I'd like to do it is like the following:. ```python; sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],; save=False, use_raw=False); ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second).; One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320:389,test,test,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320,1,['test'],['test']
Testability,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:613,log,logging,613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,1,['log'],['logging']
Testability,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:1412,test,testing,1412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404,1,['test'],['testing']
Testability,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```; # compare expression levels of mel vs all other cell types in pairwise manner; sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-32-73add1f79f3a> in <module>; 1 # save as a data frame; 2 ; ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'); 4 ; 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols); 53 d = pd.DataFrame(); 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:; ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]; 56 if pval_cutoff is not None:; 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx); 517 ; 518 def __getitem__(self, indx):; --> 519 obj = super(recarray, self).__getitem__(indx); 520 ; 521 # copy behavior of getattr, except that here. ValueError: no field of name mel; ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616:1262,log,logfoldchanges,1262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616,1,['log'],['logfoldchanges']
Testability,"Hi @transcriptomics, ; If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616:170,test,testing,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616,1,['test'],['testing']
Testability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:101,test,test,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,2,"['log', 'test']","['logging', 'test']"
Testability,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i; analysed most of them are from previous code. Regards,; Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great!; >; > The only duplicated code left is that _prepare_weighted_dataframe is very; > similar to _prepare_dataframe. I think you can delete; > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return; > categories, obs_tidy, categorical. Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:712,test,tests,712,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,5,"['assert', 'test']","['assert', 'assertions', 'test', 'tests']"
Testability,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:241,test,testing,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498,1,['test'],['testing']
Testability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:950,log,log-normalization,950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,1,['log'],['log-normalization']
Testability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:936,stub,stubs,936,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,1,['stub'],['stubs']
Testability,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/673#issuecomment-528510773:82,log,log-transformed,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673#issuecomment-528510773,1,['log'],['log-transformed']
Testability,"Hi, ; I tested your fix and it works! ; ```; scanpy==1.4.5.2.dev6+gfa408dc7 anndata==0.7.1 ; umap==0.3.10 numpy==1.17.4 scipy==1.3.1 ; pandas==0.25.2 scikit-learn==0.21.3 ; statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```; BTW:; `matplotlib==3.1.3`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599:8,test,tested,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599,1,['test'],['tested']
Testability,"Hi, looks great!. Ignore the tool, I think it’s a bit broken. I need to figure out what’s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(…)` to `categories, obs_tidy, _ = _prepare_dataframe(…)`. Other than that, there’s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so!; 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace “xyz” with whatever you want):. ```py; def test_xyz(image_comparer):; save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); […]; sc.pl.xyz(adata, …); save_and_compare_images('xyz'); ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesn’t exist. You need to copy the pngs from `scanpy/tests/figures`→`scanpy/test/_images` and `git commit` them.; 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144; 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data?. @Khalid-Usman I’m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you won’t regret doing this. You’re learning good coding practices here that wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412:550,test,tests,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412,7,"['assert', 'test']","['assert', 'assertions', 'test', 'tests']"
Testability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:645,log,log-transforming,645,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,1,['log'],['log-transforming']
Testability,"Hi, thanks for your interest in scanpy!. I’ll try to comment on your observations here with your code example:. ```; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet.; → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test).; ```; def my_scale_function(X, clip=False):; # need to make a copy of X; Y = X.copy(); mean, var = mean_var(Y, axis=0); Y -= mean; std = np.sqrt(var); #std[std == 0] = 1; Y /= std; if clip:; Y = np.clip(X, -10, 10); return np.matrix(Y); ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```; ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled)); ```. ```; Do a numpy check for closeness of floats:; False; ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that.; ```; adata.X.var(0); ```. ```; array([0.9996213 , 0.97964925, 0.29805112, ..., ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273:956,test,test,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273,1,['test'],['test']
Testability,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are; ```; scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216:36,test,tests,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216,2,['test'],['tests']
Testability,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:173,test,test,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463,2,['test'],['test']
Testability,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:46,test,test,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116,3,"['assert', 'test']","['assert', 'test']"
Testability,"Hmm, seems like `PCA` with `svd_solver='arpack'` works differently in sklearn 1.5 and flips the coordinates in some tests:. https://github.com/scikit-learn/scikit-learn/issues/28826. E.g. this used to work, now it doesn’t. ```py; from __future__ import annotations. import numpy as np; from sklearn.decomposition import PCA. data = np.asarray([[-1, 2, 0], [3, 4, 0], [1, 2, 0]]).T; expected = np.array(; [[-1.579575e-15, 1.490712], [-2.44949, -0.745356], [2.44949, -0.745356]],; dtype=np.float32,; ). pca = PCA(n_components=2, svd_solver=""arpack"", random_state=np.random.RandomState(0)); transformed = pca.fit_transform(data).astype(np.float32); np.testing.assert_almost_equal(transformed, expected, decimal=5); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3047#issuecomment-2098389353:116,test,tests,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3047#issuecomment-2098389353,2,['test'],"['testing', 'tests']"
Testability,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:342,test,test,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['test'],"['test', 'tests']"
Testability,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-479587681:8,test,tests,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-479587681,2,['test'],"['test', 'tests']"
Testability,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-460071018:48,test,test,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-460071018,2,['test'],"['test', 'tests']"
Testability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:269,test,tests,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,3,"['log', 'test']","['log', 'tests']"
Testability,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:30,log,logFC,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073,3,['log'],"['logFC', 'loged']"
Testability,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965:201,test,tests,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965,1,['test'],['tests']
Testability,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory.; Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499:484,benchmark,benchmarks,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499,1,['benchmark'],['benchmarks']
Testability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1799,log,logging,1799,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,1,['log'],['logging']
Testability,I just add the versions I used:; ```python; sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/536#issuecomment-474301705:47,log,logging,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536#issuecomment-474301705,1,['log'],['logging']
Testability,"I literally never had problems with test discovery, so idk what to look for. As said: Numpy and pandas have separated their testing utils from their tests. For the time being I want just that, no change to where the tests are. Would you accept a PR that simply moves the test utils into private submodules of `scanpy.testing` and switches the import mode to (future default, drawback-less) `importlib`?. Any change to the test layout can come later or never. I’d like to follow pytest’s recommendation (`/src/scanpy/` and `/tests/`) but this issue is orthogonal to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096566148:36,test,test,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096566148,8,['test'],"['test', 'testing', 'tests']"
Testability,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:403,log,logs,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054,1,['log'],['logs']
Testability,"I see, [densmap](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html). Hmm, I think that `method='densmap'` and `method_kwds={...}` would be a better API for us (which would then be translated into `densmap=True, densmap_kwds=method_kwds`). This also needs tests and a release note. Also we probably should just remove the umap 0.4 compatibility code, what do you think @ivirshup?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449:267,test,tests,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449,1,['test'],['tests']
Testability,"I simplified this quite a bit. I decided against blocking this on an upstream `anndata` helper for multiple reasons:. 1. we test on older anndata versions that won’t have that parameter immediately; 2. needs some designing, e.g. the API should be able to do “use `ceil(shape[0] / 2)` as chunk size for dim 0, and `-1` (=full size) for dim 1”; 3. it would make no sense for the non-dask array types, should we still add it for them?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2250061054:124,test,test,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2250061054,1,['test'],['test']
Testability,"I support the idea of tidying up plotting arguments. I think there are mainly two problems. 1) the high number of plotting arguments 2) lack of reusability of plotting ""styles"". . Chaining looks really cool and improves 1). Also, it logically partitions the plotting arguments. However, it doesn't solve 2). In other words, if we plot two figures, we'll need to copy the entire thing, and it'll be very verbose:. ```python; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). sc.pl.umap(adata2, color='fluffy').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```. One thing that comes to mind for reusability is to store the result of the chain somewhere and, well, reuse it:. ```python; style = sc.pl.styles.scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). # using simple arguments, similar to https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/lmeControl.html; sc.pl.umap(adata, color='clusters', style=style); sc.pl.umap(adata2, color='fluffy', style=style). # using context managers, similar to https://seaborn.pydata.org/tutorial/aesthetics.html#temporarily-setting-figure-style; with style:; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # overriding an existing style object; with style.legend(fontsize=12):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # or use predefined styles (?); with sc.pl.style('malte'):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). ```. WDYT?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810:233,log,logically,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810,1,['log'],['logically']
Testability,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:2,test,tested,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466,1,['test'],['tested']
Testability,I think it is more or less complete.; Here are the tutorials; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519155566:104,benchmark,benchmarks,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519155566,2,['benchmark'],['benchmarks']
Testability,"I think maybe i found a solution to solve this problem.; Maybe this problem is caused by the version of scikit—misc，when you use pip install --user scikit-misc or pip install scikit-misc，the system will install scikit-misc==0.1.4.; so,i try to install another verion of scikit-misc,you can use install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1.; In addition, this line of command needs to be used when python is greater than or equal to 3.8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497:313,test,test,313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497,1,['test'],['test']
Testability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1226,test,tested,1226,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['test'],['tested']
Testability,"I think the problem is the option `sort_order` which is True by default for; numerical data. This changes the ordering of the dots and thus it messes; up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot; > with the following code; >; > import scanpy.api as sc; > import numpy as np; > sc.settings.figdir = ""testdir""; > sc.settings.file_format_figs = ""png""; > sc.logging.print_versions(); >; > With these libraries; > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4; > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Running the following code bit. I use some dummy variable for size.; >; > somedata = sc.datasets.paul15(); > sc.pp.pca(somedata); > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); > sc.tl.leiden(somedata, resolution=0.5, random_state=42); > z = np.abs(somedata.obsm['X_pca'][:,0])**1; > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); >; > I get the following two figure as output; > [image: umapcontinuous_expr]; > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>; > [image: umapgroup_value]; > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>; >; > I would expect to see a similar size allocation/distribution but they are; > very different. I Could not really find a cause for this looking at the; > scatter plot function so it might be somewhere deeper.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/478>, or ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152:479,test,testdir,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152,2,"['log', 'test']","['logging', 'testdir']"
Testability,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:1226,benchmark,benchmarking,1226,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251,1,['benchmark'],['benchmarking']
Testability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:817,test,tested,817,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,3,"['log', 'test']","['log', 'log-ish', 'tested']"
Testability,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>; <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>; <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>; <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>; <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864:1795,test,testing,1795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864,1,['test'],['testing']
Testability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:377,test,test,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,2,"['log', 'test']","['logistic', 'test']"
Testability,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083:35,test,tests,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083,1,['test'],['tests']
Testability,"I'm also a bit confused about how the CLR is applied. In the CITE-seq paper, I think it was done within a cell (over proteins), then I think they had switched to within a protein (over cells), and now in Seurat v4 it appears to be back to within a cell. Any per cell normalization is a bit tricky because the panels will differ between datasets as well as the titration of antibodies used. The simplest thing to me seems to be a simple log transformation combined with per protein scaling, as values between proteins are not comparable to begin with. We have some additional thoughts in the appendix of our totalVI paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290:436,log,log,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290,1,['log'],['log']
Testability,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:288,test,tests,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421,2,['test'],['tests']
Testability,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:79,test,tests,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296,10,['test'],"['test', 'testing', 'tests']"
Testability,"ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5748,log,logging,5748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['log'],['logging']
Testability,It's not clear to me why these `test_10x` tests are failing here and not on master -- there shouldn't be anything in this diff that affects those tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898:42,test,tests,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898,2,['test'],['tests']
Testability,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041:242,test,test,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041,1,['test'],['test']
Testability,"Just added a test and changed the behaviour of scale a little more.; The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/cus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:13,test,test,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['test'],['test']
Testability,"Looks simple enough! Please deduplicate the tests though, they have too many identical lines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042#issuecomment-2092792623:44,test,tests,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042#issuecomment-2092792623,1,['test'],['tests']
Testability,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950:385,test,tests,385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950,1,['test'],['tests']
Testability,"Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=492) """"""; [494](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=493) Populate and run pipeline for bytecode input; [495](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=494) """"""; [496](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=495) assert self.state.func_ir is None; --> [497](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=496) return self._compile_core(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:476, in CompilerBase._compile_core(self); [474](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=473) self.state.status.fail_reason = e; [475](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=474) if is_final_pipeline:; --> [476](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=475) raise e; [477](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=476) else:; [478](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=477) raise CompilerError(""All available pipelines exhausted""). File D:\Users\xiang",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:20470,assert,assert,20470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['assert'],['assert']
Testability,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:398,test,tests,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,1,['test'],['tests']
Testability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:650,test,test,650,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,1,['test'],['test']
Testability,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:63,log,logic,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['log'],['logic']
Testability,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:118,test,test,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179,1,['test'],['test']
Testability,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475999342:319,log,logarithmize,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475999342,1,['log'],['logarithmize']
Testability,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:79,test,testing,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565,4,['test'],"['test', 'testing', 'tests']"
Testability,OS: Windows 10; Python version: 3.7.7; sc.logging.print_versions() gives; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038:42,log,logging,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038,1,['log'],['logging']
Testability,"Ok, good to read that it wasn't log-transformed!. @Koncopd, could you quickly implement these simple changes? Before continuing to work on the UMAP? These simple changes are for 1.4.1, the UMAP will be for 1.5. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478391082:32,log,log-transformed,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478391082,1,['log'],['log-transformed']
Testability,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:28,test,test,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463,2,['test'],"['test', 'tests']"
Testability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:21,test,test,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['test'],['test']
Testability,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:679,test,tests,679,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877,1,['test'],['tests']
Testability,"Please go ahead!. On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes.; > Alex is currently a bit ill I learned, which is why he probably didn’t do; > it yet. I didn’t have time to review the whole thing, but if y’all want I; > can do that too; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-463080680:153,log,logging,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-463080680,1,['log'],['logging']
Testability,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/581#issuecomment-479418054:214,test,tests,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581#issuecomment-479418054,1,['test'],['tests']
Testability,"Sergei (@Koncopd) tested it out and will get back to you. He also found a peak memory usage of 121 GB. I have to admit that I never made checks with that degree of detail and I fear that for now, I'll simply update the documentation stating that peak memory usage can go up to ~120 GB. I'm still puzzled by that, and maybe some efficiency found it's way into the code which wasn't there (simple guess: is everything in `float32`?). But we'll need some time to work it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466:18,test,tested,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466,1,['test'],['tested']
Testability,"So for a fix, we’d simply need to change. https://github.com/scverse/scanpy/blob/414092f68b4b40aa99153556377c32839b392636/scanpy/preprocessing/_highly_variable_genes.py#L197-L199. into. ```py; X = X.copy(); if 'log1p' in adata.uns_keys() and adata.uns['log1p'].get('base') is not None:; X *= np.log(adata.uns['log1p']['base']); np.expm1(X, out=X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2668#issuecomment-1766402734:295,log,log,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2668#issuecomment-1766402734,1,['log'],['log']
Testability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:457,test,test,457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,1,['test'],['test']
Testability,"Solved same problem for me as well. . For the record, the output of `sc.logging.print_versions()` in my conda environment is: . `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654:72,log,logging,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654,1,['log'],['logging']
Testability,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:317,test,test,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225,2,"['log', 'test']","['logreg', 'test']"
Testability,"Sorry @ivirshup , still not quite getting what you're after. The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Non-integer values don't necessarily mean normalised or log'd data, so I can't add a check in the code for that. I could add a check on the values of the simulated doublets, that they are the sum of the parent counts we send to scrublet's simulate function, but that seems outside the scope of these fixes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519:132,log,log,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519,2,['log'],['log']
Testability,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png); ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png); ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411:873,log,logscale,873,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411,1,['log'],['logscale']
Testability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:665,test,testing,665,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,1,['test'],['testing']
Testability,"Sorry, there is a small bug in the wilcoxon method, that might hit sometimes. @a-munoz-rojas, it should be resolved after merging your fix, don't you think so? I'd be happy to move forward as soon as the code-overhead issue around double logarithmization is fixed. Should be very simple. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/530#issuecomment-474301716:238,log,logarithmization,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530#issuecomment-474301716,1,['log'],['logarithmization']
Testability,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:238,test,test,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,3,['test'],['test']
Testability,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:187,test,test,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094,1,['test'],['test']
Testability,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and; push when it passed tests for all 5 plots. Thanks,; Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,; >; > I have removed issue from the pull request by the testing tool, now the; > tools showed me duplications, which are mostly from other code and 1-2 from; > my code. Please have a look into it. It's my first pull request and its; > taking too much time :(; >; > Thanks; > Khalid; >; > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:; >; >> Ok , thanks for letting me know. Please check the pull request. I have; >> verified my code by keeping weights 1 and it has same values when; >> observations has no weights or all weights equal to 1.; >>; >> I also suggest to update PCA for weighted sampled data.; >>; >> Thanks,; >> Khalid Usman; >>; >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; >> wrote:; >>; >>> You can just open a new one, I’ll close this one then 🙂; >>>; >>> —; >>> You are receiving this because you authored the thread.; >>> Reply to this email directly, view it on GitHub; >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >>> or mute the thread; >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >>> .; >>>; >>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520:49,test,testing,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520,4,['test'],"['tested', 'testing', 'tests']"
Testability,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:426,log,logging,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,['log'],"['log', 'logging']"
Testability,"Thanks for the PR! I've just renamed the variable to be a bit more clear. I do think this test could be a bit better (e.g. check that the structure of the object is correct), but also this is an improvement so LGTM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692:90,test,test,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692,1,['test'],['test']
Testability,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting.; * This should work with other solvers from scipy, like `lobpcg`, right?; * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that?. ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958:270,benchmark,benchmarks,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958,2,['benchmark'],"['benchmark', 'benchmarks']"
Testability,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:61,test,test,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['test'],['test']
Testability,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:81,test,tests,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808,1,['test'],['tests']
Testability,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:49,test,test,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387,2,['test'],['test']
Testability,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:265,log,logfoldchanges,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862,3,['log'],['logfoldchanges']
Testability,"Thanks! A little bit of context. We needed this aggregation for one of the projects using pseudobulks of the data. We could use scanpy aggregation methods for simple averaging, but to test the outlier-robust median aggregation, we had to write our code. scanpy didn't have it for some reason, so @farhadmd7 kindly agreed to contribute here. Perhaps someone else will find it helpful, too. @eroell, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730:184,test,test,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730,1,['test'],['test']
Testability,"The biggest advantage would be the possibility to create such panels as shown in the example, where e.g. quality metrics have different cmaps as gene expression.; Another advantage would be that cmaps could be defined globally for each parameter, resulting in simpler plotting calls. ```; adata = sc.datasets.paul15(); adata.X = adata.X.astype('float64'); sc.pp.filter_cells(adata, min_genes=100); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); adata.uns['n_counts_all_cmap'] = 'copper'; adata.uns['n_genes_cmap'] = 'copper'; sc.pl.pca(adata, color=['paul15_clusters', 'n_counts_all', 'n_genes', 'Zyx', 'calp80', 'slc43a2'], ncols=3); ```; ![test](https://user-images.githubusercontent.com/23263654/99387978-28a55100-28d5-11eb-975d-f91211370c16.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186:663,test,test,663,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186,1,['test'],['test']
Testability,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:238,test,tests,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744,1,['test'],['tests']
Testability,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py; if not issparse(X):; mean = np.mean(X, axis=0, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64); var = mean_sq - mean ** 2; elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py; if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138:123,log,logic,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138,1,['log'],['logic']
Testability,"The previous test failed but is not clear to me why, as it passes the local tests (anndata 0.7.5). It seems that on travis server, backed slicing requires integer indices and will not work with a boolean vector. I changed to sorted integers hoping that this will solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048:13,test,test,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048,2,['test'],"['test', 'tests']"
Testability,"The problem is that it does not install at all. ; When I run; ```; conda create -n test; conda activate test; conda install python=3.11; conda install -c conda-forge scanpy; ```; I get an error output for the last line, which is:; ```; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:83,test,test,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['test'],['test']
Testability,"Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding practices here that will come in handy in; > the future, I promise!; >; > Thank you for your contribution 🎉; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:1480,test,tests,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,2,['test'],"['test', 'tests']"
Testability,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/415#issuecomment-452283384:89,test,test,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415#issuecomment-452283384,2,['test'],"['test', 'tests']"
Testability,This is the output of `sc.logging.print_versions()`:; ```; scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520:26,log,logging,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520,1,['log'],['logging']
Testability,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:97,log,log,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779,3,['log'],"['log', 'logarithm']"
Testability,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```; adata = sc.read_10x_h5(""source.h5"", ""GRCh38""); adata.obs['n_counts'] = adata.X.sum(1); adata.obs['log_counts'] = np.log(adata.obs['n_counts']); adata.obs['n_genes'] = (adata.X > 0).sum(1); adata.write(""test.h5ad"", compression='gzip', compression_opts=1); ```. Then read it three different ways:. ```; In [7]: %memit a1 = sc.read(""test.h5ad""); peak memory: 2333.84 MiB, increment: 2170.77 MiB; ```. ```; In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""); peak memory: 4400.07 MiB, increment: 2137.85 MiB; ```. ```; In [9]: %memit a3 = custom_read(""test.h5ad""); peak memory: 4390.11 MiB, increment: 66.90 MiB; ```. where `custom_read` is defined as:. ```; def custom_read(filename):; adata = AnnData(); hf = h5py.File(filename); adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]; adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]; adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]; adata.obs_names = [x[0] for x in hf['obs'][()]]; adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]; adata.var_names = [x[0] for x in hf['var'][()]]; return adata; ```. and here's some verification the custom read is actually reading the data. ```; In [16]: a2.obs_keys(); Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(); Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs); Out[18]: 384000. In [19]: len(a3.obs); Out[19]: 384000. In [20]: a2.var_keys(); Out[20]: ['gene_ids']. In [21]: a3.var_keys(); Out[21]: ['gene_ids']. In [22]: len(a2.var); Out[22]: 33694. In [23]: len(a3.var); Out[23]: 33694; ```. Here's some version info I neglected in the original comment:. ```; In [28]: sc.logging.print_versions(); scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/434#issuecomment-456056835:272,log,log,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434#issuecomment-456056835,6,"['log', 'test']","['log', 'logging', 'test']"
Testability,"Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:8182,test,test,8182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"We have images which are saved which can be compared against, typically one would add a new image which you have visually confirmed to be correct, so if the behavior changes later we know. There is a little more about this in the [contribution guide](https://scanpy.readthedocs.io/en/stable/dev/testing.html#plotting-tests).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248:295,test,testing,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248,2,['test'],"['testing', 'tests']"
Testability,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:339,log,logarithm,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609,2,"['log', 'test']","['logarithm', 'tests']"
Testability,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499043170:10,log,logging,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499043170,4,['log'],['logging']
Testability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:790,benchmark,benchmarking,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['benchmark'],['benchmarking']
Testability,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:13,test,tests,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250,11,['test'],['tests']
Testability,Would be a nice use case for working on the usability of benchmarks (related to #2977),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2986#issuecomment-2044397793:57,benchmark,benchmarks,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2986#issuecomment-2044397793,1,['benchmark'],['benchmarks']
Testability,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821:62,test,test,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821,2,['test'],"['test', 'tests']"
Testability,"Yeah, I was concerned about the complexity. But AnnData doesn’t do anything wrong, it just uses chunks in both directions, which that one specific algorithm doesn’t support. But I think in general we should test for 2D chunks, that’s why I think AnnData’s test helpers work as intended. Any idea how to do this more simply?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245560938:207,test,test,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245560938,2,['test'],['test']
Testability,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here?. Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003:19,test,test,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003,1,['test'],['test']
Testability,"Yeah, plotting in python is difficult, and our plotting code doesn’t make things simpler. This is the relevant part:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_anndata.py#L273-L284. The code that behaves like advertised in the docs is in here, but that function does more things after that:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_utils.py#L364. There’s also this:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_tools/scatterplots.py#L1181. I think things should be unified so they use the same palette selection logic. But I understand that that’s a pretty complex part of our code base. I meant this comment: https://github.com/scverse/scanpy/issues/1258#issuecomment-1626690231",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114:677,log,logic,677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114,1,['log'],['logic']
Testability,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:140,benchmark,benchmarking,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169,4,"['benchmark', 'log']","['benchmarking', 'log', 'logarithmized']"
Testability,"\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\_typing.py in <module>; 82 # array-like; 83 ; ---> 84 ArrayLike = Union[""ExtensionArray"", np.ndarray]; 85 AnyArrayLike = Union[ArrayLike, ""Index"", ""Series""]; 86 . AttributeError: module 'numpy' has no attribute 'ndarray'; ```; It looks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:7470,log,logging,7470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['log'],['logging']
Testability,"^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:8987,test,test,8987,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:7712,test,test,7712,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:9786,test,test,9786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"```; sc.logging.print_versions(); scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```. ```; adata_2.raw.shape; > (5558, 2000); adata_2.X.shape; > (5558, 2000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665:8,log,logging,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665,1,['log'],['logging']
Testability,"`log_transformed` disappeared in the last commit here... Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Also: If trying to call a t-test with non-logarithmized data, a warning should be written. The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809:250,test,test,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809,3,"['log', 'test']","['logarithmized', 'test']"
Testability,"again no issue but the duplication and i; analysed most of them are from previous code. Regards,; Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great!; >; > The only duplicated code left is that _prepare_weighted_dataframe is very; > similar to _prepare_dataframe. I think you can delete; > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return; > categories, obs_tidy, categorical. Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:1068,test,tests,1068,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,1,['test'],['tests']
Testability,"all very good points, and I don't think I have a clear solution, it's more that we have to decide how to go about this:; > Question for this, what heuristics have you tried? My guess would be that min(distances_between_points) / 3 should be fine for an upper bound. this indeed could be solved, but we still would have to set this heuristics differently according to the spatial data type in question. E.g. for visium `size=1` is correct, because coordinates are in pixel measure. In the dataset I have now (seqFISH) the coordinates are essentially z-score and so would have to change for instance to the one you proposed. However, why would we want to have `circle` at all in that t case, and not just scatterplot? Since there is no real notion of size, I think a scatterplot is actually more appropriate. We discussed this already but back then we didn't have this example. > Second, I think this logic is a little convoluted, and I don't know that library_id will always be associated with visium only. Would a better check be for [""metadata""][""software_version""] or something like that?. It definitely is, there might be a better solution but I couldn't come up with it. The problem stems in the fact that we have a `library_id` key in `adata.uns[""spatial""]`. In `spatial` we also put this; ```python; {'connectivities_key': 'spatial_connectivities',; 'distances_key': 'spatial_distances',; 'params': {'n_neighbors': 6, 'coord_type': None, 'radius': None}}; ``` ; this is needed for plotting. The default in `sc.pl.spatial` now is that if library_id is `_empty`, then it iteratively search for some keys in `adata.uns[""spatial""]`.; ```python; try: # check if key is empty; spatial_data = adata.uns['spatial']; library_id = next(; (; i; for i in spatial_data.keys(); if i not in [""connectivities_key"", ""distances_key""]; ); ); except (KeyError, StopIteration) as e:; 	library_id = None; ```; The point is that it should only assign whatever key it finds that is not `[""connectivities_key"", ""distance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626:899,log,logic,899,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626,1,['log'],['logic']
Testability,"bs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""test.py"", line 14, in <module>; adata = adata[adata.obs['n_genes'] > 100, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/annd",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:2128,test,test,2128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,1,['test'],['test']
Testability,"budev1 (= 249.6-2),; libunistring2 (= 0.9.10-6),; libuuid1 (= 2.37.2-4),; libwayland-client0 (= 1.19.0-2+b1),; libwayland-cursor0 (= 1.19.0-2+b1),; libwayland-egl1 (= 1.19.0-2+b1),; libwebp6 (= 0.6.1-2.1),; libwebpdemux2 (= 0.6.1-2.1),; libwebpmux3 (= 0.6.1-2.1),; libx11-6 (= 2:1.7.2-2+b1),; libx11-data (= 2:1.7.2-2),; libx265-199 (= 3.5-2),; libxau6 (= 1:1.0.9-1),; libxcb-render0 (= 1.14-3),; libxcb-shm0 (= 1.14-3),; libxcb1 (= 1.14-3),; libxcomposite1 (= 1:0.4.5-1),; libxcursor1 (= 1:1.2.0-2),; libxdamage1 (= 1:1.1.5-2),; libxdmcp6 (= 1:1.1.2-3),; libxext6 (= 2:1.3.4-1),; libxfixes3 (= 1:5.0.3-2),; libxft2 (= 2.3.2-2),; libxi6 (= 2:1.8-1),; libxinerama1 (= 2:1.1.4-2),; libxkbcommon0 (= 1.3.1-1),; libxml2 (= 2.9.12+dfsg-5),; libxrandr2 (= 2:1.5.2-1),; libxrender1 (= 1:0.9.10-1),; libxss1 (= 1:1.2.3-1),; libz3-4 (= 4.8.12-1+b1),; libzstd1 (= 1.4.8+dfsg-3),; linux-libc-dev (= 5.14.16-1),; llvm-11 (= 1:11.1.0-4),; llvm-11-linker-tools (= 1:11.1.0-4),; llvm-11-runtime (= 1:11.1.0-4),; login (= 1:4.8.1-2),; lsb-base (= 11.1.0),; m4 (= 1.4.18-5),; mailcap (= 3.70),; make (= 4.3-4.1),; man-db (= 2.9.4-2),; mawk (= 1.3.4.20200120-2),; media-types (= 4.0.0),; mime-support (= 3.66),; mount (= 2.37.2-4),; ncurses-base (= 6.2+20210905-1),; ncurses-bin (= 6.2+20210905-1),; openssl (= 1.1.1l-1),; passwd (= 1:4.8.1-2),; patch (= 2.7.6-7),; perl (= 5.32.1-6),; perl-base (= 5.32.1-6),; perl-modules-5.32 (= 5.32.1-6),; po-debconf (= 1.0.21+nmu1),; procps (= 2:3.3.17-5),; python-matplotlib-data (= 3.3.4-2),; python-tables-data (= 3.6.1-5),; python3 (= 3.9.7-1),; python3-all (= 3.9.7-1),; python3-anndata (= 0.7.5+ds-3),; python3-asciitree (= 0.3.3-3),; python3-attr (= 20.3.0-1),; python3-certifi (= 2020.6.20-1),; python3-cffi-backend (= 1.15.0-1),; python3-chardet (= 4.0.0-1),; python3-cov-core (= 1.15.0-3),; python3-coverage (= 5.1+dfsg.1-2+b2),; python3-cryptography (= 3.3.2-1),; python3-cycler (= 0.11.0-1),; python3-dateutil (= 2.8.1-6),; python3-decorator (= 4.4.2-2),; python3-dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:10087,log,login,10087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['log'],['login']
Testability,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4533,test,tests,4533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,3,"['log', 'test']","['logreg', 'test', 'tests']"
Testability,"ctice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1380,log,log,1380,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,2,"['assert', 'log']","['assert', 'log']"
Testability,"d to figure out what’s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(…)` to `categories, obs_tidy, _ = _prepare_dataframe(…)`. Other than that, there’s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so!; 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace “xyz” with whatever you want):. ```py; def test_xyz(image_comparer):; save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); […]; sc.pl.xyz(adata, …); save_and_compare_images('xyz'); ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesn’t exist. You need to copy the pngs from `scanpy/tests/figures`→`scanpy/test/_images` and `git commit` them.; 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144; 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data?. @Khalid-Usman I’m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you won’t regret doing this. You’re learning good coding practices here that will come in handy in the future, I promise!. Thank you for your con",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412:1046,test,test,1046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412,1,['test'],['test']
Testability,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells.; I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109:392,test,tests,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109,2,"['benchmark', 'test']","['benchmark', 'tests']"
Testability,"data (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5662,test,test,5662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"does not recompute, simply saves the filtered data under; adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be; tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>; wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups; > result? Or does it recompute? The former would not alleviate the multiple; > testing burden.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471643748:134,test,tested,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471643748,2,['test'],"['tested', 'testing']"
Testability,"e 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/python3.12/site-packages/dask/threaded.py"", line 90 in get; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 662 in compute; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 376 in compute; File ""~/Dev/scanpy/tests/test_utils.py"", line 243 in test_is_constant_dask; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 159 in pytest_pyfunc_call; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 1627 in runtest; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 174 in pytest_runtest_call; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 242 in <lambda>; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:4831,test,tests,4831,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['test'],['tests']
Testability,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2561,benchmark,benchmark,2561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,2,['benchmark'],['benchmark']
Testability,"e-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:8411,test,test,8411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"e[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5544,assert,assert,5544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['assert'],['assert']
Testability,"e_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip installed by pip? False; Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:10167,test,test,10167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,2,['test'],['test']
Testability,"er(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 impor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:6812,test,tests,6812,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['test'],['tests']
Testability,"er(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:1808,log,logging,1808,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['log'],"['logg', 'logging']"
Testability,"ering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1633,test,tests,1633,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,4,['test'],"['testing', 'tests']"
Testability,"esolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func();",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:7174,test,test,7174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"esolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:7346,test,test,7346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:390,test,testing,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,1,['test'],['testing']
Testability,"exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/command",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4950,test,test,4950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"format 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:1917,log,logical,1917,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['log'],['logical']
Testability,"g an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test).; ```; def my_scale_function(X, clip=False):; # need to make a copy of X; Y = X.copy(); mean, var = mean_var(Y, axis=0); Y -= mean; std = np.sqrt(var); #std[std == 0] = 1; Y /= std; if clip:; Y = np.clip(X, -10, 10); return np.matrix(Y); ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```; ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled)); ```. ```; Do a numpy check for closeness of floats:; False; ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that.; ```; adata.X.var(0); ```. ```; array([0.9996213 , 0.97964925, 0.29805112, ..., 0.78701097, 0.9980862 ,; 0.9996219 ], dtype=float32); ```; This could happen if e.g. cells were used to scale gene expression, which were later discarded in quality control. So when calling `my_scale_function` or `sc.pp.scale`, we expect the cell-by-gene matrix to change at first. ```; mtx_rescaled_sc = sc.pp.scale(adata.X, copy=True). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled_sc)); ```; ```; Do a numpy check for closeness of floats:; False; ```; But not anymore if we call `sc.pp.scale` again. ```; mtx_rescaled_sc_II = sc.pp.scale(mtx_rescaled_sc, copy=True). print(""Do a numpy check for closeness of floats:""); print(np.allclose(mtx_rescaled_sc, mtx_rescaled_sc_II)); ```. ```; Do a numpy check for closeness of floats:; True; ```. This is the behavi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273:1809,test,test,1809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273,1,['test'],['test']
Testability,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1976,test,test,1976,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,4,['test'],"['test', 'testing']"
Testability,"hon 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1231,assert,assert,1231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['assert'],['assert']
Testability,"ioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", lin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5853,test,test,5853,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"ip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5476,test,test,5476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"ll.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:6963,test,test,6963,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"m setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5124,test,test,5124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:1790,test,tests,1790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,2,"['log', 'test']","['logic', 'tests']"
Testability,"miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:7998,test,test,7998,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"n3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 172, in _add_to_criteria; if not criterion.candidates:; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py"", line 151, in __bool__; return bool(self._sequence); ^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:6723,test,test,6723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,nssl 3.1.1 hd590300_1 conda-forge; opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge; optax 0.1.5 pyhd8ed1ab_0 conda-forge; ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge; orjson 3.9.2 py310h1e2579a_0 conda-forge; packaging 23.1 pyhd8ed1ab_0 conda-forge; pandas 2.0.3 py310h7cbd5c2_1 conda-forge; parso 0.8.3 pyhd8ed1ab_0 conda-forge; patsy 0.5.3 pyhd8ed1ab_0 conda-forge; pcre2 10.40 hc3806b6_0 conda-forge; pexpect 4.8.0 pyh1a96a4e_2 conda-forge; pickleshare 0.7.5 py_1003 conda-forge; pillow 9.4.0 py310h023d228_1 conda-forge; pip 23.2 pyhd8ed1ab_0 conda-forge; pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge; pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge; platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge; poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge; poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge; poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge; pooch 1.7.0 pyha770c72_3 conda-forge; prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-editor 1.0.4 py_0 conda-forge; python-igraph 0.10.6 py310h33b8572_0 conda-forge; python-installer 0.7.0 pyhd8ed1ab_0 conda-forge; python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:17624,stub,stubs,17624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['stub'],['stubs']
Testability,"odepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1585,test,tests,1585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['test'],['tests']
Testability,"or not isinstance(orig_clusters, Iterable):; orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]; sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering; ; new_clustering = adata.obs[orig_key].copy(); # Make new names; new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str); new_clustering.cat.add_categories(new_clusters, inplace=True); new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata; adata.obs[key_added] = new_clustering; ```. <details>; <summary> Or if you wanted something more generic: </summary>. ```python; from typing import Callable, Collection; from anndata import AnnData. def subcluster(; cluster_func: Callable[[AnnData], Collection],; adata,; orig_key,; orig_clusters,; key_added,; ):; """"""; Params; ------; cluster_func; Function that produces a clustering of observations from an anndata object.; adata; orig_key; Key in adata.obs for original clustering; orig_clusters; Set of clusters to subset to before clustering.; key_added; Key in obs to add resulting clustering to.; """"""; if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):; orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]; sub_clustering = (; pd.Categorical(cluster_func(subset)); .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)); ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment; new_clustering = (; adata.obs[orig_key].astype(""category"", copy=True); .cat.add_categories(sub_clustering.categories); ); new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata; adata.obs[key_added] = new_clustering; ; from functools import partial. subcluster_kmeans = partial(; subcluster,; lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X); ); ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:5575,assert,assert,5575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['assert'],['assert']
Testability,"pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:8772,test,test,8772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"prepare_weighted_dataframe and just change _prepare_dataframe so it does return; > categories, obs_tidy, categorical. Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding practices here that will come in handy in; > the future, I promise!; >; > Thank you for your contribution 🎉; >; > —; > You are receiving this because you were mentioned.; > Reply to this email direc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:1372,test,tests,1372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,3,['test'],"['test', 'tests']"
Testability,"rocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 204, in _get_updated_criteria; self._add_to_criteria(criteria, requirement, parent=candidate); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vend",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:6242,test,test,6242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,rty | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0.9.4; testpath | 0.5.0; texttable | 1.6.4; threadpoolctl | 3.0.0; tornado | 6.1; tqdm | 4.62.3; traitlets | 4.3.3; typing-extensions | 3.10.0.2; **umap-learn | 0.5.2**; urllib3 | 1.26.7; wcwidth | 0.2.5; webencodings | 0.5.1; wheel | 0.37.0; win-inet-pton | 1.1.0; wincertstore | 0.2; xlrd | 1.2.0; zipp | 3.6.0. </body>. </html>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:3721,test,testpath,3721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['test'],['testpath']
Testability,"ry: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:2957,test,test,2957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"s and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1419,test,testing,1419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['test'],['testing']
Testability,"s constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already done this way in `openTSNE`, I think this could help with that goal. > From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). `Seurat` does this, but I'm not sure this has been standardized much otherwise. Off the top of my head, I'd guess that the Jaccard method is going to be much more sensitive to `k` as a parameter, particularly how it relates to cluster size. I'm not really aware of any consensus made on this or benchmarks comparing approaches. When I've looked into it, using the weights (as opposed to just binarized edges) seems to give more stable results, particularly for small clusters. We've had some previous discussions on this here: #586, #240.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:2549,benchmark,benchmarks,2549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,1,['benchmark'],['benchmarks']
Testability,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:1760,log,log,1760,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['log'],['log']
Testability,"s, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding practices here that will come in handy in; > the future, I promise!; >; > Thank you for your contribution 🎉; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:1591,test,tests,1591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,2,['test'],"['test', 'tests']"
Testability,"s/pip/_internal/resolution/resolvelib/found_candidates.py"", line 155, in __bool__; return any(self); ^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 143, in <genexpr>; return (c for c in iterator if id(c) not in self._incompatible_ids); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py"", line 47, in _iter_built; candidate = func(); ^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:8578,test,test,8578,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:440,benchmark,benchmark,440,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,1,['benchmark'],['benchmark']
Testability,"sers/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip installed by pip? False; Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:9983,test,test,9983,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed correctly and modified, as well as a scale_basis argument to match the image coordinate if needed. That's it, looking forward to hear what are your thoughts and if you agree with current behaviour I'll go on and changing the docs as well as writing more tests (especially for first example where `groups` is used, where the background color is different)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:4842,test,tests,4842,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,1,['test'],['tests']
Testability,"t h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; imp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:6363,log,logging,6363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['log'],"['logg', 'logging']"
Testability,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:295,test,tests,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756,1,['test'],['tests']
Testability,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:1062,test,testing,1062,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['test'],['testing']
Testability,"the problem is that at some point the code calls np.argsort on a; pandas.Series and this returns -1 for NaN values. I will submit a PR to fix; this. Meanwhile, simpy plot as follows: sc.pl.pca(adata, color='property',; size=50, sort_order=False). On Tue, Mar 19, 2019 at 11:45 AM Fabian Rost <notifications@github.com>; wrote:. > I just add the versions I used:; >; > sc.logging.print_versions(); >; > scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/536#issuecomment-474301705>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b3l_xKITwuZQ-XpENUHSioDqLWLks5vYL_TgaJpZM4b6BYA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/536#issuecomment-474344606:371,log,logging,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536#issuecomment-474344606,1,['log'],['logging']
Testability,"the runtime checks would be too costly or impossible. to test for `List[int]`, you’d have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just can’t test it at all – if you’d iterate the thing to check the objects it yields, you exhaust it and it’s no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064:57,test,test,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064,2,['test'],['test']
Testability,"think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison.; > ; > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:1038,log,logFC,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,1,['log'],['logFC']
Testability,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2099,log,log,2099,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,2,['log'],"['log', 'logicle']"
Testability,"to make formatting easier, you should do `pre-commit install` like mentioned in the contributor guide: https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#pre-commit. Please also fill out the checklist:. - if there is an issue closed by this, please link it; - you can check the box for tests, since this function already has tests. I added the `benchmark` label so we see that it actually makes things faster; - what’s missing is a release note entry: just edit `1.10.2.md` and add a line there please",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949:299,test,tests,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949,3,"['benchmark', 'test']","['benchmark', 'tests']"
Testability,"uild/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:6036,test,test,6036,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"ution/resolvelib/factory.py"", line 206, in _make_candidate_from_link; self._link_candidate_cache[link] = LinkCandidate(; ^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 297, in __init__; super().__init__(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 162, in __init__; self.dist = self._prepare(); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 231, in _prepare; dist = self._prepare_distribution(); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py"", line 308, in _prepare_distribution; return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:9297,test,test,9297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['test'],['test']
Testability,"what’s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(…)` to `categories, obs_tidy, _ = _prepare_dataframe(…)`. Other than that, there’s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so!; 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace “xyz” with whatever you want):. ```py; def test_xyz(image_comparer):; save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); […]; sc.pl.xyz(adata, …); save_and_compare_images('xyz'); ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesn’t exist. You need to copy the pngs from `scanpy/tests/figures`→`scanpy/test/_images` and `git commit` them.; 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144; 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data?. @Khalid-Usman I’m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you won’t regret doing this. You’re learning good coding practices here that will come in handy in the future, I promise!. Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412:1284,test,tests,1284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412,9,['test'],"['test', 'tests']"
Testability,"xis()`. See following point. --------------------. > When spatial is called, it’s always shapes being drawn on an image. If there isn’t an image passed, an empty image would be generated. There would be no scatter plot case here. I played around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3355,test,test,3355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,1,['test'],['test']
Testability,"yeah, a fix could simply be. ```diff; -return np.nan_to_num(dispersion_norm) >= disp_cut_off; +return np.nan_to_num(dispersion_norm, nan=-np.inf) >= disp_cut_off; ```. but I have a hard time coming up with a test. Doing something like this doesn’t work, as it crashes earlier,; with something like “ValueError: cannot specify integer `bins` when input data contains infinity”. ```py; @pytest.mark.parametrize(""flavor"", [""seurat"", ""cell_ranger""]); def test_no_filter_genes(flavor):; """"""Test that even with 0 columns in the data, n_top_genes is respected.""""""; adata = pbmc3k(); means, _ = _get_mean_var(adata.X); assert (means == 0).any(); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); assert adata.var[""highly_variable""].sum() == 10000; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2252862491:208,test,test,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2252862491,3,"['assert', 'test']","['assert', 'test']"
Testability,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4930,test,tested,4930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['test'],['tested']
Usability," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:2205,learn,learn,2205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['learn'],['learn']
Usability," Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your conce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1469,learn,learn,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['learn'],['learn']
Usability," Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.3.2); Requirement already satisfied: fonttools>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninsta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4539,learn,learn,4539,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['learn'],['learn']
Usability," Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1001,clear,cleared,1001,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['clear'],['cleared']
Usability," behaviour of scale a little more.; The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1031,undo,undocumented,1031,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330,1,['undo'],['undocumented']
Usability," claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found.; 2. Should people be using these values for visualization?; 3. What's the workflow if people need both log normalized and pearson residuals?; 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:2003,clear,clear,2003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693,2,['clear'],['clear']
Usability," effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1850,simpl,simply,1850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['simpl'],['simply']
Usability," is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this clearly contradicts the fundamental results found in all the Coifman papers. Having said that: if the code is simple, I don't mind at all to have the possibility that you suggest, @ivirshup. Please go ahead with a pull request and I'll see whether the changes are simple enough. The user will still use the default plain knn version, which is also what is done in Seurat. But my philosophy rests the same: rather than engineering the clustering or any other aspect of the manifold analysis, one should engineer the representation. Sorry that this got a bit length and confused. I hope I addressed everything but I feel I'm still missing something - I'm currently working through a lot of issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:2371,clear,clearly,2371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,3,"['clear', 'simpl']","['clearly', 'simple']"
Usability," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:4776,learn,learn,4776,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,2,['learn'],['learn']
Usability," not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:2777,simpl,simple,2777,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['simpl'],['simple']
Usability," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:1606,clear,clear,1606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,1,['clear'],['clear']
Usability, py_0 ; r 3.6.0 r36_0 r; r-base 3.6.1 haffb61f_2 r; r-boot 1.3_20 r36h6115d3f_0 r; r-class 7.3_15 r36h96ca727_0 r; r-cluster 2.0.8 r36ha65eedd_0 r; r-codetools 0.2_16 r36h6115d3f_0 r; r-foreign 0.8_71 r36h96ca727_0 r; r-kernsmooth 2.23_15 r36ha65eedd_4 r; r-lattice 0.20_38 r36h96ca727_0 r; r-mass 7.3_51.3 r36h96ca727_0 r; r-matrix 1.2_17 r36h96ca727_0 r; r-mgcv 1.8_28 r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:15827,learn,learn-extra,15827,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['learn'],['learn-extra']
Usability," setuptools’ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what it’s for. I’d probably judge that we don’t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that we’re dealing with python packages/modules here, but that’s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, … apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that makes it easier to reason about how our test suite works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1732,clear,clear,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,2,"['clear', 'simpl']","['clear', 'simple']"
Usability," them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the cod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1402,simpl,simple,1402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['simpl'],['simple']
Usability," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1700,learn,learn,1700,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021,2,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2201?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@2c55a14`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2201 +/- ##; ========================================; Coverage ? 71.95% ; ========================================; Files ? 98 ; Lines ? 11538 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3236 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2201#issuecomment-1086301791:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2201#issuecomment-1086301791,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2202?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@2c55a14`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2202 +/- ##; ========================================; Coverage ? 71.95% ; ========================================; Files ? 98 ; Lines ? 11538 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3236 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2202#issuecomment-1086316533:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2202#issuecomment-1086316533,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2213?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@3ac9169`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 9c0054f differs from pull request most recent head 02123f2. Consider uploading reports for the commit 02123f2 to get more accurate results. ```diff; @@ Coverage Diff @@; ## 1.9.x #2213 +/- ##; ========================================; Coverage ? 71.94% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3237 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2213#issuecomment-1088659791:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2213#issuecomment-1088659791,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2221?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@6cc7541`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2221 +/- ##; ========================================; Coverage ? 71.94% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8302 ; Misses ? 3237 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2221#issuecomment-1088855272:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2221#issuecomment-1088855272,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2226?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a08c155`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2226 +/- ##; ========================================; Coverage ? 71.82% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8288 ; Misses ? 3251 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2226#issuecomment-1090215514:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2226#issuecomment-1090215514,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2241?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`master@cab9f78`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2241 +/- ##; =========================================; Coverage ? 71.74% ; =========================================; Files ? 99 ; Lines ? 11560 ; Branches ? 0 ; =========================================; Hits ? 8294 ; Misses ? 3266 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2241#issuecomment-1104534706:275,learn,learn,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2241#issuecomment-1104534706,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2275?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@5bcb539`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2275 +/- ##; ========================================; Coverage ? 71.82% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8288 ; Misses ? 3251 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2275#issuecomment-1156972689:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2275#issuecomment-1156972689,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2279?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@4d0d8be`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.9.x #2279 +/- ##; ========================================; Coverage ? 71.82% ; ========================================; Files ? 98 ; Lines ? 11539 ; Branches ? 0 ; ========================================; Hits ? 8288 ; Misses ? 3251 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2279#issuecomment-1157013971:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2279#issuecomment-1157013971,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2348?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@6b5f786`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 6fb5f26 differs from pull request most recent head fb557a1. Consider uploading reports for the commit fb557a1 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2348 +/- ##; ========================================; Coverage ? 71.77% ; ========================================; Files ? 97 ; Lines ? 11519 ; Branches ? 0 ; ========================================; Hits ? 8268 ; Misses ? 3251 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2348#issuecomment-1273857794:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2348#issuecomment-1273857794,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2350?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@cf6d820`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2350 +/- ##; ========================================; Coverage ? 71.77% ; ========================================; Files ? 97 ; Lines ? 11519 ; Branches ? 0 ; ========================================; Hits ? 8268 ; Misses ? 3251 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2350#issuecomment-1274792439:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2350#issuecomment-1274792439,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2419?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@97c2617`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 72ea692 differs from pull request most recent head 8fb038a. Consider uploading reports for the commit 8fb038a to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2419 +/- ##; ========================================; Coverage ? 71.83% ; ========================================; Files ? 98 ; Lines ? 11543 ; Branches ? 0 ; ========================================; Hits ? 8292 ; Misses ? 3251 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2419#issuecomment-1433063184:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2419#issuecomment-1433063184,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/scverse/scanpy/pull/2435?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1fbbfcd`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2435 +/- ##; ========================================; Coverage ? 71.88% ; ========================================; Files ? 98 ; Lines ? 11546 ; Branches ? 0 ; ========================================; Hits ? 8300 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2435#issuecomment-1451099582:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2435#issuecomment-1451099582,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`master@62bb643`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `74.48%`. ```diff; @@ Coverage Diff @@; ## master #1413 +/- ##; =========================================; Coverage ? 71.34% ; =========================================; Files ? 92 ; Lines ? 11186 ; Branches ? 0 ; =========================================; Hits ? 7981 ; Misses ? 3205 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `70.62% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462:277,learn,learn,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@c943b93`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `71.86%`. [![Impacted file tree graph](https://codecov.io/gh/theislab/scanpy/pull/1693/graphs/tree.svg?width=650&height=150&src=pr&token=UsIEoV0aqg)](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #1693 +/- ##; =========================================; Coverage ? 71.32% ; =========================================; Files ? 89 ; Lines ? 10969 ; Branches ? 0 ; =========================================; Hits ? 7824 ; Misses ? 3145 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_dotplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19kb3RwbG90LnB5) | `86.79% <ø> (ø)` | |; | [scanpy/plotting/\_matrixplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:176,learn,learn,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1920?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8750212`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1920 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1923?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@d0f851f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1923 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1924?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@1605f63`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1924 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8053 ; Misses ? 3193 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1935?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8c51f19`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1935 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1938?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@2883269`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1938 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1947?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@9360422`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1947 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1952?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@c9d319e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1952 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1961?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@aeaa07b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1961 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1962?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@370d1c6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1962 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669,1,['learn'],['learn']
Usability,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1966?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@b9cd8c8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1966 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703:276,learn,learn,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703,1,['learn'],['learn']
Usability,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1330,usab,usable,1330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,1,['usab'],['usable']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2516?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@2979f99`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2516 +/- ##; ========================================; Coverage ? 71.89% ; ========================================; Files ? 98 ; Lines ? 11488 ; Branches ? 0 ; ========================================; Hits ? 8259 ; Misses ? 3229 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2516#issuecomment-1596813650:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2516#issuecomment-1596813650,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2523?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@ec78ca9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 249ef59 differs from pull request most recent head c69bc0f. Consider uploading reports for the commit c69bc0f to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2523 +/- ##; ========================================; Coverage ? 71.89% ; ========================================; Files ? 98 ; Lines ? 11488 ; Branches ? 0 ; ========================================; Hits ? 8259 ; Misses ? 3229 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2523#issuecomment-1599208625:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2523#issuecomment-1599208625,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2528?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@af11c8f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2528 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2528#issuecomment-1604254568:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2528#issuecomment-1604254568,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2538?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@120dcd0`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2538 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2538#issuecomment-1609797792:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2538#issuecomment-1609797792,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2549?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@c40d2ee`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2549 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2549#issuecomment-1625396556:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2549#issuecomment-1625396556,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2567?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@759960d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2567 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2567#issuecomment-1645436066:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2567#issuecomment-1645436066,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2568?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@759960d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2568 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2568#issuecomment-1645437518:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2568#issuecomment-1645437518,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2574?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@053f47e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2574 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2574#issuecomment-1649346773:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2574#issuecomment-1649346773,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2597?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@64fab42`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2597 +/- ##; ========================================; Coverage ? 72.13% ; ========================================; Files ? 104 ; Lines ? 11648 ; Branches ? 0 ; ========================================; Hits ? 8402 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2597#issuecomment-1665680324:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2597#issuecomment-1665680324,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2606?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@b5506e1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2606 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8404 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2606#issuecomment-1670939429:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2606#issuecomment-1670939429,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2613?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a8b931a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2613 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2613#issuecomment-1677226444:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2613#issuecomment-1677226444,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2616?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a8b931a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2616 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8404 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2616#issuecomment-1677462171:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2616#issuecomment-1677462171,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2619?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@fbd73ac`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2619 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2619#issuecomment-1678888447:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2619#issuecomment-1678888447,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2620?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@a6f6c6d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2620 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2620#issuecomment-1681915003:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2620#issuecomment-1681915003,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2638?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@9ba0251`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2638 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2638#issuecomment-1691735842:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2638#issuecomment-1691735842,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2641?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@8aa93e2`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2641 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2641#issuecomment-1691930772:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2641#issuecomment-1691930772,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2643?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@6e8a8b4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2643 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2643#issuecomment-1696930445:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2643#issuecomment-1696930445,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2652?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@16e7e9f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2652 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2652#issuecomment-1706132242:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2652#issuecomment-1706132242,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2657?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > Merging [#2657](https://app.codecov.io/gh/scverse/scanpy/pull/2657?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) (2bf5f18) into [master](https://app.codecov.io/gh/scverse/scanpy/commit/3a50e60a77ced96d877448c6d9f8c27705ae949e?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) (3a50e60) will **not change** coverage.; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #2657 +/- ##; =======================================; Coverage 72.16% 72.16% ; =======================================; Files 108 108 ; Lines 11906 11906 ; =======================================; Hits 8592 8592 ; Misses 3314 3314 ; ```. | [Files Changed](https://app.codecov.io/gh/scverse/scanpy/pull/2657?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Coverage Δ | |; |---|---|---|; | [scanpy/tools/\_rank\_genes\_groups.py](https://app.codecov.io/gh/scverse/scanpy/pull/2657?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL19yYW5rX2dlbmVzX2dyb3Vwcy5weQ==) | `94.23% <ø> (ø)` | |. </details>; :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2657#issuecomment-1711823534:1546,feedback,feedback,1546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2657#issuecomment-1711823534,2,['feedback'],['feedback']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2659?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@efce8f8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head eea03bb differs from pull request most recent head 256ce44. Consider uploading reports for the commit 256ce44 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2659 +/- ##; ========================================; Coverage ? 72.18% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8405 ; Misses ? 3238 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2659#issuecomment-1712163821:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2659#issuecomment-1712163821,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2676?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@fc498c3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2676 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 104 ; Lines ? 11640 ; Branches ? 0 ; ========================================; Hits ? 8381 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2676#issuecomment-1753090073:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2676#issuecomment-1753090073,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2677?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@46969b4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2677 +/- ##; ========================================; Coverage ? 71.69% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8347 ; Misses ? 3296 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2677#issuecomment-1753261271:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2677#issuecomment-1753261271,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2686?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@418baff`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 503876d differs from pull request most recent head 5c315d4. Consider uploading reports for the commit 5c315d4 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2686 +/- ##; ========================================; Coverage ? 71.98% ; ========================================; Files ? 104 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8381 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2686#issuecomment-1764071999:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2686#issuecomment-1764071999,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2687?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`master@8353e45`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 601888e differs from pull request most recent head f257b7f. Consider uploading reports for the commit f257b7f to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #2687 +/- ##; =========================================; Coverage ? 71.97% ; =========================================; Files ? 108 ; Lines ? 11907 ; Branches ? 0 ; =========================================; Hits ? 8570 ; Misses ? 3337 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2687#issuecomment-1764131250:280,learn,learn,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2687#issuecomment-1764131250,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2690?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@3c15b99`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2690 +/- ##; ========================================; Coverage ? 71.99% ; ========================================; Files ? 104 ; Lines ? 11643 ; Branches ? 0 ; ========================================; Hits ? 8382 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2690#issuecomment-1764724800:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2690#issuecomment-1764724800,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2697?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@d71a4a9`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2697 +/- ##; ========================================; Coverage ? 72.01% ; ========================================; Files ? 104 ; Lines ? 11656 ; Branches ? 0 ; ========================================; Hits ? 8394 ; Misses ? 3262 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2697#issuecomment-1766637428:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2697#issuecomment-1766637428,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2721?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@05405f1`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2721 +/- ##; ========================================; Coverage ? 72.03% ; ========================================; Files ? 104 ; Lines ? 11659 ; Branches ? 0 ; ========================================; Hits ? 8398 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2721#issuecomment-1785251436:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2721#issuecomment-1785251436,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2722?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@4936b7e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2722 +/- ##; ========================================; Coverage ? 72.03% ; ========================================; Files ? 104 ; Lines ? 11659 ; Branches ? 0 ; ========================================; Hits ? 8398 ; Misses ? 3261 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2722#issuecomment-1787308099:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2722#issuecomment-1787308099,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2727?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@0b624b0`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2727 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 103 ; Lines ? 11641 ; Branches ? 0 ; ========================================; Hits ? 8382 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2727#issuecomment-1787571772:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2727#issuecomment-1787571772,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2728?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1083b36`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2728 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 103 ; Lines ? 11641 ; Branches ? 0 ; ========================================; Hits ? 8382 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2728#issuecomment-1787613419:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2728#issuecomment-1787613419,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2738?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@d1fe8da`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2738 +/- ##; ========================================; Coverage ? 72.00% ; ========================================; Files ? 103 ; Lines ? 11642 ; Branches ? 0 ; ========================================; Hits ? 8383 ; Misses ? 3259 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2738#issuecomment-1801509928:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2738#issuecomment-1801509928,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2751?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@ec4d79f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2751 +/- ##; ========================================; Coverage ? 71.87% ; ========================================; Files ? 103 ; Lines ? 11635 ; Branches ? 0 ; ========================================; Hits ? 8363 ; Misses ? 3272 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2751#issuecomment-1808250177:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2751#issuecomment-1808250177,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2752?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@295d889`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2752 +/- ##; ========================================; Coverage ? 72.10% ; ========================================; Files ? 103 ; Lines ? 11635 ; Branches ? 0 ; ========================================; Hits ? 8389 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2752#issuecomment-1808445091:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2752#issuecomment-1808445091,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2774?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@70d55d5`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2774 +/- ##; ========================================; Coverage ? 72.11% ; ========================================; Files ? 103 ; Lines ? 11640 ; Branches ? 0 ; ========================================; Hits ? 8394 ; Misses ? 3246 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2774#issuecomment-1838034698:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2774#issuecomment-1838034698,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2783?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; > :exclamation: No coverage uploaded for pull request base (`1.9.x@4058e36`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit).; > The diff coverage is `n/a`. > :exclamation: Current head 09a432a differs from pull request most recent head 3b44d11. Consider uploading reports for the commit 3b44d11 to get more accurate results. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2783 +/- ##; ========================================; Coverage ? 17.51% ; ========================================; Files ? 103 ; Lines ? 11645 ; Branches ? 0 ; ========================================; Hits ? 2040 ; Misses ? 9605 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2783#issuecomment-1860972298:279,learn,learn,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2783#issuecomment-1860972298,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2796?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`1.9.x@1daae5b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2796 +/- ##; ========================================; Coverage ? 71.35% ; ========================================; Files ? 103 ; Lines ? 11645 ; Branches ? 0 ; ========================================; Hits ? 8309 ; Misses ? 3336 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2796#issuecomment-1870329572:353,learn,learn,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2796#issuecomment-1870329572,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2812?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`1.9.x@518e76a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.9.x #2812 +/- ##; ========================================; Coverage ? 71.34% ; ========================================; Files ? 103 ; Lines ? 11632 ; Branches ? 0 ; ========================================; Hits ? 8299 ; Misses ? 3333 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2812#issuecomment-1893321168:353,learn,learn,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2812#issuecomment-1893321168,1,['learn'],['learn']
Usability,## [Codecov](https://app.codecov.io/gh/scverse/scanpy/pull/2972?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; All modified and coverable lines are covered by tests :white_check_mark:; > :exclamation: No coverage uploaded for pull request base (`main@3ceb740`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#section-missing-base-commit). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #2972 +/- ##; =======================================; Coverage ? 75.49% ; =======================================; Files ? 116 ; Lines ? 12911 ; Branches ? 0 ; =======================================; Hits ? 9747 ; Misses ? 3164 ; Partials ? 0 ; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2972#issuecomment-2029918769:370,learn,learn,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2972#issuecomment-2029918769,1,['learn'],['learn']
Usability,"## `X`. So, the first and third result have the same values of `X`. . ```python; assert np.array_equal(pc1.X, pc3.X); ```. The second is only slightly different:. ```python; diff = pc2.X - pc1.X; diff[diff != 0]; ```. ```; array([7.450581e-09], dtype=float32); ```. This might be adjustable by setting ""regress out"" to a fixed number of jobs. ## PCA. The results of the PCA differ more significantly, but here you should just be calling scikit-learn's implementation. Could you try calling that directly and letting us know the results?. E.g. ```python; from sklearn.decomposition import PCA. pca = PCA(n_components=50, solver=""arpack"", random_state=0); result = pca.fit_transform(adata.X); ```. If this doesn't give consistent results on your machines, the issue is upstream in scikit-learn. If you need reproducibility now, I would suggest switching out the solver for the PCA and/ or using 64 bit values for `X`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114#issuecomment-1021048765:444,learn,learn,444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114#issuecomment-1021048765,2,['learn'],['learn']
Usability,"'>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ld_impl_linux-64 conflicts for:; python=3.7 -> ld_impl_linux-64; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package louvain conflicts for:; scanpy -> louvain; Package pip conflicts for:; python=3.7 -> pip; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package libgcc-ng conflicts for:; python=3.7 -> libgcc-ng[version='>=7.2.0|>=7.3.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:13542,learn,learn,13542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['learn'],['learn']
Usability,",0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tool",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1505,learn,learning,1505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,1,['learn'],['learning']
Usability,"-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2197,clear,clear,2197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['clear'],['clear']
Usability,". in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > that this takes long and is frustrating. If this is the case, just step; > away for a while and do something else! But I think you won’t regret doing; > this. You’re learning good coding practices here that will come in handy in; > the future, I promise!; >; > Thank you for your contribution 🎉; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:2167,learn,learning,2167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,1,['learn'],['learning']
Usability,".0-1),; python3-pluggy (= 0.13.0-7.1),; python3-ply (= 3.11-5),; python3-py (= 1.10.0-1),; python3-pygments (= 2.7.1+dfsg-2.1),; python3-pyparsing (= 2.4.7-1),; python3-pytest (= 6.2.5-1),; python3-pytest-cov (= 3.0.0-1),; python3-pytoml (= 0.1.21-1),; python3-requests (= 2.25.1+dfsg-2),; python3-roman (= 3.3-1),; python3-scipy (= 1.7.1-2),; python3-seaborn (= 0.11.2-2),; python3-secretstorage (= 3.3.1-1),; python3-setuptools (= 58.2.0-1),; python3-setuptools-scm (= 6.0.1-1),; python3-sinfo (= 0.3.1-2),; python3-six (= 1.16.0-2),; python3-sklearn (= 0.23.2-5),; python3-sklearn-lib (= 0.23.2-5),; python3-skmisc (= 0.1.4+dfsg-1),; python3-slimit (= 0.8.1-4),; python3-statsmodels (= 0.12.2-2),; python3-statsmodels-lib (= 0.12.2-2),; python3-stdlib-list (= 0.8.0-4),; python3-tables (= 3.6.1-5),; python3-tables-lib (= 3.6.1-5),; python3-texttable (= 1.6.3-2),; python3-threadpoolctl (= 2.1.0-1),; python3-tk (= 3.9.8-1),; python3-toml (= 0.10.2-1),; python3-tqdm (= 4.57.0-2),; python3-tz (= 2021.3-1),; python3-urllib3 (= 1.26.5-1~exp1),; python3-wheel (= 0.34.2-1),; python3-zarr (= 2.10.2+ds-2),; python3-zipp (= 1.0.0-3),; python3.9 (= 3.9.8-2),; python3.9-minimal (= 3.9.8-2),; readline-common (= 8.1-2),; rpcsvc-proto (= 1.4.2-4),; sed (= 4.8-1),; sensible-utils (= 0.0.17),; sgml-base (= 1.30),; shared-mime-info (= 2.0-1),; sphinx-rtd-theme-common (= 1.0.0+dfsg-1),; systemd (= 249.6-2),; systemd-sysv (= 249.6-2),; sysvinit-utils (= 3.00-1),; tar (= 1.34+dfsg-1),; tk8.6-blt2.5 (= 2.5.3+dfsg-4.1),; ttf-bitstream-vera (= 1.10-8.1),; tzdata (= 2021e-1),; ucf (= 3.0043),; umap-learn (= 0.4.5+dfsg-2),; util-linux (= 2.37.2-4),; x11-common (= 1:7.7+23),; xkb-data (= 2.33-1),; xml-core (= 0.18+nmu1),; xz-utils (= 5.2.5-2),; zlib1g (= 1:1.2.11.dfsg-2); Environment:; DEB_BUILD_OPTIONS=""parallel=4""; LANG=""en_US.UTF-8""; LC_ALL=""C.UTF-8""; LC_COLLATE=""C.UTF-8""; LC_CTYPE=""en_US.UTF-8""; LC_MESSAGES=""en_US.UTF-8""; LD_LIBRARY_PATH=""/usr/lib/libeatmydata""; SOURCE_DATE_EPOCH=""1625592742""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:13854,learn,learn,13854,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['learn'],['learn']
Usability,.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (3.0.4); Requirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.3.2); Requirement already satisfied: fonttools>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4313,learn,learn,4313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['learn'],['learn']
Usability,.19.18 pypi_0 pypi; pyqtchart 5.12 pypi_0 pypi; pyqtwebengine 5.12.1 pypi_0 pypi; pyrsistent 0.17.3 py38h4d0b108_1 conda-forge; pysocks 1.7.1 py38h5347e94_2 conda-forge; pytables 3.6.1 py38h4e4ac5c_3 conda-forge; python 3.8.6 hcfdab8c_0_cpython conda-forge; python-dateutil 2.8.1 py_0 conda-forge; python-igraph 0.8.3 py38hcde0000_2 conda-forge; python-jsonrpc-server 0.4.0 pyh9f0ad1d_0 conda-forge; python-language-server 0.35.1 py_0 conda-forge; python.app 2 py38_10 ; python_abi 3.8 1_cp38 conda-forge; pytz 2020.1 pyh9f0ad1d_0 conda-forge; pyyaml 5.3.1 py38h94c058a_1 conda-forge; pyzmq 19.0.2 py38h2c785a9_2 conda-forge; qdarkstyle 2.8.1 pyh9f0ad1d_1 conda-forge; qt 5.12.9 h717870c_0 conda-forge; qtawesome 1.0.1 pyh9f0ad1d_0 conda-forge; qtconsole 4.7.7 pyh9f0ad1d_0 conda-forge; qtpy 1.9.0 py_0 conda-forge; readline 8.0 h0678c8f_2 conda-forge; requests 2.24.0 pyh9f0ad1d_0 conda-forge; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:7195,learn,learn,7195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['learn'],['learn']
Usability,"1. I've been playing around with `xarray` and finding the `Dataset` objects fairly intuitive for storing multidimensional arrays. I think it makes sense to store calculated values like this, but give easy access to a long (/tidy) dataframe (similar to that binder notebook). I think representing it internally as a tidy dataframe could be inefficient, since that's pretty close to 100% dense COO matrix. My impression is this is broadly similar to how diffxpy is representing it's results. I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good api for differential expression"". I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. 2. I'd really like to get a generalized version of this implemented. Right now, I think the biggest thing holding it back is being smart about how sparse matrices are handled, but otherwise xarray has a good model for the semantics.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487343093:83,intuit,intuitive,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487343093,1,['intuit'],['intuitive']
Usability,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output.; 2. Setting `svd_solver='arpack'` resolves that problem.; 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere.; 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again?. Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-433258438:85,learn,learn,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433258438,1,['learn'],['learn']
Usability,"3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.con",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1825,learn,learn,1825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['learn'],['learn']
Usability,3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi; tbb 2020.3 hfd86e86_0 ; tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:17893,learn,learn,17893,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['learn'],['learn']
Usability,"8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is; - a) only a question currently open (I am really not particularly an expert in R with limited bandwidth to check things there so waiting for their answer).; - b) Even if true, this does not affect our examples here. It comes into play when we try to further be as consistent with Seurat and their textual description as possible. Yes, this is confusing :) Hope I did not confuse something myself here, checked but consider it a to-the-best-of-my-current-knowledge guidance towards a working solution for you rather than peer-reviewed ground truth ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:2529,guid,guidance,2529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['guid'],['guidance']
Usability,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8531,learn,learn,8531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['learn'],['learn']
Usability,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:1292,learn,learn,1292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,2,['learn'],['learn']
Usability,"> ## `groups`; > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison.; > ; > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-scor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:742,clear,clear,742,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,1,['clear'],['clear']
Usability,"> > Docsearch; > ; > Gotta keep this (it's so much nicer). The UX is for sure, but if we could replace it with something that has similar UX but uses an index that’s built by the RTD build, I’d much prefer that. It’s silly that searching in PR builds or `stable` will result in getting redirected to the `latest` docs …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2220#issuecomment-1090258080:63,UX,UX,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2220#issuecomment-1090258080,2,['UX'],['UX']
Usability,"> @Zethson, do you think we should move those to scverse?. Do you mean to this page? https://scverse.org/learn/. Or generally all of them? We might require stronger filtering options then because with all tools combined we would have a lot of vignettes. Edit oh wait: I had no idea that we had this scanpy_usage repository. My answer is yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2260#issuecomment-1158039454:105,learn,learn,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2260#issuecomment-1158039454,1,['learn'],['learn']
Usability,"> @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`?; > ; > Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. I would also love that actually 😄 . `rank_genes_groups` results (LFC, p-val etc) and things like `mean-expression`, `frac-genes-expressed` are all cluster-specific features, which reminds me of an `obs`-like structure with clusters in rows. Right now, mean expression and fractions are calculated in a private function (`_prepare_dataframe`) in `plotting/_anndata.py` but we can move this to `utils.py` or so, call it in sc.tl.rank_genes_groups() and store the resulting data frame in ad.uns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-480385619:167,simpl,simple,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-480385619,1,['simpl'],['simple']
Usability,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:246,learn,learn,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478,1,['learn'],['learn']
Usability,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?; > ; > ; > ; > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`.; > ; > ; > ; > Thanks!; > ; > ; > ; > Thanks!. As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040743473:409,clear,clear,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040743473,1,['clear'],['clear']
Usability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:809,intuit,intuitive,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,1,['intuit'],['intuitive']
Usability,"> Ah, the problem was that the string actually contained that return type!. This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did 🙂)? Bold font and spacings around colons?. ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484026464:327,simpl,simple,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484026464,1,['simpl'],['simple']
Usability,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:168,simpl,simply,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,1,['simpl'],['simply']
Usability,> Am I right that you simply. You are right!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3307#issuecomment-2437524080:22,simpl,simply,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307#issuecomment-2437524080,1,['simpl'],['simply']
Usability,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:547,simpl,simple,547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777,2,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:889,clear,cleared,889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,1,['clear'],['cleared']
Usability,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:811,simpl,simple,811,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953,1,['simpl'],['simple']
Usability,"> Are any improvements here reasonably easy to do? I recognize that it's making two libraries talk to each other, and at least one of them can't be totally turned off, so this might be difficult. yes, both things are quite easy, I think. Event listeners can be registered in a way that they don’t capture the event, and toggling the popup is a simple `if (isModalVisible()) removeSearchModal() else showSearchModal()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889665103:344,simpl,simple,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889665103,1,['simpl'],['simple']
Usability,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553:523,simpl,simpler,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553,1,['simpl'],['simpler']
Usability,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:134,simpl,simple,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515,1,['simpl'],['simple']
Usability,"> Awesome, Gokcen, thank you! 😁; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:147,simpl,simple,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,1,['simpl'],['simple']
Usability,"> Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Fully agree. > ; > Also: If trying to call a t-test with non-logarithmized data, a warning should be written.; > . Also agree. > The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?. Oh true, it wasn't log transformed, true.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325:62,simpl,simply,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325,1,['simpl'],['simply']
Usability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:303,guid,guides,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,1,['guid'],['guides']
Usability,"> Do we give any indication of how many cells are in a group right now? I feel like this would be important for the user to even know the stats could be unreliable. `sc.pl.dotplot(..., return_fig=True).add_totals().show()`. is one way to check the cell numbers, but there is no intuitive way to do the filtering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1829#issuecomment-835850158:278,intuit,intuitive,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829#issuecomment-835850158,1,['intuit'],['intuitive']
Usability,"> Except if you plan to not update the scanpy.api module and docs section. Yes, that's the plan. `scanpy.api` is completely phased out an simply there for backwards compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450877926:138,simpl,simply,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450877926,1,['simpl'],['simply']
Usability,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:77,feedback,feedback,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677,1,['feedback'],['feedback']
Usability,"> For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above ([#794 (comment)](https://github.com/theislab/scanpy/pull/794#issuecomment-523515331)) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?. @ivirshup I see your point regarding the accurate representation of what is shown. My thoughts were more along the lines of making it easy to distinguish clusters. That would be particularly useful when you have >20 clusters that are hard to distinguish by colours alone. And the overlaps may not be as important for a large-scale overview.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-524373551:289,clear,clear,289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-524373551,1,['clear'],['clear']
Usability,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030:202,learn,learn,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030,2,['learn'],['learn']
Usability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). That’s surprising! I think numba is our most complex dependency, and umap’s dependency PyNNDescent is also compiled. I think if this isn’t a mistake and it’s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLite’s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isn’t a problem, it’s just an old package that doesn’t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:575,learn,learn,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,1,['learn'],['learn']
Usability,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). ; What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy; >; > Could does this relate to @davidsebfischer and diffxpy?. Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226:128,simpl,simple,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226,1,['simpl'],['simple']
Usability,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:110,simpl,simple,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,1,['simpl'],['simple']
Usability,"> Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with; > ; > ```; > pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; > ```; > ; > Seems to work now for me. Thank you. It just worked for me, in July 2024.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559:225,simpl,simple,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-2231704559,1,['simpl'],['simple']
Usability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:798,usab,usable,798,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,1,['usab'],['usable']
Usability,"> Hey! I just wanted to find you in your office... We should discuss and look at this in person... Sure! I’m at my gradma’s place right now, but I’ll be back tomorrow or thursday. > Non-working indentation, for instance, would be a serious problem... Yaya, that’s a very temporary hack because I just wanted a working version, nothing that stays. > in principle, I was satisfied with the docs except for referencing scanpy.api.AnnData... I still don't see the big advantage of using type annotation outside of the docstrings... The big one is that many of the bugs we had in the past and that we’ll have in the future can be prevented if your IDE/editor tells you “you can’t pass that thing here, wrong type.”. And by using type annotations in the code, it’s impossible to have them wrong (to forget a comma or so) and break the type format. I’ve seen quite some commits by you just fixing such a thing. > Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... We don’t break them. Typeless parameter annotations are still in numpy style 😉",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-380095294:947,simpl,simply,947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380095294,2,"['learn', 'simpl']","['learn', 'simply']"
Usability,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754:305,clear,clear,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754,1,['clear'],['clear']
Usability,"> Hi @JackieMium, I remember you said something similar in another issue.; > ; > If there’s things bugging you, how about making a PR that fixes it?. Not sure what you're referring to but I don't think I ever reported color pallette issue before. ; I hope I could help fix things but I am familiar with R/Seurat and Python/scanpy is a whole new universe to me. I am starting to learning the scanpy pipeline. How things work under the hood with scanpy or basically Python plotting are really beyond my capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040:378,learn,learning,378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040,1,['learn'],['learning']
Usability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:261,simpl,simpler,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920,1,['simpl'],['simpler']
Usability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:261,simpl,simpler,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457,1,['simpl'],['simpler']
Usability,"> Hi, please provide the data you use, otherwise this is not reproducible:; > ; > ```; > FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); > ```. Hey, the data is publicly available under this link: https://www.10xgenomics.com/resources/datasets/human-lung-cancer-ffpe-2-standard. I simple copied the `curl` bash script to download all the files and then unzipped the file corresponding to the images to get the ""spatial"" folder",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906:485,simpl,simple,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906,1,['simpl'],['simple']
Usability,"> However, don't you think that this could be part of a the scanpy tutorials section?. Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441474788:555,simpl,simply,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441474788,1,['simpl'],['simply']
Usability,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?. Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040013593:434,simpl,simply,434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040013593,1,['simpl'],['simply']
Usability,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:180,simpl,simply,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,1,['simpl'],['simply']
Usability,"> I am still worried that this doesn't make the relative numbers clear. It sounds like you're trying to see whether absolute numbers of cells expressing a gene is similar between clusters. I don't think this is a use case I've commonly heard of. Maybe you could explain why you're interested in this?. To me, it seems more relevant to know how common a gene is expressed within a population than how many of that cell type express that gene. Proportions of cell types vary due to tissue and collection, so I'm not sure when differences in total amount is what I want to know. If you wanted, you could try to map the size of the dot to the number of cells in the cluster expressing the gene? You can find some code for doing this kind of thing here (https://github.com/theislab/scanpy/issues/1876#issuecomment-987049315), though we would like that API to be nicer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016491736:65,clear,clear,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016491736,1,['clear'],['clear']
Usability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:329,learn,learn,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,2,['learn'],['learn']
Usability,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where “union” comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > …; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think “discriminated union/intersection of types” would make sense here. leaving out the “discriminated/tagged/disjoint” here is the problem. in C there’s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. I’d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, it’s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:925,simpl,simply,925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140,1,['simpl'],['simply']
Usability,"> I ended up with […] Does it looks ok to you?. Yeah! You could simplify though:. ```py; VMinMax = Union[str, float, Callable[[Sequence[float]], float]]. def embedding(; ...; vmin: Union[VMinMax, Sequence[VMinMax], None] = None,; vmax: Union[VMinMax, Sequence[VMinMax], None] = None,; ...; ): ... def _get_vmin_vmax(; vmin: Sequence[VMinMax],; vmax: Sequence[VMinMax],; ...; ) -> ...: ...; ```. > It's not a big deal but if we use numbers in 0-100 range for vmax-vmin, it's a percentile, so p80 makes more sense. you can also be more or less precise: `q001`→`0.001`, `q9`→`0.9`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523883673:64,simpl,simplify,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523883673,1,['simpl'],['simplify']
Usability,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:329,learn,learn,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241,1,['learn'],['learn']
Usability,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. I’d prefer “a, b, or c”, but I’ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didn’t hear of type theory. It’s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:144,learn,learn,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106,4,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simple']"
Usability,"> I quite like the saving of figures as it means people can use scanpy who otherwise aren't as familiar with data science in python. Calling a function on an axis object or saving the last axis object that was is displayed is not always intuitive to new users. How about adding a ""plotting cookbook"" section to the docs instead? `plt.rc_context` is such a neat trick (also beyond scanpy), but it wasn't obvious to me either (#1648). . Obligatory quote from the ""Zen of Python"":; ```; There should be one-- and preferably only one --obvious way to do it.; Although that way may not be obvious at first unless you're Dutch.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-841127632:237,intuit,intuitive,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-841127632,1,['intuit'],['intuitive']
Usability,"> I sent you an invitation for readthedocs.com about 2 months ago already - I just resent it. :). Well, doesn’t seem like it worked in the past: What I got now was not an invitation that I needed to click, but simply a notification that I’m now member of the team on rtd.com (which I wasn’t before). The changes look good! I would however prefer to do things via `.. include::` instead of duplicating code for the `scanpy` and `scanpy.api` sections. Except if you plan to not update the `scanpy.api` module and docs section.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450818246:210,simpl,simply,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450818246,1,['simpl'],['simply']
Usability,"> I solve this question that can’t import skmisc.loess as loess by chance。the solution is: 'python3.7'+'numpy-1.20.1+mkl-cp37-cp37m-win_amd64.whl'+'scipy==1.7.3'+'scikit-learn==1.0.2'+'scikit-misc==0.1.4'+'scanpy==1.9.1'. > It is 13 Jun ,2022 ,today.I need to creat a new evn ,in which I can run the code as **sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3')****. I meet the question that I can't **Import skmisc.loess as loess** ,while the code works fine in my last computer. So，there is a solution can solve this question ,now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1153550204:170,learn,learn,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1153550204,1,['learn'],['learn']
Usability,"> I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. I would definitely like this to be the case. I'm not sure I see . > Why? `sc.pp.neighbors` already has `method='gauss'`. To me, it’s largely of a maintenance and documentation issue. Most bugs I fix (here, and in upstream libraries) come from argument handling. The more features you lump into a function, the more complicated argument handling gets. There are questions of default values and fallbacks for different backends, and being sure users understand which arguments are valid for each backend. The use of the `Neighbors` class ends up making the `neighbors` function much more complicated than it needs to be. I think skipping out on that here can make this implementation much more simple. From an API stand point, I would like the ""blessed"" `tsne` workflow to be dead obvious. I'm thinking:. ```python; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```. How many arguments is it going to take to make this work if this functionality is in `sc.pp.neighbors`? At a minimum, `k=30, method=tsne_affinity, nn_method=""annoy""`, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450:814,simpl,simple,814,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450,1,['simpl'],['simple']
Usability,"> I think it would be fine to only cover the case of what `space ranger` actually outputs. I was thinking there could be an argument where the user manually passes an alternate path. This could be useful for cases where they've processed the image themselves some modifications to the image. space ranger doesn't output this image, as it's taken as input to assign spots and get scalefactors and metadata. This type of image is in the same folder just for chance in the 10x genomics dataset. ; In the `read_visium` function I would simply add an argument to pass the path of the image, and basically just assign it to the `adata.uns` metadata. Otherwise just assign None. THis way it's consistent for the spatial tool whichlater uses it in the image container.; It's also convenient to add it as argument so that `read_visium` could just be passed in that same way as it is now in `datasets.visium_sge`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734735910:532,simpl,simply,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734735910,1,['simpl'],['simply']
Usability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:848,simpl,simplest,848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,1,['simpl'],['simplest']
Usability,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:75,simpl,simply,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I can’t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! That’s a concrete thing I can do. I’ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:232,undo,undo,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,1,['undo'],['undo']
Usability,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:900,simpl,simpler,900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064,1,['simpl'],['simpler']
Usability,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:133,learn,learn,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652,2,['learn'],['learn']
Usability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:1251,learn,learning,1251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['learn'],['learning']
Usability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:1310,clear,clear,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,1,['clear'],['clear']
Usability,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308:79,learn,learn,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441771308,5,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simple', 'simply']"
Usability,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:366,clear,clear,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,1,['clear'],['clear']
Usability,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:265,simpl,simple,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,1,['simpl'],['simple']
Usability,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). 👍 . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:117,simpl,simple,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:334,simpl,simple,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924,2,"['simpl', 'usab']","['simple', 'usable']"
Usability,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap.; Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304:540,learn,learn,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304,1,['learn'],['learn']
Usability,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1208,guid,guidelines,1208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['guid'],['guidelines']
Usability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:472,simpl,simple,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['simpl'],['simple']
Usability,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:1016,guid,guidance,1016,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,1,['guid'],['guidance']
Usability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:23,simpl,simple,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,1,['simpl'],['simple']
Usability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:35,clear,clear,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,1,['clear'],['clear']
Usability,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:141,clear,clear,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599,1,['clear'],['clear']
Usability,"> Or just the standard matplotlib palettes, yes:; > ; > ```python; > from matplotlib import cm; > ; > sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=cm.get_cmap('Set3')); > ```; > ; > The bug is that you can’t pass the colormap name. Passing a colormap directly works. Is it then, a good idea to state this the documents for the time being to clear the confusion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1646498818:365,clear,clear,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1646498818,1,['clear'],['clear']
Usability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:614,simpl,simple,614,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,1,['simpl'],['simple']
Usability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:1029,clear,clear,1029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['clear'],['clear']
Usability,"> Should the reference object where you learn the transformation always be a subset of the data you're going to apply the transformation to? If so, instead of passing a separate object, could there be a mask of which samples to train on?; > ; > If not, what do you think about making this a separate function? Maybe `combat_by_reference`?. Thank you for your great suggestions. I think it's easier to add a mask for train/evaluate instead of splitting into 2 objects. ; I don't think it should be a separate `combat_by_reference` function, though, because the chance in the function is small and I preserved the original functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703:40,learn,learn,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703,1,['learn'],['learn']
Usability,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424271229:56,learn,learn,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424271229,1,['learn'],['learn']
Usability,"> So if I understand correctly you want to use quantile scaling to translate values to colour? If that is what you're suggesting, I'm not sure I'm such a fan of that idea. With quantile scaling you would lose all sense of gradient in your e.g. expression values. I would instead opt for trimming and scaling. The trimming could be done via a quantile threshold though. OK, I thought code was clear enough, here is more information :). vmin and vmax are used for determining the lowest and highest values of the colormap (see https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.colors.Normalize.html). By default, these values are mapped to the minimum and maximum of the color vector of the scatter plot (e.g. gene expression). Right now, we can define only one vmin/vmax value in a sc.pl.* call (e.g. `sc.pl.umap(ad, color=..., vmax=2.0)`). But when we plot multiple genes (`sc.pl.umap(ad, color=['a', 'b'], vmax=2.0)`), setting vmax to a specific value sometimes does not make sense because each gene might have a different outlier range. . What I propose is the flexibility to use quantiles to set vmin/vmax e.g. `sc.pl.umap(ad, color=['a', 'b'], vmax_quantile=0.99)` where vmax values will be calculated per panel (i.e. per gene) by Scanpy. I mean it's simply winsorization, nothing fancy :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775#issuecomment-519626141:392,clear,clear,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-519626141,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:839,clear,clear,839,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,"['clear', 'intuit']","['clear', 'intuitive']"
Usability,"> Thank you! With “tests” I mean “functions named `test_*` with `assert ...` statements inside”; ; Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, ; Khalid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243:115,guid,guidance,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243,1,['guid'],['guidance']
Usability,"> Thanks for the report.; > ; > Do you have NaN values in your expression matrix? If so, try filtering them out and seeing if that works. If not, could you report the versions of the library you're working with, and try to make a self contained example that I could run on my machine?. Hi, . Thank you for your reply. I think there is no NaN data in the matrix of the mito genes. Because I have already plotted the mitochondrial genes as follows. The version is scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0. I am sorry I don't know how to make a self-contained example. The plot is: ; ![highest_expre_genes_BHCF](https://user-images.githubusercontent.com/49381637/83712829-3ea3ab80-a5ec-11ea-8497-cc70a95a216e.png). Thank you. Best wishes,. Shangyu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1259#issuecomment-638585609:551,learn,learn,551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-638585609,1,['learn'],['learn']
Usability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:32,clear,clear,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:287,simpl,simpler,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128,2,"['clear', 'simpl']","['clear', 'simpler']"
Usability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:353,simpl,simpler,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,1,['simpl'],['simpler']
Usability,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302#issuecomment-441965409:468,responsiv,responsive,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441965409,1,['responsiv'],['responsive']
Usability,"> The tsne test is giving me a headache. There are some small difference even setting a `random_state`. I will remove the test. I completely get this... the exact UMAP and tSNE plots are simply not _exactly_ reproducible, just very similar... fortunately, clustering (even though that's also a greedy algorithm) and everything else are exactly reproducible. I also removed the tests for UMAP: https://github.com/theislab/scanpy/blob/1df151f678c50b9f85f5d65e7a47d061e4e6784b/scanpy/tests/notebooks/pbmc3k.py#L88-L91 :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837:187,simpl,simply,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-423783837,1,['simpl'],['simply']
Usability,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:844,simpl,simple,844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664,2,"['clear', 'simpl']","['clearly', 'simple']"
Usability,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:662,learn,learn,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729,1,['learn'],['learn']
Usability,"> This is the standard way of writing a numpydoc returns section. […] This solution is dropping support for them. It certainly shouldn’t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I don’t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I don’t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons?. I’ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the “optional” is redundant. What would it even mean to have “a parameter that isn’t optional but has a default value”?. I’m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417:487,simpl,simple,487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417,1,['simpl'],['simple']
Usability,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:77,learn,learn,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220,1,['learn'],['learn']
Usability,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:25,simpl,simpler,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026,3,"['learn', 'simpl']","['learn', 'simpler']"
Usability,"> To be able to reproduce and help, it is a big aid for us if you can supply a code sample that we can run: that is, with some dummy data (the datasets scanpy readily supplies are great for that), and the error/unexpected behaviour you get. Can you show such an example, with data? It is not immediately clear to me what specific you are trying to add or construct; I'm not sure whether basically the dataframe gets destroyed by the operation you intend to perform, or whether it is the violin plot failing (if the dataframe is crooked, it would be this to be fixed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546:304,clear,clear,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546,1,['clear'],['clear']
Usability,"> UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. Both of those clustering algorithms just use whatever graph is passed, so this shouldn't be an issue. > t-SNE, on the other hand, calculates 90 nearest neighbors by default.; > openTSNE does something similar to UMAP for adding new samples to existing embeddings. Could there just be a separate function for computing neighbors for tsne? `sc.pp.neighbors` can be considered to be ""compute nearest neighbors and connectivity as expected by UMAP"", while a separate function could use methods and defaults appropriate to openTSNE. @Koncopd would have more to say on how this should work w.r.t. `ingest`. > Is relying on a single k=15 from UMAP for everything really ok?. Ultimately, up to the user. There is an element of consistency and simplicity to using the same representation of the data for multiple parts of the analysis. I think there would have to be a good reason for using a different connectivity matrix for the 2d embedding and for the clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-657406330:890,simpl,simplicity,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-657406330,1,['simpl'],['simplicity']
Usability,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:103,simpl,simply,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,1,['simpl'],['simply']
Usability,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > I’d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:746,simpl,simply,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973,1,['simpl'],['simply']
Usability,"> What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. Regarding this -- yes, that's what we thought to do. Do you think it makes sense? However, for computational efficiency, I would threshold the genes by expression and take e.g. 1000 or even 100 genes with the highest average expression (for the purpose of estimating theta). Lowly-expressed genes don't really constrain theta much, it's highly-expressed ones that do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-792059556:34,learn,learning,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-792059556,1,['learn'],['learning']
Usability,"> Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. My argument was mostly about API. But one big benefit of using kNN with k=15 is that it's *much faster* then using k=90 (with perplexity=30) which is what t-SNE is using by default (for historical reasons). It's not about uniform vs non-uniform, it's about k=15 vs k=90. Uniform on k=15 just happens to give almost the same results as perplexity=30 on k=90. So it's a lucky coincidence that I thought we could benefit from. > Second API: I'm not sure I completely agree with this. I think it would be the most clear for sc.pp.neighbors to essentially mean ""build umap's connectivity graph"", and functions like sc.tl.tsne or sc.tl.umap to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via connectivities_key) are the weights that get used. I understand what you saying, but the situation won't be symmetric because `neighbors` already exists, is *not* called `neighbors_umap`, and all users of Scanpy are very familiar with this function. I'd like to make it very easy to use t-SNE in scanpy and that it naturally fits into the scanpy's established workflow. That's why I think simply running `tsne()` after running `neighbors()` should be possible. Please note that t-SNE requires normalized weights (they should sum to 1). The weights constructed by UMAP in `neighbors` are not normalized. So if you run `neighbors()` and then `tsne()` then t-SNE should do *something* in order to be able to use this graph. My suggestion is that it discards the weights and uses normalized affinity matrix. I am not sure what exactly is your suggestion? Take UMAP weights and normalize them? This has never been explored in the literature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575:643,clear,clear,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"> You can always choose a palTete like 'Blues', 'Reds', 'binary' that will give you a gradient from a clear to a darker color. Maybe that helps but I agree with Philipp, 120 clusters is a lot to visualize with different colors.; > […](#); > On Tue, May 28, 2019 at 4:21 PM Philipp A. ***@***.***> wrote: Closed #156 <#156>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA> .; > -- Fidel Ramirez. Thanks for your great idea, I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-523779587:102,clear,clear,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-523779587,1,['clear'],['clear']
Usability,"> absolute numbers of cells expressing a gene is similar between clusters as a use case. I am aware that this is a bit of a niche problem, and I am not particularly happy with domino plots as a solution either. I have no better vehicle to discuss this than opening an issue :( Hopefully this inspires the next person who deals with this problem. As to the question, maybe sticking to this example will help me explain:. I am looking at the expression of Hb9/Mnx in my whole-body dataset. I notice from the feature scatter that it seems to be somewhat expressed in clusters 0, 2, and 18. Wanting to be sure, I look at the dotplot. The dotplot tells me that there is a greater proportion of cells in cluster 18 that express it, compared to 0 and 2. The dotplot might make me believe that Hb9 is a marker for cluster 18, and if I do an in-situ hybridisation, these are the cells I would be staining. However, the truth is that the vast majority of cells that express Hb9 are actually in clusters 0 and 2, different cell types than 18. The number of cells in each cluster correlates with the number of cells in the organism, so if I performed the in-situ I would get lots of cells that I could mistakenly all identify as cluster 18. Does this make more sense?. EDIT: I am not advocating for domino plots to be part of ScanPy. I am simply trying to start a discussion, and trying to see if there was an easy fix that I missed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889:1327,simpl,simply,1327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889,1,['simpl'],['simply']
Usability,"> hi @yotamcons ,; > ; > thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,; > ; > Thank you!. Would love to starting November, ping me if thats still relevant",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531:46,feedback,feedback,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531,1,['feedback'],['feedback']
Usability,"> it would make sense if I mirror the change in normalize_pearson_residuals(), right? I believe doing that will even simplify the function further. If ivirshup agrees, I could quickly do that :). Sounds good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-791140191:117,simpl,simplify,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-791140191,1,['simpl'],['simplify']
Usability,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:158,simpl,simply,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,2,"['intuit', 'simpl']","['intuitive', 'simply']"
Usability,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:177,clear,clear,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:561,guid,guidelines,561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681,1,['guid'],['guidelines']
Usability,"> thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. No worries!. I think communicating about the ideas we have for these tools can be fraught. > in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot). I don't think this is the case. . First, I believe there are non-visium grid based spatial methods (I remember seeing a product page for one, but can't find it atm). Second, I think you don't need segmentation info to use this function. You just need coordinates (probably derived from segmentation) and possibly an image. Like this:. > unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units. But I think a user already having done the segmentation, then coming to scanpy is a reasonable workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703:66,clear,clear,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703,1,['clear'],['clear']
Usability,"> thanks @jlause ! Really excited for this!; > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this?; > ; > thank you!. cool, looking forward to your feedback!; I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506:219,feedback,feedback,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506,2,"['feedback', 'simpl']","['feedback', 'simplify']"
Usability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:623,simpl,simply,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,1,['simpl'],['simply']
Usability,"> we don't have pre-commit in place, we are discussing it here #1563 . I do check flake8 but clearly didn't do it this time. I thought at one point you guys were checking flake8 with CI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679#issuecomment-784310828:93,clear,clearly,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-784310828,1,['clear'],['clearly']
Usability,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:2930,learn,learn,2930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['learn'],['learn']
Usability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:464,simpl,simple,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,1,['simpl'],['simple']
Usability,"@Koncopd sorry for the late feedback, but I don't see the ""neighbors_key"" in the scanpy.tl.paga function. It'd be great to make sure that everything that uses the neighbor graph is covered :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1118#issuecomment-616757843:28,feedback,feedback,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-616757843,1,['feedback'],['feedback']
Usability,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances; * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191:202,learn,learn,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191,1,['learn'],['learn']
Usability,"@Koncopd, can we merge this without the `neighbors_update` function and without writing the `rp_forest` to the AnnData object? Your code is good, but we should put it into another PR. > Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR. Is what I wrote in the beginning. I think it turned out tricky and is a case for https://github.com/theislab/scanpy/issues/562#issuecomment-487409358. So, let's keep this PR really simple and just be about removing the legacy code. Your statement about ""all tests pass except for the PAGA tests"" is still true? Did you manually inspect the PAGA notebook and does it look consistent? Just a few cosmetic things should have changed, I guess. If yes, we'll merge this, now that `1.4.1` is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487410333:465,simpl,simple,465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487410333,1,['simpl'],['simple']
Usability,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:242,clear,clear,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259,1,['clear'],['clear']
Usability,"@LuckyMD I am wondering have you tried it before? or did someone try it with the pbmc data with regards to the t-cells that you mentioned? I agree with you that it can not be an option for answering biological questions but in the case that there is no clear biological knowledge like looking for new sub-types or so it can be an option to get some idea (mathematically). . p.s. it just crossed my mind so I am pushing a technical question also here 😄 Since we would need to calculate the distance matrix, would the input for the function be adata.X ? should it be the raw file?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498068973:253,clear,clear,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498068973,1,['clear'],['clear']
Usability,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494539920:56,simpl,simply,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494539920,1,['simpl'],['simply']
Usability,"@LuckyMD Yeah, we can definitely do the differences in mean expression on the log-scale as a quick fix. However, I think it's a little bit more intuitive to express fold-changes with a base of 2, as opposed to a base of e, which is the transformation used for the data matrix. This also mimics what other packages usually report, but I'm happy either way!. For the differential testing itself, I completely agree with you about the assumption of normality with t-tests, so we definitely shouldn't change that. For the wilcoxon test, however, normality isn't assumed, so do you think it would be ok to run on raw counts?. Either way, we should definitely fix the double-log for the fold-change reporting. Is there a preference on whether to keep it was loge(base e) difference or log2 difference? For simplicity, this change in scale would only affect the fold-change reporting, all other tests I think we should keep in loge to keep it consistent. One final thought I wanted to ask your opinion on: log-mean vs mean-log. (e.g. log(mean(count)) vs mean(log(count)). For t-tests I think it makes sense to use mean-log, however, when calculating fold-change differences in mean expression, I'm not entirely sure what's preferred (particularly for methods like wilcoxon that don't use the mean expression to calculate scores). Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-470250609:144,intuit,intuitive,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-470250609,2,"['intuit', 'simpl']","['intuitive', 'simplicity']"
Usability,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-439105490:377,simpl,simply,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-439105490,1,['simpl'],['simply']
Usability,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:29,simpl,simply,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854,1,['simpl'],['simply']
Usability,"@LuckyMD: Yes, scran's size factor calculation would be very nice-to-have and should be a simple task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-469643226:90,simpl,simple,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-469643226,1,['simpl'],['simple']
Usability,"@PGmajev, I would recommend setting up pre-commit for the repo (as described in the contributing guide [here](https://scanpy.readthedocs.io/en/latest/dev/getting-set-up.html#pre-commit)). It should save you from dealing with these formatting errors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1074431745:97,guid,guide,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1074431745,1,['guid'],['guide']
Usability,"@VolkerBergen ; No, similar lines also should be changed in [`randomized_range_finder`](https://github.com/scikit-learn/scikit-learn/blob/3a884c5ee507f735e2df384727340c72c5219a8e/sklearn/utils/extmath.py#L148), which is used by `randomized_svd` function. Or the whole `safe_sparse_dot` function. But copying the file extmath.py (or the part of it related to svd) and changing this lines would be sufficient, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446527430:114,learn,learn,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446527430,2,['learn'],['learn']
Usability,@VolkerBergen can you type a simple example on how to use this new functionality. I think that I want to use this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428458237:29,simpl,simple,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428458237,1,['simpl'],['simple']
Usability,"@VolkerBergen, is this really important? I had the intend of allowing passing a precomputed `counts_per_cell` vector, but I think it wasn't really ever used... So, for a simpler function and cleaner code, it would be nice to get rid of it; as @Koncopd did for the new version. Any objections?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-457869063:170,simpl,simpler,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-457869063,1,['simpl'],['simpler']
Usability,"@Zethson Ready to merge. Thanks for your feedback; I added to the release notes, and rebased on master.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184:41,feedback,feedback,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184,1,['feedback'],['feedback']
Usability,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:789,clear,clear,789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161,1,['clear'],['clear']
Usability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:136,simpl,simple,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,1,['simpl'],['simple']
Usability,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:258,simpl,simply,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215,2,['simpl'],['simply']
Usability,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:418,simpl,simply,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240,1,['simpl'],['simply']
Usability,"@dawe a cell cycle scoring function would be great! everything that's a bit more extensive and non-standard should go into [sc.tl](https://github.com/theislab/scanpy/tree/master/scanpy/tools), everything that's really just simple preprocessing and stats with a few lines can go to [sc.pp](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/simple.py). usually, there should be a plotting function in sc.pl that presents a canonical visualization of the annotation added in with the tool... writing a test for your function would also be great ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398:223,simpl,simple,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-363250398,2,['simpl'],['simple']
Usability,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095:13,feedback,feedback,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095,1,['feedback'],['feedback']
Usability,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:739,guid,guidelines,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874,4,"['clear', 'guid']","['clear', 'clearer', 'guidelines', 'guidelines-for-repository-contributors']"
Usability,"@falexwolf ; Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange; ```; X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]); normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7); array([[1. , 0. , 1. ],; [3. , 0. , 1. ],; [0.71428571, 0.85714286, 0.14285714]]); ```; Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/301#issuecomment-438749595:180,simpl,simple,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301#issuecomment-438749595,1,['simpl'],['simple']
Usability,"@falexwolf @ivirshup it would be good to clear this up soon. If this is correct, then we choose HVGs incorrectly every time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705#issuecomment-520516186:41,clear,clear,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-520516186,1,['clear'],['clear']
Usability,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:587,simpl,simple,587,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,1,['simpl'],['simple']
Usability,"@falexwolf Thanks for pointing out this package, I'll give it a try. I'll also give the pull request a shot - I'm still learning my way around this package but if I can do it I'll submit it for sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420358808:120,learn,learning,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420358808,1,['learn'],['learning']
Usability,@falexwolf regarding #204 the image that didn't work for you is this? I will address the conflict once this is clear because they are related. ![image](https://user-images.githubusercontent.com/4964309/42776678-05350dc6-8938-11e8-8109-901e94abbfee.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405339417:111,clear,clear,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405339417,1,['clear'],['clear']
Usability,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730:26,feedback,feedback,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730,1,['feedback'],['feedback']
Usability,"@falexwolf thanks for the feedback. As @maximilianh suggested, I was able to export the expression matrix from the cellbrowser export function. Thank you for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-461540169:26,feedback,feedback,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-461540169,1,['feedback'],['feedback']
Usability,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-441809122:66,clear,clear,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-441809122,1,['clear'],['clear']
Usability,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:253,simpl,simple,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154,1,['simpl'],['simple']
Usability,"@falexwolf, yes I think overall fold changes with 2 as a basis are more intuitive than with e as a basis for most people, similar to basis=10, but basis 2 gives a more sensible dynamic range than 10 does on gene expression data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/446#issuecomment-457617601:72,intuit,intuitive,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/446#issuecomment-457617601,1,['intuit'],['intuitive']
Usability,"@fidelram FYI, this defines an explicit plotting submodule in `plotting/__init__.py`, which also contains the docs for it. There is no need to reexport to `api/pl.py` anymore. Everything is backwards compat and if you want to use new functionality, simply `import scanpy` instead of `import scanpy.api`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/406#issuecomment-450268629:249,simpl,simply,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/406#issuecomment-450268629,1,['simpl'],['simply']
Usability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:227,simpl,simply,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['simpl'],['simply']
Usability,@fidelram lemme know if the new comments make things more clear,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-526651900:58,clear,clear,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-526651900,1,['clear'],['clear']
Usability,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:235,simpl,simply,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,1,['simpl'],['simply']
Usability,@flying-sheep I quite like your style guidelines. It might be a good idea to designate a particular function that does it well and is complex enough to include all of these options. That way it's easy to follow style when writing a new function. Something like a contributors template.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523589188:38,guid,guidelines,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523589188,1,['guid'],['guidelines']
Usability,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642:125,simpl,simply,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441619642,1,['simpl'],['simply']
Usability,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:36,clear,clear,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,1,['clear'],['clear']
Usability,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436:147,simpl,simply,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437729436,1,['simpl'],['simply']
Usability,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:103,learn,learn,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135,1,['learn'],['learn']
Usability,"@flying-sheep that user experience seems pretty reasonable. I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for `expression_atlas` would have a reference to `dataset_dir`?. Also on point 4, I've definitely had conda exit with helpful errors when I ran out of space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437:19,user experience,user experience,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478225437,1,['user experience'],['user experience']
Usability,"@flying-sheep the first is easy to fix. For 2. , it's not clear to me what you're asking for here. It's been a while since I worked on this. Am I supposed to import this function in the `__init__.py` for `tl` and `pl`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947264348:58,clear,clear,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947264348,1,['clear'],['clear']
Usability,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:218,clear,clear,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973,1,['clear'],['clear']
Usability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:481,simpl,simple,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,3,"['clear', 'intuit', 'simpl']","['clear', 'intuitively', 'simple']"
Usability,"@flying-sheep, any idea why `__version__` wouldn't work? Are we sure `importlib.metadata.version('umap-learn')` works when `__version__` doesn't? I'd be worried if they were basing the reported version on different sources.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-963430528:103,learn,learn,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963430528,1,['learn'],['learn']
Usability,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753#issuecomment-804517933:55,clear,clear,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-804517933,1,['clear'],['clear']
Usability,"@flying-sheep, does this still need to get merged?. It looks like a simple rebase, and the CI plot comparison feature is now active on azure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1047#issuecomment-777239623:68,simpl,simple,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047#issuecomment-777239623,1,['simpl'],['simple']
Usability,"@flying-sheep: Do you agree that we should add this to the travis setup? I thought about creating a `requirements_tests.txt` as for `anndata` and simply adding the line to `.travis.yml`. Good solution? Maybe it's even your solution, I don't remember. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457869443:146,simpl,simply,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457869443,1,['simpl'],['simply']
Usability,"@frederikziebell thanks for the feedback, we will discuss this in the next core meeting. An automated cross-package CI could be a solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2366951904:32,feedback,feedback,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2366951904,1,['feedback'],['feedback']
Usability,"@giovp I haven't been able to work around the issue. When I rebuild a container with different versions of scanorama, scanpy, numpy, scikit-learn I end up with errors. Most of the time it's this ValueError about the wrong shape. The only different error I noticed is when I tried scanorama 1.6 and there was an error about `concatenate()` not being an available function. . Whenever you find time to update the tutorial, that will be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633:140,learn,learn,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633,1,['learn'],['learn']
Usability,"@giovp thanks for your reply, I agree on all points :) Have a good vacation!; @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG).; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459:116,feedback,feedback,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459,1,['feedback'],['feedback']
Usability,"@giovp this issue can be closed since the documentation already states that ""To preserve the original structure of adata.uns[‘rank_genes_groups’], filtered genes are set to NaN."" . Users can simply drop the NANs for each cluster column in the adata.uns[‘rank_genes_groups_filtered’] dataframe.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1446#issuecomment-2211244159:191,simpl,simply,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-2211244159,1,['simpl'],['simply']
Usability,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:138,learn,learn,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512,2,['learn'],['learn']
Usability,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:766,clear,clearly,766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,1,['clear'],['clearly']
Usability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:535,clear,clear,535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,3,"['clear', 'intuit', 'learn']","['clear', 'intuitive', 'learn']"
Usability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:91,simpl,simply,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['simpl'],['simply']
Usability,"@havardtl, sorry for the late feedback, but this would be a great first PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434#issuecomment-783193441:30,feedback,feedback,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434#issuecomment-783193441,1,['feedback'],['feedback']
Usability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:484,simpl,simply,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,1,['simpl'],['simply']
Usability,"@ilan-gold, I believe `map_blocks` offers several advantages, the most significant being its simplicity. If you don't require `futures`, using `map_blocks` is likely the better choice. I'll delve more into this once I begin working on the multi-GPU version for RSC.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980697488:93,simpl,simplicity,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980697488,1,['simpl'],['simplicity']
Usability,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240:153,learn,learn,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240,2,['learn'],['learn']
Usability,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831:75,clear,clearly,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831,1,['clear'],['clearly']
Usability,"@ivirshup I simplified the conditionals a bit and there are only two sets now. One to check for various `{Value/Import}Error`s and another to do the `clustering_kwargs` building. I think this is cleaner and faster since no code will run that doesn't have to. I didn't really see a way to do it with only one set of conditionals without code duplication. There's some code that's just common to both, but that shouldn't be run in the case of one of the `{Value/Import}Error`s .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952548908:12,simpl,simplified,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952548908,1,['simpl'],['simplified']
Usability,"@ivirshup I'd actually be interest in hearing those. My packages also have the test folder outside the package, but I am happy to learn why many major packages have theirs in the actual package and why that might be a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545:130,learn,learn,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545,1,['learn'],['learn']
Usability,"@ivirshup Thank you for the feedback. I will add a release note soon. I also thought about the naming of the parameter. However, if we assume that also in the future it is mostly used to subset the number of PCs in PCA arrays stored under different names, it should be fine?. I can not comment further on what these changes may break, at least ideally they should make the use of n_pcs more consistent and as expected. Would you suggest, I implement some further, more comprehensive tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928:28,feedback,feedback,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928,1,['feedback'],['feedback']
Usability,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:182,simpl,simple,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106,1,['simpl'],['simple']
Usability,@ivirshup You are right about some of the pre-processing plots. I should have created a separate branch for those. I simply think hte outputs have changed and we never caught it but would be curious to see what you get. It could be my M1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952455566:117,simpl,simply,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1952455566,1,['simpl'],['simply']
Usability,@ivirshup `integrated_anterior` came from `scanorama.correct_scanpy()` like you mentioned. I'm running my jupyter notebook out of a container. I rebuilt my container with scanorama=1.7 but faced the same issue. Is it possible the problem is the preprocessed data at `https://hmgubox.helmholtz-muenchen.de/f/4ef254675e2a41f89835/?dl=1` is simply of the wrong shape once it's read by scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051031200:338,simpl,simply,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051031200,1,['simpl'],['simply']
Usability,"@ivirshup and I went over this, it just simplifies the doc setup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2561#issuecomment-1693364079:40,simpl,simplifies,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2561#issuecomment-1693364079,1,['simpl'],['simplifies']
Usability,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417:36,clear,clear,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417,1,['clear'],['clear']
Usability,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:76,simpl,simply,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735,1,['simpl'],['simply']
Usability,"@jamestwebber Thank you for reintroducing the functionality! :smile:. @Koncopd, could you check whether it still works as you're the one who introduced it in the first place?. Functional programming (scikit-learn): Yes, I assumed you were referring to functional programming. Also, I completely agree that it makes sense that a class method like `.fit(X)` should return `self` in order to be able to further operate on the modified object. But I wouldn't call that _functional programming_ and for a non-class-method, I'd say this is a completely different case. I think we're interpreting the Wikipedia article and all the tutorials on functional vs. object-oriented a bit differently, which is not problematic, of course. 🙂. Functional programming (Scanpy): When thinking about Scanpy's structure in the first place, I thought I'd write it in a largely functional way - where I mean that the central operations are based on functions on some fixed data container, not on objects that contain both data and functionality for the central operations - for two reasons: (1) the data structure was clear from the beginning, it was not terribly complicated but sufficiently complicated to write some non-trival infrastruture (`anndata.AnnData`) (2) the needed functionality of the toolkit was not clear and is not entirely clear to date. Also, a lot of different functionality is needed and putting all of it into one object would require to define a generic god class, which wouldn't be very transparent. Functional programming (Keras): I'd be very astonished if `processed_sequences = TimeDistributed(model)(input_sequences)` modified `input_sequences` - this is taken from the tutorial you linked and summarizes the central operation. Hence, I think that the Keras functional API doesn't use `inplace` functions, as most APIs. Then, of course, there should be a return value. But maybe I'm mistaken. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077:207,learn,learn,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403310077,4,"['clear', 'learn']","['clear', 'learn']"
Usability,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:69,learn,learn,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293,2,['learn'],"['learn', 'learning']"
Usability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:56,learn,learn-intelex,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,1,['learn'],['learn-intelex']
Usability,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:467,clear,clear,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714,1,['clear'],['clear']
Usability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:173,learn,learn,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['learn'],['learn']
Usability,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:213,clear,clear,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301,1,['clear'],['clear']
Usability,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?. More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks!. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040009925:393,clear,clear,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040009925,1,['clear'],['clear']
Usability,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:123,simpl,simple,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604,1,['simpl'],['simple']
Usability,"A simple way to do it is by creating a new `.obs` categorical column as follows:. ```python; adata = sc.datasets.pbmc68k_reduced(); # create new categorical column called `selection`; adata.obs['selection'] = pd.Categorical((adata.obs_vector('CD3G') > 2) & (adata.obs_vector('CD4') < 3)); # adjust colors; adata.uns['selection_colors'] = ['blue', 'yellow']; sc.pl.umap(adata, color='selection', add_outline=True, s=20); ```; ![image](https://user-images.githubusercontent.com/4964309/77539317-75996a80-6ea1-11ea-8762-bb29b00d8e43.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1120#issuecomment-603826676:2,simpl,simple,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1120#issuecomment-603826676,1,['simpl'],['simple']
Usability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:531,clear,clear,531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,1,['clear'],['clear']
Usability,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/692#issuecomment-504362945:160,simpl,simply,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692#issuecomment-504362945,1,['simpl'],['simply']
Usability,"Actually, very good point about `scater`. I think the name can be kept. It's more the description that I found slightly convoluted and confusing. For reference, here is the description of `percent.top` in `scater`:. > An integer vector specifying the size(s) of the top set of high-abundance genes. Used to compute the percentage of library size occupied by the most highly expressed genes in each cell. (https://rdrr.io/github/LTLA/scuttle/man/perCellQCMetrics.html). alongside the description of `percent_top` in `scanpy`. > Which proportions of top genes to cover. If empty or None don’t calculate. Values are considered 1-indexed, percent_top=[50] finds cumulative proportion to the 50th most expressed gene. (https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.calculate_qc_metrics.html). I find it clearer when the description starts with ""integer"" rather than ""proportions"". Here's my Pythonized suggestion for scanpy:. > A list of integers specifying the sizes of the top sets of genes used to compute the percentage of library size occupied by the most highly expressed genes in each cell. Happy to hear feedback and further edits!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2842#issuecomment-1935690589:812,clear,clearer,812,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2842#issuecomment-1935690589,2,"['clear', 'feedback']","['clearer', 'feedback']"
Usability,"Agreed. I don’t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:103,simpl,simple,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247,1,['simpl'],['simple']
Usability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:746,intuit,intuitive,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,1,['intuit'],['intuitive']
Usability,"Ah sorry, I should have told you that. All of the preprocessing functions can be migrated to only work with AnnData; there is no important setting in which you want to pass an array or a sparse matrix. That's also remniscient from the early days when I thought people might not like to use AnnData. But that's of course stupid, they wouldn't use Scanpy in that case, anyway. I'm merging this for now so that we have something working for 1.4.1, but if you want to simplify and reduce this to AnnData-only, happy to merge a PR on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016:464,simpl,simplify,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016,1,['simpl'],['simplify']
Usability,"Ah, I cannot push to your fork it seems. I think you would have had to set ""allow maintainers to edit"" or something on the right-hand side. I'm simply merging this and editing after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479741041:144,simpl,simply,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479741041,1,['simpl'],['simply']
Usability,"Ah, sorry for being in the way here with the unrelated logging changes. Alex is currently a bit ill I learned, which is why he probably didn’t do it yet. I didn’t have time to review the whole thing, but if y’all want I can do that too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-462858876:102,learn,learned,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-462858876,1,['learn'],['learned']
Usability,"Ah, sorry, maybe this wasn't clear. You need to set the `.raw` attribute of `AnnData` for doing that at some point.; ```; adata.raw = adata # at the point during preprocessing at which you wish store a copy for visualization and differential testing; ```. You can then set `use_raw=False` in several functions, if you want to acess `.X` instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629:29,clear,clear,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395589629,1,['clear'],['clear']
Usability,"Ah, what I missed is the statement about metrics: I know that many people play around with different metrics. But then you mix ""representation engineering"" (preprocessing or machine learning) with manifold analysis. I'd say the cleanest is to always just use Euclidean distance and all the other work should be done already before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416730034:182,learn,learning,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416730034,1,['learn'],['learning']
Usability,"Alright! I've got a little example case I'd probably be using for a test case [here](https://gist.github.com/ivirshup/2a0d9a785339b719e7d372027ae2df31) (doublet prediction by simulation and projection). My current thoughts:. * Since we need to be working in the same feature space, we'll at least need PCA projection, but this is pretty easy:. <details>; <summary> Basic PCA projection </summary>. ```python; def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings (just whether to center?) from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt; ```. </details>. * Are you planning on storing the UMAP object in the AnnData? That would make transformation easier, but I see how on-disk representation could get complicated.; * What order should we do this in? Would you like everything to be accomplished by this PR or should we break it up?; * Are we introducing a general transfer learning api? Probably worth considering that a bit. Some relevant questions:; * Does the syntax still work for cases other than 1-to-1 transfer? ; * How do we deal with concatenation/ joins? The current `concatenate` doesn't join things like `obsm`.; * Alternatively, does everything have to be in the same AnnData? It would solve issues with having `var` be the same, but could complicate a lot of other code (many functions would need some kind of masking argument).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842:1087,learn,learning,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-481525842,1,['learn'],['learning']
Usability,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/371#issuecomment-441479438:202,simpl,simple,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-441479438,1,['simpl'],['simple']
Usability,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138:170,guid,guidelines,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138,1,['guid'],['guidelines']
Usability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:389,intuit,intuitive,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,1,['intuit'],['intuitive']
Usability,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/917#issuecomment-563302687:88,user experience,user experience,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917#issuecomment-563302687,1,['user experience'],['user experience']
Usability,"And, you're right: [line 486](https://github.com/theislab/scanpy/blob/17141d02ad19ad10aedd8361633be3cd670b3001/scanpy/preprocessing/simple.py#L486) could be a bug. It should be `zero_center is None` and not `zero_center is not None`. Hm, @Koncopd, could it be that the function does the opposite as wished? Did this happen when introducing the `chunked` version a couple of months ago or was it present from the beginning? It would be quite a serious bug... And @VolkerBergen would be right in this observation... Damn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446371088:132,simpl,simple,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446371088,1,['simpl'],['simple']
Usability,Any news when diffxpy will be up on biorxiv? The package looks super interesting and I'd like to learn more about the background to the methods you implemented.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-462890590:97,learn,learn,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-462890590,1,['learn'],['learn']
Usability,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:118,usab,usable,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952,1,['usab'],['usable']
Usability,"Are you talking about the `collections.abc.Mapping` in this case? [From the 3.7 docs](; https://docs.python.org/3/library/typing.html#classes-functions-and-decorators):. > In general, `isinstance()` and `issubclass()` should not be used with types. Additionally, those functions just throw an error for subscripted generics, so you definitely can't do `isinstance(m, Mapping[str, int])`. I don't think I'm totally clear on the differences in intended use cases for `ABC`s vs `Type`s. Are types only for annotation? Should ABCs not be used for annotations?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460:414,clear,clear,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445102460,1,['clear'],['clear']
Usability,"Arrrgh, this prepending of the 0s is a bug in scikit-learn. Thank you very much for pointing it out. Let me briefly think about solving this elegantly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/264#issuecomment-423784002:53,learn,learn,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-423784002,1,['learn'],['learn']
Usability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:1239,usab,usable,1239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,1,['usab'],['usable']
Usability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:62,learn,learn,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,1,['learn'],['learn']
Usability,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673:69,learn,learn,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673,2,['learn'],['learn']
Usability,"As mentioned before, can you move everything you currently have in `/preprocessing/simple.py` to `qc.py`? We shouldn't grow that file even larger...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433422519:83,simpl,simple,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433422519,1,['simpl'],['simple']
Usability,"As mentioned in the commit, fantastic! :smile:. Which style mentioned in https://github.com/theislab/scanpy/pull/610#issuecomment-484124854 did you decide for?. Most manual corrections I saw in your PR pointed to solution (1) [`**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)`]. That's fine, but we should mention it in `CONTRIBUTING.md`. 🙂What do you think, @flying-sheep?. ----. Why did you decide against the sklearn-style solution, which would have been simple to get without manual fixes (adding `**name**`)? https://github.com/theislab/scanpy/pull/610#issuecomment-484027492",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484418235:483,simpl,simple,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484418235,1,['simpl'],['simple']
Usability,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:225,simpl,simply,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844,2,"['intuit', 'simpl']","['intuitive', 'simply']"
Usability,At some point we moved scvelo's loom reading to anndata/scanpy and then simply called that one from within scvelo. . `scvelo.read` and `scanpy.read` are ecaxtly the same. And `read_loom` is called internally within `read`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1074#issuecomment-592336678:72,simpl,simply,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074#issuecomment-592336678,1,['simpl'],['simply']
Usability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:202,usab,usable,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,3,['usab'],"['usability', 'usable']"
Usability,"Awesome idea! :). Sorry that I haven't merged all your other PRs, yet. I got back to work on Scanpy yesterday or so after being sick for 2 weeks and another 2 weeks of complete chaos (sick babies)... ;). You'll get feedback very soon, but at first sight, all of them looked fine, anyway. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/489#issuecomment-470934197:215,feedback,feedback,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/489#issuecomment-470934197,1,['feedback'],['feedback']
Usability,"Awesome, Gokcen, thank you! :grin:. Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357785692:131,simpl,simple,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357785692,1,['simpl'],['simple']
Usability,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:364,simpl,simply,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797,1,['simpl'],['simply']
Usability,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:336,simpl,simpler,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,1,['simpl'],['simpler']
Usability,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:396,simpl,simple,396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732,1,['simpl'],['simple']
Usability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:294,guid,guides,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,1,['guid'],['guides']
Usability,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575:96,clear,clear,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575,1,['clear'],['clear']
Usability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:410,simpl,simply,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,2,"['feedback', 'simpl']","['feedback', 'simply']"
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2901""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104,1,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2962""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203,1,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2974""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380,1,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2984""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743,1,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3120""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017,1,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3216""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331,1,['feedback'],['feedback']
Usability,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3222""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303:303,feedback,feedback,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303,1,['feedback'],['feedback']
Usability,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python; import scanpy as sc; import matplotlib as mpl; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata, color=[""louvain"", ""HES4""]); sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds); ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/476#issuecomment-462582018:467,clear,clearly,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476#issuecomment-462582018,1,['clear'],['clearly']
Usability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:418,simpl,simplicity,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['simpl'],['simplicity']
Usability,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424799480:132,simpl,simple,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424799480,1,['simpl'],['simple']
Usability,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:192,simpl,simple,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485,1,['simpl'],['simple']
Usability,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:221,simpl,simpler,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109,1,['simpl'],['simpler']
Usability,Could you provide some more details? It'd be useful to see a script that can reproduce this behavior from scratch. There are some guides on how to write this up in the [contributing section](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md#before-filing-an-issue). Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-554926531:130,guid,guides,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-554926531,1,['guid'],['guides']
Usability,"Could you tell us about how you've found it useful, or point us towards some literature on it being used? I think this would be easy enough to implement, but when I tried a naive implementation the results weren't that compelling. It's very possible I should've played around with the parameters more. @flying-sheep, do you have thoughts on this?. Here's a simple implementation:. ```python; def ica(adata, n_components, inplace=True, **kwargs): ; from sklearn.decomposition import FastICA ; ica_transformer = FastICA(n_components=n_components, **kwargs) ; x_ica = ica_transformer.fit_transform(adata.X) ; if inplace:; adata.obsm[""X_ica""] = x_ica ; adata.varm[""ICs""] = ica_transformer.components_.T ; else:; return ica_transformer ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-519071187:357,simpl,simple,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519071187,1,['simpl'],['simple']
Usability,"Dear @fbrundu,. very sorry for the late response. I took a couple of days off with the family. Everything is dictated by the order in `.obs['mycategorical'].cat.categories`. If you're starting off from a string `.obs['mystring']` annotation, then this will default to `natsorted` categories. If you pandas `.reorder_categories` then this will be reflected, too. Now, in `AnnData`, we have the additional possibility to store colors for each category. The corresponding array matches `.obs['mycategorical'].cat.categories` just that instead of the category names, it stores the colors. Let me know if this clears things up for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/218#issuecomment-408749326:605,clear,clears,605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/218#issuecomment-408749326,1,['clear'],['clears']
Usability,"Dear @wflynny, sorry for the late response. And sorry that I don't feel able to comment on imputation techniques, it's simply something that I don't have a lot of experience with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-403629451:119,simpl,simply,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-403629451,1,['simpl'],['simply']
Usability,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks!. Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405:283,simpl,simple,283,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405,1,['simpl'],['simple']
Usability,"Did you recompute using `tl.paga` in between?. You're passing something from the 14 cluster calculation to the 4-cluster call, as is evident from ; > 'c' argument has 14 elements, which is not acceptable for use with 'x' with size 4, 'y' with size 4. It has nothing to do with anything in matplotlib. If the docs aren't clear enough, please let me know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/381#issuecomment-443398454:320,clear,clear,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381#issuecomment-443398454,1,['clear'],['clear']
Usability,"Did you simply set the verbosity to a higher level to get timings or did you have to use profiling or modify the code? If the latter, it might be helpful for more people to do more fine-grained timings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528250968:8,simpl,simply,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528250968,1,['simpl'],['simply']
Usability,"Do you guys think this PR makes sense or is it too much to add R packages to the travis setup? @ivirshup @flying-sheep . There are bunch of useful R packages out there that will most likely not be reimplemented in Python (limma-voom pseudobulk DE, Liger, MAST etc.). I think it'd be cool to revive rtools and add access to such packages. I'm not sure if this is the right way but, any guidance is appreciated :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854:385,guid,guidance,385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854,1,['guid'],['guidance']
Usability,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:330,learn,learn,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843,1,['learn'],['learn']
Usability,Does this problem also happens with the previous code? this is just to guide me on a solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/309#issuecomment-431269990:71,guid,guide,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309#issuecomment-431269990,1,['guid'],['guide']
Usability,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496837522:20,learn,learn,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496837522,1,['learn'],['learn']
Usability,"Encountered this same error, not clear what is causing it. . ``` ; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata.raw = adata; sc.pp.scale(adata, max_value=10); sc.pp.filter_genes(adata, min_cells=2); sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=2000); ; ```; With the same error:. ```; File ""/opt/venv/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 400, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n""; ValueError: Bin edges must be unique: array([ -inf, -4.47034836e-08, -2.98023224e-08, -2.98023224e-08,; -1.49011612e-08, -8.38190317e-09, 1.00000000e-12, 1.00000000e-12,; 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 4.09781933e-09,; 1.49011612e-08, 1.49011612e-08, 1.49011612e-08, 1.49011612e-08,; 2.98023224e-08, 4.47034836e-08, 4.47034836e-08, 5.96046448e-08,; inf]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. Very possibly an input error, but the output doesn't point to anything useful as a starting point to debug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/509#issuecomment-1147252922:33,clear,clear,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509#issuecomment-1147252922,1,['clear'],['clear']
Usability,"Even something simple doesn't work anymore, without going through h5ad:. ```; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; Traceback (most recent call last):; File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>; cellbrowser.cbScanpyCli(); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli; adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices; obs = _normalize_index(obs, self._adata.obs_names); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index; positions = positions[index]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__; return self._get_with(key); File ""/cluster/home/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138:15,simpl,simple,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138,1,['simpl'],['simple']
Usability,"Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867898442:86,guid,guidance,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867898442,1,['guid'],['guidance']
Usability,Exactly same error here.; info:; scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371:130,learn,learn,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371,1,['learn'],['learn']
Usability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:596,learn,learn,596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,1,['learn'],['learn']
Usability,"FWIW, confirming `umap-learn 0.5.1` works with `numba==0.53.1` on my machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846837983:23,learn,learn,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846837983,1,['learn'],['learn']
Usability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:27,guid,guidance,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,1,['guid'],['guidance']
Usability,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512171197:82,learn,learn,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512171197,1,['learn'],['learn']
Usability,"Flit has a very tiny surface area. You can learn its full CLI in literally 2 minutes, as it doesn’t include any kind of new concept (like Poetry’s venv management).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904146:43,learn,learn,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904146,1,['learn'],['learn']
Usability,"Following @Koncopd 's idea, wouldn't it be sufficient to simply have line 340 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py changed to ; ```; mu = M.mean(1).A1 if issparse(M) else M.mean(1); B = safe_sparse_dot(Q.T, M) - safe_sparse_dot(Q.T, mu[:, None]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446396916:57,simpl,simply,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446396916,3,"['learn', 'simpl']","['learn', 'simply']"
Usability,"For general help like this, please go to https://scanpy.discourse.group/. This is also what the issue template says. How could we have made the text more clear so that you’d have found your way there?. ![grafik](https://user-images.githubusercontent.com/291575/69068901-e0cfbd80-0a25-11ea-8095-52e567b86574.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-555083714:154,clear,clear,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-555083714,1,['clear'],['clear']
Usability,"For me the documentation seems clear, but the example may be confusing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905#issuecomment-557090359:31,clear,clear,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905#issuecomment-557090359,1,['clear'],['clear']
Usability,"For pca of sparse matrices i think it should work this way; 1) Create class for lazy evaluation of X - mean, i.e store X, mean separately and implement multiplication by some dense B as `sparse.csr_matrix.dot(X, B) - mean.dot(B)`.; 2) Pass instance of this class to [randomized_svd](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L233)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-445972448:309,learn,learn,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-445972448,2,['learn'],['learn']
Usability,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333#issuecomment-434298485:114,learn,learn,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-434298485,1,['learn'],['learn']
Usability,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:137,learn,learn,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745,1,['learn'],['learn']
Usability,"From a quick look at the Seurat code this was borrowed from, I think normalised (but not scaled) data is used, so maybe the scaling isn't needed https://github.com/satijalab/seurat/blob/656fc8b562d53e5d0cedda9e09d9dda81e8c00e9/R/utilities.R#L192. Either way it would be good for this to be clear in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2909#issuecomment-1996819867:290,clear,clear,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2909#issuecomment-1996819867,1,['clear'],['clear']
Usability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:852,simpl,simple,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['simpl'],['simple']
Usability,"Given that UMAP can be used for manifold learning, shouldn’t be possibile to align experiments using UMAP? Who wants to join me in this evaluation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424605733:41,learn,learning,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424605733,1,['learn'],['learning']
Usability,"Good to hear! Looking forward to learning more about it.; PS: Having a doublet detection tool in `tl` would be fine, I'd say... `pp` and `tl` are just meant to give a rough orientation for users... in some cases, it's not completely clear what *preprocessing* and what *downstream* analysis is...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845:33,learn,learning,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400277845,2,"['clear', 'learn']","['clear', 'learning']"
Usability,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:215,simpl,simply,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,2,['simpl'],['simply']
Usability,Gotcha! I'll prioritize getting the benchmarks up and then I'll need some guidance on how to organize it to fit in scanpy's codebase. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655:74,guid,guidance,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655,1,['guid'],['guidance']
Usability,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598:763,learn,learn,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598,2,['learn'],['learn']
Usability,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441598257:426,simpl,simply,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441598257,1,['simpl'],['simply']
Usability,"Great! :smile:. Sure, next Wednesday is fine. Also, we can always simply make 1.4.2. I just need to check one tiny thing before we actually make the release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355:66,simpl,simply,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355,1,['simpl'],['simply']
Usability,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:340,learn,learning,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,3,['learn'],"['learned', 'learning']"
Usability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:1347,simpl,simple,1347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,1,['simpl'],['simple']
Usability,"Great, thanks for the feedback. Hopefully this merge and commit fix everything. I wasn't able to see what errors were causing the readthedocs build fail as the ""Details"" link just took me to a page that said ""SORRY / This page does not exist yet."", so let me know if there are any other issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338:22,feedback,feedback,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338,1,['feedback'],['feedback']
Usability,"Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with ; ```; pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; ```. Seems to work now for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996:216,simpl,simple,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996,1,['simpl'],['simple']
Usability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:126,clear,clear,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,1,['clear'],['clear']
Usability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:1230,simpl,simple,1230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,1,['simpl'],['simple']
Usability,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/710#issuecomment-507120679:78,learn,learn,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710#issuecomment-507120679,2,['learn'],['learn']
Usability,"Hello @Koncopd ,; Thanks for the response!. Step 1: I created a fresh new environment (py3.8.12); ```python; !pip install scanpy[leiden]. Successfully installed anndata-0.7.8 cycler-0.11.0 fonttools-4.28.5 h5py-3.6.0 igraph-0.9.9 joblib-1.1.0 kiwisolver-1.3.2 leidenalg-0.8.8 llvmlite-0.38.0 matplotlib-3.5.1 natsort-8.0.2 networkx-2.6.3 numba-0.55.0 numexpr-2.8.1 numpy-1.21.5 pandas-1.3.5 patsy-0.5.2 pillow-9.0.0 pynndescent-0.5.5 python-igraph-0.9.9 scanpy-1.8.2 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.13.1 stdlib-list-0.8.0 tables-3.7.0 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:474,learn,learn-,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['learn'],['learn-']
Usability,"Hello All,; I am a totally new one in learning single cell RNA_seq. When I was doing quality control, I met this problem. Can anyone help me? Thank you so much",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/647#issuecomment-492876929:38,learn,learning,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647#issuecomment-492876929,1,['learn'],['learning']
Usability,"Hello, I am running into exactly the same bug when using both **scanpy-1.5.1** or **scanpy-1.5.0**. . **Versions**: ; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.3.2 pandas==1.0.4 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.0. **Input**: ; `import time; t0 = time.time(); sc.external.exporting.spring_project(adata, './SPRING',; 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; custom_color_tracks=['total_counts']); print(time.time() - t0)`. **Output**: ; `WARNING: root:Overwriting the files in SPRING.; Writing subplot to SPRING\all; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-59-9c683583ff59> in <module>; 1 import time; 2 t0 = time.time(); ----> 3 sc.external.exporting.spring_project(adata, './SPRING',; 4 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; 5 custom_color_tracks=['total_counts']). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite); 179 ; 180 # Write graph in two formats for backwards compatibility; --> 181 edges = _get_edges(adata, neighbors_key); 182 _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges); 183 _write_edges(subplot_dir / 'edges.csv', edges). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in _get_edges(adata, neighbors_key); 217 ; 218 def _get_edges(adata, neighbors_key=None):; --> 219 neighbors = NeighborsView(adata, neighbors_key); 220 if 'distances' in neighbors: # these are sparse matrices; 221 matrix = neighbors['distances']. NameError: name 'NeighborsView' is not defined`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1260#issuecomment-645395239:207,learn,learn,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260#issuecomment-645395239,1,['learn'],['learn']
Usability,"Hello,. thank you for your help. I was able to find the issue, I have >50 categories that were sorted, and I did not want to input them manually into a list (like your example). After sorting I got an object type 'pandas.core.arrays.categorical.Categorical', that dotplot is able to read the order from, but stacked_violin interpreted differently, using the 'Categories' section. My solution was as simple as converting to list with:; correct_order = new_order.tolist(). Thank you so much for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2611#issuecomment-1677594358:399,simpl,simple,399,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611#issuecomment-1677594358,1,['simpl'],['simple']
Usability,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:463,simpl,simple,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,1,['simpl'],['simple']
Usability,"Here is the old package information, in which I run Scanpy very well before.; Windows 10 x64 bit, 20H2. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.3 pandas==1.3.5 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5; scvelo==0.2.4 scanpy==1.8.2 anndata==0.7.8 loompy==3.0.6 numpy==1.20.3 scipy==1.7.3 matplotlib==3.5.1 sklearn==1.0.1 pandas==1.3.5; cellrank==1.5.0 scanpy==1.8.2 anndata==0.7.8 numpy==1.20.3 numba==0.54.1 scipy==1.7.3 pandas==1.3.5 pygpcca==1.0.2 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 scvelo==0.2.4 pygam==0.8.0 matplotlib==3.5.1 seaborn==0.11.2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012619831:193,learn,learn,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012619831,2,['learn'],['learn']
Usability,"Here is the summary:; * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`; * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`; * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150:512,simpl,simplified,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150,1,['simpl'],['simplified']
Usability,"Here's [test code](https://gist.github.com/jorvis/da877d89fd159b2fb7dfba26705f7ceb) and my output is:. ```pytb; Initial shape: 737280x28002; After min_genes: 5128x28002; After max_genes: 1431x28002; Traceback (most recent call last):; File ""/tmp/test_cell_and_gene_filter.py"", line 22, in <module>; sc.pp.filter_genes(adata, min_cells=3); File ""/home/jorvis/git/scanpy/scanpy/preprocessing/simple.py"", line 152, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. Note that this same error displays on both of the following lines:. ```python; sc.pp.filter_genes(adata, min_cells=3); sc.pp.filter_genes(adata, max_cells=1000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317:390,simpl,simple,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364468317,1,['simpl'],['simple']
Usability,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:218,simpl,simply,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,2,['simpl'],"['simple', 'simply']"
Usability,"Hey @a-munoz-rojas,. I normally wouldn't redo the batch correction. That can go wrong (or better tbh)... for scanorama it could be better, but for DL-based methods you would have fewer data points for learning the difference between batch and bio effects. So unless you have a large dataset, it might generate problems for those methods. Therefore I try to stay consistent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1060433444:201,learn,learning,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1060433444,1,['learn'],['learning']
Usability,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:308,feedback,feedback,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133,1,['feedback'],['feedback']
Usability,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:644,feedback,feedback,644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292,1,['feedback'],['feedback']
Usability,"Hey @ivirshup , could you please give me a feedback on this if there should be any improvements?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-603993724:43,feedback,feedback,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-603993724,1,['feedback'],['feedback']
Usability,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up!; Cheers,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007:462,feedback,feedback,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007,1,['feedback'],['feedback']
Usability,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it.; 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,; Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1775#issuecomment-813543994:31,feedback,feedback,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775#issuecomment-813543994,1,['feedback'],['feedback']
Usability,"Hey @ivirshup,. thanks for your comments, that looks like good feedback! Hope your moving went well :); I'm busier than expected this week, but will take some time next week to respond / make the changes you suggested.; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-959860881:63,feedback,feedback,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-959860881,1,['feedback'],['feedback']
Usability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:783,usab,usable,783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,1,['usab'],['usable']
Usability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:1153,simpl,simple,1153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,1,['simpl'],['simple']
Usability,"Hey Isaac - thanks for the tip, I did not know about `add_totals`! Violin plots are, in principle, exactly what I would like to have, but they are very hard to read for genes with lots of zeros or for clusters with a lot of cells. I am still worried that this doesn't make the relative numbers clear. For instance, something like 40% of CD14+ monocytes express LDHB, while maybe 80% of dendritic cells do. This looks like it might be more ""relevant"", but in reality, the number of monocytes that express the gene is three times the number of total dendritic cells. Does this make sense?. I guess that in the end the onus is on me to avoid or highlight such ambiguities in the analysis, and remain cognisant of them while looking at the data. I will keep thinking about this and post here if I have an epiphany.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016453592:294,clear,clear,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1016453592,1,['clear'],['clear']
Usability,"Hey Lisa!; I think I can see that connection now. But I had initially interpreted the ""If provided"" to mean if I specified, but seems like it also applies to the case where `mpl.rcParams[""axes.prop_cycle""]` is provided for me.; Instead of ""If provided, values of adata.uns[""{var}_colors""] will be set."" I would find it more clear to say something like ""adata.uns[""{var}_colors""] will be set if not already stored""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1258452834:324,clear,clear,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1258452834,1,['clear'],['clear']
Usability,Hey all!. Just wanted to let you know that I've also made a PR for the tutorial as @giovp suggested:. https://github.com/theislab/scanpy-tutorials/pull/43. Let me know what's left to do - looking forward to your feedback!. Jan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-889285607:212,feedback,feedback,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-889285607,1,['feedback'],['feedback']
Usability,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:99,feedback,feedback,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467,2,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:30,feedback,feedback,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,1,['feedback'],['feedback']
Usability,"Hey! I just wanted to find you in your office... We should discuss and look at this in person... Non-working indentation, for instance, would be a serious problem... I'd suggest waiting a little bit longer before you write more hacks... in principle, I was satisfied with the docs except for referencing `scanpy.api.AnnData`... I still don't see the big advantage of using type annotation outside of the docstrings... As they are right now, they look very good when rendered as html and they look good as plain text when invoked within a Jupyter notebook... Of course, you usually have the better arguments on such questions in the end, but I'm not fully convinced at this stage. So, let's discuss in person. :smile:. PS: Evidently, scanpy's style for docstrings simply imitates numpy, pandas, seaborn, scikit-learn. I'm not sure whether one should break these conventions... Also, you imagine how much work it was - many iterations over the past 12 months - to get all the type annotations, the ""optional"" keyword and the default value into the docstrings... as I'm busy like crazy with other stuff, I'm again hesitant to make such big changes... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-380066080:763,simpl,simply,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-380066080,2,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Hey! Sorry for the late reply:; 1. Yes, a separate file, please.; 2. Put both in the same function. I wouldn't call it coexpression though. Something along the lines of `cell_selection_by_genes()` or just `cell_selection()`.; 3. There is a `sort_order` keyword for plotting which works for continuous covariates. I imagine that should work.; 4. That may be overkill... but it would definitely be interesting. I think MAGIC is in `sc.external` and DCA is also easily usable in this framework. They are not part of the core package though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328:466,usab,usable,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328,1,['usab'],['usable']
Usability,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:989,clear,clearer,989,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347,1,['clear'],['clearer']
Usability,"Hey!; I wrote this function a while ago... it was definitely not the cleanest or quickest implementation. And it did take a while to run on ~5k cells at the time, but I thought it would be useful to have this functionality in scanpy. Just wanted to note that the intention was definitely to implement this without resampling. I clearly missed that the default was to use resampling in `np.random.choice`. Thanks for spotting this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/340#issuecomment-435326551:328,clear,clearly,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435326551,1,['clear'],['clearly']
Usability,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:534,intuit,intuitive,534,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527,1,['intuit'],['intuitive']
Usability,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:1551,feedback,feedback,1551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207,1,['feedback'],['feedback']
Usability,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:850,guid,guide,850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790,1,['guid'],['guide']
Usability,"Hey,. indeed there seems to be an [issue](https://github.com/mwaskom/seaborn/issues/3522) with our current usage of `seaborn`, not working with `seaborn 0.13.0`.; This has been fixed on the main branch [here](https://github.com/scverse/scanpy/pull/2661), and we'll eventually take over the newest `seaborn` version once this is cleared. For users running into this issue now ; - first check if you indeed have `seaborn 0.13.0`. If yes, then do; - `pip install seaborn==0.12.2` if using pip or; - `conda install seaborn=0.12.2` if using conda. this makes sure you are using the working version of seaborn. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215:328,clear,cleared,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215,1,['clear'],['cleared']
Usability,"Hi @GMaciag,. This looks like a simple function that people may like to use. Do you want to write a small helper function for this maybe? This might be nice to add to `sc.tl`. One way you could make it display nicely in `sc.pl.umap()` is by turning the values into `pd.Categorical`. In the end you want to show which cells are co-expressing your genes. . Also, this may be a good use for imputation methods. Otherwise you may struggle with the sparsity of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-587532154:32,simpl,simple,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-587532154,1,['simpl'],['simple']
Usability,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:213,simpl,simple,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,1,['simpl'],['simple']
Usability,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:189,simpl,simple,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216,1,['simpl'],['simple']
Usability,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:417,simpl,simple,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172,2,['simpl'],['simple']
Usability,"Hi @a-munoz-rojas, please use [fenced code blocks](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown) for code. The “language” for tracebacks is “pytb”: ```` ```pytb````. @ivirshup do you know what’s happening here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/884#issuecomment-545328026:59,guid,guides,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884#issuecomment-545328026,1,['guid'],['guides']
Usability,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:154,simpl,simple,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211,1,['simpl'],['simple']
Usability,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:521,simpl,simplicity,521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,1,['simpl'],['simplicity']
Usability,"Hi @atarashansky and everyone following this interesting discussion!. I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`!. Looking forward to you thoughts on this :); Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382:403,simpl,simplified,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382,3,"['learn', 'simpl']","['learn', 'learns', 'simplified']"
Usability,"Hi @chris-rands I hope so. . I'm one of the authors of `solo`. I'm currently working on getting `solo` into `scvi-tools`. After that it should be relatively simple to add to scanpy. Best,; Nick",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1432#issuecomment-788402755:157,simpl,simple,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-788402755,1,['simpl'],['simple']
Usability,"Hi @davidsebfischer, I am writing a simple jupyter notebook where I am analysing the 10x_pbmc68k_reduced.h5ad data. I selected only clusters 0 and 1:; `twoClusters = adata[np.logical_or(adata.obs.louvain == '0', adata.obs.louvain == '1')]; `. Running `sc.tl.rank_genes_groups(twoClusters, groupby='louvain, method='wilcoxon', corr_method=''bonferroni)`, I obtained the following genes. ![image](https://user-images.githubusercontent.com/26186755/54123532-ec2f0b80-43f7-11e9-8c2f-f506b9170e55.png). Trying with `diffxxy` library,; `test = de.test.wilcoxon(data=twoClusters, grouping=""louvain""); `; there is the following error: _All numbers are identical in mannwhitneyu_. > @andrea-tango please use dev right now. For this test, I used the version downloaded with pip.; I can clone the repository and use the diffxpy dev branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124:36,simpl,simple,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471519124,1,['simpl'],['simple']
Usability,"Hi @falexwolf ; I am sending you the simpler code ,my anndata which can be reproduced in some way, and csv marker file calculated by Seurat to your email, you might to repeat my marker analysis if you would like. Sorry for doing it in this way, I not familiar about how to make public notebook...and our data is too preliminary to be public.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471373445:37,simpl,simpler,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471373445,1,['simpl'],['simpler']
Usability,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:235,clear,clear,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773,1,['clear'],['clear']
Usability,"Hi @fidelram,; One way in which I'd like to do it is like the following:. ```python; sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],; save=False, use_raw=False); ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second).; One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320:509,clear,clear,509,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463771320,1,['clear'],['clear']
Usability,"Hi @fidelram,; Thanks for your prompt reply! Sorry I didn't make my question clear. ; I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/732#issuecomment-510146221:77,clear,clear,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732#issuecomment-510146221,1,['clear'],['clear']
Usability,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:653,learn,learn,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,1,['learn'],['learn']
Usability,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:699,simpl,simple,699,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404,1,['simpl'],['simple']
Usability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:253,simpl,simpler,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,1,['simpl'],['simpler']
Usability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:846,learn,learned,846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['learn'],['learned']
Usability,"Hi @ivirshup . Thanks for the help. In terms of the use cases here:. (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. . I'm a bit confused why Seurat or ScanPy never did this....but then I realize that Pagoda2 didn't either: https://github.com/kharchenkolab/pagoda2/blob/main/R/Pagoda2.R#L900. (There's a bit of multithreading there at the end...). Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). (2) In terms of our use case, an interactive way to run DE via the client is too slow. We've just started to implement the above ourselves. . **RE: pertpy**. Could does this relate to @davidsebfischer and diffxpy?. Best, Evan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1387503006:706,simpl,simple,706,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1387503006,1,['simpl'],['simple']
Usability,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```; # compare expression levels of mel vs all other cell types in pairwise manner; sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-32-73add1f79f3a> in <module>; 1 # save as a data frame; 2 ; ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'); 4 ; 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols); 53 d = pd.DataFrame(); 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:; ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]; 56 if pval_cutoff is not None:; 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx); 517 ; 518 def __getitem__(self, indx):; --> 519 obj = super(recarray, self).__getitem__(indx); 520 ; 521 # copy behavior of getattr, except that here. ValueError: no field of name mel; ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616:67,clear,clear,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616,1,['clear'],['clear']
Usability,"Hi @ivirshup,. Yes, both of them are me. I incorporated feedback from the help forum which suggested that this function has a bug, hence, the bug report.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1485#issuecomment-722483698:56,feedback,feedback,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485#issuecomment-722483698,1,['feedback'],['feedback']
Usability,"Hi @jorvis ,. ProjectR as far as I'm aware relies on some form of matrix decompositon (PCA, NMF) and do transfer learning with learnt weights. In some sense, it's similar to ingest. However, it would be a bit complicated to port it from R. ; It doesn't seem super complicated to have it in scanpy as an additional tool, but it's not really a priority now. If you have anything in mind and would want to try with submitting a PR, it would be very much appreciated and we would definitely have a look! ; I'll close this for now, but pls feel free to re-open if you want to continue discussion or would like to discuss implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075:113,learn,learning,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075,2,['learn'],"['learning', 'learnt']"
Usability,"Hi @transcriptomics, ; If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616,2,"['guid', 'learn']","['guide', 'learn']"
Usability,"Hi @venomandvenus, it's not clear if this is really a bug or you are just getting a bunch of warnings from seaborn, which can happen quite often. Did `sc.pl.violin` eventually output the plot you wanted? Also can you post a link to the tutorial you are following?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1742#issuecomment-802755481:28,clear,clear,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742#issuecomment-802755481,1,['clear'],['clear']
Usability,"Hi @vitkl . thanks a lot for the feedback, all noted, we'll work toward enabling large tissue image available for storing+plotting. Will keep this open for reference!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782:33,feedback,feedback,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782,1,['feedback'],['feedback']
Usability,"Hi Alex!. Sorry for this long delay, I just forgot completely.; Maybe I wasn't clear enough in my original post, here is where the issue lies:; ; When I run . ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'); ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'); ```. then I get the following error. ```pytb; 100 groups_order = [str(n) for n in groups_order]; 101 if reference != 'rest' and reference not in set(groups_order):; --> 102 groups_order += [reference]; 103 if (reference != 'rest'; 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list; ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py; sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']); ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624:79,clear,clear,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-445219624,1,['clear'],['clear']
Usability,"Hi David, . thanks for your reply and your interest!. > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or cust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:150,learn,learned,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,2,['learn'],['learned']
Usability,"Hi David,; it's currently not a focus, at least for me... Our general perspective is to replace all of the manual preprocessing with some ""deep learning-based preprocessing""... We will soon have something on this... If it works, more advanced preprocessing becomes obsolete, I guess. If it doesn't, we'll definitely add more advanced stuff... Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/107#issuecomment-374180584:144,learn,learning-based,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/107#issuecomment-374180584,1,['learn'],['learning-based']
Usability,"Hi Davide,. thank you! Currently, we use the default behavior of pandas concatenate: see [here](https://github.com/theislab/anndata/blob/562954b43a9b8faa969e0ec01707bc56cbc021b0/anndata/base.py#L1371-L1400). I'll not be able to fix this during the next days. @flying-sheep, could you have a look and maybe figure out a meaningful option or meaningful default to circumvent this? It should be easy to simply pass an option to DataFrame.concat(). @dawe Thanks again for your pull request. I also put a new anndata version that incorporates it on PyPI. Cheers, ; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354371241:400,simpl,simply,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354371241,1,['simpl'],['simply']
Usability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:639,learn,learn,639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,1,['learn'],['learn']
Usability,"Hi Joshua, can you upload an example dataset somewhere? So that I can reproduce the figure above? I'm confident that I can speed this up...; PS: Still consolidating all the gene correlation stuff... Everything works, but we do not want to expose things to the user that have not been checked 3 times... in particular the conventions need to be intuitive etc.; PPS: The new cell cycle example could interest you:; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html; https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes_cell_cycle.html; Both link to the notebook in the ""Examples"" section; https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899:344,intuit,intuitive,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-365873899,1,['intuit'],['intuitive']
Usability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:254,simpl,simply,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['simpl'],['simply']
Usability,"Hi Michael:. I compare the `umap` in `scanpy` with the original [`umap`](https://umap-learn.readthedocs.io/en/latest/plotting.html) (https://umap-learn.readthedocs.io/en/latest/plotting.html) using the same dataset, the original `umap` works well, I think the problem is in `scanpy`. I edited my question to include this.; Do you have some suggestions?; Thanks. Dan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364160150:86,learn,learn,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364160150,2,['learn'],['learn']
Usability,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:202,clear,clearly,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498,1,['clear'],['clearly']
Usability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:240,simpl,simply,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,5,['simpl'],"['simple', 'simply']"
Usability,"Hi all! I've been using `scanpy` and ran into a similar problem. I was wondering if there's an easy / appropriate work-around, other than simply deleting `obsm[X_diffmap]`?. Thank you all for developing `scanpy`, it's a really wonderful piece of software.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-739622198:138,simpl,simply,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-739622198,1,['simpl'],['simply']
Usability,"Hi all,. thanks for the nice discussion!. Phenograph is cited not for the specific implementation but for suggesting to use community detection for clustering in single-cell data. This is what the docs state ""The Louvain algorithm has been proposed for single-cell analysis by [Levine15]."". My opinion on clustering algorithms: use something that respects the topology of the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:645,clear,clearly,645,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['clear'],['clearly']
Usability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1520,simpl,simpler,1520,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['simpl'],['simpler']
Usability,"Hi! Good to read! :smile:. The PAGA edges simply mean that clusters are topologically connected - in the single-cell graph, there is a significant number of inter-cluster-edges, above noise-level. They absolutely don't have an orientation. Regarding velocyto: yes, it's possible to use it to orient the edges in PAGA. You can get that functionality following [this](https://github.com/falexwolf/paga/blob/master/planaria/planaria_paga_velocyto.ipynb); however, until this becomes really well-documented etc. this will still take a while... the model behind this will also be subject to change, I guess... Get Scanpy 1.1 for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393738263:42,simpl,simply,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393738263,1,['simpl'],['simply']
Usability,Hi! I added a more extensive explanation in the [docs](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap). . The line in the code where the sigmas are calculated is [here](https://github.com/theislab/scanpy/blob/e78062a0e4f02888cab080f8ed2571ff7764efc7/scanpy/neighbors/__init__.py#L725). There is no explicit way of changing this parameter... you can only implicitly change it via the number of neighbors. See the [examples](https://scanpy.readthedocs.io/en/latest/examples.html#simple-pseudotime). Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/120#issuecomment-380016607:523,simpl,simple-pseudotime,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380016607,1,['simpl'],['simple-pseudotime']
Usability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:117,clear,clear,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,1,['clear'],['clear']
Usability,"Hi! Sorry for frustrating you :( if you want I can fix and merge it manually. You're doing great work!. You simply need to import the things you're using in the annotations, then it'll work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-439104094:108,simpl,simply,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439104094,1,['simpl'],['simply']
Usability,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/673#issuecomment-528510773:221,simpl,simplify,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673#issuecomment-528510773,1,['simpl'],['simplify']
Usability,"Hi! Sorry, my bad. That parameter was undocumented, and I added it to the wrong docstring building block. It’s available in all scatterplots for dimension reductions, like `pca`, `tsne` and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/545#issuecomment-475158480:38,undo,undocumented,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545#issuecomment-475158480,1,['undo'],['undocumented']
Usability,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place!. Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-538903445:434,simpl,simply,434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-538903445,1,['simpl'],['simply']
Usability,"Hi! thank you for the praise! I’m afraid this is a documentation bug!. We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/875#issuecomment-545958927:336,simpl,simplify,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875#issuecomment-545958927,1,['simpl'],['simplify']
Usability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:87,learn,learning,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,1,['learn'],['learning']
Usability,"Hi!. Thanks for reaching out!. We have an option to compute connectivity based on minimum distance, right. The default choice, however, is based on edge-statistics (actual inter-edges between clusters vs. expected number of edges in random connections). Currently, I'm working on the revision of the algorithm. The option for minimum distance will disappear and everything will become much cleaner. I'm also trying to improve the statistical model for connectivity and provide a clearer option for its significance threshold. Right now, the only relevant option in the whole AGA [given the single-cell graph is computed and clustered] is `tree_based_confidence=True`; if you set this to `False`, the significance value for edges to appear will be much lower and you'll get a much sparser abstracted graph. However, this graph is sometimes too sparse. If `tree_based_confidence=True`, as per default, this works fine on very connected datasets, but sometimes gives results that are too dense on disconnected datasets. For now, you could simply try setting `tree_based_confidence=False` and see whether this is satisfying. If not; probably too sparse, it would be great if you could try the new AGA version in a couple of days. Also, I'd be very happy to run the method on your data and look at specific issues. You can also approach me via email... Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-370139940:479,clear,clearer,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-370139940,2,"['clear', 'simpl']","['clearer', 'simply']"
Usability,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:86,simpl,simply,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113,1,['simpl'],['simply']
Usability,"Hi, ; I tested your fix and it works! ; ```; scanpy==1.4.5.2.dev6+gfa408dc7 anndata==0.7.1 ; umap==0.3.10 numpy==1.17.4 scipy==1.3.1 ; pandas==0.25.2 scikit-learn==0.21.3 ; statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```; BTW:; `matplotlib==3.1.3`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599:157,learn,learn,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599,1,['learn'],['learn']
Usability,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. ; Could you please let me know what kind of solution will be good for this issue?. Thanks! ; <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">; <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1103#issuecomment-633326096:55,learn,learning,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103#issuecomment-633326096,1,['learn'],['learning']
Usability,"Hi, here my settings:. scanpy==1.0.4 anndata==0.6.1 numpy==1.14.2 scipy==1.1.0 pandas==0.23.0 ; scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1. Ivan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390656723:103,learn,learn,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390656723,1,['learn'],['learn']
Usability,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help?. This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058:101,learn,learn,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058,1,['learn'],['learn']
Usability,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:1592,learn,learn,1592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889,1,['learn'],['learn']
Usability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1397,simpl,simple,1397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,1,['simpl'],['simple']
Usability,"Hi, thanks for the contribution!. The converting to dense is quite iffy, we should probably add real support for sparse here. [We could use this as a base](https://github.com/scikit-learn/scikit-learn/blob/45cf8ec555a026c4263e8bef12850755a83df10e/sklearn/utils/sparsefuncs.py#L685). Do you think you can do that or should we help?. This is also missing release notes in 1.11.0.md",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449:182,learn,learn,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449,2,['learn'],['learn']
Usability,"Hi, thanks for the feedback! Another interesting option is [AUCell](https://github.com/aertslab/pySCENIC/blob/master/src/pyscenic/aucell.py) from the [SCENIC workflow](https://www.nature.com/articles/nmeth.4463) that does the comparison based on ranked gene expression - haven't tried it myself though. Best wishes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/638#issuecomment-491638466:19,feedback,feedback,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638#issuecomment-491638466,1,['feedback'],['feedback']
Usability,"Hi, thanks for the report!. Note that the plots in the documentation are generated on the fly when building the documentation. The plot you currently see on https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.dotplot.html has therefore been created with scanpy 1.10.1. Must be a dependency issue, I’ll try to reproduce with the environment you provided. /edit: I can reproduce it with that environment:. <details><summary>environment.yml</summary>. ```yaml; name: scanpy-3062; channels:; - conda-forge; dependencies:; - ipykernel. - python==3.10.10; - anndata==0.10.7; - scanpy==1.10.1; - IPython==8.13.2; - pillow==10.0.0; - astunparse==1.6.3; - backcall==0.2.0; - cffi==1.15.1; - cloudpickle==2.2.1; - colorama==0.4.4; - cycler==0.10.0; - cytoolz==0.12.0; - dask==2023.10.1; #- dateutil==2.8.2; - decorator==5.1.1; - defusedxml==0.7.1; - dill==0.3.6; - entrypoints==0.4; - exceptiongroup==1.1.1; - executing==1.2.0; - fasteners==0.17.3; - gmpy2==2.1.2; - h5py==3.8.0; #- icu==2.11; - python-igraph==0.11.2; - jedi==0.19.1; - jinja2==3.1.2; - joblib==1.2.0; - kiwisolver==1.4.4; - leidenalg==0.10.2; - llvmlite==0.42.0; - lz4==4.3.2; - markupsafe==2.1.2; - matplotlib==3.8.3; - mpmath==1.3.0; #- msgpack==1.0.5; - natsort==8.3.1; - numba==0.59.1; - numcodecs==0.11.0; - numexpr==2.7.3; - numpy==1.26.4; - packaging==23.1; - pandas==1.5.3; - parso==0.8.3; - pexpect==4.8.0; - pickleshare==0.7.5; - plotly==5.14.1; - prompt_toolkit==3.0.38; - psutil==5.9.5; - ptyprocess==0.7.0; - pure_eval==0.2.2; - pyarrow==10.0.1; - pydot==1.4.2; - pygments==2.15.1; - pyparsing==3.0.9; - pytz==2023.3.post1; - scipy==1.13.0; #- session_info==1.0.0; #- setuptools==67.7.2; - six==1.16.0; - scikit-learn==1.2.2; - stack_data==0.6.2; - sympy==1.11.1; - tblib==1.7.0; - texttable==1.6.7; - threadpoolctl==3.1.0; #- tlz==0.12.0; - toolz==0.11.2; #- pytorch==2.1.1; - tqdm==4.65.0; - traitlets==5.9.0; - wcwidth==0.2.6; #- yaml==5.4.1; - zarr==2.14.2; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516:1691,learn,learn,1691,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516,1,['learn'],['learn']
Usability,"Hi, thanks for your interest in scanpy!. For user questions, it would be great if you could ask your question on [Discourse](https://discourse.scverse.org/); this is the designated discussion forum for user questions regarding scverse tools (such as scanpy). This way, here at GitHub the focus can be put on development, while on Discourse user questions can be answered in more detail and in a manner that future users can better find previous questions. If you think your question is related to a development issue or I misinterpreted it as a user question, we're happy to look into it here on GitHub!. Hope this gives you a guidance for receiving helpful support for your question!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2626#issuecomment-1748396363:627,guid,guidance,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626#issuecomment-1748396363,1,['guid'],['guidance']
Usability,"Hi, thanks for your interest in scanpy!. I’ll try to comment on your observations here with your code example:. ```; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet.; → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test).; ```; def my_scale_function(X, clip=False):; # need to make a copy of X; Y = X.copy(); mean, var = mean_var(Y, axis=0); Y -= mean; std = np.sqrt(var); #std[std == 0] = 1; Y /= std; if clip:; Y = np.clip(X, -10, 10); return np.matrix(Y); ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```; ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled)); ```. ```; Do a numpy check for closeness of floats:; False; ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that.; ```; adata.X.var(0); ```. ```; array([0.9996213 , 0.97964925, 0.29805112, ..., ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273:838,simpl,simply,838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273,1,['simpl'],['simply']
Usability,"Hi,. I have modified version 1.4, but i think git only allow latest version to; fork. Is there any other simple way, so that i can share my code for Scanpy; version 1.4. Thanks,; Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc; > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>); > reverts a lot of changes we made since.; >; > i assume you just copied all your code over the current master branch, and; > not the version of the master branch as it was when you made the changes.; >; > you need to find the version of scanpy that you downloaded before you made; > your changes and modify that one to have just the changes you want to; > commit. otherwise we have no idea what your actual changes are.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492076397:105,simpl,simple,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492076397,1,['simpl'],['simple']
Usability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:97,learn,learn,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,3,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Hi,. Thanks @carversh for opening the discussion!. Trying to get things a bit into order here, as at the moment some wrong impressions are around I think:. **1. Incorrect comparisons done here**; To my current knowledge,; - `sc.pp.highly_variable_genes(…, flavor=“seurat”)` mimics `FindVariableFeatures(…, method=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:949,guid,guidance,949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['guid'],['guidance']
Usability,"Hi,; We don't have a simple function for this atm. But you could check out the dotplot, maybe that already does what you'd like. I'm not sure about Seurat, as I predominantly use scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-572642526:21,simpl,simple,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-572642526,1,['simpl'],['simple']
Usability,"Hi,; Your question is actually quite difficult to answer. It depends on how you define cell identity. For example, depending on how you set the resolution parameter for your clustering you can get a very different number of clusters. As clusters often have super- and sub-structure, you cannot easily say when a sub- or superstructure is not meaningful (e.g., T-cells vs CD4+ and CD8+ T-cells). Unfortunately many clustering techniques do not incorporate an assessment of uncertainty to tell you when you are fitting noise. And even when they do, this is based on a model of what a cell cluster should look like, which does not have to conform to the biological reality. The heuristic that tends to be used is that if you can biologically interpret your clusters based on marker genes and other signatures, then they are meaningful. That does however not mean that sub- or superstructures involving these clusters are not also meaningful. Sorry, I can't give a clearer answer than that. In terms of assessing differences between clusters, this is a somewhat related problem. An approach that you could use is to look at the distribution of Euclidean distances in gene- or PCA-space. However, that comes with the caveat that Euclidean distance does not take into account the biological manifold on which the cells lie (and on which distances will be more informative). You can try to use pseudotime as a measure for distance over this manifold, however that would require you to have sampled enough of the manifold to fit it in your data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874:961,clear,clearer,961,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874,1,['clear'],['clearer']
Usability,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:96,simpl,simply,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,1,['simpl'],['simply']
Usability,"Hm, I can’t reproduce this, but I also can’t reproduce your environment, since scipy 1.9.1 doesn’t build on my system. With this environment, everything worked instantly. Can you try updating your environment?. ```; anndata==0.9.1; asttokens==2.2.1; backcall==0.2.0; contourpy==1.1.0; cycler==0.11.0; decorator==5.1.1; executing==1.2.0; fonttools==4.40.0; h5py==3.9.0; igraph==0.10.4; ipython==8.14.0; jedi==0.18.2; joblib==1.2.0; kiwisolver==1.4.4; llvmlite==0.40.1; louvain==0.8.0; matplotlib==3.7.1; matplotlib-inline==0.1.6; natsort==8.4.0; networkx==3.1; numba==0.57.1; numpy==1.24.3; packaging==23.1; pandas==2.0.2; parso==0.8.3; patsy==0.5.3; pexpect==4.8.0; pickleshare==0.7.5; Pillow==9.5.0; prompt-toolkit==3.0.38; ptyprocess==0.7.0; pure-eval==0.2.2; Pygments==2.15.1; pynndescent==0.5.10; pyparsing==3.1.0; python-dateutil==2.8.2; python-igraph==0.10.4; pytz==2023.3; scanpy==1.9.3; scikit-learn==1.2.2; scipy==1.11.0; seaborn==0.12.2; session-info==1.0.0; six==1.16.0; stack-data==0.6.2; statsmodels==0.14.0; stdlib-list==0.9.0; texttable==1.6.7; threadpoolctl==3.1.0; tqdm==4.65.0; traitlets==5.9.0; tzdata==2023.3; umap-learn==0.5.3; wcwidth==0.2.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1607052906:902,learn,learn,902,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1607052906,2,['learn'],['learn']
Usability,"Hm, I researched a bit more. psutil doesn't seem to cause problems and also, this has not been a problem within Scanpy for any user up to now. If you start a terminal with `python` and type; ```; import psutil; psutil.process_iter(); ```; does this throw an error? I'd really like to know what's going on. If you want a quick fix; you can simply comment out line 773 in your file `/ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py`; this should cause no problem for your applications.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821:339,simpl,simply,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324476821,1,['simpl'],['simply']
Usability,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['learn'],['learn']
Usability,"Hm, how about simply ranking things yourself, like ; ```; sort_idcs = np.argsort(adata.var['PCs'][:, 0]); genes_ranked_by_loading_in_PC1 = adata.var_names[sort_idcs]; ```; This is what the plotting functions do internally.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/338#issuecomment-435641437:14,simpl,simply,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/338#issuecomment-435641437,1,['simpl'],['simply']
Usability,"Hm, it was decided to suspend this pr earlier.; There is an analogous pr in scikit-learn, but i'm not sure it will got forward.; I'm not sure what to do with this pr...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-573320770:83,learn,learn,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-573320770,1,['learn'],['learn']
Usability,"Hm, my impression is also that `bioservices` is relatively mature but tries to achieve a lot of things. `pybiomart` looks minimalistic and evidently only focuses on interfacing biomart and has a high coverage, which is nice. For that reason, I tend to prefer @ivirshup implementation over the @fbrundu's original `bioservices`-based one. But I don't have a strong opinion. I would not have implementations for two ""backends"" here for such a simple thing (querying biomart). @fbrundu, would you be alright with changing this to @ivirshup implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457868949:441,simpl,simple,441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457868949,1,['simpl'],['simple']
Usability,"Hm, simply removing `.. automodule:: scanpy` is not possible in the case of `scanpy/plotting/__init__.py` as then sphinx doesn't seem to know anymore where all the `pl.*` functions come from. Also, `docs/api/index.rst` renders completely fine: https://scanpy.readthedocs.io/en/latest/api/index.html. The problem is with `scanpy/api/__init__.py` (which doesn't contain `..automodule::`, as we're just documenting the functions defined in that directory) and `scanpy/plotting/__init__.py` (which does contain it, as we're documenting 'scanpy-level' functions).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/408#issuecomment-450882561:4,simpl,simply,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408#issuecomment-450882561,1,['simpl'],['simply']
Usability,"Hm, strange, the notebook is in the tests... I also just ran it through myself, manually, everything got me exactly the same results as available online: my versions are; ```; scanpy==1.3.7+86.g2c80c7a anndata==0.6.17+1.ga0cd0c6 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Could it be that you're using an older anndata or scanpy or something? I think I added the notebook to the tests around Scanpy 1.3 or so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216:278,learn,learn,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445#issuecomment-457346216,1,['learn'],['learn']
Usability,"Hm, that’s probably the doing of [`sklearn.utils.check_random_state`][]:. https://github.com/scverse/scanpy/blob/c3cfa74b1316d780568411175316cd9f139efb22/scanpy/_utils/__init__.py#L71-L72. @ilan-gold seems like using the legacy RandomState class wasn’t ideal. The new [`Generator`](https://numpy.org/doc/stable/reference/random/generator.html) actually emits 64 bit ints by default and I bet has other advantages. [`sklearn.utils.check_random_state`]: https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_random_state.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085057657:467,learn,learn,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085057657,1,['learn'],['learn']
Usability,"Hm, this is really strange. Sorry about this. The docker image might be out-of-date now - you compiled, @flying-sheep, what do you think?. But you shouldn't need the docker image to exactly reproduce the results. Which versions do you run?. The latest version of the tutorial says; ```; scanpy==1.0.2 anndata==0.5.8 numpy==1.14.1 scipy==1.0.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Your error is a Pandas error - do you run an older or more recent version of Pandas that might cause the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495:365,learn,learn,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636495,1,['learn'],['learn']
Usability,"Hm, very strange. Generally: `groups` is somehow reminiscent of times when AnnData didn't have views. Today, I'd say one should simply pass a subsetted AnnData (by default a view). What about deprecating the `groups` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/231#issuecomment-412140748:128,simpl,simply,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-412140748,1,['simpl'],['simply']
Usability,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:34,simpl,simpler,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463,1,['simpl'],['simpler']
Usability,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:527,clear,clearer,527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116,1,['clear'],['clearer']
Usability,"Hmm, seems like `PCA` with `svd_solver='arpack'` works differently in sklearn 1.5 and flips the coordinates in some tests:. https://github.com/scikit-learn/scikit-learn/issues/28826. E.g. this used to work, now it doesn’t. ```py; from __future__ import annotations. import numpy as np; from sklearn.decomposition import PCA. data = np.asarray([[-1, 2, 0], [3, 4, 0], [1, 2, 0]]).T; expected = np.array(; [[-1.579575e-15, 1.490712], [-2.44949, -0.745356], [2.44949, -0.745356]],; dtype=np.float32,; ). pca = PCA(n_components=2, svd_solver=""arpack"", random_state=np.random.RandomState(0)); transformed = pca.fit_transform(data).astype(np.float32); np.testing.assert_almost_equal(transformed, expected, decimal=5); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3047#issuecomment-2098389353:150,learn,learn,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3047#issuecomment-2098389353,2,['learn'],['learn']
Usability,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830832355:550,clear,clearly,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830832355,1,['clear'],['clearly']
Usability,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:208,simpl,simplest,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,1,['simpl'],['simplest']
Usability,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:671,learn,learn,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350,1,['learn'],['learn']
Usability,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:498,clear,clear,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542,1,['clear'],['clear']
Usability,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:140,clear,clear,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['clear'],['clear']
Usability,"Huh? `obs` and `var` have been `DataFrame`s for a long time now. and `uns` is a dict. The reason why they haven’t originally been that is that `DataFrame`s are super complicated and have a giant API. We thought having something simpler would be easier to work with, but in the end convenience won.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-483216799:228,simpl,simpler,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-483216799,1,['simpl'],['simpler']
Usability,I added tests for both louvain and leiden with restrict parameter. Please review the test code to be sure it is clear and working.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-479587681:112,clear,clear,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-479587681,1,['clear'],['clear']
Usability,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:234,learn,learn,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309,2,['learn'],['learn']
Usability,I am facing the same issue. I have recently updated my scanpy to the latest version.; I think that it was working before that. Here is rest of my software versions; scanpy==1.4.6 anndata==0.7.1 umap==0.3.9 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992:255,learn,learn,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992,1,['learn'],['learn']
Usability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:502,learn,learn,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,1,['learn'],['learn']
Usability,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:304,learn,learn,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994,2,['learn'],['learn']
Usability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:558,learn,learn,558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,1,['learn'],['learn']
Usability,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555:317,learn,learn,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555,1,['learn'],['learn']
Usability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:866,clear,clear,866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,1,['clear'],['clear']
Usability,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:89,user experience,user experience,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,1,['user experience'],['user experience']
Usability,"I completely agree. It should simply go in the `test` extra. @tomwhite, would you do that? It might that the tests don't run through on Travis for some reason and then, I guess, it would be great if you could look into it (would for sure be a problem that would pop elsewhere, too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-460071018:30,simpl,simply,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-460071018,1,['simpl'],['simply']
Usability,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:894,learn,learn,894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671,1,['learn'],['learn']
Usability,"I could not reproduce this bug, I am using . `scanpy==1.5.1 anndata==0.7.4 umap==0.3.10 numpy==1.19.2 scipy==1.5.2 pandas==1.1.2 scikit-learn==0.23.2 statsmodels==0.12.0 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.7.0`. @giovp maybe be a good idea to close this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1300#issuecomment-718771033:136,learn,learn,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1300#issuecomment-718771033,1,['learn'],['learn']
Usability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:336,learn,learn,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['learn'],['learn']
Usability,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:203,simpl,simply,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,1,['simpl'],['simply']
Usability,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:309,learn,learn,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500,2,['learn'],['learn']
Usability,"I don't know how they do it Seurat, but I'd simply do; ```; filenames = ['name0.h5', 'name1.h5', 'name2.h5']; adatas = [sc.read_10x_h5(filename) for filename in filenames]; adata = adatas[0].concatenate(adatas[1:]); ```; Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-424547172:44,simpl,simply,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-424547172,1,['simpl'],['simply']
Usability,I don't think that `legend_loc` would be the correct parameter. You are not even interacting with it during the example that @LisaSikkema posted (thanks for that btw!). I see two options:; 1. Adding a general statement to the `color` argument stating that reordering the categorical column to color by can customize the legend order; 2. Adding your use-case to the example section in the pl.umap documentation. Leaning more towards the second option. What do you think? Would be great if you could attempt to submit a PR then. Happy to provide some guidance if required.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257189686:549,guid,guidance,549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257189686,1,['guid'],['guidance']
Usability,"I don't think that what you want is possible. sc.pl.tracksplot plots data in a way that resemble a genome browser track; but ist not because it does not understand coordinates. It simple groups; the cells by the given groupby condition and then plots the value of each; gene in a separate track. The y value is the gene expression (or in your; case the ATAC-seq value). The x coordinate, simple puts all cells one after; the other without any ordering. On Sat, Jan 26, 2019 at 12:55 AM manarai <notifications@github.com> wrote:. > Hi,; >; > Thanks for this amazing package.; >; > I have been playing with scanpy on scATACSeq data generated from 10x. And; > in comparison to the cellranger analysis, I think analysis scanpy does; > pretty descent job and adds more possibilities. I would like to displays; > some peaks that are highly present if some clusters using the genome; > browser which scanpy seem to be able to do ""I think"" ( as shown below). Is; > it possible to the same thing but with the peak averaged for all cells; > within the same cluster?; >; > import matplotlib.pyplot as plt; > genes =['chr15:101708546_101718131','chr11:117961932_117970696',; > 'chr19:5821847_5852441','chr15:101422873_101429606',; > 'chr17:39842811_39849028','chr13:6108971_6109684']; > sc.pl.tracksplot(adata,genes,groupby='louvain', figsize=[40,50]); >; > [image: atacseq]; > <https://user-images.githubusercontent.com/39877296/51778958-d7e4c700-2147-11e9-88cd-78f3e100c75f.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/447>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VUU1fUPbHKqadt-yXNKrA4amCXQks5vG5lngaJpZM4aT2VQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/447#issuecomment-457810196:180,simpl,simple,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/447#issuecomment-457810196,2,['simpl'],['simple']
Usability,"I encounter the same error as well when i tried sc.pp.normalize_total(adata, target_sum=5e4). . Environment:; scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. [conda list]; https://github.com/phamidko/codesnippets/blob/master/scanpy-conda-list.txt; [ipynb]; https://github.com/phamidko/codesnippets/blob/master/Tissue-Tcell-activation.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477:199,learn,learn,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477,1,['learn'],['learn']
Usability,"I face the same problem. However I used latest version ; </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-860196125:148,learn,learn,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-860196125,1,['learn'],['learn']
Usability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:429,simpl,simply,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,1,['simpl'],['simply']
Usability,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-791524373:496,guid,guide,496,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-791524373,1,['guid'],['guide']
Usability,"I follow your argumentation on ""good clusters"". However, I also like the concept that putting k=35 means you make it harder to detect clusters of size < 35, as you 'over-connect' those clusters in a way. The weighted case is less interpretable in that way. However, here it clearly outperforms the unweighted case. I am still a little on the fence (due to interpretability), but I'd be okay with weighting I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-488610378:274,clear,clearly,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-488610378,1,['clear'],['clearly']
Usability,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:315,learn,learn,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116,1,['learn'],['learn']
Usability,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:252,intuit,intuitive,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073,1,['intuit'],['intuitive']
Usability,I got the same error with scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.2 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1; But I am so glad to find answer here and thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485:130,learn,learn,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485,1,['learn'],['learn']
Usability,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494730898:179,simpl,simply,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494730898,1,['simpl'],['simply']
Usability,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:285,learn,learning,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342,4,['learn'],"['learn', 'learned', 'learning']"
Usability,"I guess what I mean is a metric to describe how well the data points are clustered in their own cluster relative to every other cluster and/or data point. But what you both said makes sense. Looking at marker expression and cell type classification seems to be the most obvious, practical way to assign clusters. At the end of the day, it's the biology we care about. . Thanks for your detail explanations everyone, I'm new to this but am continuing to learn a lot. . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409996535:453,learn,learn,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409996535,1,['learn'],['learn']
Usability,"I have pytorch and tensorflow alongside scanpy in several conda envs. I would close this for now, also because it's not clear what ""probably it does not finish"" means. ; Feel free to reopen it if problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1286#issuecomment-702367885:120,clear,clear,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-702367885,1,['clear'],['clear']
Usability,"I have to admit that I'm still not an expert in these forums. I'm happy if we go with https://gitter.im/scanpyhelp as a solution. I also know discourse is super popular among many people and I'm happy if we go with it if all three of you, @outlace, @ivirshup and @flying-sheep, think this could be a better place. I simply can't judge myself as I haven't used either of them. Most importantly, let's put what you guys choose on the top of the webpage and properly announce it; it would be terrible to have several of these chat rooms; one on gitter, one discourse, etc. with potentially even different names `scanpyhelp`, `scanpy` etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-509026804:316,simpl,simply,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-509026804,1,['simpl'],['simply']
Usability,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965:77,simpl,simplest,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428240965,1,['simpl'],['simplest']
Usability,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory.; Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499:242,simpl,simple,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478499,1,['simpl'],['simple']
Usability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1223,learn,learn,1223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,2,['learn'],['learn']
Usability,I just add the versions I used:; ```python; sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/536#issuecomment-474301705:150,learn,learn,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536#issuecomment-474301705,1,['learn'],['learn']
Usability,"I just encountered this as well. It seems like it's not running UMAP at all unless I give it a `maxiter` parameter. Not clear why that is, but passing an argument there worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1261103537:120,clear,clear,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1261103537,1,['clear'],['clear']
Usability,"I just tried again with the reprex above and it works for me; ```python; import scanpy as sc; adata = sc.datasets.paul15(); sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.dpt(adata); sc.tl.paga(adata, groups='paul15_clusters'); sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']); ```; ![image](https://user-images.githubusercontent.com/25887487/107772027-f1133d00-6d3b-11eb-8866-f514acea297a.png). I'm on a separate branch but it's on par with current master; ```; scanpy==1.7.0rc2.dev25+g56303580.d20210212 anndata==0.7.4 umap==0.4.6 numpy==1.19.4 scipy==1.5.2 pandas==1.1.4 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778186427:604,learn,learn,604,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778186427,1,['learn'],['learn']
Usability,"I like @VolkerBergen's suggestion. On the other Hand, @LuckyMD uses the scran estimate of size factors for normalization. Processing something like that would need a `counts_per_cell` argument (which I'd call `normalization_factor` today, I guess). If one needs to manually compute the `counts_per_cell` before calling the function, then the whole convenience and purpose of the function is gone, though. So, I'd say the convenience of an argument `by_initial` absolutely outweighs the flexibility of an argument `normalization_factor` (`size_factor`). In case we have another size factor estimator in Scanpy, it will definitely not occur in `normalize_total` or `normalize_quantile` (the names already suggest that this is simple normalization) but in a new function `normalize_...`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-460612249:724,simpl,simple,724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-460612249,1,['simpl'],['simple']
Usability,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:34,simpl,simple,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884,2,"['pause', 'simpl']","['pause', 'simple']"
Usability,"I like the idea and I see your point, however I wonder how intuitive the concept of a style is for beginners. Also you'd have to know exactly what you are looking to plot during your analysis to set up a style for all plots for the future (e.g. do you need arrows? Will you use a marker gene dotplot?). Context managers definitely look clean (I love that I have my own ^^), but it would make things a bit more difficult for beginners to get an intuitive feel for scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956#issuecomment-567412942:59,intuit,intuitive,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567412942,2,['intuit'],['intuitive']
Usability,I like this idea a lot. I recently learned about automated linting and blacking of code per commit and have started using it for the single cell open problems project upon the suggestion of @scottgigante.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-753945848:35,learn,learned,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-753945848,1,['learn'],['learned']
Usability,"I literally never had problems with test discovery, so idk what to look for. As said: Numpy and pandas have separated their testing utils from their tests. For the time being I want just that, no change to where the tests are. Would you accept a PR that simply moves the test utils into private submodules of `scanpy.testing` and switches the import mode to (future default, drawback-less) `importlib`?. Any change to the test layout can come later or never. I’d like to follow pytest’s recommendation (`/src/scanpy/` and `/tests/`) but this issue is orthogonal to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096566148:254,simpl,simply,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096566148,1,['simpl'],['simply']
Usability,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:313,clear,clearly,313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559,1,['clear'],['clearly']
Usability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:487,learn,learn,487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,1,['learn'],['learn']
Usability,"I meant apart from the long label at ""right margin"", basically adding a number to each label and display that number ""on data"". Right margin would look like this:; 1. Cell type A; 2. Cell type B; 3. Cell type C. And the UMAP will simply show 1, 2, 3 ... on the plot. If the numbers could be automatically generated when one runs `sc.pl.umap`, and added to the plot, that would be awesome. The issue is adding the text label on the plot make them overlap with each other, but the corresponding numbers would do just fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2112#issuecomment-1023417880:230,simpl,simply,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2112#issuecomment-1023417880,1,['simpl'],['simply']
Usability,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing!. ```python; In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X; Out[6]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a; Out[7]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X; Out[12]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a; Out[13]: ; array([[ 0., 1., 2., 3.],; [ 4., 5., 6., 7.],; [ 8., 9., 10., 11.],; [12., 13., 14., 15.]], dtype=float32); ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1415#issuecomment-694916916:103,clear,clearly,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-694916916,1,['clear'],['clearly']
Usability,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:403,simpl,simply,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,1,['simpl'],['simply']
Usability,I quite like the saving of figures as it means people can use scanpy who otherwise aren't as familiar with data science in python. Calling a function on an axis object or saving the last axis object that was is displayed is not always intuitive to new users.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-735835548:235,intuit,intuitive,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-735835548,1,['intuit'],['intuitive']
Usability,"I really like this! I was trying to figure out how to easily do this within the current `plot_scatter` framework, but I couldn't figure out how. One thing though... You could add an option `group='all'`, which should be the default. That would be much more user-friendly than the current setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/628#issuecomment-488715711:257,user-friendly,user-friendly,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628#issuecomment-488715711,1,['user-friendly'],['user-friendly']
Usability,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:116,progress bar,progress bars,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054,2,"['progress bar', 'simpl']","['progress bars', 'simply']"
Usability,I renamed pts and pts_rest to `fraction_group` and `fraction_rest`. I'd like to merge this PR if there is no other feedback. We can think about how to provide aggregate statistics somewhere else. Then we can revisit this function and merge this info too.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-678829691:115,feedback,feedback,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-678829691,1,['feedback'],['feedback']
Usability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:74,simpl,simpler,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['simpl'],['simpler']
Usability,"I see! OK, so `.. automodule` [doesn’t by default include the members](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), and we don’t have [`autodoc_default_options`](http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_default_options) set. > All three directives [(`automodule` and so on)] will by default only insert the docstring of the object itself. However I don’t understand what you mean by. > as then sphinx doesn't seem to know anymore where all the pl.* functions come from. So will it simply not link them? Because that can have other reasons as well…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/408#issuecomment-450887824:545,simpl,simply,545,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/408#issuecomment-450887824,1,['simpl'],['simply']
Usability,"I see, [densmap](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html). Hmm, I think that `method='densmap'` and `method_kwds={...}` would be a better API for us (which would then be translated into `densmap=True, densmap_kwds=method_kwds`). This also needs tests and a release note. Also we probably should just remove the umap 0.4 compatibility code, what do you think @ivirshup?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449:30,learn,learn,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449,1,['learn'],['learn']
Usability,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py; def _plot_legend(self, legend_ax, return_ax_dict, normalize): ; self._plot_colorbar(legend_ax, normalize) ; return_ax_dict['color_legend_ax'] = color_legend_ax; ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829:230,simpl,simply,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829,1,['simpl'],['simply']
Usability,"I simplified this quite a bit. I decided against blocking this on an upstream `anndata` helper for multiple reasons:. 1. we test on older anndata versions that won’t have that parameter immediately; 2. needs some designing, e.g. the API should be able to do “use `ceil(shape[0] / 2)` as chunk size for dim 0, and `-1` (=full size) for dim 1”; 3. it would make no sense for the non-dask array types, should we still add it for them?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2250061054:2,simpl,simplified,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2250061054,1,['simpl'],['simplified']
Usability,I simply meant to do what I described in that issue: https://github.com/theislab/scanpy/issues/271#issuecomment-431634492,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-435734288:2,simpl,simply,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-435734288,1,['simpl'],['simply']
Usability,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584222835:236,simpl,simpler,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584222835,1,['simpl'],['simpler']
Usability,"I support the idea of tidying up plotting arguments. I think there are mainly two problems. 1) the high number of plotting arguments 2) lack of reusability of plotting ""styles"". . Chaining looks really cool and improves 1). Also, it logically partitions the plotting arguments. However, it doesn't solve 2). In other words, if we plot two figures, we'll need to copy the entire thing, and it'll be very verbose:. ```python; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). sc.pl.umap(adata2, color='fluffy').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```. One thing that comes to mind for reusability is to store the result of the chain somewhere and, well, reuse it:. ```python; style = sc.pl.styles.scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). # using simple arguments, similar to https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/lmeControl.html; sc.pl.umap(adata, color='clusters', style=style); sc.pl.umap(adata2, color='fluffy', style=style). # using context managers, similar to https://seaborn.pydata.org/tutorial/aesthetics.html#temporarily-setting-figure-style; with style:; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # overriding an existing style object; with style.legend(fontsize=12):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # or use predefined styles (?); with sc.pl.style('malte'):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). ```. WDYT?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810:956,simpl,simple,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810,1,['simpl'],['simple']
Usability,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:162,feedback,feedback,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466,1,['feedback'],['feedback']
Usability,"I thing that is not so difficult to achieve this. I submit a PR soon. Fidel Ramírez . > On 26 Mar 2019, at 22:02, Alex Wolf <notifications@github.com> wrote:; > ; > @fidelram, as discussed today, could we adopt pl.rank_genes_groups_dotplot so that it reads this information from .uns['rank_genes_groups']?; > ; > Maybe just a simple switch? Or having arguments color and size be a choice from a selection {pvals, pvals_adj, log2FC, expression, frac-genes-expressed}.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-477015393:326,simpl,simple,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-477015393,1,['simpl'],['simple']
Usability,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674:790,simpl,simply,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674,1,['simpl'],['simply']
Usability,"I think anndata’s `rename_categories` should accept non-unique values as argument. Then one could simply do things like. ```py; cluster_markers = {; 'CD4 T': {'IL7R'},; 'CD14+\nMonocytes': {'CD14', 'LYZ'},; 'B': {'MS4A1'},; 'CD8 T': {'CD8A'},; 'NK': {'GNLY', 'NKG7'},; 'FCGR3A+\nMonocytes': {'FCGR3A', 'MS4A7'},; 'Dendritic': {'FCER1A', 'CST3'},; 'Mega-\nkaryocytes': {'PPBP'},; }; marker_matches = sc.tl.marker_gene_overlap(adata, cluster_markers); adata.rename_categories('leiden', marker_matches.idxmax()); ```. As it stands, things like the `pbmc3k` tutorial are super flaky because they hardcode things like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-1153213330:98,simpl,simply,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-1153213330,1,['simpl'],['simply']
Usability,I think it is more or less complete.; Here are the tutorials; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519155566:223,simpl,simple,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519155566,1,['simpl'],['simple']
Usability,"I think maybe i found a solution to solve this problem.; Maybe this problem is caused by the version of scikit—misc，when you use pip install --user scikit-misc or pip install scikit-misc，the system will install scikit-misc==0.1.4.; so,i try to install another verion of scikit-misc,you can use install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1.; In addition, this line of command needs to be used when python is greater than or equal to 3.8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497:327,simpl,simple,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497,1,['simpl'],['simple']
Usability,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:166,clear,clear,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,2,['clear'],['clear']
Usability,"I think pbmc68k_reduced was processed something like. ```py; sc.pp.normalize_total(adata, target_sum=1e6); sc.pp.log1p(adata); sc.pp.scale(adata); ```. still no idea what’s in “raw” as it’s clearly not counts …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3051#issuecomment-2107394169:190,clear,clearly,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3051#issuecomment-2107394169,1,['clear'],['clearly']
Usability,"I think that the best is to have separate scatter functions. For the; remaining use cases of scatter a much simpler version can be devised. But; meanwhile, @bebatut <https://github.com/bebatut> why did you try that; combination of parameters? Is this in some tutorial?. On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good; > situation. @bebatut <https://github.com/bebatut> components does only; > make sense if you provide basis as an argument. a list-like color was; > also only meant for that case. both is now deprecated as @fidelram; > <https://github.com/fidelram> wrote a whole new scatter backend; however,; > which for now, misses the x and y parameters...; >; > In any case, pl.scatter should continue to work with the canonical calls; > and also with non-list-like color.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-432533052:108,simpl,simpler,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-432533052,1,['simpl'],['simpler']
Usability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:38,simpl,simple,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"I think the problem is the option `sort_order` which is True by default for; numerical data. This changes the ordering of the dots and thus it messes; up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot; > with the following code; >; > import scanpy.api as sc; > import numpy as np; > sc.settings.figdir = ""testdir""; > sc.settings.file_format_figs = ""png""; > sc.logging.print_versions(); >; > With these libraries; > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4; > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Running the following code bit. I use some dummy variable for size.; >; > somedata = sc.datasets.paul15(); > sc.pp.pca(somedata); > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); > sc.tl.leiden(somedata, resolution=0.5, random_state=42); > z = np.abs(somedata.obsm['X_pca'][:,0])**1; > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); >; > I get the following two figure as output; > [image: umapcontinuous_expr]; > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>; > [image: umapgroup_value]; > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>; >; > I would expect to see a similar size allocation/distribution but they are; > very different. I Could not really find a cause for this looking at the; > scatter plot function so it might be somewhere deeper.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/478>, or ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152:671,learn,learn,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152,1,['learn'],['learn']
Usability,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196:37,guid,guide,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196,3,['guid'],"['guide', 'guides']"
Usability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:363,simpl,simplexes,363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,1,['simpl'],['simplexes']
Usability,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:275,learn,learning,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251,1,['learn'],['learning']
Usability,"I think this is getting to a good place for an initial addition. Parts that definitely need expanding include:. * Code guidelines; * These are pretty minimal at the moment.; * Documentation; * Information on restructured text ; * sphinx extensions we use; * More on the structure of a doc-string; * Updated examples (I took these from the existing `CONTRIBUTING.md`). I think these can be expanded at a later date. The main goal here was to make sure there was some base organization for a contributing guide and dev-docs. . I'm probably also not the best person to expand on the documentation, since I still barely understand sphinx :wink:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865:119,guid,guidelines,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865,2,['guid'],"['guide', 'guidelines']"
Usability,"I think this looks good and simple enough. Could you please fix the CI errors?. Also there’s 3 added optional deps: cuml, cudf, and cugraph. I assume they’re all different CUDA packages. Could you add them into an `extra` in setup.py?. The RAPIDS umap branch doesn’t use a metric argument. Does it support metrics other than euclidean?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279:28,simpl,simple,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279,1,['simpl'],['simple']
Usability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:140,clear,clear,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['clear'],['clear']
Usability,I think we should remove the that uses the divergent colormap and keep things simpler.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/512#issuecomment-470914298:78,simpl,simpler,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512#issuecomment-470914298,1,['simpl'],['simpler']
Usability,"I think you misunderstood a bit. 1. There’s three *types* of return sections – prose, tuple, added anndata.obs/var fields.; 2. There’s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. I’ll automate the other cases in scanpydoc, then we can remove the manually formatted ones.; 3. There’s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484427693:233,simpl,simple,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484427693,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"I think you're right that Keras doesn't do in-place modifications (it's just building a computation graph there so there is nothing to modify). And the scikit-learn example is a bit different because it's a class method (although it would also make sense for `log1p` to be a class method, given that it only needs to exist for `AnnData` objects). Your rationale for the Scanpy structure makes a lot of sense for the reasons you mentioned, but to me functional programming means immutable objects and functions that return their result with no side effects. Of course, memory constraints come into play here and immutability isn't viable, but the ""return your result"" part is still possible. Personally I don't mind the possibility of multiple references to one object and so I prefer the option of either assigning the result or chaining functions together. In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. This behavior is what `numpy.log1p` itself is doing here, for that matter–with an `out` argument it still returns the array. Anyway, obviously it's your call on how to design Scanpy, but I thought I'd give my perspective. Looking forward to see where you go with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403311910:159,learn,learn,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403311910,1,['learn'],['learn']
Usability,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114:25,learn,learn,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114,1,['learn'],['learn']
Usability,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699:85,clear,clear,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699,1,['clear'],['clear']
Usability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,1,['learn'],['learn']
Usability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:691,clear,clear,691,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,1,['clear'],['clear']
Usability,"I was eventually able to contact the maintainer and he's looking into making a new release. Will see what happens. Nevertheless, it might not be a bad idea to simplify the code and to only support `igraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980:159,simpl,simplify,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980,1,['simpl'],['simplify']
Usability,I was having the same issue these days. Happy to find this thread.; Fixed the issue by simply using the latest version of scanpy. Thanks a lot!!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1020766690:87,simpl,simply,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1020766690,1,['simpl'],['simply']
Usability,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>; <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>; <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>; <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>; <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864:687,clear,clear,687,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864,1,['clear'],['clear']
Usability,"I would agree the results of `sc.pp.filter_genes(..., inplace=False)` are not the most intuitive. Instead of returning a filtered anndata, it returns which cells would have been filtered and the stats which were used to make this decision. This is documented under the `Returns` section for these functions. What you might want is. ```python; mask, _ = sc.pp.filter_cells(adata, min_genes=200, inplace=False); a = adata[mask].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137:87,intuit,intuitive,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137,1,['intuit'],['intuitive']
Usability,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:584,clear,clearer,584,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,1,['clear'],['clearer']
Usability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:54,clear,clear,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,1,['clear'],['clear']
Usability,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:251,simpl,simple,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973,1,['simpl'],['simple']
Usability,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083:147,clear,clear,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083,1,['clear'],['clear']
Usability,"I'd like to bump the version requirement down a bit, since it seems like it's not that uncommon to pin `map-learn` lower than 0.5.5: https://github.com/search?q=%2F%5B%22%27%5D%3Fumap-learn%5B%22%27%5D%3F%5B%3D%3E%3C%5D%2B%2F&type=code. The cellxgene one is worrying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2870#issuecomment-1957629969:108,learn,learn,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2870#issuecomment-1957629969,2,['learn'],['learn']
Usability,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue?. This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914:443,simpl,simple,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914,1,['simpl'],['simple']
Usability,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:; ```; X = data; if isinstance(data, AnnData):; X = data.X; ```; or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433397997:301,simpl,simply,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433397997,2,['simpl'],['simply']
Usability,I'll fix this PR up in ~10 days. Appreciate your comments @ivirshup and think that some of your feedback could have been mitigated by me not going for short cuts :). Will request (final hopefully) review then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-789127611:96,feedback,feedback,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-789127611,1,['feedback'],['feedback']
Usability,"I'll reopen this cause I think it's quite relevant still and could be very straightforward to implement with [sklearn resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html). also, there is an entire package for subsampling strategies which is probably quite relevant: https://github.com/scikit-learn-contrib/imbalanced-learn. line here for reference: https://github.com/theislab/scanpy/blob/48cc7b38f1f31a78902a892041902cc810ddfcd3/scanpy/preprocessing/_simple.py#L857",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098:143,learn,learn,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098,3,['learn'],"['learn', 'learn-contrib']"
Usability,"I'm also a bit confused about how the CLR is applied. In the CITE-seq paper, I think it was done within a cell (over proteins), then I think they had switched to within a protein (over cells), and now in Seurat v4 it appears to be back to within a cell. Any per cell normalization is a bit tricky because the panels will differ between datasets as well as the titration of antibodies used. The simplest thing to me seems to be a simple log transformation combined with per protein scaling, as values between proteins are not comparable to begin with. We have some additional thoughts in the appendix of our totalVI paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290:394,simpl,simplest,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290,2,['simpl'],"['simple', 'simplest']"
Usability,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027:619,simpl,simple,619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027,1,['simpl'],['simple']
Usability,"I'm enjoying this brainstorming session... let's continue. I think the most difficult part would be the grid plotting in the end, but let's continue with the points in order. 1. We may need to rethink the scaling. At the moment it's scaled such that 1 is the highest density in one sample. Maybe it's more informative to make all sample densities to sum to 1 for this comparison? I didn't implement that currently as it's currently calculated over cells... if it were over grid points, then each condition would have the same number of grid points and this would be feasible again. 2. I'm not even sure if sampling is necessary... you could just as well take average densities in a grid square. That would make the calculation fairly easy. The issue, as I alluded to above, is plotting that grid I think. At the moment I can easily use scanpy's inherent `plot_scatter()` function... I'm not aware of any grid plotting function. That would probably need to be custom made. 3. A simple statistic would just be to use the outliers of the differences... but there are definitely better ideas...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-478296037:977,simpl,simple,977,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-478296037,1,['simpl'],['simple']
Usability,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782,1,['learn'],['learn']
Usability,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:1035,simpl,simple,1035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['simpl'],['simple']
Usability,"I'm hesitant to add another embedding method without clear benefits over existing implementations. Could you give some detail on benefits here, ideally with direct comparisons?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1050880241:53,clear,clear,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1050880241,1,['clear'],['clear']
Usability,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:272,simpl,simply,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,1,['simpl'],['simply']
Usability,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python; self.n_samples_, self.n_features_ = n_samples, n_features; self.components_ = V; self.n_components_ = n_components. # Get variance explained by singular values; self.explained_variance_ = (S ** 2) / (n_samples - 1); total_var = np.var(X, ddof=1, axis=0); self.explained_variance_ratio_ = \; self.explained_variance_ / total_var.sum(); self.singular_values_ = S.copy() # Store the singular values.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978:95,learn,learn,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978,2,['learn'],['learn']
Usability,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:540,simpl,simple,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['simpl'],['simple']
Usability,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246:197,progress bar,progress bars,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246,1,['progress bar'],['progress bars']
Usability,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:236,simpl,simplification,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421,1,['simpl'],['simplification']
Usability,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:152,learn,learn,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181,2,['learn'],['learn']
Usability,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:635,clear,clear,635,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296,1,['clear'],['clear']
Usability,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129:87,feedback,feedback,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129,1,['feedback'],['feedback']
Usability,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:698,simpl,simplified,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096,2,"['guid', 'simpl']","['guide', 'simplified']"
Usability,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825:25,simpl,simple,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825,1,['simpl'],['simple']
Usability,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:19,learn,learn,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206,1,['learn'],['learn']
Usability,"If you're planning to look into the code: There will be a new version of PAGA in Scanpy 1.2, which will feature two connectivity models... The code will be much clearer. We'll also see whether we can upload an extensive revision of the preprint - unfortunately, the review process at the journal took ages and coming up with the revision, too. All of this should happen in the next days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393819773:161,clear,clearer,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393819773,1,['clear'],['clearer']
Usability,"In my experience, this happens if batch key is not None and one or more batches have low number of cells. Does it make sense to catch this error and simply skip the problematic batch or inform the user that batch doesn't have enough cells?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060:149,simpl,simply,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060,1,['simpl'],['simply']
Usability,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553490520:221,simpl,simple,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553490520,1,['simpl'],['simple']
Usability,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:265,simpl,simple,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['simpl'],['simple']
Usability,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:996,usab,usability,996,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['usab'],['usability']
Usability,Is there any feedback？,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951#issuecomment-885545697:13,feedback,feedback,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-885545697,1,['feedback'],['feedback']
Usability,"It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of `sc.pl.umap(adata, color='clusters')` -> `sc.pl.umap(adata, 'clusters')`. About the changes that you suggest: I have concerns with breaking previous functionality and I wonder what is your position with respect to this. The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage from the very firsts versions of scanpy but maybe you have some good ideas for transitioning this from a string to a tuple. . The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. . For the plots being the product of `color` and `components`: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. . For your question about replacing `components` by `dimensions`. We need to be careful here because in many places the use of components is in the context of PCA as in `sc.pp.neighbors` with the parameter `n_pcs`. I think that the replacement of `components` by `dimensions` should only be done for the embedding functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392:115,simpl,simple,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392,1,['simpl'],['simple']
Usability,"It looks like your adata object is corrupted. You should be able to type; `adata.X` to get the matrix. How are you generating the adata object?. On Thu, Dec 6, 2018 at 5:56 PM ltosti <notifications@github.com> wrote:. > Hi there,; >; > When running sc.pp.highly_variable_genes(adata.X) I get the following; > error:; >; > AttributeError: X not found; >; > I then ran sc.pp.highly_variable_genes(adata) and got the following:; >; > ValueError: Bin edges must be unique: array([nan, inf, inf, inf, inf, inf,; > inf, inf, inf, inf, inf, inf, inf,inf, inf, inf, inf, inf, inf, inf, inf]).; > You can drop duplicate edges by setting the duplicates kwarg; >; > The older sc.pp.filter_genes_dispersion(adata.X) works fine.; >; > Do you know how to fix this?; >; > Thank you!; >; > *Info*: scanpy==1.3.4 anndata==0.6.13 numpy==1.15.3 scipy==1.1.0; > pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > louvain==0.6.1; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/391>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RPErIznAoUd0DwpbdlEjkOUyjTdks5u2Uw4gaJpZM4ZG6Jw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-444950693:864,learn,learn,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-444950693,1,['learn'],['learn']
Usability,"It seems that upgrading from 1.8.1 to 1.8.2 introduce an error on umap version checking, mentioned by #1978 (and a potential solution); > Why are we using umap.__version__ instead of importlib.metadata.version('umap-learn')?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681:216,learn,learn,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681,1,['learn'],['learn']
Usability,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:499,learn,learn,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497,2,['learn'],['learn']
Usability,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,3,"['clear', 'learn']","['clearer', 'learn']"
Usability,It turned out that this is definitely something wrong with my system setup. After I circumvented the bug above by clearing `README.rst` I found another package that spits out the same error (`louvain`).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102:114,clear,clearing,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-342897102,1,['clear'],['clearing']
Usability,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:232,simpl,simply,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453,1,['simpl'],['simply']
Usability,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:321,intuit,intuitive,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,2,"['intuit', 'learn']","['intuitive', 'learn']"
Usability,It's not clear to me why these `test_10x` tests are failing here and not on master -- there shouldn't be anything in this diff that affects those tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898:9,clear,clear,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898,1,['clear'],['clear']
Usability,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041:218,simpl,simply,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041,1,['simpl'],['simply']
Usability,"I’m not sure why it came up now, I saw changes to the chunking in https://github.com/scverse/anndata/pull/1550 and assumed that introduced it, but maybe not. regarding complexity and parameters, we could simplify things by doing. ```py; def maybe_rechunk_1d(a: NDArray | DaskArray | ...) -> NDArray | DaskArray | ...:; if isinstance(a, DaskArray):; return a.rechunk((a.chunksize[0], -1)); return a; ```. and using that in all the functions that fail (after running things through `array_type`). Would you prefer that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245646540:204,simpl,simplify,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245646540,1,['simpl'],['simplify']
Usability,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:293,simpl,simple,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026,1,['simpl'],['simple']
Usability,"Jaccard metric may lead to very different graphs where Louvain communities are not really distinct compared to those in graphs built with euclidean metric. For example:. Graph with euclidean metric and clusters:. ![image](https://user-images.githubusercontent.com/1140359/41617435-c02baf88-7400-11e8-8629-31db4e8e16f1.png). Graph with jaccard metric and clusters (or lack thereof). ![image](https://user-images.githubusercontent.com/1140359/41617406-aff29fb4-7400-11e8-8d44-4219de42a9e9.png). So in this case, there is simply no communities in the Jaccard graph (at least with the default resolution). However, tSNE always uses the euclidean distance to compute a kNN graph from scratch (completely discarding the metric user specifies in sc.pp.neighbors) and the clusters computed on the Jaccard graph are not representative any more due to very different topology of the graph. In your data, something similar is going on, therefore using `sc.tl.draw_graph` would show how Louvain clusters really look like because it uses the kNN graph built with the metric specified by the user. . @falexwolf does it make sense to print a warning in `sc.tl.tsne` if `sc.pp.neighbors` is called with a a metric other than `euclidean` like ""a non-euclidean metric is used to construct the graph, Louvain clusters may not be representative in tSNE"" or would it be too hacky? tSNE is apparently misleading in these cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-398506015:519,simpl,simply,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398506015,1,['simpl'],['simply']
Usability,"Joining on the ""smart subsample"" part which we talked about a few months ago. The under/oversampling methods of [imbalanced-learn](https://imbalanced-learn.org/stable/index.html) was something we chatted about back then I remember. I opened a small dummy scverse-package draft [here](https://github.com/eroell/scimb) just to check how well this can be transferred for AnnData, but never really got to push it much. Not sure if we somehow could find ground to join on making something for stuff like that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862#issuecomment-1971428782:124,learn,learn,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862#issuecomment-1971428782,2,['learn'],['learn']
Usability,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:123,simpl,simple,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434,2,['simpl'],['simple']
Usability,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:314,simpl,simple,314,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807,2,"['simpl', 'usab']","['simple', 'usable']"
Usability,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:64,learn,learn,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994,3,['learn'],['learn']
Usability,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:194,simpl,simple,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140,1,['simpl'],['simple']
Usability,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:848,simpl,simply,848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698,1,['simpl'],['simply']
Usability,"Just saw this by chance. As we're planning to merge with scvelo's plotting modules soonish, that would simply become; `scv.pl.scatter(adata, groups=[[c] for c in adata.obs['clusters'].cat.categories], color='clusters', ncols=4)`, simply passing a list of lists to `groups` (without copying a whole anndata object). Will that be sufficient?. ![image](https://user-images.githubusercontent.com/31883718/71019675-10dcb000-20fb-11ea-80da-78301d37b958.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/955#issuecomment-566661312:103,simpl,simply,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566661312,2,['simpl'],['simply']
Usability,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051:64,learn,learn,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051,1,['learn'],['learn']
Usability,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):; ```; adata.obs['new_clusters'] = (; adata.obs[""old_clusters""]; .map(lambda x: {""a"": ""b""}.get(x, x)); .astype(""category""); ); ```; Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:; ```; adata.obs['new_clusters'] = (; adata.obs[""old_clusters""]; .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)); .astype(""category""); ); ```; @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-941184403:503,simpl,simply,503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-941184403,1,['simpl'],['simply']
Usability,"Just to be clear, is **n_genes** = `nFeature_RNA` and **n_genes_by_counts** = `nCounts_RNA `?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434#issuecomment-2029488023:11,clear,clear,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434#issuecomment-2029488023,1,['clear'],['clear']
Usability,"Long term, I'd prefer to just use pynndescent since it would be a more simple implementation. That could change some results. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959398:71,simpl,simple,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959398,1,['simpl'],['simple']
Usability,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`?. If we do that or plan to in the future, how should the added `weights` parameter to all these functions work?. I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesn’t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494718710:272,simpl,simply,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494718710,1,['simpl'],['simply']
Usability,Looks good to me! Docstring is also clearer now. Do you think it's worth adding a warning at a hard minimum threshold of 5-10 cells per category or sth like this? Although it would probably also just look strange to anyone plotting it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1936#issuecomment-875418541:36,clear,clearer,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1936#issuecomment-875418541,1,['clear'],['clearer']
Usability,"Looks great! You can get rid of `expect_warning`, as you can just simply check `if expected_warning_message is not None` instead, otherwise pretty ideal!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2563#issuecomment-1682177199:66,simpl,simply,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563#issuecomment-1682177199,1,['simpl'],['simply']
Usability,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:35,undo,undoubtedly,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085,1,['undo'],['undoubtedly']
Usability,"Looks simple enough! Please deduplicate the tests though, they have too many identical lines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042#issuecomment-2092792623:6,simpl,simple,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042#issuecomment-2092792623,1,['simpl'],['simple']
Usability,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800:133,simpl,simple,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800,1,['simpl'],['simple']
Usability,"MAP(); pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]); dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""); sc.tl.embedding_density(dblt, ""umap""); ```; </details>. <details> ; <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python; pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data; dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pbmcdf[""density""] = pbmc.obs[""umap_density""].values; dbltdf[""density""] = dblt.obs[""umap_density""].values; ```. Get plotting imports and canvas:. ```python; import datashader as ds; from datashader import transfer_functions as tf; from bokeh import palettes. canvas = ds.Canvas(plot_width=300, plot_height=300,; x_range=(pbmcdf[""x""].min() - .5, pbmcdf[""x""].max() + .5), ; y_range=(pbmcdf[""y""].min() - .5, pbmcdf[""y""].max() + .5),; x_axis_type='linear', y_axis_type='linear'); ```. </details>. First, something simple. Basically just a 2d histogram with 300 x 300 bins:. ```python; real = canvas.points(pbmcdf, 'x', 'y', ds.count()); sim = canvas.points(dbltdf, 'x', 'y', ds.count()). tf.Images(; tf.shade(real, name=""pbmcs""),; tf.shade(sim, name=""doublet""),; tf.shade(sim / (real + sim)),; ); ```. <img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/55789263-532a0800-5afd-11e9-8c58-4ecde66a2717.png"">. Using the weights from your method, while making the plots look more similar (though there's something weird going on with non-overlapping areas here):. ```python; real_density = canvas.points(pbmcdf, 'x', 'y', ds.mean(""density"")); sim_density = canvas.points(dbltdf, 'x', 'y', ds.mean(""density"")). tf.Images(; tf.spread(tf.shade(real_density, name=""pbmc""), px=2),; tf.spread(tf.shade(sim_density, name=""doublet""), px=2),; tf.spread(; tf.shade(; sim_density / (sim_density + real_density), ; cmap=palettes.Viridis256; ), ; px=2,; name=""sim / (real + sim)""; ),; tf.spread",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:3766,simpl,simple,3766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['simpl'],['simple']
Usability,Makes sense! Thanks it's clear now why the corr matrix,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1614#issuecomment-771145250:25,clear,clear,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614#issuecomment-771145250,1,['clear'],['clear']
Usability,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950:457,simpl,simply,457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950,1,['simpl'],['simply']
Usability,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-673944797:16,clear,clear,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-673944797,1,['clear'],['clear']
Usability,"Maybe the documentation is not clear; ```; counts_per_cell_after : float or None, optional (default: None); If None, after normalization, each cell has a total count equal to the median of the counts_per_cell before normalization.; ```; I thought that only ""counts_per_cell_after"" would multiply by the median, but the argument is:; ```; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); ```; not ""counts_per_cell_after"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905#issuecomment-552922125:31,clear,clear,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905#issuecomment-552922125,1,['clear'],['clear']
Usability,"Mmh no, it does work as you expect, just need to turn the dendrogram off.; ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""CD79A"", ""MS4A1""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=False,; swap_axes=True,; ); ```. or just pass the list of markers (list, not mapping); ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_list = [""GNLY"", ""NKG7""]. sc.pl.heatmap(; pbmc,; marker_genes_list,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. If you pass a dict with incomplete annotation and request dendrogram, then it fails, and the warning says this clearly:; ```; WARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.; categories: 0, 1, 2, etc.; var_group_labels: 1, 0, 2, etc.; ```; and it still produces a plot (yet unordered). The `var_group_labels` is also described in `help(sc.pl.heatmap)` as you pointed out. I think the misunderstanding is that passing a mapping or a list the behaviour is different, although potentially expected since a mapping and a list are different things. This could probably be explained clearer in the `var_names` argument yes. Just to go back to your original problem, in your case you were using as mapping categories that were not present in your `groupby` key altogether. This is a different issue, and probably the function should have thrown an error saying `var_group_labels` are not present in `categories`. . If you feel like opening a PR for this, we would really appreciate!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024:909,clear,clearly,909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024,2,['clear'],"['clearer', 'clearly']"
Usability,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:105,simpl,simply,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,1,['simpl'],['simply']
Usability,Most interesting things would be (from Severin's conversations) would be autocorrelation (via NN graph). Current workflow is to just use squidpy by it would be simple to add/port from squidpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3137#issuecomment-2275898105:160,simpl,simple,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3137#issuecomment-2275898105,1,['simpl'],['simple']
Usability,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:3,intuit,intuition,3,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146,1,['intuit'],['intuition']
Usability,"My main point is that having an implicit mapping between colors and the categories is not that user or developer friendly. It seems to me like it'd be simpler to just have the mapping be explicit. This wouldn't change much from right now, except for cutting down on some boiler plate in a bunch of plotting functions. That example was just to demonstrate that it could even be simpler to have an explicit mapping, since we don't have to do:. ```python; dict(zip(adata.obs[key].cat.categories, adata.uns[key + ""_colors""])); # and instead could just do:; adata.uns[key + ""_colors""]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480739679:151,simpl,simpler,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480739679,2,['simpl'],['simpler']
Usability,"My main reasoning was that:. * Use dask array/ delayed is more simple; * One can go from dask array/ dask delayed -> `Future` with `client.compute`, but the other way around isn't really possible since `Future`s kick off computation immediately. So I'd like to use the higher level interface as much as possible, and only use lower level when necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332:63,simpl,simple,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332,1,['simpl'],['simple']
Usability,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:29,simpl,simple,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738,2,['simpl'],"['simple', 'simply']"
Usability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:16,intuit,intuitive,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,1,['intuit'],['intuitive']
Usability,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:1464,simpl,simpler,1464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['simpl'],['simpler']
Usability,"Nice to hear that it worked. As a side product, you can now create a cell; browser html directory from the generated directory. On Thu, Feb 7, 2019 at 10:21 AM aditisk <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> thanks for the feedback. As; > @maximilianh <https://github.com/maximilianh> suggested, I was able to; > export the expression matrix from the cellbrowser export function. Thank; > you for your help.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262#issuecomment-461540169>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TXGCkdaQWO8ks_x7uOm-P2_ISArRks5vLG6RgaJpZM4Wne7Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-461557503:262,feedback,feedback,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-461557503,1,['feedback'],['feedback']
Usability,"Nice! Thank you! That's certainly very meaningful. It should just go somewhere else, not in `preprocessing.simple`... Let me think about where to put these queries... We'll have more of this sort of thing in the future! I'll certainly merge this and move it around... However, we'll not make bioservices a hard requirement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-386769886:107,simpl,simple,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-386769886,1,['simpl'],['simple']
Usability,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:467,learn,learn,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179,4,"['guid', 'learn', 'simpl']","['guide', 'learn', 'simpler']"
Usability,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:410,intuit,intuitive,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315,1,['intuit'],['intuitive']
Usability,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:423,clear,clear,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483,4,"['clear', 'user-friendly']","['clear', 'user-friendly']"
Usability,"No, not right now. You have to do this with numpy or pandas. adata is just a collection of numpy arrays and a dict (`print(adata)`) So you can simply do this manually. If you explain in a more detailed way what you want, I can probably also quickly implement it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-310073367:143,simpl,simply,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310073367,1,['simpl'],['simply']
Usability,"No, there is no way to produce a single file with data and metadata. Having genes as rows can simply be achieved by transposing the matrix (`adata.T.write_csvs(...)`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-460475684:94,simpl,simply,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-460475684,1,['simpl'],['simply']
Usability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:541,simpl,simple,541,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,2,['simpl'],"['simple', 'simply']"
Usability,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:555,learn,learn,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,1,['learn'],['learn']
Usability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:182,learn,learn,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,1,['learn'],['learn']
Usability,"Not sure, according to [this page](https://joblib.readthedocs.io/en/latest/auto_examples/parallel/distributed_backend_simple.html#sphx-glr-auto-examples-parallel-distributed-backend-simple-py) function code should be changed. It would be handy to add the backend as option to `regress_out`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781#issuecomment-1860968991:182,simpl,simple-py,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781#issuecomment-1860968991,1,['simpl'],['simple-py']
Usability,"Nothing should be hardcoded `np.float32`, but it might be that some functions still do that from an early time, where, for instance, scikit-learn's PCA was silently transforming to `float64` (and Scanpy silently transformed back etc.). Nothing should change the dtype that the user wants, except, for instance, when we logarithmize an integer matrix etc. Here, there should be a default `dtype='float32'` parameter. [PS: In algorithms that inherently are unstable and would profit more from higher precision, one could think about increasing precision.]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475999342:140,learn,learn,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475999342,1,['learn'],['learn']
Usability,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091:39,learn,learn,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091,1,['learn'],['learn']
Usability,"OK guys, I'm happy with all that!. I suggested `datasetsdir` over `datasetdir` because the module is called `datasets` and it's a place where many datasets end up. But, Isaac, you're the native speaker, so you're choice. Regarding making a settings a class: happy to if you feel you want to do that already in this PR. Really just change `scanpy/settings.py` to `scanpy/_settings.py`, put a `Settings` class in that file and generate an instance upon importing Scanpy. You could also just make `datasetdir` the only property and add all other attributes of the current `scanpy/settings` module as simple attributes. Then, all of this should be 10 min of work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479385067:597,simpl,simple,597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479385067,1,['simpl'],['simple']
Usability,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right?. Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565:32,simpl,simply,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317#issuecomment-435729565,1,['simpl'],['simply']
Usability,"OK, issues like this are almost always either memory or dependency problems: Something’s miscompiled or compiled for the wrong architecture (e.g. a newer CPU than you have) or simply buggy. We have no native code in Scanpy, so we don’t cause segfaults. If there’s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108:176,simpl,simply,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108,1,['simpl'],['simply']
Usability,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672:83,learn,learn,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672,2,['learn'],['learn']
Usability,"OK, so now the question is: should this become part of legacy-api-wrap?. I’d rather have the API fixed once than using multiple decorators. I think It’s clearer to see what the new API is like if you don’t have to think about the order of multiple decorators being applied. Also, I think. ```py; @renamed_args(new=""old""); ```. feels more natural than. ```py; @deprecated_arg_names({""old"": ""new""}); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422:153,clear,clearer,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422,1,['clear'],['clearer']
Usability,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:98,learn,learn,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962,3,['learn'],['learn']
Usability,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:1043,simpl,simply,1043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672,1,['simpl'],['simply']
Usability,"OK; now I have more time. The error thrown at ; ```; 207 df['dispersion_norm'] = (df['dispersion'].values # use values here as index differs; --> 208 - disp_mean_bin[df['mean_bin']].values) \; 209 / disp_std_bin[df['mean_bin']].values; ```; astonishes me. The line has been working for me on pandas 0.19.2 and 0.20.3 and for others for other versions for many months already. Do you have an old pandas version? The line should work as `disp_mean_bin` has been computed from `disp_grouped = df.groupby('mean_bin')['dispersion']` [here](https://github.com/theislab/scanpy/blob/65503d34d6b9d0a1d23e831d6daeba86856b3eee/scanpy/preprocessing/simple.py#L215); i.e., the Series 'mean_bin' was used to initialize the index of `disp_mean_bin`. Hence, you should be able to index with 'mean_bin'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115:637,simpl,simple,637,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324469115,1,['simpl'],['simple']
Usability,OS: Windows 10; Python version: 3.7.7; sc.logging.print_versions() gives; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038:165,learn,learn,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038,1,['learn'],['learn']
Usability,Of course that's fine. You can simply push such things to master. ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/591#issuecomment-480219397:31,simpl,simply,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/591#issuecomment-480219397,1,['simpl'],['simply']
Usability,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616845594:33,clear,clear,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616845594,1,['clear'],['clear']
Usability,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)); colors = pd.Series(data[:,0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:934,learn,learning,934,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,1,['learn'],['learning']
Usability,Oh that's reaaaally bad. I did a quick git bisect on sklearn:. ![image](https://user-images.githubusercontent.com/1140359/58111323-40582800-7bbf-11e9-8905-e7f3a73cd057.png). Here is the commit that broke our umaps: https://github.com/scikit-learn/scikit-learn/pull/13554,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494454599:241,learn,learn,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494454599,2,['learn'],['learn']
Usability,"Oh, damn... It wasn't clear to me that you were actually saying that this is broke... Sorry, I went over a few pull requests too quickly. Anyways, so the master branch was broke for a day. It's resolved in https://github.com/theislab/scanpy/commit/96890730972162aa531c3289b38ad728a7585c85. The next pull request goes smoother... :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-368081444:22,clear,clear,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-368081444,1,['clear'],['clear']
Usability,"Oh. Well, that’s just not right. They need to undo that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933#issuecomment-874661288:46,undo,undo,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874661288,1,['undo'],['undo']
Usability,Ok @ivirshup I think we're ready to go here. Thanks for the guidance!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1665700077:60,guid,guidance,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1665700077,1,['guid'],['guidance']
Usability,"Ok, good to read that it wasn't log-transformed!. @Koncopd, could you quickly implement these simple changes? Before continuing to work on the UMAP? These simple changes are for 1.4.1, the UMAP will be for 1.5. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478391082:94,simpl,simple,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478391082,2,['simpl'],['simple']
Usability,"Ok, now I see why you are confused! The x-axis in the figure above is somehow confusingly labeled 'dpt_order' even though it simply corresponds to `range(adata.n_smps)`, as evident from the code snippet I posted above; posting it again with the weird axis labeling that confuses you; ```; import matplotlib.pyplot as pl; pl.plot(range(adata.n_smps), adata.smp['dpt_pseudotime'][adata.smp['dpt_order']]); pl.xlabel('dpt_order'); pl.show(); ```. I definitely have to change this naming. Still it is meaningful that `adata.smp['dpt_order']` is an index vector that ""generates"" the order. OK, very soon, I could do the following. I'll rename the index array that generates the order `adata.smp['dpt_generate_order']` and the order you see in the plot `adata.smp['dpt_ordering_id']`, if you think this is better. So, one probably fast computation of this is the following; ```; ordering_id = np.zeros(adata.n_smps, dtype=int); for count, idx in adata.smp['dpt_generate_order']:; ordering_id[idx] = count; ```. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/27#issuecomment-314744802:125,simpl,simply,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314744802,1,['simpl'],['simply']
Usability,"Okay, I solved the issue. In my environment, scanpy==1.7.2 does not work with umap==0.5.2. I pip uninstall scanpy and umap-learn. Next I installed umap-learn through conda which was umap==0.5.1. When installing scanpy again it works as expected. . Not sure if its worth looking further into it but pip install scanpy also installs umap==0.5.2 (which does not work at least for me). . Many thanks! ; ![umap_0 5 1](https://user-images.githubusercontent.com/20926246/148250255-5ac00a46-cbc9-4608-a893-0472e32f5fb5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077:123,learn,learn,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077,2,['learn'],['learn']
Usability,"On `legacy_api_wrap`, I don't think I have enough experience maintaining stable APIs to make a call. I'm not too worried about the api for this decorator if it's just in scanpy. Since it'd be only meant for internal use there aren't any promises about the api that should be kept. It also means the issue of multiple decorators can be dealt with when it occurs, and an example case could help guide the decision. I think that I prefer passing a dict to using `kwargs` because it might make sense to give this decorator keyword arguments of its own. For example, if you can specify the version it'll be removed. If keyword arguments we used I agree `new=""old""` would make sense to me, but with a `dict` I see ""old"" maps to ""new"" as more intuitive.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-473500100:393,guid,guide,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-473500100,2,"['guid', 'intuit']","['guide', 'intuitive']"
Usability,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-674659520:139,clear,clear,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-674659520,1,['clear'],['clear']
Usability,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:1483,simpl,simplify,1483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688,1,['simpl'],['simplify']
Usability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:300,user experience,user experience,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,1,['user experience'],['user experience']
Usability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:436,learn,learn,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,1,['learn'],['learn']
Usability,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:102,clear,clearly,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463,1,['clear'],['clearly']
Usability,"PS: If you have a conda environment for Python 3, you do not need to use `pip3`; simply use `pip`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/29#issuecomment-321767948:81,simpl,simply,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321767948,1,['simpl'],['simply']
Usability,"PS: You can of course also simply upload here on GitHub in a comment, as you want. ; PPS: The canonical way of saving AnnData's is via `.write('myfile.h5ad')`. 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/365#issuecomment-440424406:27,simpl,simply,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440424406,1,['simpl'],['simply']
Usability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:153,simpl,simply,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['simpl'],['simply']
Usability,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/277#issuecomment-426845759:365,clear,clear,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-426845759,1,['clear'],['clear']
Usability,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:107,simpl,simplifies,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877,3,"['learn', 'simpl']","['learn', 'simplifies']"
Usability,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:361,simpl,simple,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924,1,['simpl'],['simple']
Usability,"Please go ahead!. On Tue, Feb 12, 2019 at 6:41 PM Philipp A. <notifications@github.com> wrote:. > Ah, sorry for being in the way here with the unrelated logging changes.; > Alex is currently a bit ill I learned, which is why he probably didn’t do; > it yet. I didn’t have time to review the whole thing, but if y’all want I; > can do that too; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/425#issuecomment-462858876>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RE9LYK4sL6sLFd586y_cpEBQKxwks5vMvzRgaJpZM4Z-M3d>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-463080680:203,learn,learned,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-463080680,1,['learn'],['learned']
Usability,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:739,pause,pausefilter,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937,1,['pause'],['pausefilter']
Usability,"Quick question to you @ivirshup, can't we simply replace all the `adata_neighbors` stuff with `scanpy.datasets.pbmc68k_reduced`? It already has the neighbor graph etc. in it and is smaller, that is, would speed up tests considerably.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/581#issuecomment-479418054:42,simpl,simply,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581#issuecomment-479418054,1,['simpl'],['simply']
Usability,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:324,simpl,simply,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932,1,['simpl'],['simply']
Usability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:171,simpl,simply,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"Regarding your other bug: `scanpy.plotting` used to have the attribute and `scanpy.api.plotting` would simply import the module. To make the [overview of the API](https://scanpy.readthedocs.io/en/latest/api.html) work, I had to introduce a [dummy module](https://github.com/theislab/scanpy/blob/master/scanpy/api/pl.py). In order to avoid duplication, I removed all exports from `scanpy.plotting.__init__`. I readded it to fix the bug on the development branch, but I need to think of a better solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094:103,simpl,simply,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324378094,1,['simpl'],['simply']
Usability,"Relevant issues about `scverse.org/learn`: . * https://github.com/scverse/scverse-tutorials/issues/58; * https://github.com/scverse/scverse-tutorials/issues/60. Tutorial registry is [here](https://github.com/scverse/scverse-tutorials/tree/main/tutorial-registry) and works in principle, but needs to be filled with content as described in https://github.com/scverse/scverse-tutorials/issues/58",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2502#issuecomment-1580575932:35,learn,learn,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502#issuecomment-1580575932,1,['learn'],['learn']
Usability,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:358,guid,guidance,358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637,1,['guid'],['guidance']
Usability,"Retrieve cell names in the Seurat object used to create the embedding, then simply reorder AnnData accordingly (adata = adata[cell_names]).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1718#issuecomment-791195880:76,simpl,simply,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-791195880,1,['simpl'],['simply']
Usability,Same issue here. . scanpy==1.6.0 anndata==0.7.4 umap==0.4.4 numpy==1.19.0 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1411#issuecomment-695065780:108,learn,learn,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411#issuecomment-695065780,1,['learn'],['learn']
Usability,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:537,learn,learn,537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518,1,['learn'],['learn']
Usability,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014:39,simpl,simply,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014,1,['simpl'],['simply']
Usability,"Same issue: . ```; UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:280,learn,learn,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012,2,['learn'],['learn']
Usability,"Same thing: If the line is under-indented, the first line summary can’t be properly extracted. I’ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:122,clear,clear,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728,1,['clear'],['clear']
Usability,"Same! Seems like -maxiter gets set/clobbered to 1. I'm seeing it on one machine (which I have limited access too, its a galaxy installation using scanpy scripts) but not another (my local), both of which are apparently running scanpy 1.8.1. . Im wondering if there's a umap-learn version issue? In order to set the umap n_epochs(aka maxiter) default , it looks like older versions of umap-learn expected 0 (e.g. https://github.com/lmcinnes/umap/blob/0.5.0/umap/umap_.py), whereas the newer expect None. My working installation has umap-learn 0.5.2 (which seems to expect None), and I'm not sure about the one on the other server. Might be barking up the wrong tree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544:274,learn,learn,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544,3,['learn'],['learn']
Usability,"Seems like an ""old"" X_diffmap has only length 2056. But that should not be the case, of course, as that would be invalid. I just learned about a bug in the storage of the graph `.uns['neighbors']['connectivities']` that appears when you do subsetting on an AnnData and want to access the original object. That could explain what is happening... I'll submit a bug for that in the next couple of hours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708:129,learn,learned,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381650708,1,['learn'],['learned']
Usability,Seems like this is still an issue. I am getting this error below randomly. It disappears after trying sometime later with the exact code and files... scanpy==1.9.4 anndata==0.9.2 umap==0.5.3 numpy==1.23.4 scipy==1.11.2 pandas==2.1.4 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 louvain==0.8.1 pynndescent==0.5.10. `adata=sc.read_h5ad('foo.h5ad')`; Error:; `AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.group.Group'> from /.`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1890353810:240,learn,learn,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1890353810,1,['learn'],['learn']
Usability,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:564,learn,learn,564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294,1,['learn'],['learn']
Usability,"Sergei (@Koncopd) tested it out and will get back to you. He also found a peak memory usage of 121 GB. I have to admit that I never made checks with that degree of detail and I fear that for now, I'll simply update the documentation stating that peak memory usage can go up to ~120 GB. I'm still puzzled by that, and maybe some efficiency found it's way into the code which wasn't there (simple guess: is everything in `float32`?). But we'll need some time to work it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466:201,simpl,simply,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466,2,['simpl'],"['simple', 'simply']"
Usability,"Should the reference object where you learn the transformation (currently `adata`) always be a subset of the data you're going to apply the transformation to (`adata2`)? If so, instead of passing a separate object, could there be a mask of which samples to train on?. If not, what do you think about making this a separate function? Maybe `combat_by_reference`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047,1,['learn'],['learn']
Usability,"Should this be relayed to scikit-learn then? If so, that should probably be done by someone who knows where in the `sc.pp.neighbors()` function this is breaking...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494457995:33,learn,learn,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494457995,1,['learn'],['learn']
Usability,Similar pull request exists already in sklearn.; https://github.com/scikit-learn/scikit-learn/pull/12841; Will watch.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-460239298:75,learn,learn,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-460239298,2,['learn'],['learn']
Usability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:12,learn,learn,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,1,['learn'],['learn']
Usability,Sklearn has its implementation of [CCA](; http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](; https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423799757:56,learn,learn,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423799757,1,['learn'],['learn']
Usability,Slightly unrelated feedback: I think that your API documentation is broken or incomplete: https://bento-tools.readthedocs.io/en/latest/api.html#tools. Would be great to have it complete so that people can use your tool easily.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2334#issuecomment-1291804266:19,feedback,feedback,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2334#issuecomment-1291804266,1,['feedback'],['feedback']
Usability,"So I switched everything to Napoleon after all. Far simpler and prettier. I no longer parse the docstrings, and opted for a different approach: If a line contains only the name of a parameter, it will be amended with type info. Since no fancy parsing is involved, that could theoretically go wrong. On the other hand, the docstrings now stay in the same format and the probability for a line to consist of just the parameter name (and e.g. no `.` behind it) is relatively low and can be fixed when discovered.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404504765:52,simpl,simpler,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404504765,1,['simpl'],['simpler']
Usability,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conveni",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:788,simpl,simple,788,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178,1,['simpl'],['simple']
Usability,"So for a fix, we’d simply need to change. https://github.com/scverse/scanpy/blob/414092f68b4b40aa99153556377c32839b392636/scanpy/preprocessing/_highly_variable_genes.py#L197-L199. into. ```py; X = X.copy(); if 'log1p' in adata.uns_keys() and adata.uns['log1p'].get('base') is not None:; X *= np.log(adata.uns['log1p']['base']); np.expm1(X, out=X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2668#issuecomment-1766402734:19,simpl,simply,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2668#issuecomment-1766402734,1,['simpl'],['simply']
Usability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:249,learn,learn,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,1,['learn'],['learn']
Usability,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:87,learn,learn,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310,1,['learn'],['learn']
Usability,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:7,user experience,user experience,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,2,"['learn', 'user experience']","['learn', 'user experience']"
Usability,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:61,clear,clear,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,1,['clear'],['clear']
Usability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:171,learn,learn,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,1,['learn'],['learn']
Usability,"Solved same problem for me as well. . For the record, the output of `sc.logging.print_versions()` in my conda environment is: . `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654:233,learn,learn,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654,1,['learn'],['learn']
Usability,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:209,simpl,simplifying,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225,1,['simpl'],['simplifying']
Usability,"Sorry @ivirshup , still not quite getting what you're after. The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Non-integer values don't necessarily mean normalised or log'd data, so I can't add a check in the code for that. I could add a check on the values of the simulated doublets, that they are the sum of the parent counts we send to scrublet's simulate function, but that seems outside the scope of these fixes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519:212,simpl,simple,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519,1,['simpl'],['simple']
Usability,Sorry about that. Any idea how we can be clearer?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531833776:41,clear,clearer,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531833776,1,['clear'],['clearer']
Usability,"Sorry about the delay, I've been working on some writing about this stuff (though from a different perspective). I'm not sure `k` is ""meant"" to have any particular effect, since these methods weren't designed for KNN graphs. I'd also argue if the parameters are analogous, there's an advantage of simplicity to just choosing one of them. I've got some plots for the effect of resolution and number of neighbors on the size of clusters which are found. This is for the 10x example dataset with 10k pbmcs using the v3 chemistry. What I've done is build the networks at 5 different values of k, four times each (different random seeds). For each of those networks, I ran clustering at 50 different resolutions (`np.geomspace(0.05, 20, 50)`). Here are the maximum cluster sizes found for each combination of k and resolution for the unweighted and weighted graph (color bar is logscale, from 1 to 6000, but I couldn't get useful ticks to work):. ![image](https://user-images.githubusercontent.com/8238804/56872793-2c158500-6a70-11e9-91fd-ee7aac91b811.png); ![image](https://user-images.githubusercontent.com/8238804/56872794-2d46b200-6a70-11e9-9607-67147d7493f9.png). Overall, pretty similar. Now, the minimum cluster sizes (color scales are different, but you'll see why):. ![image](https://user-images.githubusercontent.com/8238804/56872836-a34b1900-6a70-11e9-9a60-7b2a51b53da2.png); ![image](https://user-images.githubusercontent.com/8238804/56872837-a6460980-6a70-11e9-8722-be6576f605e7.png). This looks to me like using the weighted graph allows identifying small clusters even at low resolutions. The cluster of 25 cells looks like megakaryocytes, and are being detected at pretty much every clustering (996 out of 1000) using the weighted graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411:297,simpl,simplicity,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-487432411,1,['simpl'],['simplicity']
Usability,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:30,clear,clear,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813,1,['clear'],['clear']
Usability,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:154,learn,learn,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575,1,['learn'],['learn']
Usability,Sorry for the lack of feedback! I got caught up in my own work. * Have you made any progress on the travis issue?; * Is it an issue of getting different results from computing variance?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182#issuecomment-624644914:22,feedback,feedback,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-624644914,1,['feedback'],['feedback']
Usability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:374,simpl,simplest,374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,1,['simpl'],['simplest']
Usability,"Sorry for the late response! This seems to have come just after I went through the issues last weekend...; ; It looks great! :smile:. Some small notes:; * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...); * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916:852,clear,clear,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916,1,['clear'],['clear']
Usability,"Sorry for the late response!. In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622244074:235,simpl,simplifying,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622244074,1,['simpl'],['simplifying']
Usability,Sorry to be unresponsive for a while. I think exporting `plot_scatter` as `pl.scatter_embedding` and keeping `pl.scatter` sounds a like a simple and good solution!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-490000909:138,simpl,simple,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-490000909,1,['simpl'],['simple']
Usability,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:191,simpl,simplify,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620,1,['simpl'],['simplify']
Usability,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:218,clear,clear,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572,1,['clear'],['clear']
Usability,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:153,clear,clear,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549,1,['clear'],['clear']
Usability,"Sorry, there is a small bug in the wilcoxon method, that might hit sometimes. @a-munoz-rojas, it should be resolved after merging your fix, don't you think so? I'd be happy to move forward as soon as the code-overhead issue around double logarithmization is fixed. Should be very simple. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/530#issuecomment-474301716:280,simpl,simple,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530#issuecomment-474301716,1,['simpl'],['simple']
Usability,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:364,learn,learn,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['learn'],['learn']
Usability,"Sounds great!. Re tidy: Storing things internally in tidy format also seems inefficient to me... I remember a long discussion with Philipp more than 2 years ago... :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution?. > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being sa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:731,learn,learn,731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['learn'],['learn']
Usability,"Sounds like a good idea. Since we have a hexagonal grid, we can just connect the centers of the hexagons in a regular fashion instead of running delauney triangulation. But it’s fast enough to do that too if we want to have it easy and there’s a delauney implementation in something we already import (e.g. scipy maybe?). @giovp the result would simply be a smoothly changing shading. Like this, but with a hex grid instead of a square grid:. ![](https://upload.wikimedia.org/wikipedia/commons/f/f5/Interpolation-bicubic.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1287#issuecomment-706180849:346,simpl,simply,346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1287#issuecomment-706180849,1,['simpl'],['simply']
Usability,"Sure! I made it as easy as possible, by adding it directly to the [autosummary template](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/_templates/autosummary/base.rst#L3). `api_image` is defined [in `scanpy/docs/conf.py` here](https://github.com/theislab/scanpy/blob/181240602f2face3c49532f40b2fad4f300e3685/docs/conf.py#L213-L218), and simply inserts a `..image` directive if a file exists at the right place. Just add your (.png) image to [`scanpy/docs/api`](https://github.com/theislab/scanpy/tree/master/docs/api) using the qualified name of the function, e.g. `scanpy/docs/api/scanpy.api.pl.dotplot.png`. Maybe we should set up [Contribution guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) where people can read this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161:377,simpl,simply,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-406512161,3,"['guid', 'simpl']","['guidelines', 'guidelines-for-repository-contributors', 'simply']"
Usability,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:195,clear,clear,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361,2,"['clear', 'guid']","['clear', 'guidance']"
Usability,"Tbh, I found out about `groups` after writing the function and looking for a way to put the dots in front. Maybe there is a simpler way to do this... But then the command you suggest gives an error on my own data if I don't also specify `color='bulk_labels'` (works for the pbmc68k, but doesn't colour anything in), and then it just puts all the labels on the same plot and doesn't create small multiples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331:124,simpl,simpler,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331,1,['simpl'],['simpler']
Usability,"Thank you all for the feedback!. In the end the best solution has been to store activities in `.obsm` and then use the plotting functions via an `extract` function like in `squidpy`. Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-807060037:22,feedback,feedback,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-807060037,1,['feedback'],['feedback']
Usability,"Thank you all for this! In particular, @Koncopd!. I know that this will cause a little more headache, but could we consider renaming to `rank_genes`?. The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573:448,simpl,simple,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573,2,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/319#issuecomment-432482743:23,feedback,feedback,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319#issuecomment-432482743,1,['feedback'],['feedback']
Usability,"Thank you both for taking the time to answer. I was hoping there is a python port/re-implementation of clustree, but this is clearly not the case. Nonetheless, the workarounds you suggest are helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785816727:125,clear,clearly,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785816727,1,['clear'],['clearly']
Usability,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:480,simpl,simple,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901,1,['simpl'],['simple']
Usability,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:445,simpl,simple,445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,2,['simpl'],"['simple', 'simply']"
Usability,"Thank you for these thoughts!. I guess the high documentation quality in R stems from the Bioconductor project, which really set some standards. Nothing like this exists in Python - everyone just does what he or she wants. There are few people thinking about setting up something similar to Bioconductor for Python - but this will likely take some time... With Scanpy, we try to provide documentation at the Standards of the big packages: numpy, scipy, statsmodels, seaborn, scikit-learn, h5py, pytables, etc. There are many more and all of them have great docs. I think, with Scanpy, one can still do a lot better. Tuturials tend to be too short. Also, there should be a properly rendered html output of the notebooks - with a button where you can simply download it and then run it yourself to start playing around with it. Hope we will have this in a couple of weeks. And yes, other packages maybe just need to take time. But I'd guess that this will get much better soon...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498:482,learn,learn,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364055498,2,"['learn', 'simpl']","['learn', 'simply']"
Usability,Thank you for this; maybe there was one version where this was inconsistent and I simply don't remember... @AnatoleKing have you used version 0.4.4?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370147087:82,simpl,simply,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370147087,1,['simpl'],['simply']
Usability,"Thank you for your kind replies, and sorry for my poor descriptions. This is what happend on my env. ![image](https://user-images.githubusercontent.com/19543497/52568903-597c6a80-2e53-11e9-8d9a-3e530bd6f991.png). And now I solved by just adding this,. ```python; import matplotlib as mpl; mpl.rcParams['figure.facecolor'] = 'white'; ```. I verified the same thing on another environment, but it didn't happen.; So this might be critically relating to my personal environment.; I apologize that I didn't verify on another computer before the issue. Lastly, this is a simple example for the thing. ```python; %matplotlib inline. # 2 lines below solved the facecolor problem.; # import matplotlib as mpl; # mpl.rcParams['figure.facecolor'] = 'white'. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/473#issuecomment-462346509:566,simpl,simple,566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473#issuecomment-462346509,1,['simpl'],['simple']
Usability,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:129,simpl,simple,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['simpl'],['simple']
Usability,"Thank you so much for the feedback!; I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using ; conda create -n scanpy python=3.6 scanpy; conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341:26,feedback,feedback,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341,1,['feedback'],['feedback']
Usability,"Thank you very much for the super clean examples here! Is fixed via https://github.com/theislab/scanpy/commit/0ed304a64038f7d2c11b36fe5883ab9765ffba57 (@yugeji, you fed a pandas series into scikit learn, which is generally a bad idea :wink: ).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/273#issuecomment-427033148:197,learn,learn,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273#issuecomment-427033148,1,['learn'],['learn']
Usability,"Thank you! I'll rename the result according to the scikit-learn convention, though. If they call it `explained_variance_`, we should also call it that way - we can add in the docs, that it equals the eigenvalues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/87#issuecomment-366189008:58,learn,learn,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/87#issuecomment-366189008,1,['learn'],['learn']
Usability,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:99,guid,guides,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336,2,['guid'],['guides']
Usability,"Thank you! This is great and very helpful! :smile:. Yes, it would be nicer if the user were allowed to pass both. But isn't it possible simply by virtue of not passing `gene_symbols`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/430#issuecomment-456008629:136,simpl,simply,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/430#issuecomment-456008629,1,['simpl'],['simply']
Usability,Thank you!; It was just not clear from the tutorial in the beginning that the same thing is used to define colors in plots :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/383#issuecomment-445461792:28,clear,clear,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-445461792,1,['clear'],['clear']
Usability,"Thank you, @giovp . I see. So when using heatmap, the mapping of marker genes must represent all clusters.; It won't work if markers are supplied for only a subset of the clusters. As far as I can tell, though, this is not apparent from the documentation of the function `help(sc.pl.heatmap)`.; Also, in the plotting tutorial, which I referred to in the beginning, only a subset of markers are supplied.; So, either the documentation and tutorial should be adjusted to make clear that markers must be supplied for all clusters. Or, which I would find great, heatmap could be adjusted to be able to display markers for a subset of the clusters, which I had assumed to be the case in the beginning. Btw. from looking at the tutorial, I believe the same issue may apply to some other plotting functions, e.g. for [Tracksplot](https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html#Tracksplot)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723037276:474,clear,clear,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723037276,1,['clear'],['clear']
Usability,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:66,simpl,simple,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969,1,['simpl'],['simple']
Usability,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user; want to sample data with some other weighting technique. So we should ask; them to just put the weights for observations, then we need to modify PCA; as well and i think my code will support most of plots and marker genes,; but not PCA, because my input is PCA matrix with weights for each; observations. Thanks,; Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all; > the time is possible, but not very nice for users. So a few questions come; > to mind:; >; > Should we add scanpy.pp.coreset, which would create a sampling and add; > adata.obs['coreset_weights'] or simply adata.obs['weights']?; >; > If we do that or plan to in the future, how should the added weights; > parameter to all these functions work?; >; > I think it might default to 'coreset_weights', and the functions would; > automatically use that .obs column if it exists. Users should also still; > be able to specify weights manually as in this PR.; >; > So the type of the parameter would be Union[str, pd.DataFrame,; > Sequence[Union[float, int]]].; > ------------------------------; >; > All of that doesn’t really affect this PR, as we can merge it as it is and; > include anndata-stored weights later.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494724138:763,simpl,simply,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494724138,1,['simpl'],['simply']
Usability,"Thanks @adamgayoso, it's clear now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1630#issuecomment-775835519:25,clear,clear,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630#issuecomment-775835519,1,['clear'],['clear']
Usability,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:38,feedback,feedback,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094,1,['feedback'],['feedback']
Usability,"Thanks a lot, @falexwolf. . I have one more question. Suppose, I have clustered cells with a restricted gene set & arrived at 8 Louvain clusters (considering one like the example above). . `adata.obsm['X_geneset1`] = adata[:, ['Tmem72', 'Adgrg2', 'Scn4b', 'Ust', 'Htr1d', 'Prnp', 'Rom1', 'Gpr158', 'Robo2', 'Cntn2', 'Rab11fip5', 'Lhfpl2', 'Emb', 'Gabrg2', 'Ptk7', 'Prkar1a', 'Ehd3', 'Kitl', 'Slc6a19', 'Spry4', 'Nceh1', 'Lin7a',....,'Lpin1']].X`. ![Input_subgroup_030519](https://user-images.githubusercontent.com/44576210/57128124-bae91100-6d92-11e9-96d8-6d3d322d5f12.png). Now, for marker identification, I have ranked genes in each community using Logistic regression & Wilcoxon. Naturally, there can be genes with a very high score (those were not used to cluster the cells). ; ![Input_CMarkers_LogReg](https://user-images.githubusercontent.com/44576210/57132746-d5c28200-6da0-11e9-816b-e3b71ee01a0f.png). Now, if I find a few genes in a cluster that acts as contradictory markers (like, the markers for vascular cells in a dendritic cluster) will I be able to eliminate contaminating cells from the group that expresses those vascular genes? To simply put it, I want to cluster genes with ['gene1', 'gene2', 'gene3', 'gene4',..'gene400'] but I don't want to include cells that express ['gene10' AND 'gene35' AND 'gene100'..etc]. Will I be able to omit all such cells & rerun the cluster? I can do one gene at a time. . `ldata_modified = ldata[ldata[: , 'Rpl13'].X > 0.5, :] `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-489055148:1150,simpl,simply,1150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-489055148,1,['simpl'],['simply']
Usability,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and; push when it passed tests for all 5 plots. Thanks,; Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,; >; > I have removed issue from the pull request by the testing tool, now the; > tools showed me duplications, which are mostly from other code and 1-2 from; > my code. Please have a look into it. It's my first pull request and its; > taking too much time :(; >; > Thanks; > Khalid; >; > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:; >; >> Ok , thanks for letting me know. Please check the pull request. I have; >> verified my code by keeping weights 1 and it has same values when; >> observations has no weights or all weights equal to 1.; >>; >> I also suggest to update PCA for weighted sampled data.; >>; >> Thanks,; >> Khalid Usman; >>; >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; >> wrote:; >>; >>> You can just open a new one, I’ll close this one then 🙂; >>>; >>> —; >>> You are receiving this because you authored the thread.; >>> Reply to this email directly, view it on GitHub; >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >>> or mute the thread; >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >>> .; >>>; >>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520:41,learn,learned,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520,1,['learn'],['learned']
Usability,Thanks for all the good feedback @ivirshup - I'll work on it and re-request review when I'm done.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-726918992:24,feedback,feedback,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-726918992,1,['feedback'],['feedback']
Usability,"Thanks for bearing with me Isaac 😅 🙏 took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:327,simpl,simply,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,1,['simpl'],['simply']
Usability,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:; ```; WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)); Reason for being yanked: License Violation; ```; Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615:452,simpl,simple,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615,1,['simpl'],['simple']
Usability,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:562,learn,learn,562,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,1,['learn'],['learn']
Usability,"Thanks for the PR! I've just renamed the variable to be a bit more clear. I do think this test could be a bit better (e.g. check that the structure of the object is correct), but also this is an improvement so LGTM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692:67,clear,clear,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692,1,['clear'],['clear']
Usability,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting.; * This should work with other solvers from scipy, like `lobpcg`, right?; * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that?. ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958:113,learn,learn,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958,1,['learn'],['learn']
Usability,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:266,simpl,simply,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256,2,['simpl'],['simply']
Usability,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/727#issuecomment-508614967:84,guid,guides,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508614967,1,['guid'],['guides']
Usability,"Thanks for the clarification, @falexwolf. The documentation is clear, not sure how I missed it. I agree that euclidean distance is well-approximated by PCA (as long as populations are sufficiently large). For other metrics, that may not be the case (and for Jaccard it bails hard), and so maybe a warning would be appropriate in those cases rather than changing the behavior of `choose_representation`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-400070267:63,clear,clear,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-400070267,1,['clear'],['clear']
Usability,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:34,clear,clear,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['clear'],['clear']
Usability,"Thanks for the detailed feedback @ivirshup , I've started to and will be working on that in the next days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-900656064:24,feedback,feedback,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-900656064,1,['feedback'],['feedback']
Usability,"Thanks for the feedback and sorry for the delay. The code snippet using `sklearn.decomposition.FastICA` gave me quite similar results to PCA for my data in terms of the downstream UMAP visualisations (when I simply embedded 50 components in the neighbourhood graph). One difference is that the ICA was slower to compute. I am not confident to create a vignette, as I'm unclear what the 'correct' results should look like. I have not tried the `picard` implementation yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004,2,"['feedback', 'simpl']","['feedback', 'simply']"
Usability,"Thanks for the feedback! It astonishes me a bit, though, that Monocle 2 doesn't work well on that example. For the Paul et al, Cell (2015), it should give very nice results ([link to preprocessing](https://github.com/theislab/scanpy/blob/a2a330fa4640fdd4847fb48970a743242936e1df/scanpy/examples/builtin.py#L183-L199) / [link to plots](https://github.com/theislab/scanpy_usage/blob/master/EXAMPLES.md#paul15)), as they show in the recent preprint I reference above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/11#issuecomment-302212941:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11#issuecomment-302212941,1,['feedback'],['feedback']
Usability,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808,2,"['feedback', 'learn']","['feedback', 'learn']"
Usability,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:15,feedback,feedback,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387,1,['feedback'],['feedback']
Usability,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:934,learn,learning,934,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,1,['learn'],['learning']
Usability,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317:554,simpl,simply,554,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317,1,['simpl'],['simply']
Usability,"Thanks for the quick reaction. While the parameters seem to be respected in Scnapy 0.3.2, there is still some weird caching issue. Everything works OK when the ```write``` directory is empty, but when running multiple simulations in a row, e.g.:. ```; adam_krumsiek11_2 = sc.tl.sim('krumsiek11.txt', nrRealizations=1); sc.pl.sim(adam_krumsiek11_2). adam_krumsiek11_2 = sc.tl.sim('krumsiek11.txt', nrRealizations=2); sc.pl.sim(adam_krumsiek11_2); ```. I sometime get the same result (and both calls report reading from the very same simulation result file). However, when I clear the ```write``` directory between the calls to ```sc.tl.sim```, the results are as expected. This problem occurs only for certain parameters (for example, varying seed this way works as expected - I get two different figures).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/52#issuecomment-348160053:573,clear,clear,573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348160053,1,['clear'],['clear']
Usability,"Thanks for the quick response, @flying-sheep!. I can confirm that updating _pandas-2.2.2_ does fix this. I totally missed this possibility; it's not clear to me why the dots would change ordering, but the totals wouldn't (maybe _scanpy_ relies on default _pandas_ behaviour that changed between 1.x and 2.x?). That said, _pandas-2.x_ unfortunately breaks some dependencies in our environment, so I'll either pin _scanpy_ or use your workaround. Regarding the ordering and issue title change. Maybe a nit, but it's my understanding that the default ordering is alphabetical (which makese perfect sense as a default!). If this is correct, then I'd suggest that the wrong ordering is not the totals, but the categories themselves. Given this, the workaround that gives me the expected behaviour would be `dp.categories_order = dp.dot_color_df.index.sort_values()`:; <img width=""439"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/5192495/6f7622f5-14b5-4ea5-a44f-288c4507c4f0"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629:149,clear,clear,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629,1,['clear'],['clear']
Usability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:99,simpl,simple,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,1,['simpl'],['simple']
Usability,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:456,simpl,simple,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398,4,"['clear', 'intuit', 'simpl']","['clear', 'intuitive', 'simple', 'simply']"
Usability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:30,clear,clear,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,Thanks for you detailed guidance. All I want is to display zero count cells as some color (gray) and this is what cellranger and seurat do. I can do it now following the above code. This is just what I want !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550#issuecomment-748139666:24,guid,guidance,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748139666,1,['guid'],['guidance']
Usability,"Thanks for your comment Jason!; Don't you think the sentence before the one you quoted:; ""If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""].""; in combination with the default being ""None"" would make this clear?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845:290,clear,clear,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845,1,['clear'],['clear']
Usability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:290,learn,learning,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,2,"['learn', 'simpl']","['learning', 'simply']"
Usability,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed.; ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png); ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354:852,simpl,simply,852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-426424354,1,['simpl'],['simply']
Usability,Thanks for your reply. I will try that and may given more feedback. Cheers!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312780478:58,feedback,feedback,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312780478,1,['feedback'],['feedback']
Usability,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:876,simpl,simple,876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862,1,['simpl'],['simple']
Usability,"Thanks very much for your reply. . Yes, I am aware ‘dpi’ can help increase the resolution. However, I noticed that with the same ‘dpi’ , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing ‘dpi’ will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1791#issuecomment-815761921:335,simpl,simply,335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791#issuecomment-815761921,1,['simpl'],['simply']
Usability,"Thanks! A little bit of context. We needed this aggregation for one of the projects using pseudobulks of the data. We could use scanpy aggregation methods for simple averaging, but to test the outlier-robust median aggregation, we had to write our code. scanpy didn't have it for some reason, so @farhadmd7 kindly agreed to contribute here. Perhaps someone else will find it helpful, too. @eroell, what do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730:159,simpl,simple,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2258670730,1,['simpl'],['simple']
Usability,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:422,simpl,simple,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550,1,['simpl'],['simple']
Usability,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:24,learn,learn,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435,1,['learn'],['learn']
Usability,"Thanks, however I think the [documentation](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups) is not perfectly clear about it:. > groups : str, Iterable[str]; Subset of groups, e.g. ['g1', 'g2', 'g3'], to which comparison shall be restricted, or 'all' (default), for all groups. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531820364:167,clear,clear,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531820364,1,['clear'],['clear']
Usability,"Thanks, now is becoming clearer. Can you specify the maximum percentage; value or this is taken from the data?. What would be a nice name for a parameter to add to the function?; `max_fraction` ?. I would not be able to work on this for the next days but at least we can; throw some ideas. The change is the code is probably quite simple. On Fri, Dec 14, 2018 at 7:14 PM a-munoz-rojas <notifications@github.com>; wrote:. > Sure - this is an output from a Seurat analysis. You can see that the; > scale of the dot size goes from 10% to 60%, such that the group with 60%; > expressing cells is scaled to the max dot size:; >; > [image: pastedgraphic-3]; > <https://user-images.githubusercontent.com/37122760/50017697-d9dd3700-ff9a-11e8-8c28-fb6cd7f064e7.png>; >; > As a comparison, this is a (different) output from scanpy that; > automatically scales to 100% and causes the dots to be too small:; > [image: dotplotex]; > <https://user-images.githubusercontent.com/37122760/50019765-df8a4b00-ffa1-11e8-9b49-30d5057898bb.png>; >; > Please let me know if this makes sense!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/396#issuecomment-447408166>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Xirz8yk_WM-3MJPaKKrbq2rm19Bks5u4-qegaJpZM4ZSMYu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/396#issuecomment-447898456:24,clear,clearer,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/396#issuecomment-447898456,2,"['clear', 'simpl']","['clearer', 'simple']"
Usability,"Thanks, this is very helpful to pinpoint what is happening!. So what's going on here is that within one gene (column), all cells (rows) have the almost same float value.; This causes the variance per gene to be ~0, usually just a tiny value, in this dataset e.g. on the order of 1e-21.; Handling such tiny values, tiny offsets due to numerics can yield negative values for the variance. When doing np.sqrt() [here](https://github.com/scverse/scanpy/blob/fdfb9a1a48d480a30c23e5f14499a18a6388e418/src/scanpy/preprocessing/_scale.py#L186C5-L186C23) on such a nan, this yields this entire gene column to obtain nans. From our side, this could be addressed by considering to set such tiny negative values to 0. `sklearn` circumvents this by directly [computing the standard deviation](https://github.com/scikit-learn/scikit-learn/blob/2621573e60c295a435c62137c65ae787bf438e61/sklearn/preprocessing/_data.py#L248) from numpy, which likely has such a mechanism within it directly. However, trying to ""scale"" a feature of a constant value should be omitted in the first place very likely: as scaling involves dividing by standard deviation (which is ~0 then), the resulting numbers obtained are not actual biology, but just artifacts from a stability correction. I'd assume that in scRNAseq, this typically is a gene which is never observed (that is, 0 all the time), and as such could be filtered for in the preprocessing with e.g. `sc.pp.filter_genes(adata, min_counts=...)`, where `a` would be a number > 0. We might consider to raise a warning here though, as just introducing the nans without further comment can be quite confusing... What do you think about this @sophiamaedler?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2199782706:806,learn,learn,806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2199782706,2,['learn'],['learn']
Usability,That could also work. But this would require a bit of a rewrite. I think the current solution is simpler and also really fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797:97,simpl,simpler,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797,1,['simpl'],['simpler']
Usability,That doc change looks good to me!. For any change to `reference` it would be good to keep it simple. Maybe it could accept a list of groups?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-747197402:93,simpl,simple,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-747197402,1,['simpl'],['simple']
Usability,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:942,learn,learn,942,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,1,['learn'],['learn']
Usability,That should be simply `groupby='sample'` instead of multiple plot calls.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609649845:15,simpl,simply,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609649845,1,['simpl'],['simply']
Usability,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:331,simpl,simple,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092,1,['simpl'],['simple']
Usability,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:462,simpl,simple,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702,1,['simpl'],['simple']
Usability,"That's good to know! That means, `filter_genes` would simply annotate the genes kept just like `highly_variable_genes`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/429#issuecomment-460689393:54,simpl,simply,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/429#issuecomment-460689393,1,['simpl'],['simply']
Usability,"That's still the case (at least for randomized PCA @Koncopd linked above), though it looks like there may be a another path forward using other solvers: https://github.com/scikit-learn/scikit-learn/issues/12794. Still needs an implementation though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-577471793:179,learn,learn,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-577471793,2,['learn'],['learn']
Usability,"The ""choice"" mentioned in the source code regards how to compute sigma from the distances of the k nearest neighbors. In the default setting (sparse knn graph), this uses the median. In the completely different original dense and smooth setting of Coifman and Laleh's first diffmap paper, this uses the distance to the k-th neighbor divided by 4... Both of theses choices are completely ad hoc... The only goal that they should achieve is give you some smoothing already *among* the k nearest neighbors, and not just beyond (there are no further neighbors in the sparse setting). In destiny, some additional flexibility is allowed. You can choose the number of nearest neighbors and the sigma. Even though in principle it's ok to have both parameters, from the interpretation view it's not very clear. Both parameters affect the size of your effective local neighborhoods. In the dense setting, varying sigma makes a huge difference in destiny as it controls the neighborhood size alone, you can achieve the same by varying `n_neighbors` in scanpy. In the sparse setting, sigma does not influence the result a lot any more, if you don't choose pathological values, but `k` (called `n_neighbors` in scanpy) dramatically determines the size of the local neighborhood. So Scanpy tries to not burden the user with a parameter sigma that might take some thinking to understand and destroys consistency between the sparse-dense setting. Having said that, I'd generally recommend using umap or draw_graph for the things that you did with diffmap up to now... It's so much easier to have everything in 2 dimensions... Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/120#issuecomment-380350482:795,clear,clear,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/120#issuecomment-380350482,1,['clear'],['clear']
Usability,"The best thing to do would just be to put a link to the data here, and paste the code. It's actually easier to debug the more you can simplify the data that replicates the bug. Ideally, you could just send code that generates the data to replicate the bug. If that isn't working out, you could send me a DM on the discourse forum?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1170#issuecomment-618177408:134,simpl,simplify,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1170#issuecomment-618177408,1,['simpl'],['simplify']
Usability,"The biggest advantage would be the possibility to create such panels as shown in the example, where e.g. quality metrics have different cmaps as gene expression.; Another advantage would be that cmaps could be defined globally for each parameter, resulting in simpler plotting calls. ```; adata = sc.datasets.paul15(); adata.X = adata.X.astype('float64'); sc.pp.filter_cells(adata, min_genes=100); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); adata.uns['n_counts_all_cmap'] = 'copper'; adata.uns['n_genes_cmap'] = 'copper'; sc.pl.pca(adata, color=['paul15_clusters', 'n_counts_all', 'n_genes', 'Zyx', 'calp80', 'slc43a2'], ncols=3); ```; ![test](https://user-images.githubusercontent.com/23263654/99387978-28a55100-28d5-11eb-975d-f91211370c16.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186:260,simpl,simpler,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186,1,['simpl'],['simpler']
Usability,"The checks shouldn't take any time. But you're right; you're also slicing the `.var` and the `.varm` annotations if you make this call and we could check whether this perceivably slows down things (only for very large data, I guess). If it does, it would be very simple to add an accessor `.slice_X` that enables convenient slicing of the data matrix. That's a bit ugly but would vanish in the plotting function. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480:263,simpl,simple,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-422400480,1,['simpl'],['simple']
Usability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:288,clear,clear,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['clear'],['clear']
Usability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:778,learn,learn,778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,2,['learn'],['learn']
Usability,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:251,clear,clearly,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744,1,['clear'],['clearly']
Usability,The new documentation index looks great! It's exactly what I was thinking about! Thanks so much for being so responsive!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/407#issuecomment-451605378:109,responsiv,responsive,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407#issuecomment-451605378,1,['responsiv'],['responsive']
Usability,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py; if not issparse(X):; mean = np.mean(X, axis=0, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64); var = mean_sq - mean ** 2; elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py; if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138:38,learn,learn,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138,2,['learn'],['learn']
Usability,"The previous test failed but is not clear to me why, as it passes the local tests (anndata 0.7.5). It seems that on travis server, backed slicing requires integer indices and will not work with a boolean vector. I changed to sorted integers hoping that this will solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048:36,clear,clear,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048,1,['clear'],['clear']
Usability,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-487917974:72,simpl,simpler,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-487917974,2,['simpl'],"['simpler', 'simplified']"
Usability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:370,clear,clearly,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['clear'],['clearly']
Usability,"The same error happens to me, @Blohrer . **Versions:**. > scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557:147,learn,learn,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557,1,['learn'],['learn']
Usability,"The slight difference is due to scikit learn's implementation of PCA. If the top PCs are enough for you, you get perfect reproducibility, as in the tutorials [clustering](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) and [trajectory inference](https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html). Higher PCs are subject to the instability of the underlying iterative methods for computing them. You'll always see slight inconsistencies. However, I've never seen this to affect any conclusion drawn from an analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-474290381:39,learn,learn,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-474290381,1,['learn'],['learn']
Usability,"The tSNE looks ""good"" if parts of the data that you expect to be connected actually look connected in the tSNE and do not cluster apart. In your case, it looks a bit too clustered, but seeing the DiffMaps, everything turns out to be fine. The second example simply doesn't seem to have a branching.; PS: There will soon be a new default tSNE that will ensure that the correspondence between DiffMap and tSNE is better. Still, of course, both visualization methods focus on different aspects of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-310611678:258,simpl,simply,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310611678,1,['simpl'],['simply']
Usability,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-605811146:63,simpl,simple,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-605811146,1,['simpl'],['simple']
Usability,"The way pynndescent does it looks simply wrong: They knew that one specific backend caused problems, but instead of changing it only when that backend is set, they do it indiscriminately, which is pretty rude. I might be missiong something, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667061:34,simpl,simply,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667061,1,['simpl'],['simply']
Usability,"Their guide is also applicable to us, even if it uses a lot of pandas examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666343302:6,guid,guide,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666343302,1,['guid'],['guide']
Usability,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:192,clear,clear,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639,1,['clear'],['clear']
Usability,"There's my initial attempt, but it fails with:. ```pytb; File ""/usr/local/lib/python3.6/dist-packages/scanpy/preprocessing/simple.py"", line 169, in filter_genes; adata.var['n_cells'] = number; File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2519, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2585, in _set_item; value = self._sanitize_column(key, value); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 2760, in _sanitize_column; value = _sanitize_index(value, self.index, copy=False); File ""/usr/local/lib/python3.6/dist-packages/pandas/core/series.py"", line 3121, in _sanitize_index; raise ValueError('Length of values does not match length of ' 'index'); ValueError: Length of values does not match length of index; ```. If it isn't obvious to you what's wrong I'll return to it after my conference this weekend. Every hour is critical right now. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/77#issuecomment-363822066:123,simpl,simple,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/77#issuecomment-363822066,1,['simpl'],['simple']
Usability,"These appear to be consistent, simple changes and I assume they would be covered by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_highly_variable_genes.py. If you have any doubts about this, let's discuss before making a PR. Otherwise, I'm happy if you move forward with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/415#issuecomment-452283384:31,simpl,simple,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415#issuecomment-452283384,1,['simpl'],['simple']
Usability,"Thinking about this more I think ideally we wouldn't really on the obs column name ""--"" convention and rather place and filter this based on data put in .uns, right? Just have to learn how to do that ....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-367770658:179,learn,learn,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-367770658,1,['learn'],['learn']
Usability,"This certainly looks strange. Would you mind making a minimal complete example? If you need, [here's a useful guide](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) to making one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/530#issuecomment-474171747:110,guid,guide,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530#issuecomment-474171747,1,['guid'],['guide']
Usability,"This error is certainly caused by ""scikit-learn"". I abandoned this conda environment and created a new one by `conda create -n Scanpy -c conda-forge scikit-learn`[https://scikit-learn.org/stable/install.html](url).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729:42,learn,learn,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729,3,['learn'],['learn']
Usability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:404,simpl,simply,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['simpl'],['simply']
Usability,This is great! 👍 . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924:73,simpl,simply,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924,1,['simpl'],['simply']
Usability,"This is identical with #2339. The simplest way is to downgrade `python-igraph` to `0.9.9`, https://github.com/scverse/scanpy/issues/2339#issuecomment-1261132252",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341#issuecomment-1271333881:34,simpl,simplest,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341#issuecomment-1271333881,1,['simpl'],['simplest']
Usability,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797848694:328,clear,clear,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797848694,1,['clear'],['clear']
Usability,This is the output of `sc.logging.print_versions()`:; ```; scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520:150,learn,learn,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496828520,2,['learn'],['learn']
Usability,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as ; ```; <dl class=""field-list simple"">; <dt class=""field-odd"">Parameters</dt>; ```; whereas; ```; <p class=""rubric"">Notes</p>; ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need; ```; .rst-content dl:not(.docutils) dl dt {; font-weight: bold;; font-size: 16px;; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484428516:188,simpl,simply,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484428516,3,"['learn', 'simpl']","['learn', 'simple', 'simply']"
Usability,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811:416,simpl,simple,416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811,1,['simpl'],['simple']
Usability,"This is very interesting! It would be awesome if you linked to a small example to learn what you do exactly! I guess, a PR would then be more than welcome! 🙂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-437729769:82,learn,learn,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-437729769,1,['learn'],['learn']
Usability,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:304,clear,clear,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596,1,['clear'],['clear']
Usability,"This makes sense. Negative means lead to negative dispersion values and hence NaNs upon taking a log. https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L299-L312. One simply shouldn't take a logarithm in this case, so, I suggest you pass `log=False` to the function. Should we clarify the documentation: https://github.com/theislab/scanpy/blob/457d3aa8b7dd7344914edc7991618067cab34dde/scanpy/preprocessing/simple.py#L241-L243? Maybe put this right at the top so that it's not left unnoticed anymore?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779:204,simpl,simple,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398710779,3,['simpl'],"['simple', 'simply']"
Usability,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:75,learn,learn,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129,1,['learn'],['learn']
Usability,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:134,guid,guides,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141,1,['guid'],['guides']
Usability,This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1509#issuecomment-748156450:59,learn,learn,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-748156450,1,['learn'],['learn']
Usability,"This should be fixed via: https://github.com/theislab/scanpy/issues/407. We're super happy if you check out the new: https://scanpy.readthedocs.io/en/latest/api/ and give us your feedback! We know that we still have an issue with the return sections, which was introduced in the past couple of months as we changed the docs generator. We're working on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/407#issuecomment-451494959:179,feedback,feedback,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/407#issuecomment-451494959,1,['feedback'],['feedback']
Usability,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst; parameter : some.type; Description; ```. Later I’ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then I’ll check if there’s a section like. ```rst; one_identifier; Desc; second_identifier; Desc; ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value; 2. The above for returning a tuple; 3. “this function adds some AnnData.obs/var fields”. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``; 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```; 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484124854:188,simpl,simple,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484124854,1,['simpl'],['simple']
Usability,"Those are good! Short and simple. On Sat, Mar 30, 2019 at 9:02 PM Philipp A. <notifications@github.com> wrote:. > The ones from vscode are pretty good:; > https://github.com/Microsoft/vscode/issues/new/choose; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/568#issuecomment-478229664>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AH221DYPmPqIrPkmsxrzuc2eWtN_QRLzks5vbzY9gaJpZM4cRcJj>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/568#issuecomment-478230333:26,simpl,simple,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568#issuecomment-478230333,1,['simpl'],['simple']
Usability,"To add on to what @ivirshup said, reading the file with a custom approach results in significantly less memory used:. I created the `h5ad` file with:. ```; adata = sc.read_10x_h5(""source.h5"", ""GRCh38""); adata.obs['n_counts'] = adata.X.sum(1); adata.obs['log_counts'] = np.log(adata.obs['n_counts']); adata.obs['n_genes'] = (adata.X > 0).sum(1); adata.write(""test.h5ad"", compression='gzip', compression_opts=1); ```. Then read it three different ways:. ```; In [7]: %memit a1 = sc.read(""test.h5ad""); peak memory: 2333.84 MiB, increment: 2170.77 MiB; ```. ```; In [8]: %memit a2 = sc.read(""test.h5ad"", backed=""r""); peak memory: 4400.07 MiB, increment: 2137.85 MiB; ```. ```; In [9]: %memit a3 = custom_read(""test.h5ad""); peak memory: 4390.11 MiB, increment: 66.90 MiB; ```. where `custom_read` is defined as:. ```; def custom_read(filename):; adata = AnnData(); hf = h5py.File(filename); adata.obs['n_counts'] = [x[1] for x in hf['obs'][()]]; adata.obs['log_counts'] = [x[2] for x in hf['obs'][()]]; adata.obs['n_genes'] = [x[3] for x in hf['obs'][()]]; adata.obs_names = [x[0] for x in hf['obs'][()]]; adata.var['gene_ids'] = [x[1] for x in hf['var'][()]]; adata.var_names = [x[0] for x in hf['var'][()]]; return adata; ```. and here's some verification the custom read is actually reading the data. ```; In [16]: a2.obs_keys(); Out[16]: ['n_counts', 'log_counts', 'n_genes']. In [17]: a3.obs_keys(); Out[17]: ['n_counts', 'log_counts', 'n_genes']. In [18]: len(a2.obs); Out[18]: 384000. In [19]: len(a3.obs); Out[19]: 384000. In [20]: a2.var_keys(); Out[20]: ['gene_ids']. In [21]: a3.var_keys(); Out[21]: ['gene_ids']. In [22]: len(a2.var); Out[22]: 33694. In [23]: len(a3.var); Out[23]: 33694; ```. Here's some version info I neglected in the original comment:. ```; In [28]: sc.logging.print_versions(); scanpy==1.3.3 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/434#issuecomment-456056835:1886,learn,learn,1886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434#issuecomment-456056835,1,['learn'],['learn']
Usability,"To answer the following question:. > 2. what you said: The original approach samples from the full list of genes in each bin, then restricts the sample to valid ones. Your approach samples from the valid genes in each bin.; >; > So if a bin e.g. contains mostly invalid genes, the original code adds only a few genes for that bin, while yours adds the maximum possible number.; >; > So the questions is: is the sampling bias introduced in the original code wanted? If not, you not only made the code more resilient, but also more objective. After going through the original [code from Seurat](https://github.com/satijalab/seurat/blob/c54e57d3423b3f711ccd463e14965cc8de86c31b/R/utilities.R#L280C3-L303), it seems to me that there's not equivalent to removing genes to be scored from the control gene set.; From what I can tell, if one of the genes to be scored happens to be chosen as the background, it will be included in the calculation.; But please correct me if that's not the case. So if the original implementation does not remove score genes from the control gene set, we would simply need to remove the following line: https://github.com/scverse/scanpy/blob/ec4457470618efd85da3c7b29f951cab01a49e3a/scanpy/tools/_score_genes.py#L169. (Note: if we want to keep the current behaviour, we should still remove the line above, since it would be redundant)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358:1085,simpl,simply,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2875#issuecomment-2015316358,1,['simpl'],['simply']
Usability,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:507,simpl,simplicity,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,4,['simpl'],['simplicity']
Usability,Underlying wish: Why don't we simply ditch list-based coloring everywhere :D,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1392#issuecomment-680962922:30,simpl,simply,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392#issuecomment-680962922,1,['simpl'],['simply']
Usability,"Unfortunately, all of this discussion here was not really further pursued, I have to admit. In principle, these are very simple things. However, I'm a bit afraid of offering a canonical function as I fear that there are also a lot of bad ways of visualizing gene correlation plots and I don't feel capable of judging this. If no one else wants to make a pull request for that (maybe using what @tcallies already did, but I fear it's not really serving the purpose of the discussion here: [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/top_genes.py), [here](https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/plotting/top_genes_visual.py)) it would be cool if someone sent me an example case, which clearly shows what you want. Maybe @jorvis, you can send images for the examples you have in mind?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-399897165:121,simpl,simple,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-399897165,2,"['clear', 'simpl']","['clearly', 'simple']"
Usability,"Unfortunately, changing; ```; .rst-content dl:not(.docutils) dl dt {; font-weight: normal;; ```; to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though; ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484027492:250,learn,learn,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484027492,1,['learn'],['learn']
Usability,"We have images which are saved which can be compared against, typically one would add a new image which you have visually confirmed to be correct, so if the behavior changes later we know. There is a little more about this in the [contribution guide](https://scanpy.readthedocs.io/en/stable/dev/testing.html#plotting-tests).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248:244,guid,guide,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248,1,['guid'],['guide']
Usability,"We should make dynamic 3D plots ;-) . If I remember correctly, in the past we have the issue that the categorical colors were given by the adata.obs order and we change them such that they follow the order of the categories. Yet, I agree that a good mix of categorical colors is good sometimes. To address this issue I think that we can simply randomize the order if `sort_order=False` to avoid adding any new parameters. . Isaac's solution looks great for dealing with of lots of cells, something that I imagine will become more frequent. I think we should have a 'cookbook' where we can keep this and other information. I find this better than adding more and more functionality to the scatter plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279:337,simpl,simply,337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279,1,['simpl'],['simply']
Usability,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:861,learn,learning,861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609,2,"['learn', 'simpl']","['learning', 'simply']"
Usability,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:569,guid,guides,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984,1,['guid'],['guides']
Usability,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499043170:28,undo,undocumented,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499043170,1,['undo'],['undocumented']
Usability,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:216,learn,learn,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,1,['learn'],['learn']
Usability,"What about comparing communities between for example CPM and RBERVertexPartition at the same resolution, using modularity score? This way we are not comparing scores obtained by optimization functions, just simple ""external"" measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127:207,simpl,simple,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127,1,['simpl'],['simple']
Usability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:43,simpl,simple,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['simpl'],['simple']
Usability,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:416,learn,learn,416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825,1,['learn'],['learn']
Usability,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:1431,learn,learn,1431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,2,['learn'],['learn']
Usability,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:144,learn,learn,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316,1,['learn'],['learn']
Usability,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:288,learn,learn,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,2,['learn'],['learn']
Usability,Why are we using `umap.__version__` instead of `importlib.metadata.version('umap-learn')`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898544845:81,learn,learn,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898544845,1,['learn'],['learn']
Usability,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:471,learn,learn,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250,2,['learn'],['learn']
Usability,"Why not simply as in the [tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)?; ```; sc.tl.rank_genes_groups(adata, 'louvain', groups=['0'], reference='1'); ```; Or am I missing your problem? A few lines of code documenting your call wouldn't hurt.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/346#issuecomment-436337697:8,simpl,simply,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346#issuecomment-436337697,1,['simpl'],['simply']
Usability,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302#issuecomment-441476438:92,simpl,simply,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-441476438,1,['simpl'],['simply']
Usability,"With respect to the heatmap, indeed it is possible to transpose the matrix.; Currently, this option is only available for `stacked_violin`. I thought; about adding this option to other plots like heatmap, matrixplot and; dotplot but I have not find the time and it is always possible to save the; figure and rotate it so it has low priority for me. The changes are not as; trivial as simply rotating the matrix as all other elements need to be; adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this...; > 😄; >; > —; > You are receiving this because you were mentioned.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839:384,simpl,simply,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-436548839,1,['simpl'],['simply']
Usability,Would be a nice use case for working on the usability of benchmarks (related to #2977),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2986#issuecomment-2044397793:44,usab,usability,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2986#issuecomment-2044397793,1,['usab'],['usability']
Usability,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1601#issuecomment-763294926:69,simpl,simple,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601#issuecomment-763294926,1,['simpl'],['simple']
Usability,"Wow! Again simply awesome! :smile:. PS: Sorry for the late response, I was on holidays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/175#issuecomment-398683241:11,simpl,simply,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175#issuecomment-398683241,1,['simpl'],['simply']
Usability,"Wow, I learned something new! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/896#issuecomment-547448945:7,learn,learned,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547448945,1,['learn'],['learned']
Usability,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821:41,clear,clear,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821,1,['clear'],['clear']
Usability,"Yeah, I agree. Go for it ;) Saturday, 23 January 2021, 11:48AM +01:00 from Philipp A. notifications@github.com :. >Flit has a very tiny surface area. You can learn its full CLI in literally 2 minutes, as it doesn’t include any kind of new concept (like Poetry’s venv management).; >—; >You are receiving this because you commented.; >Reply to this email directly, view it on GitHub , or unsubscribe .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904369:158,learn,learn,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904369,1,['learn'],['learn']
Usability,"Yeah, I think I invented that convention in the early days of destiny, and it sloshed over to here. It recently completely confused a destiny user, which made me realize that this is *not* canonical and intuitive use, but just something I came up with so long ago that I forgot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-456715454:203,intuit,intuitive,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-456715454,1,['intuit'],['intuitive']
Usability,"Yeah, I was concerned about the complexity. But AnnData doesn’t do anything wrong, it just uses chunks in both directions, which that one specific algorithm doesn’t support. But I think in general we should test for 2D chunks, that’s why I think AnnData’s test helpers work as intended. Any idea how to do this more simply?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245560938:316,simpl,simply,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3162#issuecomment-2245560938,1,['simpl'],['simply']
Usability,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here?. Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003:40,simpl,simple,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003,1,['simpl'],['simple']
Usability,"Yeah, plotting in python is difficult, and our plotting code doesn’t make things simpler. This is the relevant part:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_anndata.py#L273-L284. The code that behaves like advertised in the docs is in here, but that function does more things after that:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_utils.py#L364. There’s also this:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_tools/scatterplots.py#L1181. I think things should be unified so they use the same palette selection logic. But I understand that that’s a pretty complex part of our code base. I meant this comment: https://github.com/scverse/scanpy/issues/1258#issuecomment-1626690231",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114:81,simpl,simpler,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114,1,['simpl'],['simpler']
Usability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:389,simpl,simply,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,1,['simpl'],['simply']
Usability,"Yes , the sampling is done with weights and I used the coreset technique; for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>; wrote:. > I understand the benefits of sampling regarding computational speed up.; > What I'm not clear on is how you choose your weights for the calculations; > you perform here. You mentioned that you get wrong marker gene results when; > you sample and don't use weights. That makes sense if you get a; > non-representative set of cells in your sample. I wonder how you select the; > weights to fix this. I guess you don't just try a lot of different values; > until one works, right?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494:254,clear,clear,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494,1,['clear'],['clear']
Usability,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:; ```; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version; return distribution(package).version; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution; return Distribution.from_name(package); File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name; raise PackageNotFoundError(name); importlib_metadata.api.PackageNotFoundError: umap-learn; ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512160342:323,learn,learn,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512160342,2,['learn'],['learn']
Usability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:627,simpl,simply,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,1,['simpl'],['simply']
Usability,"Yes, but I’m not happy about the spaghetti code in pl.scatter. We should make pl.embedding and pl.scatter share most of their code, and make that code as simple as @VolkerBergen’s.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553945910:154,simpl,simple,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553945910,1,['simpl'],['simple']
Usability,"Yes, we already have a good mask for sparse scaling. Boolean arrays are very effective for indicating where computations should be performed, as they eliminate the need for copying and reintegration. One clear example is the `tl.score_genes` function. masks there as booleans for the nanmean is a lot more efficent but less pythonic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711:204,clear,clear,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711,1,['clear'],['clear']
Usability,"Yes, we can simply add @bebatut's packages as an `ext` flag in the setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-435734577:12,simpl,simply,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-435734577,1,['simpl'],['simply']
Usability,"Yes, you simply pass a single gene name to the violin plot... :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370355474:9,simpl,simply,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370355474,1,['simpl'],['simply']
Usability,"Yes, you're of course right. It's not the best default and has been written like that for historic reasons (figuring out the difference and benchmarking vs seurat and vs cell ranger). But simply changing something here messes up everything people have done. I have to think of a quick way of checking whether data is logarithmized or not... Maybe it's enough to simply check whether the data matrix still contains integers - then `log` should be `True`. Otherwise, a warning should be raised if `log` is `True`. Makes sense?. At some point, one might rethink the structure of `filter_genes_dispersion`... one could deprecate it at some point in favor of a new function `extract_highly_variable_genes` or something like this. Or one indeed changes the default behavior and prints a lot of warnings... Not very desirable though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169:188,simpl,simply,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398726169,2,['simpl'],['simply']
Usability,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:471,responsiv,responsive,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,2,"['learn', 'responsiv']","['learning', 'responsive']"
Usability,"You can always choose a palTete like 'Blues', 'Reds', 'binary' that will; give you a gradient from a clear to a darker color. Maybe that helps but I; agree with Philipp, 120 clusters is a lot to visualize with different; colors. On Tue, May 28, 2019 at 4:21 PM Philipp A. <notifications@github.com> wrote:. > Closed #156 <https://github.com/theislab/scanpy/issues/156>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156?email_source=notifications&email_token=ABF37VNHLRL6TEP3I7BBLEDPXU5X7A5CNFSM4FAIXFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORVQ5OJI#event-2371999525>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDXU3HBFBIZ6LII73PXU5X7ANCNFSM4FAIXFAA>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-496543294:101,clear,clear,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-496543294,1,['clear'],['clear']
Usability,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846:60,learn,learn,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846,1,['learn'],['learn']
Usability,"You can pass `legend_loc=None`. Thanks for noting this. Looking at the docs, I can definitely see how this isn't clear. ```; legend_loc : str, optional (default: 'right margin'); Location of legend, either `'on data'`, `'right margin'` or a valid keyword; for the `loc` parameter of :class:`~matplotlib.legend.Legend`.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502#issuecomment-730259879:113,clear,clear,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-730259879,1,['clear'],['clear']
Usability,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```; def sparse_pca(X,npcs,mu = None):; # X -- scipy sparse data matrix; # npcs -- number of principal components; # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features; if mu is None: ; mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means; mmat = mdot = mu.dot ; # dot product operator for the transposed means; mhmat = mhdot = mu.T.dot ; # dot product operator for the data; Xmat = Xdot = X.dot ; # dot product operator for the transposed data; XHmat = XHdot = X.T.conj().dot ; # dot product operator for a vector of ones; ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means; def matvec(x): ; return Xdot(x) - mdot(x); def matmat(x): ; return Xmat(x) - mmat(x); def rmatvec(x): ; return XHdot(x) - mhdot(ones(x)); def rmatmat(x): ; return XHmat(x) - mhmat(ones(x)); ; # construct the LinearOperator; XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,; matmat = matmat,; shape = X.shape,; rmatvec = rmatvec, rmatmat = rmatmat); ; u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs); ; # i like my eigenvalues sorted in decreasing order; idx = np.argsort(-s); S = np.diag(s[idx]); # principal components; pcs = u[:,idx].dot(S) ; # equivalent to PCA.components_ in sklearn ; components_ = v[idx,:] ; return pcs,components_; ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727:100,learn,learn,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727,1,['learn'],['learn']
Usability,"You just need to specify `key=` to `rank_genes_groups_dotplot`, as described in the **Examples** section of the [documentation of `filter_rank_genes_groups`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.filter_rank_genes_groups.html):. > ```py; > sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered'); > # visualize results using dotplot; > sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered'); > ```. I don’t know if we can make it clearer. If you have any suggestions, or if I misinterpreted what you want, please leave a comment!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3049#issuecomment-2106955398:479,clear,clearer,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3049#issuecomment-2106955398,1,['clear'],['clearer']
Usability,"You would like the legend on the right to have both the short and long names, while the ""on data"" annotation would only have the short names – right?. I'm not sure I can think of a great way to do this without making the process complicated. It may be easier for you to add the text labels to the plot yourself. Here is some code for adding the labels with `""adjustText""` (which needs some parameter fiddling to look nice) #1513. ```python; def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).indices.items():; if g in exclude:; continue; medians[g] = np.median(adata.obsm[""X_umap""][g_idx], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs); ```. which can be simplified if you're alright with just plotting on the medians. I personally think interactivity and hover-over becomes quite useful at this point, though that can be difficult to scale and share.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2112#issuecomment-1015716949:1067,simpl,simplified,1067,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2112#issuecomment-1015716949,1,['simpl'],['simplified']
Usability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:970,simpl,simple,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['simpl'],['simple']
Usability,\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (7.1.1); Requirement already satisfied: networkx>=2.3 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.6.3); Requirement already satisfied: importlib-metadata>=0.7 in c:\users\charles\anaconda3\lib\si,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:1273,learn,learn,1273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['learn'],['learn']
Usability,"```; sc.logging.print_versions(); scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```. ```; adata_2.raw.shape; > (5558, 2000); adata_2.X.shape; > (5558, 2000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665:124,learn,learn,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665,1,['learn'],['learn']
Usability,"`log_transformed` disappeared in the last commit here... Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Also: If trying to call a t-test with non-logarithmized data, a warning should be written. The overflow and 0 warnings: are you sure you used logarithmized data, Gökcen?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809:117,simpl,simply,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809,1,['simpl'],['simply']
Usability,"`pd.crosstab` is a function from `pandas`, and would only work on a pandas dataframe. You can find some documentation on that [here](http://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html#cross-tabulations). I'm not sure why you would be losing any info. I'd also note this seems less related to scanpy, and more related to pandas, since `adata.obs` is just a pandas dataframe. Have you tried looking through the [pandas user guide](https://pandas.pydata.org/pandas-docs/stable/) to figure out how to do what you want?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/584#issuecomment-482929037:441,guid,guide,441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/584#issuecomment-482929037,1,['guid'],['guide']
Usability,"`pl.scatter` is, unfortunately, not in the best shape anymore. Since Fidel rewrote a large part of the plotting API, it's not used by any of the frequently used embeddings scatters anymore and would need to be simplified a lot, before actually adding new functionality (like `ncols`). Would you mind correcting the docs? What are you using `pl.scatter` for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476000764:210,simpl,simplified,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476000764,1,['simpl'],['simplified']
Usability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:177,guid,guide,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,1,['guid'],['guide']
Usability,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:137,simpl,simple,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,2,['simpl'],['simple']
Usability,"about your first questions: i’m using the numpydoc extension to parse the strings into a structure that I can modify. the normal way to render this structure back to a string is doing `str(numpydoc_object)`. This however doesn’t give us the original format, but instead the reStructuredText outpt of numpydoc. What do you mean with `auto` parameters? using `None` is the pythonic way to do things like this. i.e. if a simple string or int or so isn’t sufficient to explain the default case, use `None` and do something complex, which you then describe in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404199758:418,simpl,simple,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404199758,1,['simpl'],['simple']
Usability,"age pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package xz conflicts for:; python=3.6 -> xz[version='>=5.2.3,<6.0a0|>=5.2.4,<6.0a0']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:1844,learn,learn,1844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012,2,['learn'],['learn']
Usability,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973:466,clear,clear,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973,1,['clear'],['clear']
Usability,"all very good points, and I don't think I have a clear solution, it's more that we have to decide how to go about this:; > Question for this, what heuristics have you tried? My guess would be that min(distances_between_points) / 3 should be fine for an upper bound. this indeed could be solved, but we still would have to set this heuristics differently according to the spatial data type in question. E.g. for visium `size=1` is correct, because coordinates are in pixel measure. In the dataset I have now (seqFISH) the coordinates are essentially z-score and so would have to change for instance to the one you proposed. However, why would we want to have `circle` at all in that t case, and not just scatterplot? Since there is no real notion of size, I think a scatterplot is actually more appropriate. We discussed this already but back then we didn't have this example. > Second, I think this logic is a little convoluted, and I don't know that library_id will always be associated with visium only. Would a better check be for [""metadata""][""software_version""] or something like that?. It definitely is, there might be a better solution but I couldn't come up with it. The problem stems in the fact that we have a `library_id` key in `adata.uns[""spatial""]`. In `spatial` we also put this; ```python; {'connectivities_key': 'spatial_connectivities',; 'distances_key': 'spatial_distances',; 'params': {'n_neighbors': 6, 'coord_type': None, 'radius': None}}; ``` ; this is needed for plotting. The default in `sc.pl.spatial` now is that if library_id is `_empty`, then it iteratively search for some keys in `adata.uns[""spatial""]`.; ```python; try: # check if key is empty; spatial_data = adata.uns['spatial']; library_id = next(; (; i; for i in spatial_data.keys(); if i not in [""connectivities_key"", ""distances_key""]; ); ); except (KeyError, StopIteration) as e:; 	library_id = None; ```; The point is that it should only assign whatever key it finds that is not `[""connectivities_key"", ""distance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626:49,clear,clear,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626,1,['clear'],['clear']
Usability,"anpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1104,learn,learn,1104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,2,['learn'],['learn']
Usability,"anpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.13.0); Requirement already satisfied: numpy>=1.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.17.4); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:8611,learn,learn,8611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['learn'],['learn']
Usability,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:2001,feedback,feedback,2001,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004,1,['feedback'],['feedback']
Usability,"arger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks abo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3057,learn,learning,3057,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['learn'],['learning']
Usability,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:85,simpl,simply,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896,1,['simpl'],['simply']
Usability,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2552,feedback,feedback,2552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475,1,['feedback'],['feedback']
Usability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:192,simpl,simplification,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['simpl'],['simplification']
Usability,"c neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration fact",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1979,learn,learning,1979,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['learn'],['learning']
Usability,"cc: @mumichae about our conversation the other day. I've been thinking that a good entry point here could just be a notebook that demonstrates using these packages on single cell data that we could point to on https://scverse.org/learn (hosted on https://github.com/scverse/scverse-tutorials). This could be a good starting point for anyone who wants to jump in to investigate further. And, for completeness, I would also want to point out https://github.com/brianhie/geosketch as another promising subsampling method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862#issuecomment-2024910951:230,learn,learn,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862#issuecomment-2024910951,1,['learn'],['learn']
Usability,"cgutils.py?line=192) index=index)); [194](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/cgutils.py?line=193) self._builder.store(value, ptr). TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); e:\Drosophila\ST\04.yao\E8-10_b_analysis\E8-10_b_analysis\8_cellbin_clustering\cellbin_scsq.ipynb Cell 10' in <cell line: 1>(); ----> [1](vscode-notebook-cell:/e%3A/Drosophila/ST/04.yao/E8-10_b_analysis/E8-10_b_analysis/8_cellbin_clustering/cellbin_scsq.ipynb#ch0000016?line=0) from umap import UMAP. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\__init__.py:2, in <module>; [1](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=0) from warnings import warn, catch_warnings, simplefilter; ----> [2](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=1) from .umap_ import UMAP; [4](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=3) try:; [5](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/__init__.py?line=4) with catch_warnings():. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\umap_.py:41, in <module>; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=33) from umap.utils import (; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=34) submatrix,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=35) ts,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=36) csr_unique,; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=37) fast_knn_indices,; [39",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:8685,simpl,simplefilter,8685,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['simpl'],['simplefilter']
Usability,"ckages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package libstdcxx-ng conflicts for:; python=3.7 -> libstdcxx-ng[version='>=7.2.0|>=7.3.0']; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11789,learn,learn,11789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['learn'],['learn']
Usability,conda-forge; protobuf 4.23.4 pypi_0 pypi; psutil 5.9.5 py311h2582759_0 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pydantic 1.10.11 pypi_0 pypi; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pypi_0 pypi; pynndescent 0.5.10 pypi_0 pypi; pyparsing 3.0.9 pypi_0 pypi; pyro-api 0.1.2 pypi_0 pypi; pyro-ppl 1.8.5 pypi_0 pypi; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.11.4 hab00c5b_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-editor 1.0.4 pypi_0 pypi; python-igraph 0.10.6 pypi_0 pypi; python-levenshtein 0.21.1 pypi_0 pypi; python-multipart 0.0.6 pypi_0 pypi; python_abi 3.11 3_cp311 conda-forge; pytorch-lightning 2.0.5 pypi_0 pypi; pytz 2023.3 pypi_0 pypi; pyyaml 6.0.1 pypi_0 pypi; pyzmq 25.1.0 py311h75c88c4_0 conda-forge; rapidfuzz 3.1.2 pypi_0 pypi; readchar 4.0.5 pypi_0 pypi; readline 8.2 h8228510_1 conda-forge; requests 2.28.1 pypi_0 pypi; rich 13.4.2 pypi_0 pypi; scanpy 1.9.3 pypi_0 pypi; scikit-learn 1.3.0 pypi_0 pypi; scipy 1.11.1 py311h64a7726_0 conda-forge; scirpy 0.13.0 pypi_0 pypi; scmisc 0.0.1 pypi_0 pypi; scvi-tools 1.0.2 pypi_0 pypi; seaborn 0.12.2 pypi_0 pypi; session-info 1.0.0 pypi_0 pypi; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; singlecellhaystack 0.0.5 pypi_0 pypi; six 1.16.0 pyh6c4a22f_0 conda-forge; sniffio 1.3.0 pypi_0 pypi; soupsieve 2.4.1 pypi_0 pypi; sparse 0.14.0 pypi_0 pypi; squarify 0.4.3 pypi_0 pypi; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pypi_0 pypi; starsessions 1.3.0 pypi_0 pypi; statsmodels 0.14.0 pypi_0 pypi; stdlib-list 0.9.0 pypi_0 pypi; sympy 1.11.1 pypi_0 pypi; tensorstore 0.1.40 pypi_0 pypi; texttable 1.6.7 pypi_0 pypi; threadpoolctl 3.2.0 pypi_0 pypi; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pypi_0 pypi; torch 2.0.1+cu118 pypi_0 pypi; torchaudio 2.0.2+cu118 pypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:6680,learn,learn,6680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['learn'],['learn']
Usability,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4098,clear,clear,4098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,3,"['clear', 'learn', 'simpl']","['clear', 'learn', 'simply']"
Usability,"d: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:3855,learn,learn,3855,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['learn'],['learn']
Usability,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells.; I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109:341,simpl,simplified,341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109,1,['simpl'],['simplified']
Usability,"data.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1622,simpl,simplefilter,1622,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['simpl'],['simplefilter']
Usability,"does not recompute, simply saves the filtered data under; adata.uns['rank_genes_groups_filtered']. Thus, different parameters can be; tested quickly. Off course, sc.tl.rank_genes_groups has to be call first. On Mon, Mar 11, 2019 at 3:47 PM MalteDLuecken <notifications@github.com>; wrote:. > does sc.tl.filter_rank_genes_groups filter the sc.tl.rank_genes_groups; > result? Or does it recompute? The former would not alleviate the multiple; > testing burden.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/460#issuecomment-471569285>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1debC8DotLkQywhO8zJpEvfkBbSHks5vVmxpgaJpZM4ahuSs>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471643748:20,simpl,simply,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471643748,1,['simpl'],['simply']
Usability,"done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:1130,learn,learn,1130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,2,['learn'],['learn']
Usability,"e first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:1279,learn,learned,1279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884,1,['learn'],['learned']
Usability,"eady satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:3725,learn,learn,3725,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['learn'],['learn']
Usability,ed1ab_0 conda-forge; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pyhd8ed1ab_0 conda-forge; starsessions 1.3.0 pyhd8ed1ab_0 conda-forge; statsmodels 0.14.0 py310h278f3c1_1 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; suitesparse 5.10.1 h9e50725_1 conda-forge; sympy 1.12 pypyh9d50eac_103 conda-forge; tbb 2021.9.0 hf52228f_0 conda-forge; texttable 1.6.7 pyhd8ed1ab_0 conda-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.12 h27826a3_0 conda-forge; tomli 2.0.1 pyhd8ed1ab_0 conda-forge; tomlkit 0.11.8 pyha770c72_0 conda-forge; toolz 0.12.0 pyhd8ed1ab_0 conda-forge; torchaudio 2.0.2 py310_cu118 pytorch; torchmetrics 0.11.4 pyhd8ed1ab_0 conda-forge; torchtriton 2.0.0 py310 pytorch; torchvision 0.15.2 py310_cu118 pytorch; tornado 6.3.2 py310h2372a71_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; trove-classifiers 2023.7.6 pyhd8ed1ab_0 conda-forge; typing 3.10.0.0 pyhd8ed1ab_0 conda-forge; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023c h71feb2d_0 conda-forge; umap-learn 0.5.3 py310hff52083_1 conda-forge; unicodedata2 15.0.0 py310h5764c6d_0 conda-forge; urllib3 1.26.15 pyhd8ed1ab_0 conda-forge; uvicorn 0.23.1 py310hff52083_0 conda-forge; virtualenv 20.24.1 pyhd8ed1ab_0 conda-forge; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; webencodings 0.5.1 py_1 conda-forge; websocket-client 1.6.1 pyhd8ed1ab_0 conda-forge; websockets 11.0.3 py310h2372a71_0 conda-forge; wheel 0.40.0 pyhd8ed1ab_1 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pyhd8ed1ab_0 conda-forge; xlrd 1.2.0 pyh9f0ad1d_1 conda-forge; xorg-libxau 1.0.11 hd590300_0 conda-forge; xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; yaml 0.2.5 h7f98852_2 conda-forge; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hd590300_5 conda-forge; zstd 1.5.2 hfc55251_7 conda-forge. </p>; </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:21080,learn,learn,21080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['learn'],['learn']
Usability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4823,learn,learn,4823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,4,['learn'],"['learn', 'learn-']"
Usability,"em, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:1941,clear,clear,1941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,1,['clear'],['clear']
Usability,"epodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts for:; scanpy -> natsort; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1769,learn,learn,1769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,2,['learn'],['learn']
Usability,"esired e.g. he want to focus one cell type etc. So we should support; weights generally rather specifically. Thanks,; Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,; >; > But i will suggest to just support weights instead of coreset, may be user; > want to sample data with some other weighting technique. So we should ask; > them to just put the weights for observations, then we need to modify PCA; > as well and i think my code will support most of plots and marker genes,; > but not PCA, because my input is PCA matrix with weights for each; > observations.; >; > Thanks,; > Khalid; >; > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>; > wrote:; >; >> Long-term, we should think about the design here: Specifying weights all; >> the time is possible, but not very nice for users. So a few questions come; >> to mind:; >>; >> Should we add scanpy.pp.coreset, which would create a sampling and add; >> adata.obs['coreset_weights'] or simply adata.obs['weights']?; >>; >> If we do that or plan to in the future, how should the added weights; >> parameter to all these functions work?; >>; >> I think it might default to 'coreset_weights', and the functions would; >> automatically use that .obs column if it exists. Users should also still; >> be able to specify weights manually as in this PR.; >>; >> So the type of the parameter would be Union[str, pd.DataFrame,; >> Sequence[Union[float, int]]].; >> ------------------------------; >>; >> All of that doesn’t really affect this PR, as we can merge it as it is; >> and include anndata-stored weights later.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,; >> or mute the thread; >> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004:1457,simpl,simply,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004,1,['simpl'],['simply']
Usability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:31,simpl,simple,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,1,['simpl'],['simple']
Usability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2522,simpl,simple,2522,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['simpl'],['simple']
Usability,"fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspectives on the general topic here 👍🏻",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:2117,learn,learning,2117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['learn'],['learning']
Usability,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2571,simpl,simple,2571,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,1,['simpl'],['simple']
Usability,"he first is `OneOf`. So these are different things. My point is (repeating what Philipp said): in practice (in all the numerical stuff that I've done so far, including Scanpy), I have never encountered the need for defining such an intersection object on the typing level. I just overload functions using `OneOf` and account for differences in the passed objects attributes via `if isinstance(...):`... If I need a function that only eats a ""weird intersection type object"", I'll go and define the corresponding class and throw an error if the function gets fed something different. Fortunately, that happens quite rarely; but yeah, I had cases where I only wanted an `OrderedDict` but neither a `dict` or a `list`. But I'd never call this an ""intersection type"". @ivirshup You didn't explain the ""type lattice"": but according to what I learned about `Union` and `Intersection` in this thread, the sets involved in the mentioned ""set operations on the type lattice"" should have elements that are ""properties"" of types (as they are not restricted to actual class attributes, this, unfortunately, doesn't tell you right away which ""property"" you are intersecting: ""being ordered"", ""having a key accesor"", ""having a certain numerical range""). Right? Union and Intersection then refer to the maximal set of properties of the objects you pass. As each passed object can be characterized by a set of properties, all that naming makes sense. But for someone reading the docs, who isn't expected to know about all the properties of all each object that comes along the way, it's a really sophisticated concept. As a user, I want to characterize things with a simple name for a class, like `AnnData` or `OrderedDict`. And these are the things that I want to see in the docs. Don't you agree? By contrast, I'm fine with having the sophisticated in the code; even though I still think that a plain old school untyped function signature looks more beautiful and its content can immediately be grasped by a human.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884:2093,simpl,simple,2093,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443966884,1,['simpl'],['simple']
Usability,"he full dataset) with ```sc.pp.filter_genes(adata, min_counts=1)``` and now the zero variance genes are gone but still same warning, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/aues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1097,learn,learn,1097,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['learn'],['learn']
Usability,"he user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibility but we should probably stick with it. hence, another argument for simply using a dataframe and putting it in the unstructured annotation `.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:1311,simpl,simple,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458,2,['simpl'],"['simple', 'simply']"
Usability,"hey @WeilerP ,; > umap-learn>=0.5.1 should work (see here). I think this approach would be best, since numba>=0.53 supports python>=3.9. I agree this is the best solution, and don't think there is any drawback from scanpy side. Can @Koncopd @ivirshup comment on this? If so, I think it would be easy to inlcude it in 1.8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-845733520:23,learn,learn,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845733520,1,['learn'],['learn']
Usability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:20,feedback,feedback,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['feedback'],['feedback']
Usability,"hi @yotamcons ,. thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561:38,feedback,feedback,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561,1,['feedback'],['feedback']
Usability,"hmm, we could just make the upload conditional to the file existing and merge this into `main`. Then we can debug things as soon as the problem occurs somewhere, and finally undo this PR when commiting the actual fix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3069#issuecomment-2202643573:174,undo,undo,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3069#issuecomment-2202643573,1,['undo'],['undo']
Usability,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py; import numba; import warnings. with warnings.catch_warnings():; warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):; do_thing(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/688#issuecomment-504451555:162,simpl,simplefilter,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688#issuecomment-504451555,1,['simpl'],['simplefilter']
Usability,i just learned that OSX sends its locale per default when connecting to a server. so is it a local ubuntu or on a server?. what does `locale` (executed from a terminal) return?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344015034:7,learn,learned,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344015034,1,['learn'],['learned']
Usability,"ies'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:2512,learn,learn,2512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,2,['learn'],['learn']
Usability,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2627,learn,learning,2627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['learn'],['learning']
Usability,"it's not clear what the problem here sorry, can you copy the error and report a reproducible example? thank you!; I'll close this for now, feel free to reopen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437:9,clear,clear,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437,1,['clear'],['clear']
Usability,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:2256,feedback,feedback,2256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297,1,['feedback'],['feedback']
Usability,"later development versions will then again show `0.1+NUMCOMMITS.gHASH` and it will be clear for the user which kind of version she/he uses. i think it makes sense, do you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298315116:86,clear,clear,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298315116,1,['clear'],['clear']
Usability,"layed around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed corr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3551,simpl,simply,3551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,1,['simpl'],['simply']
Usability,"lkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3836,learn,learn,3836,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['learn'],['learn']
Usability,"ll of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. It’s a big improvement to no longer have ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:1030,learn,learn,1030,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,3,['learn'],['learn']
Usability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:2344,simpl,simply,2344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['simpl'],['simply']
Usability,"looks really great, I'd say Yes! to pretty much everything, thanks for looking into this.; > Getting the spot size even if the image isn't used. so this is something we are going back and forth a lot, I still think it has pros and cons, and also agree with the point below re having a separate argument `spot_size`. ; If `sc.pl.spatial` *always* plot `circles` and not `scatter`, then the size of the radius needs to be inferred from the data: this makes me a bit worried for the different cases that could arise. I understand that is much nicer that the function returns always the same type of plot (circles), but it might be a bit forcing in this context (given the heterogeneity of the data). What I would agree instead is to pass the `size_spot` and use circles, otherwise simply use scatter (and then set size). What do you think? . > Using ax.set_aspect(""equal"") when there is no image, so that the aspect ratio is equivalent (coordinates are assumed to be pixel space); this is really nice, I'd say yes in principle, would like to try it out though for couple of plots. > If crop_coords is not passed, use the cropping matplotlib would have used if there was no image. This is done by getting the axis limits before the image is added. this is also fine and probably cleaner than having an heuristic for the offset. > I feel like it would make sense for these to crop to the same part of the image or embedding:. missed this, ok yes it makes sense, then metric of `crop_coord` is the same as `adata.obsm[""spatial""]`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-756087364:778,simpl,simply,778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-756087364,1,['simpl'],['simply']
Usability,"lorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.1; matplotlib 3.3.2; matplotlib_inline NA; ml_collections NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.1; multipledispatch 0.6.0; nats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3765,learn,learn,3765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['learn'],['learn']
Usability,"manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2263,clear,clearly,2263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,1,['clear'],['clearly']
Usability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3269,clear,clear,3269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['clear'],['clear']
Usability,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:1876,clear,clearer,1876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,2,"['clear', 'simpl']","['clearer', 'simply']"
Usability,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1656,clear,clearer,1656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,1,['clear'],['clearer']
Usability,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4926,learn,learn,4926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['learn'],['learn']
Usability,"ng and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:2273,simpl,simply,2273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['simpl'],['simply']
Usability,"ng with the `svd_solver='arpack'` parameter. I attach some examples:. ```python; >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,; 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,; 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],; dtype=float32),; 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0. , 0. , 0. ,; 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,; 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,; 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],; dtype=float32)}. >>> sc.tl.pca(adata_h); Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,; 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,; 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,; 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,; 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/264#issuecomment-423300225:1224,learn,learn,1224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264#issuecomment-423300225,1,['learn'],['learn']
Usability,"ning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1852,learn,learning,1852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545,1,['learn'],['learning']
Usability,"nt, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obvi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2775,learn,learning,2775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['learn'],['learning']
Usability,"nto two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1290,simpl,simple,1290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['simpl'],['simple']
Usability,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2952,learn,learning,2952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['learn'],['learning']
Usability,"oh, and versions:; ```; scanpy==1.4.3+116.g0075c62 ; anndata==0.6.22.post2.dev80+g72c2bde ; umap==0.3.9 ; numpy==1.17.2 ; scipy==1.3.0 ; pandas==0.24.1 ; scikit-learn==0.21.3 ; statsmodels==0.10.0rc2 ; ython-igraph==0.7.1 ; louvain==0.6.1; ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/887#issuecomment-545561793:161,learn,learn,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561793,1,['learn'],['learn']
Usability,"ok that's great, thank you! The `radius_neighbors` have very clear applications in fish-like data, and we are assmebling a tutorial to show exactly that. The n-rings as well especially in the context of cell-cell communication (although did not check that systematically yet). So shall we add this functionality to spatial-tools ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707604861:61,clear,clear,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707604861,1,['clear'],['clear']
Usability,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:530,clear,clear,530,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112,1,['clear'],['clear']
Usability,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:64,learn,learn,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006,2,['learn'],['learn']
Usability,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-795155685:34,simpl,simple,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795155685,1,['simpl'],['simple']
Usability,"or:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package pytables conflicts for:; scanpy -> pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:2023,learn,learn,2023,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,2,['learn'],['learn']
Usability,orkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3391,learn,learn,3391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['learn'],['learn']
Usability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1168,user experience,user experience,1168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['user experience'],['user experience']
Usability,"pandas transparently handles gzipping. We could also simply add support for gzipped files:. ```py; def maybe_zipped_path(path: Path) -> Path:; zipped_path = Path(f'{path}.gz'); if zipped_path.is_file(): return zipped_path; return path; ```. And then we use it:. https://github.com/scverse/scanpy/blob/a38a22ab85074f17788b8d1effa89c1373e0c978/scanpy/readwrite.py#L488. to. ```py; genefile_exists = maybe_zipped_path(path / f'{prefix}genes.tsv').is_file(); ```. and . https://github.com/scverse/scanpy/blob/a38a22ab85074f17788b8d1effa89c1373e0c978/scanpy/readwrite.py#L525. to. ```py; genes = pd.read_csv(maybe_zipped_path(path / f'{prefix}genes.tsv'), header=None, sep='\t'); ```. and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916#issuecomment-1102500257:53,simpl,simply,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1102500257,1,['simpl'],['simply']
Usability,r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-serializinghtml 1.1.4 pyhd3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi;,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:16192,simpl,simplegeneric,16192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['simpl'],['simplegeneric']
Usability,rmation of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leid,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1101,learn,learn,1101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['learn'],['learn']
Usability,"rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 fro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:1946,simpl,simplefilter,1946,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['simpl'],['simplefilter']
Usability,rty | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0.9.4; testpath | 0.5.0; texttable | 1.6.4; threadpoolctl | 3.0.0; tornado | 6.1; tqdm | 4.62.3; traitlets | 4.3.3; typing-extensions | 3.10.0.2; **umap-learn | 0.5.2**; urllib3 | 1.26.7; wcwidth | 0.2.5; webencodings | 0.5.1; wheel | 0.37.0; win-inet-pton | 1.1.0; wincertstore | 0.2; xlrd | 1.2.0; zipp | 3.6.0. </body>. </html>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:3507,learn,learn,3507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,2,['learn'],['learn']
Usability,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2436,simpl,simple,2436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['simpl'],['simple']
Usability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:22,usab,usable,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,1,['usab'],['usable']
Usability,"scale_spot is a float to scale the size of the spots from users. We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Regarding examples, I've made a tutorial that I will push to scanpy-tutorial. I'm waiting for @Mirkazemi read function that I need to complete the notebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580139642:166,simpl,simply,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580139642,1,['simpl'],['simply']
Usability,seems like the `--deps` flag can be used to select dependencies. Default is all dependencies. ```bash; $ beni --deps production pyproject.toml; channels:; - conda-forge; dependencies:; - pip:; - flit; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; name: scanpy; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811:376,learn,learn,376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811,2,['learn'],['learn']
Usability,"some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1110,learn,learn,1110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['learn'],['learn']
Usability,"source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed correctly and modified, as well as a scale_basis argument to match the image coordinate if needed. That's it, looking forward to hear what are your thoughts and if you agree with current behaviour I'll go on and changing the docs as well as writing more tests (especially for first example where `groups` is used, where the background color is different)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:4451,simpl,simply,4451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514,1,['simpl'],['simply']
Usability,"ssion"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1285,simpl,simply,1285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['simpl'],['simply']
Usability,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:267,simpl,simply,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"t: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`?. ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these?. ```python; sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns; sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]); ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:2912,intuit,intuitive,2912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['intuit'],['intuitive']
Usability,"ta analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the fol",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:1205,intuit,intuitive,1205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,1,['intuit'],['intuitive']
Usability,tconsole 5.0.3 pyhd3eb1b0_0 ; qtpy 1.9.0 py_0 ; r 3.6.0 r36_0 r; r-base 3.6.1 haffb61f_2 r; r-boot 1.3_20 r36h6115d3f_0 r; r-class 7.3_15 r36h96ca727_0 r; r-cluster 2.0.8 r36ha65eedd_0 r; r-codetools 0.2_16 r36h6115d3f_0 r; r-foreign 0.8_71 r36h96ca727_0 r; r-kernsmooth 2.23_15 r36ha65eedd_4 r; r-lattice 0.20_38 r36h96ca727_0 r; r-mass 7.3_51.3 r36h96ca727_0 r; r-matrix 1.2_17 r36h96ca727_0 r; r-mgcv 1.8_28 r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:15790,learn,learn,15790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['learn'],['learn']
Usability,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:3028,intuit,intuitively,3028,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['intuit'],['intuitively']
Usability,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:64,clear,clear,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:430,simpl,simple,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119,5,"['clear', 'simpl']","['clear', 'clearer', 'simple', 'simply']"
Usability,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it?. enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:497,clear,clear,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,1,['clear'],['clear']
Usability,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON; old_to_new = dict(; old_cluster1='new_cluster1',; old_cluster2='new_cluster1',; old_cluster3='new_cluster2',; ); adata.obs['new_clusters'] = (; adata.obs['old_clusters']; .map(old_to_new); .astype('category'); ); ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/925#issuecomment-555069647:17,simpl,simply,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-555069647,1,['simpl'],['simply']
Usability,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:40,learn,learn,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671,1,['learn'],['learn']
Usability,"the problem is that at some point the code calls np.argsort on a; pandas.Series and this returns -1 for NaN values. I will submit a PR to fix; this. Meanwhile, simpy plot as follows: sc.pl.pca(adata, color='property',; size=50, sort_order=False). On Tue, Mar 19, 2019 at 11:45 AM Fabian Rost <notifications@github.com>; wrote:. > I just add the versions I used:; >; > sc.logging.print_versions(); >; > scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/536#issuecomment-474301705>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b3l_xKITwuZQ-XpENUHSioDqLWLks5vYL_TgaJpZM4b6BYA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/536#issuecomment-474344606:479,learn,learn,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536#issuecomment-474344606,1,['learn'],['learn']
Usability,"the runtime checks would be too costly or impossible. to test for `List[int]`, you’d have to traverse the whole thing and check every element. And for e.g. `Iterator[int]`, you just can’t test it at all – if you’d iterate the thing to check the objects it yields, you exhaust it and it’s no longer usable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064:298,usab,usable,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445749064,1,['usab'],['usable']
Usability,"thod=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is; - a) only a question currently open (I am really not particularly an expert in R with limited bandwidth to check things there so waiting for their answer).; - b) Even if true, this does not affect our examples here. It comes into play when we t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:1322,clear,clear,1322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['clear'],['clear']
Usability,"tigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal tSNE embeddings. The same about initialization: UMAP smartly uses Laplacian Eigenmaps to initialize, but sklearn/multicore tSNE use random init, which is simply a bad choice (as again shown in that paper). openTSNE now uses PCA init by default which is much more sensible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:1733,learn,learning,1733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833,3,"['learn', 'simpl']","['learning', 'simply']"
Usability,"tion with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3857,simpl,simply,3857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['simpl'],['simply']
Usability,tisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.13.0); Requirement already satisfied: numpy>=1.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.17.4); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:8123,learn,learn,8123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['learn'],['learn']
Usability,"to make formatting easier, you should do `pre-commit install` like mentioned in the contributor guide: https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#pre-commit. Please also fill out the checklist:. - if there is an issue closed by this, please link it; - you can check the box for tests, since this function already has tests. I added the `benchmark` label so we see that it actually makes things faster; - what’s missing is a release note entry: just edit `1.10.2.md` and add a line there please",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949:96,guid,guide,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949,1,['guid'],['guide']
Usability,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:14,learn,learn,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609,2,['learn'],['learn']
Usability,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:1611,clear,clear,1611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,2,['clear'],['clear']
Usability,"well, we should warn the user when they do something like this then. lanzcos is an implementation detail that our API hides, so it’s pure coincidence for an user to learn about it beforehand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/170#issuecomment-398737615:165,learn,learn,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/170#issuecomment-398737615,1,['learn'],['learn']
Usability,"what’s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(…)` to `categories, obs_tidy, _ = _prepare_dataframe(…)`. Other than that, there’s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so!; 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace “xyz” with whatever you want):. ```py; def test_xyz(image_comparer):; save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); […]; sc.pl.xyz(adata, …); save_and_compare_images('xyz'); ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesn’t exist. You need to copy the pngs from `scanpy/tests/figures`→`scanpy/test/_images` and `git commit` them.; 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144; 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data?. @Khalid-Usman I’m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you won’t regret doing this. You’re learning good coding practices here that will come in handy in the future, I promise!. Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412:1958,learn,learning,1958,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493907412,1,['learn'],['learning']
Usability,wow. I was thinking...maybe the PAGA graph can be used as the simplified graph for color assignment. But will try your solution here!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-763156349:62,simpl,simplified,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763156349,1,['simpl'],['simplified']
Usability,y>Installed scanpy on jupyter notebook/ anaconda: </summary>. ```; pip install scanpy. Requirement already satisfied: scanpy in c:\users\charles\anaconda3\lib\site-packages (1.7.2); Requirement already satisfied: numba>=0.41.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:1034,learn,learn,1034,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['learn'],['learn']
Usability,"yeah, a fix could simply be. ```diff; -return np.nan_to_num(dispersion_norm) >= disp_cut_off; +return np.nan_to_num(dispersion_norm, nan=-np.inf) >= disp_cut_off; ```. but I have a hard time coming up with a test. Doing something like this doesn’t work, as it crashes earlier,; with something like “ValueError: cannot specify integer `bins` when input data contains infinity”. ```py; @pytest.mark.parametrize(""flavor"", [""seurat"", ""cell_ranger""]); def test_no_filter_genes(flavor):; """"""Test that even with 0 columns in the data, n_top_genes is respected.""""""; adata = pbmc3k(); means, _ = _get_mean_var(adata.X); assert (means == 0).any(); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); assert adata.var[""highly_variable""].sum() == 10000; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2252862491:18,simpl,simply,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2252862491,1,['simpl'],['simply']
Usability,"yes, I know, that's non-ideal... the sparseness issue is circumvented by only returning top-scoring genes... I see that you make suggestions for how the user can get dataframes but I tend to say that he shouldn't have to do some extra work for this. i think we should continue to return a table with groups vs. top-scoring genes. this is also what all others (Seurat, Pagoda, ...) do and what, I guess, feels most intuitive. a sparse object is likely to confuse users. if we start changing this, we should also talk to @mbuttner, who has written a function for transforming the recarrays to a single dataframe to write them to a csv or xls file and send it out to collaborators... we should also talk to @tcallies, who worked a lot on `rank_genes_groups`; ; our current workflow often involves showing collaborators tables of marker genes for different cell groups. these can get quite long as, e.g., transcription factors are not much differentially expressed, hence not top-scoring and appear further down the tabular. the tabular therefore has to be easily inspectable. currently, you can quickly turn a single rearray into a dataframe as shown [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). `rank_genes_groups` returns a recarray for historical reasons: there is a simple hdf5-backing via the recarray. these days, since the hdf5-backing of categorical data types within anndata works well, we could think about returning a dataframe directly. i guess this would be the way to go requiring only minor modifactions in that the hdf5-backing also accepts dataframes in `.uns` and not only in `.obs` and `.var`. very generally: I think that it would be a decent convention to only allow strings to denote groups/categories. this was also the convetion before using dataframes for the annotation. now we use the category dtype of pandas, which - in contrast to R - allows arbitrary data types for denoting categories. I don't see much advantage of this flexibi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458:414,intuit,intuitive,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/61#issuecomment-355082458,1,['intuit'],['intuitive']
Usability,yhd8ed1ab_0 conda-forge; python-editor 1.0.4 py_0 conda-forge; python-igraph 0.10.6 py310h33b8572_0 conda-forge; python-installer 0.7.0 pyhd8ed1ab_0 conda-forge; python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.10 3_cp310 conda-forge; pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch; pytorch-cuda 11.8 h7e8668a_5 pytorch; pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge; pytorch-mutex 1.0 cuda pytorch; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyyaml 6.0 py310h5764c6d_5 conda-forge; pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge; rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge; re2 2023.03.02 h8c504da_0 conda-forge; readchar 4.0.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; referencing 0.30.0 pyhd8ed1ab_0 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge; rich 13.4.2 pyhd8ed1ab_0 conda-forge; rpds-py 0.9.2 py310hcb5633a_0 conda-forge; scanpy 1.9.3 pyhd8ed1ab_0 conda-forge; scikit-learn 1.3.0 py310hf7d194e_0 conda-forge; scipy 1.11.1 py310ha4c1d20_0 conda-forge; scvi-tools 1.0.2 pyhd8ed1ab_0 conda-forge; seaborn 0.12.2 hd8ed1ab_0 conda-forge; seaborn-base 0.12.2 pyhd8ed1ab_0 conda-forge; secretstorage 3.3.3 py310hff52083_1 conda-forge; session-info 1.0.0 pyhd8ed1ab_0 conda-forge; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; shellingham 1.5.1 pyhd8ed1ab_0 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; sleef 3.5.1 h9b69904_2 conda-forge; sniffio 1.3.0 pyhd8ed1ab_0 conda-forge; soupsieve 2.3.2.post1 pyhd8ed1ab_0 conda-forge; sparse 0.14.0 pyhd8ed1ab_0 conda-forge; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pyhd8ed1ab_0 conda-forge; starsessions 1.3.0 pyhd8ed1ab_0 conda-forge; statsmodels 0.14.0 py310h278f3c1_1 conda-forge; stdlib-list 0.8.0 pyhd8ed1ab_0 conda-forge; suitesparse 5.10.1 h9e50725_1 conda-forge; sympy 1.12 pypyh9d50eac_103 conda-forge; tbb 2021.9.0 hf52228f_0 conda-forge; texttable 1.6.7 pyhd8ed1ab_0 conda-forge; threadp,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:19387,learn,learn,19387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['learn'],['learn']
Usability,"you wanted to say “implicit” right?. and I disagree, transposing here is exactly the right thing to do: when you want to use your genes as observations in a plot you should transpose your AnnData so that they are the dimension made for observations!. also, there’s no “technical restriction”. it’s about API design; we could also introduce a `switch_var_and_obs = False` parameter, but i feel `.T` is simpler.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441244349:401,simpl,simpler,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441244349,1,['simpl'],['simpler']
Usability,"ypi_0 pypi; setuptools 68.0.0 pyhd8ed1ab_0 conda-forge; singlecellhaystack 0.0.5 pypi_0 pypi; six 1.16.0 pyh6c4a22f_0 conda-forge; sniffio 1.3.0 pypi_0 pypi; soupsieve 2.4.1 pypi_0 pypi; sparse 0.14.0 pypi_0 pypi; squarify 0.4.3 pypi_0 pypi; stack_data 0.6.2 pyhd8ed1ab_0 conda-forge; starlette 0.27.0 pypi_0 pypi; starsessions 1.3.0 pypi_0 pypi; statsmodels 0.14.0 pypi_0 pypi; stdlib-list 0.9.0 pypi_0 pypi; sympy 1.11.1 pypi_0 pypi; tensorstore 0.1.40 pypi_0 pypi; texttable 1.6.7 pypi_0 pypi; threadpoolctl 3.2.0 pypi_0 pypi; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pypi_0 pypi; torch 2.0.1+cu118 pypi_0 pypi; torchaudio 2.0.2+cu118 pypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; triton 2.0.0 pypi_0 pypi; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023.3 pypi_0 pypi; umap-learn 0.5.3 pypi_0 pypi; urllib3 1.26.13 pypi_0 pypi; uvicorn 0.23.1 pypi_0 pypi; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; websocket-client 1.6.1 pypi_0 pypi; websockets 11.0.3 pypi_0 pypi; wheel 0.41.0 pyhd8ed1ab_0 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pypi_0 pypi; xz 5.2.6 h166bdaf_0 conda-forge; yamlordereddictloader 0.4.0 pypi_0 pypi; yarl 1.9.2 pypi_0 pypi; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:7884,learn,learn,7884,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['learn'],['learn']
