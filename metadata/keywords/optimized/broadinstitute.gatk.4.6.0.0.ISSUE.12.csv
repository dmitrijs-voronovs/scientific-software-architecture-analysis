quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 10:20:24.995 INFO gcnvkernel.tasks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:7448,variab,variables,7448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['variab'],['variables']
Modifiability,"211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTabl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2092,config,config,2092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['config'],['config']
Modifiability,"21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3899,Config,ConfigFactory,3899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"27 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5062,Config,ConfigFactory,5062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utiliz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1706,variab,variables,1706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,2Engine.callRegion(Mutect2Engine.java:283); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:300); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -tumor HAP1_1 --germline-resource gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz -L gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF --gcs-project-for-requester-pays broad-firecloud-ccle; ```. #### Steps,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:7177,variab,variable,7177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['variab'],['variable']
Modifiability,"3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in com",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1977,inherit,inheritance,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,1,['inherit'],['inheritance']
Modifiability,341 DEBUG Mutect2 - Processing assembly region at chrM:13637-13936 isActive: true numReads: 110273; 12:09:09.957 DEBUG ReadThreadingGraph - Recovered 24 of 26 dangling tails; 12:09:10.041 DEBUG ReadThreadingGraph - Recovered 6 of 14 dangling heads; 12:09:10.602 DEBUG Mutect2Engine - Active Region chrM:13637-13936; 12:09:10.608 DEBUG Mutect2Engine - Extended Act Region chrM:13537-14036; 12:09:10.613 DEBUG Mutect2Engine - Ref haplotype coords chrM:13537-14036; 12:09:10.617 DEBUG Mutect2Engine - Haplotype count 128; 12:09:10.621 DEBUG Mutect2Engine - Kmer sizes count 0; 12:09:10.625 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:51.290 DEBUG Mutect2 - Processing assembly region at chrM:13937-13944 isActive: true numReads: 54773; 12:13:53.989 DEBUG ReadThreadingGraph - Recovered 29 of 59 dangling tails; 12:13:54.004 DEBUG ReadThreadingGraph - Recovered 0 of 35 dangling heads; 12:13:54.432 DEBUG Mutect2Engine - Active Region chrM:13937-13944; 12:13:54.440 DEBUG Mutect2Engine - Extended Act Region chrM:13837-14044; 12:13:54.447 DEBUG Mutect2Engine - Ref haplotype coords chrM:13837-14044; 12:13:54.452 DEBUG Mutect2Engine - Haplotype count 128; 12:13:54.456 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:54.462 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:55.715 DEBUG Mutect2 - Processing assembly region at chrM:13945-14244 isActive: false numReads: 54745; 12:13:56.962 DEBUG Mutect2 - Processing assembly region at chrM:14245-14544 isActive: false numReads: 0; 12:13:56.973 DEBUG Mutect2 - Processing assembly region at chrM:14545-14844 isActive: false numReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:20978,Extend,Extended,20978,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,"35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4598,Config,ConfigFactory,4598,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5184,Config,ConfigFactory,5184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: Pat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6227,Config,ConfigFactory,6227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,3:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.CREATE_MD5 : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.CUSTOM_READER_FACTORY :; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3335,Config,ConfigFactory,3335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3Rlci5qYXZh) | `89.773% <0.000%> (-0.881%)` | :arrow_down: |; | [...tools/walkers/sv/JointGermlineCNVSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/7863/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L0pvaW50R2VybWxpbmVDTlZTZWdtZW50YXRpb24uamF2YQ==) | `86.047% <0.000%> (-0.752%)` | :arrow_down: |; | [...der/tools/walkers/sv/SVClusterIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7863/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3RlckludGVncmF0aW9uVGVzdC5qYXZh) | `99.496% <0.000%> (-0.004%)` | :arrow_down: |; | [...itute/hellbender/tools/LocalAssemblerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7863/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Mb2NhbEFzc2VtYmxlclVuaXRUZXN0LmphdmE=) | `92.448% <0.000%> (ø)` | |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7863/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <0.000%> (+0.035%)` | :arrow_up: |; | ... and [2 more](https://codecov.io/gh/broadinstitute/gatk/pull/7863/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7863#issuecomment-1133159561:4974,Adapt,AdaptiveChainPruner,4974,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7863#issuecomment-1133159561,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,3Zxc3IvVHJhbmNoZS5qYXZh) | `62.921% <0%> (-7.349%)` | `18% <0%> (ø)` | |; | [.../hellbender/tools/walkers/vqsr/TrancheManager.java](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVHJhbmNoZU1hbmFnZXIuamF2YQ==) | `67.347% <0%> (-2.983%)` | `18% <0%> (+3%)` | |; | [...ols/walkers/contamination/ContaminationRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vQ29udGFtaW5hdGlvblJlY29yZC5qYXZh) | `87.302% <0%> (-2.698%)` | `9% <0%> (+4%)` | |; | [...nder/transformers/SimpleRepeatMaskTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvU2ltcGxlUmVwZWF0TWFza1RyYW5zZm9ybWVyLmphdmE=) | `94.286% <0%> (ø)` | `11% <0%> (?)` | |; | [...ellbender/transformers/AdapterTrimTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQWRhcHRlclRyaW1UcmFuc2Zvcm1lci5qYXZh) | `92.857% <0%> (ø)` | `12% <0%> (?)` | |; | [...ools/spark/pathseq/PSFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `80% <0%> (+1.429%)` | `2% <0%> (ø)` | :arrow_down: |; | [...s/spark/pathseq/PSBuildReferenceTaxonomyUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTQnVpbGRSZWZlcmVuY2VUYXhvbm9teVV0aWxzLmphdmE=) | `90.541% <0%> (+1.579%)` | `80% <0%> (+41%)` | :arrow_up: |; | ... and [7 more](https://codecov.io/gh/broadinstitute/gatk/pull/3388?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3388#issuecomment-319175054:3054,Adapt,AdapterTrimTransformer,3054,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3388#issuecomment-319175054,1,['Adapt'],['AdapterTrimTransformer']
Modifiability,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2949:2181,parameteriz,parameterize,2181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949,3,"['config', 'parameteriz']","['config', 'parameterize', 'parameterized']"
Modifiability,"4); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2446,refactor,refactored,2446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactored']
Modifiability,"4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 858, in make_c_thunk; output_storage=node_output_storage); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1217, in make_thunk; keep_lock=keep_lock); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1157, in __compile__; keep_lock=keep_lock); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1623, in cthunk_factory; module = get_module_cache().module_from_key(; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 48, in get_module_cache; return cmodule.get_module_cache(config.compiledir, init_args=init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1587, in get_module_cache; _module_cache = ModuleCache(dirname, **init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 703, in __init__; self.refresh(); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 794, in refresh; files = os.listdir(root); FileNotFoundError: [Errno 2] No such file or directory: '/spin1/home/linux/gatk_users1/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.5.1804-Core-x86_64-3.6.2-64/tmp3mkfuhpw'; 	at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.exe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:11989,config,config,11989,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['config'],['config']
Modifiability,"4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 858, in make_c_thunk; output_storage=node_output_storage); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1217, in make_thunk; keep_lock=keep_lock); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1157, in __compile__; keep_lock=keep_lock); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1623, in cthunk_factory; module = get_module_cache().module_from_key(; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 48, in get_module_cache; return cmodule.get_module_cache(config.compiledir, init_args=init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1587, in get_module_cache; _module_cache = ModuleCache(dirname, **init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 703, in __init__; self.refresh(); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 794, in refresh; files = os.listdir(root); FileNotFoundError: [Errno 2] No such file or directory: '/spin1/home/linux/gatk_users1/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.5.1804-Core-x86_64-3.6.2-64/tmpmy0w17z3'; 00:34:39.396 DEBUG ScriptExecutor - Result: 1; 00:34:39.397 INFO DetermineGermlineContigPloidy - Shutting down engine; [October 27, 2019 12:34:39 AM EDT] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.66 minutes.; Runtime.totalMemory()=21516",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:3922,config,config,3922,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['config'],['config']
Modifiability,"4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 858, in make_c_thunk; output_storage=node_output_storage); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1217, in make_thunk; keep_lock=keep_lock); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1157, in __compile__; keep_lock=keep_lock); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 1623, in cthunk_factory; module = get_module_cache().module_from_key(; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cc.py"", line 48, in get_module_cache; return cmodule.get_module_cache(config.compiledir, init_args=init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1587, in get_module_cache; _module_cache = ModuleCache(dirname, **init_args); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 703, in __init__; self.refresh(); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 794, in refresh; files = os.listdir(root); FileNotFoundError: [Errno 2] No such file or directory: '/spin1/home/linux/sangj2/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.5.1804-Core-x86_64-3.6.2-64/tmpyvtzjxzm'; 00:50:23.369 DEBUG ScriptExecutor - Result: 1; 00:50:23.370 INFO GermlineCNVCaller - Shutting down engine; [October 29, 2019 12:50:23 AM EDT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=2335703040; org.broadinstitute.hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-547440019:6545,config,config,6545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-547440019,1,['config'],['config']
Modifiability,"43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4122,Config,ConfigFactory,4122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:19005,refactor,refactor,19005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['refactor'],['refactor']
Modifiability,49s][m[39D[1B[1m> :testOnPackagedReleaseJar > 1727 tests completed[m[50D[1B[1m> :testOnPackagedReleaseJar > Executing test org...help.DocumentationGeneration[m[79D[1B[3A src/main/java/org/broadinstitute/hellbender/utils/variant/writers/SomaticGVCFWriter.java:4: error: package com.google.common.collect does not exist[0K; 2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; 2022-08-16T00:09:07.4436380Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4436641Z location: class CommandLineProgram; 2022-08-16T00:09:07.4436930Z src/main/java/org/broadinstitute/hellbender/engine/FeatureInput.java:120: error: cannot find symbol; 2022-08-16T00:09:07.4437094Z @VisibleForTesting; 2022-08-16T00:09:07.4437369Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4437519Z location: class FeatureInput<T>; 2022-08-16T00:09:07.4437725Z where T is a type-variable:; 2022-08-16T00:09:07.4437925Z T extends Feature declared in class FeatureInput; 2022-08-16T00:09:07.4438276Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:251: error: cannot find symbol; 2022-08-16T00:09:07.4438417Z @VisibleForTesting; 2022-08-16T00:09:07.4438677Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4438873Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4439223Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:271: error: cannot find symbol; 2022-08-16T00:09:07.4439362Z @VisibleForTesting; 2022-08-16T00:09:07.4439618Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4439806Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4465668Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/DefaultGATKVariantAnnotationArgumentCollection.java:3: error: package com.google.common.collect does not exist; 2022-08-,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:20843,variab,variable,20843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['variab'],['variable']
Modifiability,5); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionRe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4083,config,configuration,4083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5823,Config,ConfigFactory,5823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: Pat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6866,Config,ConfigFactory,6866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6944,Config,ConfigFactory,6944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isAct,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:14377,Extend,Extended,14377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,"64e; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 08:04:19 2017 -0500. mkl. commit 43e2a65201286161fcd5bfe7dbb21ae888e19dac; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 06:56:20 2017 -0500. added cpu argument for germline tasks. commit 4433a62c2173c7f29d0f264c084bbaf2f6738782; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 02:45:38 2017 -0500. revert travis yml forks; verbose logging germline wdl. commit ae05801e33c37b3bf2685fba202032a270804873; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:55:14 2017 -0500. updated somatic PoNs for PreprocessIntervals drop Ns. commit cff64984d9fb42364001bda4c73d54cf68d85a5c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:37:24 2017 -0500. sudo travis yml. commit 89025941febd2089d426cfa1e0f0aa6a6712e2a9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Fri Dec 8 00:23:22 2017 -0500. travis/Docker config update (g++-6, Miniconda3); python test group assignment. commit 31f96398106c2b8577b8c25d110abea3ebe7f836; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:44:53 2017 -0500. WDL test bugfix. commit 9b2fb820536ec355bea0256471bd093d547f5c99; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:20:36 2017 -0500. update WDL test JSON files. commit e3d97644d1a2c40a5c364a96f8b67246154179c9; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:18:14 2017 -0500. assertions in inference task base; removed a ASCII > 128 character in log messages. commit 526cf92e623a3bbd5f9d375132b6ca046fc47620; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 20:03:04 2017 -0500. redirect tqdm progress bar to python logger. commit 2e45bd30968b921fae225de3901fb97ece690b0c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 19:45:49 2017 -0500. more arg related fixes. commit bb89a3bb338d88199881e8aca65f656f2acd7c0a; Author: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:4206,config,config,4206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['config'],['config']
Modifiability,6f6105708ded7f86c96c830781; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:35:33 2017 -0500. annotated intervals kebab case; updated germline WDL workflows. commit 29cc6234dbfb8db12559217a650c6ceb170c5797; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 13:15:28 2017 -0500. cleanup test files. commit 08a35bb4e65eceb735adcd41a91132e9a34d2b66; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:50:19 2017 -0500. update WDL scripts. commit 12bcfa192ee6fa6da21239ebf5b513633efe974f; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 02:47:33 2017 -0500. significant updates to GermlineCNVCaller; integration tests for GermlineCNVCaller w/ sim data in both run modes. commit 151416a4af735ca721bd75e4b54a780c17ac9397; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Thu Dec 7 01:42:05 2017 -0500. hybrid ADVI abstract argument collection w/ flexible default values; hybrid ADVI argument collection for contig ploidy model; hybrid ADVI argument collection for germline denoising and calling model. commit 56e21bf955d3dc0c52aceb384f28cf6173959de0; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 23:18:39 2017 -0500. rewritten python-side coverage metadata table reader using pandas to fix the issues with comment line; change criterion for cohort/case based on whether a contig-ploidy model is provided or not; simulated test files for ploidy determination tool; proper integration test for ploidy determination tool and all edge cases; updated docs for ploidy determination tool. commit 7fa104b2e9170770cfc5b338835e41215d7fd39c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Dec 6 18:43:17 2017 -0500. kabab case for gCNV-related tools; removed short args (this also partially affected PlotDenoisedCopyRatios and PlotModeledSegments and their integration tests). commit f02cb024331a986213cfd9fae2da706bbc5ddbd9; Author: Mehrtash Babadi <mehrtash@broadins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:7178,flexible,flexible,7178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['flexible'],['flexible']
Modifiability,"7:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5750,Config,ConfigFactory,5750,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"7:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6389,Config,ConfigFactory,6389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"8:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4990,Config,ConfigFactory,4990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,91 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_pac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3260,Config,ConfigFactory,3260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,93YWxrZXJzL3Zxc3IvVHJhbmNoZU1hbmFnZXIuamF2YQ==) | `67.347% <0%> (-2.983%)` | `18% <0%> (+3%)` | |; | [...hellbender/utils/haplotype/HaplotypeBAMWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oYXBsb3R5cGUvSGFwbG90eXBlQkFNV3JpdGVyLmphdmE=) | `97.531% <0%> (-0.546%)` | `15% <0%> (+5%)` | |; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `68.595% <0%> (-0.092%)` | `29% <0%> (+5%)` | |; | [...r/transformers/BaseQualityClipReadTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQmFzZVF1YWxpdHlDbGlwUmVhZFRyYW5zZm9ybWVyLmphdmE=) | `100% <0%> (ø)` | `19% <0%> (+5%)` | :arrow_up: |; | [...hellbender/utils/haplotype/SAMFileDestination.java](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oYXBsb3R5cGUvU0FNRmlsZURlc3RpbmF0aW9uLmphdmE=) | `100% <0%> (ø)` | `6% <0%> (+3%)` | :arrow_up: |; | [...nder/transformers/SimpleRepeatMaskTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvU2ltcGxlUmVwZWF0TWFza1RyYW5zZm9ybWVyLmphdmE=) | `94.286% <0%> (ø)` | `11% <0%> (?)` | |; | [...ellbender/transformers/AdapterTrimTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQWRhcHRlclRyaW1UcmFuc2Zvcm1lci5qYXZh) | `92.857% <0%> (ø)` | `12% <0%> (?)` | |; | ... and [14 more](https://codecov.io/gh/broadinstitute/gatk/pull/3398?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3398#issuecomment-319512377:3668,Adapt,AdapterTrimTransformer,3668,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3398#issuecomment-319512377,1,['Adapt'],['AdapterTrimTransformer']
Modifiability,"95). @akiezun I defer to @cmnbroad on whether the issues with `crai` are bad enough to warrant such an approach. . You should be able to make a `bai` on the cram by running GATK `PrintReads` on it. ---. @cmnbroad commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hook",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2850:2910,variab,variable,2910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850,1,['variab'],['variable']
Modifiability,: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3301,plugin,plugins,3301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['plugin'],['plugins']
Modifiability,: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:1323,Config,ConfigCache,1323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,1,['Config'],['ConfigCache']
Modifiability,:+1: Thanks for fixing this. Just the comment on the comment. Feel free to merge when that is updated. I do think this test class is still very confusing in general though. . I don't understand why it's testing on a random combinatorial set of inputs instead of choosing a few explicitly. Surely showing that a method throws on null input once is just as good as showing it 10 time. Refactoring the whole thing seems out of scope though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/743#issuecomment-124566502:383,Refactor,Refactoring,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/743#issuecomment-124566502,1,['Refactor'],['Refactoring']
Modifiability,":+1: looks good, merging -- brown drink waived because environment variables are just not fair :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1804#issuecomment-216880872:67,variab,variables,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1804#issuecomment-216880872,1,['variab'],['variables']
Modifiability,":19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5941,Config,ConfigFactory,5941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,":22 UTC 2019] picard.analysis.CollectWgsMetrics done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=6996099072; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; at htsjdk.samtools.util.AbstractRecordAndOffset.validateOffset(AbstractRecordAndOffset.java:109); at htsjdk.samtools.util.EdgingRecordAndOffset$StartEdgingRecordAndOffset.getBaseQuality(EdgingRecordAndOffset.java:112); at picard.analysis.FastWgsMetricsCollector.excludeByQuality(FastWgsMetricsCollector.java:189); at picard.analysis.FastWgsMetricsCollector.processRecord(FastWgsMetricsCollector.java:144); at picard.analysis.FastWgsMetricsCollector.addInfo(FastWgsMetricsCollector.java:105); at picard.analysis.WgsMetricsProcessorImpl.processFile(WgsMetricsProcessorImpl.java:93); at picard.analysis.CollectWgsMetrics.doWork(CollectWgsMetrics.java:231); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /opt/conda/envs/base-v1.4.1/share/gatk4-4.1.3.0-0/gatk-package-4.1.3.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=8 -Xms6G -Xmx10G -Djava.io.tmpdir=. -jar /opt/conda/envs/base-v1.4.1/share/gatk4-4.1.3.0-0/gatk-package-4.1.3.0-local.jar CollectWgsMetrics --INPUT=example.bam --OUTPUT=example.seq_metrics.txt --REFERENCE_SEQUENCE=ucsc.hg19.fasta --USE_FAST_ALGORITHM=true --LOCUS_ACCUMULATION_CAP 25000 --COVERAGE_CAP=100; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163:3636,variab,variable,3636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163,1,['variab'],['variable']
Modifiability,":37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5215,Config,ConfigFactory,5215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,":37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflate",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4452,Config,ConfigFactory,4452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,":55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6580,Config,ConfigFactory,6580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,":e98d186c-96db-46ae-92e5-c326e7aa05d9;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,19] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] Wo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:1353,config,configured,1353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configured']
Modifiability,"; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.exe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4132,Config,ConfigFactory,4132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3855,Config,ConfigFactory,3855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,; 12:11:28.277 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.426 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.771 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.877 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 12:11:28.882 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.883 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 12:11:28.905 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.906 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2021-03-24 12:11:28 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:28.910 INFO DataSourceUtils - Resolved data source file path: file:///gatk/cosmic_tissue.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg19/cosmic_tissue.tsv; 12:11:28.930 INFO DataSourceUtils - Resolved data source file path: file:///gatk/cosmic_fusion.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/cosmic_fusion/hg19/cosmic_fusion.tsv; 12:11:28.932 INFO DataSourceUtils - R,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:11916,config,config,11916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['config'],['config']
Modifiability,"; 16:58:10.116 INFO PrintVariantsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:58:10.116 INFO PrintVariantsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:58:10.116 INFO PrintVariantsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:58:10.116 INFO PrintVariantsSpark - Deflater: IntelDeflater; 16:58:10.116 INFO PrintVariantsSpark - Inflater: IntelInflater; 16:58:10.116 INFO PrintVariantsSpark - GCS max retries/reopens: 20; 16:58:10.116 INFO PrintVariantsSpark - Requester pays: disabled; 16:58:10.116 WARN PrintVariantsSpark - . ?[1m?[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintVariantsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!?[0m. 16:58:10.116 INFO PrintVariantsSpark - Initializing engine; 16:58:10.116 INFO PrintVariantsSpark - Done initializing engine; 19/02/18 16:58:10 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/02/18 16:58:10 INFO org.spark_project.jetty.util.log: Logging initialized @8431ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: Started @8536ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 19/02/18 16:58:11 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 19/02/18 16:58:12 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:3977,config,configuration,3977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1444,Config,ConfigurationResolver,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Config'],['ConfigurationResolver']
Modifiability,; at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3572,Plugin,PluginManager,3572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginManager']
Modifiability,"; at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:277); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java:774); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1037); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Duplicate key 0, for input source: cadd.config; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:263); at htsjdk.tribble.TribbleIndexedFeatureReader.&lt;init&gt;(TribbleIndexedFeatureReader.java:102); at htsjdk.tribble.TribbleIndexedFeatureReader.&lt;init&gt;(TribbleIndexedFeatureReader.java:127); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:120); at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:350); ... 14 more; Caused by: java.lang.IllegalStateException: Duplicate key 0; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1254); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250); at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110); at java.util.Split",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:2067,config,config,2067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"= ""Split intervals into sub-interval files."",; > + oneLineSummary = ""Split intervals into sub-interval files."",; > + programGroup = VariantProgramGroup.class; > +); > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; >; > @samuelklee <https://github.com/samuelklee> is the boss of the copy; > number code, but personally I don't see the need to be extremely concise; > with short names and would prefer width.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646054>:; >; > > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; > + public static final String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; >; > binWidth would be a more readable variable name. There's nothing wrong; > with the command line argument and the variable being identical.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646097>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = tr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:1526,extend,extends,1526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,2,"['extend', 'variab']","['extends', 'variable']"
Modifiability,"=Code) impl. [IOUtils](https://github.com/broadinstitute/gatk/blob/94ac626218e073b77156a3eff076003d26be318c/src/main/java/org/broadinstitute/hellbender/utils/io/IOUtils.java#L535). Today, org.apache.hadoop.fs.{FileSystem, Path} is much broadly used in the Big Data world, and most of vendors of distribution storage provider already provide impl of org.apache.hadoop.fs.{FileSystem, Path} include AWS, Google and Alibaba. There are huge customers of Hadoop already work on hadoop.fs for years, if GAKT on spark could rely on org.apache.hadoop.fs.{FileSystem, Path} , I guess GAKT could acquire more existing customers of Hadoop on Cloud much faster . . Maybe we could consider migrating java.nio.file.FileSystem impl to org.apache.hadoop.fs.{FileSystem, Path} impl in [SparkContextFacto]r(https://github.com/broadinstitute/gatk/blob/73f2a62bee52518b57a985717770ed3a64d83243/src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java), otherwise we could support both nio and hadoop thru env variable, Let me know your thought!. ```; scala> stringRdd.saveAsTextFile(""oss://eric-new/testwrite10""). scala> val stringRdd = sc.parallelize(Seq(""Test String"")); stringRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:24. scala> stringRdd.saveAsTextFile(""oss://eric-new/testwrite11""); ```. ``` oss nio is missing; 18/01/03 17:13:13 INFO NewHadoopRDD: Input split: oss://eric-new/resources/NA12878.chr17_69k_70k.dictFix.bam:1632-3770875903; 18/01/03 17:13:13 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0); java.nio.file.ProviderNotFoundException: Provider ""oss"" not found; 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.crea",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-354989381:1659,variab,variable,1659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-354989381,1,['variab'],['variable']
Modifiability,"> > @mcovarr @koncheto-broad Do all these files exist as they are here in the ah_var_store branch currently?; > ; > Yes, delta the refactoring required to separate the VCF writing into a subclass. Would it be easier/preferable for you all to move those changes into the var store branch and I'll work on the PGEN stuff there instead of in master?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634754805:131,refactor,refactoring,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634754805,1,['refactor'],['refactoring']
Modifiability,> @davidbenjamin Can I assume that you didn't change NearbyKmerErrorCorrector (other than renaming and refactoring). Yes you can.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6470#issuecomment-592715339:103,refactor,refactoring,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6470#issuecomment-592715339,1,['refactor'],['refactoring']
Modifiability,"> @mcovarr @koncheto-broad Do all these files exist as they are here in the ah_var_store branch currently?. ~Yes, delta the refactoring required to separate the VCF writing into a subclass.~; Yes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634742056:124,refactor,refactoring,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634742056,1,['refactor'],['refactoring']
Modifiability,"> A fix for another often-reported issue where Mutect2 could emit MNPs despite --max-mnp-distance being 0, causing downstream errors in GenomicsDB about MNPs not being supported. Is this fix is only dedicated to the Mutect2 module and not extended for the HaplotypeCaller/HaplotypeCallerSpark/CombineGVCFs functions?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-671790528:239,extend,extended,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-671790528,1,['extend'],['extended']
Modifiability,"> First, if a datasource config has a space in it, throw an error and tell the user to remove the space. Yes, but trim trailing and leading spaces first. Do this when initializing the datasource. So if a config file specifies a leading or trailing whitespace, it is discarded. > Second - remove spaces from funcotation field names when outputting the funcotations (could still happen with XSVs with spaces in column headers). Actually, since we know the output field names before we do any work, I would recommend doing something similar as above (i.e. Remove trailing or leading spaces and throw error if any other spaces are found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5937#issuecomment-507304973:25,config,config,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937#issuecomment-507304973,2,['config'],['config']
Modifiability,"> For some reason, the environment variable is not getting passed to GenomicsDB at all. To help debug the issue, can you do the following and see if any consolidation lock is created at all?; > ; > ```; > find /path/to/genomicsdb_workspace -name "".__consolidation_lock""; > ```; > ; > Also, what type of Posix filesystem is your GenomicsDB workspace? Is it NFS or Lustre? How is file locking configured on the system?. Thank you nalinigans.I want to confirm that 'genomicsdb_workspace’ is the result of GenomicsDBImport like; > -[ 201] callset.json; -[4.0K] chr22$1$50818468; |-[4.0K] __3d7b0c29-9d8b-4748-a08a-c1e0a9136cf647572466251520_1590412882146; ||-[133K] AD.tdb; ||-[ 54K] AD_var.tdb; ||-[3.2M] ALT.tdb; ||-[ 77K] ALT_var.tdb; ||-[ 85K] BaseQRankSum.tdb; ||-[192K] __book_keeping.tdb.gz; ||-[4.1M] __coords.tdb; ||-[1011K] DP_FORMAT.tdb; ||-[114K] DP.tdb; ||-[3.5M] END.tdb; ||-[108K] ExcessHet.tdb; ||-[ 64K] FILTER.tdb; ||-[ 0] FILTER_var.tdb; ||-[1.1M] GQ.tdb; ||-[2.9M] GT.tdb; ||-[120K] GT_var.tdb; ||-[ 64K] ID.tdb; ||-[ 0] ID_var.tdb; ||-[ 64K] InbreedingCoeff.tdb; ||-[1.0M] MIN_DP.tdb; ||-[128K] MLEAC.tdb; ||-[ 32K] MLEAC_var.tdb; ||-[128K] MLEAF.tdb; ||-[ 36K] MLEAF_var.tdb; ||-[ 82K] MQRankSum.tdb; ||-[ 98K] PGT.tdb; ||-[5.8K] PGT_var.tdb; ||-[ 99K] PID.tdb; ||-[ 15K] PID_var.tdb; ||-[2.9M] PL.tdb; ||-[3.5M] PL_var.tdb; ||-[ 77K] PS.tdb; ||-[143K] QUAL.tdb; ||-[163K] RAW_MQandDP.tdb; ||-[ 84K] ReadPosRankSum.tdb; ||-[3.2M] REF.tdb; ||-[655K] REF_var.tdb; ||-[174K] SB.tdb; ||-[ 0] __tiledb_fragment.tdb; |-[ 535] __array_schema.tdb; |-[4.0K] genomicsdb_meta_dir; | -[ 60] genomicsdb_meta_a793298a-95f2-475e-b10b-726695483e3a.json; -[ 0] __tiledb_workspace.tdb; -[ 33K] vcfheader.vcf; -[ 39K] vidmap.json. If yes, I regret that I did not find "".__consolidation_lock"" in my resultfile.; And I'm sorry I don't know what type my GenomicsDB workspace is, but I guess it is lustre，Hope this information can help you：; > Filesystem Type Size Used Avail Use% Mounted on; /dev/sda3 ext",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627#issuecomment-637578697:35,variab,variable,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627#issuecomment-637578697,2,"['config', 'variab']","['configured', 'variable']"
Modifiability,"> Hi @icemduru Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use.; > ; > Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs.; > ; > One final reminder. We strongly recommend users to set th; [slurm-22680938.out_text.txt](https://github.com/user-attachments/files/16608314/slurm-22680938.out_text.txt); e temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files.; > ; > I hope this helps. Thank you for your help, but unfortunately it didn't resolve the issue. I've already tried allocating 10GB of memory using the -Xmx10g flag and redirecting the temporary directory away from /tmp. However, GATK is still attempting to consume more than 48GB of RAM, resulting in the termination of my run.; [slurm-22680938.out_text.txt](https://github.com/user-attachments/files/16608325/slurm-22680938.out_text.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2287941632:58,config,configured,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2287941632,1,['config'],['configured']
Modifiability,"> How can F be a probability when it takes on negative values?. It's the probability of alleles being IBD *provided that inbreeding is the only source of deviation from HWE* and in the limit of infinite sample size washing out statistical noise. Under these assumptions it's always positive. How about I rewrite the docs to be much, much clearer about this?. > Also, I've never heard of it being called the Fixation Score. I hadn't heard of it, either, but Wikipedia told me so: https://en.wikipedia.org/wiki/F-statistics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5768#issuecomment-470249511:304,rewrite,rewrite,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5768#issuecomment-470249511,1,['rewrite'],['rewrite']
Modifiability,"> If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right?. GenomicsDB does store the END as a separate attribute for the interval, so the information is present even if the GenomicsDB array region does not span that far. The other questions I will leave it to @droazen and/or @mlathara to answer. Hopefully, you are able to make progress.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1231929214:67,extend,extends,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1231929214,1,['extend'],['extends']
Modifiability,"> OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by; > ; > `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`; > ; > This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:; > ; > ```; > [global]; > base_compiledir = PATH/TO/COMPILEDIR_#; > ```; > ; > Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?; > ; > This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort.; > ; > However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:286,variab,variable,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"> Overall the refactoring looks good and makes sense… but I'm not seeing how this fixes the problem of eating exceptions we saw during a recent run. Can you explain what was happening before, and how the new code addresses it?. Sure! This code (besides refactoring so that it was only in one place) aims to fix two issues:; 1. if query results in an error, it gets run three more times and then, because of `while len(retry_delay) > 0`, it doesn’t run again and the `raise err` line never gets executed, so no error is ever raised; 2. if the query fails for a reason that has no chance of being fixed by a retry (eg. 404), it will still run three more times. I probably missed some errors that should be ""retry-able"" (maybe `Aborted `? `BadGateway`? `Cancelled `? [full list here](https://googleapis.dev/python/google-api-core/latest/exceptions.html)), but I still think it makes sense to not treat all errors the same.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-930239576:14,refactor,refactoring,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-930239576,2,['refactor'],['refactoring']
Modifiability,"> Still interested in the reference sample increase results. I wrote a unit test for that. QUAL decreases slowly as ref samples increase (e.g. one million ref samples decrease QUAL by about log_10 10^6 = 6), which I believe is the correct behavior. Overwhelming evidence that a variant is rare _ought_ to make any genotyper more skeptical about it. Note that this will only lose variants with very weak evidence. As for the command line unification that @vruano suggested, I put in a few hours and it's trickier than expected. If we use `pnrm` then we have to add an instance of `AFCalculatorImplementation`, which would need a `Supplier<AlleleFrequencyCalculator>`. But unlike the old exact model `AFCalculator`s the new one knows the heterozygosity priors (needed for Dirichlet prior pseudocounts) and needs to get those in its constructor. But this would mean that `AFCalculatorImplementation` would have to know about the heterozygosity command line arguments which would would in turn demand that it be refactored as something other than an `Enum`. As inelegant as the current hack of having a separate argument that overrides `pnrm` is, I think it's actually cleaner than the alternative.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242264411:1008,refactor,refactored,1008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242264411,1,['refactor'],['refactored']
Modifiability,"> The constructors (some of them anyway) use the args during constructor execution. That's a theoretical problem but in this case they dont. Anyway, I restored my earlier code to special-case CommandLineException and throw that. . You may have seen this, but there was a failure in AlleleFrequencyQCTest, which I addressed here: https://github.com/broadinstitute/gatk/pull/6973/commits/8a9f5db6eccbc82600f8ec46e7656bcf92bedf5b. The general change is to stop stashing two variables locally within AlleleFrequency, which allows AlleleFrequencyQC to set their values later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827880315:471,variab,variables,471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827880315,1,['variab'],['variables']
Modifiability,"> The new changes to the base image Dockerfile look good to me, @kevinpalis ! Can you tell us how many layers we have total after these changes? Is there any value in pursuing a full squash, or do you think that with this patch most users' issues will be resolved?. @droazen , the total layers is now down to **16** (from 44). I honestly don't see the value of doing a full squash, mainly because if we are hosting this in a premium ACR, the limit is 10,000 readOps per minute. So with 16 layers, you get around 625 pulls per minute. Also, this will be able to still take advantage of parallel pulls (default is 3, but at most 16 threads in this case, I believe) as opposed to one big layer which will not download in parallel. There's the potential of that being a lot slower and subsequent jobs falling into the same ""minute"" because others are not done, making it easier to hit that 10k readOps limit. Lastly, people using GATK outside data pipelines will not be able to take advantage of layer caching too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2102891281:103,layers,layers,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2102891281,3,['layers'],['layers']
Modifiability,"> While we're at it, can we rename all of the ""is_xxx"" variables?. @takutosato Agreed. I renamed them all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4238#issuecomment-360180392:55,variab,variables,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4238#issuecomment-360180392,1,['variab'],['variables']
Modifiability,"> Yes, GenotypeGVCFs prunes alleles that lack sufficient evidence. The force output intervals only guarantees that the site is emitted, but it may be emitted as monomorphic. OK, but I think my test shows that's not happening. In the --force-output-intervals case of testForceOutputNonRef(), all VCs in actualVC2 are not polymorphic. Despite this, some of them have alternate alleles? see the new test case I added at GenotypeGVCFsIntegrationTest line 614",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-577501077:320,polymorphi,polymorphic,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-577501077,1,['polymorphi'],['polymorphic']
Modifiability,"> Yes, setting the GATK_LOCAL_JAR and/or GATK_SPARK_JAR environment variables will cause the gatk script to use that jar, instead of looking in its directory for a jar. The naming of the jar itself also doesn't matter if you use the environment variable method. @droazen I think this leaves one more question: what about running locally? Since cromwell doesn't take in the docker image in that case (right?) wouldn't we need to pass in a pass to the gatk script?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3968#issuecomment-353610830:68,variab,variables,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3968#issuecomment-353610830,2,['variab'],"['variable', 'variables']"
Modifiability,"> number code, but personally I don't see the need to be extremely concise; > with short names and would prefer width.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646054>:; >; > > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; > + public static final String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; >; > binWidth would be a more readable variable name. There's nothing wrong; > with the command line argument and the variable being identical.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646097>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > This tool extends GATKTool, which means that it inherits an; > IntervalArgumentCollection that already includes a padding argument. A; > new one is not needed. BTW @samuelklee <https://github.com/samuelklee>; > does this come up elsewhere in the CNV code? It could be a holdover from; > the days of porting ReCapSeg when",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:2146,variab,variable,2146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['variab'],['variable']
Modifiability,"> user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:1970,flexible,flexible,1970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,2,"['flexible', 'parameteriz']","['flexible', 'parameterized']"
Modifiability,">--af_of_alleles_not_in_resource: is this allele frequency used only in certain contexts, e.g. with matched normal analyses, or towards tumor sample variant alleles, etc.? I need to add to the doc details how this argument factors into calculations. This is the population AF assigned to a variant not found in the germline resource and is inversely proportional to the number of samples used to create that resource. For example, gnomAD contains about 16,000 wgs samples, or 32,000 homologs per locus. Thus the AF of an allele not found in gnomAD is probably 1/32,000 or less. This is useful because it lets us use absence from the germline resource as evidence that an allele is *not* a germline variant. >I need a sentence or two describing the new algorithmic improvement on the new Mutect2 integration over uncertainty. Since allele depths (ADs) are subject to statistical error -- i.e. the exact number of reads for an allele is a random variable -- alleles' allele fractions are not known. Previously, M2 estimated allele fractions ffrom the ADs and proceeded with the likelihoods calculation. Now M2 uses nifty math to account for the uncertainty in the allele fraction when calculating likelihoods. In statistical language (which *will* be familiar to a decent-sized minority of users) we marginalize over allele fractions instead of using a maximum likelihood estimate. >The WDLs do not include use of a contamination.table and so I did not include it in the commands. Is this something we want to nudge users to use, i.e. should I put in a sentence in the documentation section about the new tool CalculateContamination?. `CalculateContamination` is invoked inside the `Filter` task in mutect2.wdl. We want users to use the new tool. You might mention that in the interest of speed you can pass it `-L 1` to use only variants on chromosome 1, which gives a very good estimate in a couple of minutes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2816#issuecomment-306107618:944,variab,variable,944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2816#issuecomment-306107618,1,['variab'],['variable']
Modifiability,"@AJDCiarla . Ah, I see. Ok, so the reason was the following: I didn't use any 3rd party blockers or add-ons, but the Firefox has built-in ""enhanced tracking protection"" which I had to disable for it to work. I guess it's because of cross-site tracking required for this zendesk provider. ; Sorry, this was the first time it actually broke a website in 1-2 years of usage, so I didn't expect to be a culprit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1337288833:139,enhance,enhanced,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1337288833,1,['enhance'],['enhanced']
Modifiability,"@Aqoolare Hello. There are a a few things going on here. The `unrecognized runtime attribute keys` warning is coming from cromwell. It's telling you that the cromwell **local** backend doesn't understand those keys, which is true. That means it's just ignoring them. I think the actual problem is different though. You're running the spark tool in spark local mode, which in this case isn't configured to use the correct amount of cores or memory. I think the intent of this wdl script was that it would be run in a container on a cluster and the container would restrict the cores and memory options. In any case, it's not configured correctly for what you need. I would skip running cromwell and just invoke gatk directly since this wdl only executes a single job. Since this is going to run spark in local mode you need to specify the number of cores using the `--spark-master` argument, and set the memory using the `--java-options ""-Xmx""` arguments. For example:. ```; gatk ReadsPipelineSpark ; --java-options ""-Xmx16G"" ; --spark-master 'local[8]'; --I yourbam.; ... etc; ```. The above command is specifying to use 8 (that's what the local[**8**] means) cores for spark and give it 16G of memory. Your job was accidentally using 200 cores so it doesn't surprise me that it would run into memory issues. Using spark with more than 16ish cores in a single process is going to bog down a lot. I think 8 is a good starting place to try. If you want to go wider you should really look into running a proper cluster (or using dataproc), but there's pretty heavy diminishing returns. Try 8 or 16 and tune the memory from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771:391,config,configured,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771,2,['config'],['configured']
Modifiability,"@DanishIntizar Hello! Thank you for this pr. This is great to see an official plugin from amazon available. I appreciate that you took the time to make it an optional include. I think if we're going to include it we might as well just add it as one of our normal dependencies though. Assuming there aren't any dependency conflicts it **should** (always a risky statement) be independent from everything else. . Thanks also for identifying the different issues you mentioned. It's expected that it won't work with most picard tools as you discovered, but we're actively in the process of updating more of them too support Paths instead of Files so that will slowly improve. The second issue is more worrisome. We regularly use an equivalent provider with google to read reference files through the exact same code, so I suspect there is either some sort of mismatched assumptions in the way they are handling things. Maybe something strange with the Path.resolve methods or the like. (Or in in the much worse potential case a bug in their look ahead caching.). I'd like to look into that before we'd merge this. Ideally we would have tests for this. Are there any public AWS paths we could read from without any secret authentication?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721:78,plugin,plugin,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721,1,['plugin'],['plugin']
Modifiability,"@DarioS ; In general we don't see this kind of behavior. Can you provide the names of these threads and the command line you are using the run M2?. One possibility is that you are running in a JVM configured with a small amount of RAM. . Consider running the JVM with -Xms2G -Xmx5G, this should be sufficient for most M2 runs. I typically run with lower amounts of RAM for a single core, but there may be something about your data that is requiring a larger amount than what we normally see. Is there anything unusual about your data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-803595912:197,config,configured,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-803595912,1,['config'],['configured']
Modifiability,@DarioS Thanks for reporting this. It seems like there is an object in the tool which accumulates a record for every variant in the -V input. Since you're using gnomad it's very large. This is a design issue and seems like it should be fixed to stream the output instead of accumulating it all in memory and then dumping it at the end. @davidbenjamin I aimed this at you since it seems like it's your tool. From a lazy glance it seems like it should be doable to rewrite it to stream output instead of keeping it all in memory.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7606#issuecomment-1004309450:463,rewrite,rewrite,463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7606#issuecomment-1004309450,1,['rewrite'],['rewrite']
Modifiability,"@KPS-Malpe To start it would be useful to know what commandline you used, what version of GATK you're using, basic configuration info like that. If you could include the output of the commands you ran that could also be pertinent. It is interesting that not all the intervals you specified in your bed file show up in the Genomicsdb workspace. Along the lines of the question asked by @droazen, I presume you checked that the missing intervals do actually have data in the gvcfs?. Lastly, you could try running SelectVariants on the Genomicsdb workspace -- that should return the data that was ingested into the genomicsdb, i.e., matching the input gvcfs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196207156:115,config,configuration,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196207156,1,['config'],['configuration']
Modifiability,"@LeeTL1220 . This seems to be running into a cromwell / WDL error:. ```; java.lang.IllegalArgumentException: Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); ```. Isn't cromwell supposed to handle `gs://` URLs for localizing files? Do you have any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481836556:762,config,configure,762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481836556,1,['config'],['configure']
Modifiability,@LeeTL1220 Can you review? This fixes your bug. I'm not putting in a regression test yet because i the upcoming filtering refactoring it will become much easier to unit test for bugs like this. @madduran Yours too.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5595:122,refactor,refactoring,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5595,1,['refactor'],['refactoring']
Modifiability,"@LeeTL1220 It sounds like you're suggesting two things. . First, if a datasource config has a space in it, throw an error and tell the user to remove the space. . Second - remove spaces from funcotation field names when outputting the funcotations (could still happen with XSVs with spaces in column headers). Is this correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5937#issuecomment-507298072:81,config,config,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937#issuecomment-507298072,1,['config'],['config']
Modifiability,@LeeTL1220 This gets rid of a bunch of false positives with no effect on sensitivity. It also does some useful refactoring of the filtering engine in order to exploit Takuto's two-pass formalism for more filters than just OB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5092:111,refactor,refactoring,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5092,1,['refactor'],['refactoring']
Modifiability,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/742). Should now be `CallAllelicSplits`. Rename the task configuration as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2903:155,config,configuration,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2903,1,['config'],['configuration']
Modifiability,"@MattMcL4475 We've merged a PR that reduces the number of layers in our docker image from 44 down to 16: https://github.com/broadinstitute/gatk/pull/8808. See comments on that PR for reasons why this approach might be preferable to a full squash. If there are still too many layers for your use case, please feel free to reopen this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-2103231924:58,layers,layers,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-2103231924,2,['layers'],['layers']
Modifiability,@PlatonB I agree that this would be a nice feature. We're currently refactoring GATK to use a new type for tool inputs that may eventually enable us to support stdin as an input. @cmnbroad Can you comment on the feasibility of adding stdin support once the path migration is complete?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6749#issuecomment-679301688:68,refactor,refactoring,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6749#issuecomment-679301688,1,['refactor'],['refactoring']
Modifiability,@RWilton I agree that this is a confusing filter and maybe should be renamed... As I have said this is a specific utility is for SVs where they might only want to look at long range read pairs not HaplotypeCaller. In that context I would say that the nuance about TLEN vs left aligned start position is a relatively small one. I agree with you that it would be useful to have some functionality for filtering out highly distant read pairs form our short variant calling as an option. I would have to think on whether its better to refactor this filter to be more general or to make a second filter that is the reverse of this one. . @RWilton can you make an issue ticket and describe what you would want out of such a filter for HaplotypeCaller? I would put some thought into the case where the TLEN field has not been populated since it often isn't (especially since the cases where TLEN might not get computed overlaps with discordant/distant mates which are part of the target for such a filter).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1104014142:531,refactor,refactor,531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1104014142,1,['refactor'],['refactor']
Modifiability,"@SHuang-Broad can you create two new issues, for 1) the error you saw on one of the runs about the missing bwa index file -- maybe we could verify that it's there on all the nodes or do retries and 2) the variability in the number of kmers found and variants discovered? You can assign the latter one to @tedsharpe .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157463:205,variab,variability,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157463,1,['variab'],['variability']
Modifiability,@SHuang-Broad does this refactor to the inversion filter look better?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-243196129:24,refactor,refactor,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-243196129,1,['refactor'],['refactor']
Modifiability,@SHuang-Broad refactored some code in response to your comments. how does this look now?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-243457466:14,refactor,refactored,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-243457466,1,['refactor'],['refactored']
Modifiability,"@SQ	SN:HLA-A*01:16N	LN:2985	M5:10150ad21301a29f92e1521530fdd3f5	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; @SQ	SN:HLA-A*01:20	LN:3105	M5:05dc0384da2f751afe549a9bfdbc3037	AS:38	UR:/seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta	SP:Homo sapiens; ```. If the contig name is the issue, then this is a bug of sorts that, depending on your project, may or may not be an issue. If you expect all of your samples (PoN normals and tumor samples) to have coverage for specific alternate contigs and need to include such contigs in your analysis, then we can request a fix for this parsing error. I think this case would be unusual. If alternate contigs are not needed for your research, or you expect sporadic coverage for the contigs across samples, then you can move ahead by limiting your analysis to the [primary assembly](https://gatkforums.broadinstitute.org/gatk/discussion/7857/reference-genome-components). For a typical somatic CNV analysis, because of the way the PoN is pruned, when working with GRCh38 alignments, you want to be sure to limit your counting to the primary assembly. You want to use the `-L` argument with an intervals file that only lists the primary assembly and excludes alternate and decoy contigs. This is really important. Any apparent arm/contig level event with variable coverage across your samples will cause CreatePanelOfNormals to raise a _red flag_ for the sample. The tool considers the sample suspect, in that it interprets the arm/contig level event as somatic and in that it expects some amount of coverage for each contig for each normal sample. Suspect samples get tossed from the PoN. Because the workflow uses proportional counts, this means that for your tumor sample you must count over the same genomic intervals as the PoN normal samples. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40740#Comment_40740",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3357:3931,variab,variable,3931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357,1,['variab'],['variable']
Modifiability,"@Stikus Yes, this is expected, and is mentioned in the release notes for 4.1.8.0:. * More flexible matching of dbSNP variants during variant annotation (#6626); * Add all dbsnp id's which match a particular variant to the variant's id, instead of just the first one found in the dbsnp vcf.; * Be less brittle to variant normalization issues, and match differing variant representations of the same underlying variant. This is implemented by splitting and trimming multiallelics before checking for a match, which I suspect are the predominant cause of these types of matching failures. For more details see the original pull request here: https://github.com/broadinstitute/gatk/pull/6626",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6690#issuecomment-653119026:90,flexible,flexible,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6690#issuecomment-653119026,1,['flexible'],['flexible']
Modifiability,"@Stikus right, if you take a look at the PR, you’ll see that only defaults for certain arguments of the FilterIntervals, DetermineGermlineContigPloidy, and GermlineCNVCaller tools were changed. So if you just manually specify the old values for these arguments, hopefully that should reproduce your previous results. Take care to make the appropriate changes to both COHORT and CASE modes for the latter two tools. It’s possible that this could be easily done in some kind of config file, e.g., if your pipeline is implemented in WDL you could just make the appropriate changes to your JSON config files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1857781749:476,config,config,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1857781749,2,['config'],['config']
Modifiability,"@TedBrookings which formats are you using, in particular? If there is a format out there that nicely fits our needs, we should adopt it. In the CNV package, I've taken pains to unify how tabular data are represented in Java, depending on whether each record is Locatable or whether the collection of records can be associated with a sample name or sequence dictionary. This allows us to represent records that extend Locatable with *multidimensional numerical or non-numerical annotations* along with some metadata (sample name and sequence dictionary) with a minimum of boilerplate. There are also base methods for producing interval trees, etc. This pretty much satisfies all of the CNV team's needs (and is, in my opinion, a necessary improvement over the horrowshow of read/write utility methods for each file format that we had previously...). However, this unification effort was a quick push I made before release, so some polishing or redesigning may be warranted. We may also want to add more forms of metadata, etc. if other teams would require more features. Another downside is that this code lacks the indexing, NIO support, etc. that some of the other standardized/Tribble formats enjoy. For CNV data, this isn't a huge issue, but I think it would be nice to unify how we represent such data GATK-wide. As I said above, I don't think VCF is the correct answer, but certainly it could fit into whatever framework we adopt or come up with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468:410,extend,extend,410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468,1,['extend'],['extend']
Modifiability,"@Unip0rn Thank you for the pr. I'm not sure I understand what you're trying to do here though. currently when I run `./gatk --list` it prints the list of gatk tools. ex:; ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run.; CollectIlluminaLaneMetrics (Picard) Collects Illumina lane metrics for the given BaseCalling analysis directory.; ExtractIlluminaBarcodes (Picard) Tool determines the barcode for each read in an Illumina lane.; IlluminaBasecallsToFastq (Picard) Generate FASTQ file(s) from Illumina basecall read data. ...; ```. With this change it instead prints the gatk launcher help, which is not the intended result. ; ```; Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after --; GCS: run using Google cloud dataproc; commands after the --",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030:442,adapt,adapters,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030,1,['adapt'],['adapters']
Modifiability,@Z-Zen It wouldn't be to hard to make the precision of the numbers configurable. How would you propose to be able to specify it that would be best for you? What is the use case in which you need more decimal places?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7699#issuecomment-1054457277:67,config,configurable,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7699#issuecomment-1054457277,1,['config'],['configurable']
Modifiability,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1768:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768,3,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"@akiezun @lindenb I can help. It's probably going to be a slightly complicated process though, especially actually building the cross platform jar. I assume we're targeting OSX and x86-64 to start with, and then hopefully expanding to POWER8 in the future? . The general idea is to prebuild the c code for whatever platforms we want to support. Then package that in a structured way into a jar, and write some java code which will detect the platform at runtime and extract the correct executable into a temporary location. Then we can publish that jar as a maven artifact. We have an example of how to do the extraction in the `VectorLoglessPairHMM` constructor. It's not perfect and probably needs a bit of refactoring to make it more general but it's the right idea. Other libraries that package native code have similar examples. I.e. Snappy-java https://github.com/xerial/snappy-java. We're going to be performing similar packaging for other native dependencies that we have, so standardizing it is a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135:709,refactor,refactoring,709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-215820135,1,['refactor'],['refactoring']
Modifiability,@akiezun Added support for intervals. But all this stuff should be refactored. An inefficiency in the ADAM case here is that a `SAMRecord` object must be constructed for each `AlignmentRecord` that is read in before we decide whether it is even in a target `Interval` or not.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152107916:67,refactor,refactored,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152107916,1,['refactor'],['refactored']
Modifiability,"@akiezun BTW, if you want to experiment with different numbers of threads in the current code, you can use the `OMP_THREAD_LIMIT` environment variable to set the maximum number of threads. For example, this command would run with 2 threads:. ```; OMP_THREAD_LIMIT=2 ./gatk-launch HaplotypeCaller ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218785574:142,variab,variable,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218785574,1,['variab'],['variable']
Modifiability,"@akiezun Can you determine whether you're using the HDFS -> GCS adapter in your test case? The adapter historically did have performance problems of this magnitude. As @jean-philippe-martin mentioned, we should benchmark the new NIO -> GCS support as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213097025:64,adapt,adapter,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213097025,2,['adapt'],['adapter']
Modifiability,@akiezun I removed a few more things. Up to you where you think AdapterPair and the newly extracted IlluminaAdapatorPair should live.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/470#issuecomment-97517175:64,Adapt,AdapterPair,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/470#issuecomment-97517175,1,['Adapt'],['AdapterPair']
Modifiability,"@akiezun I've gone over this for shallow things. It seems like there's a bunch of opportunities for refactoring, but it sounds like you want to match the original code for now without to many changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1555#issuecomment-193443611:100,refactor,refactoring,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1555#issuecomment-193443611,1,['refactor'],['refactoring']
Modifiability,"@akiezun If you think the problem might extend beyond `FilterVcf`, and that our sequence dictionary detection might be broken in general in some way for vcfs, could you create a separate ticket to investigate, and assign to @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1728#issuecomment-212036904:40,extend,extend,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1728#issuecomment-212036904,1,['extend'],['extend']
Modifiability,"@akiezun If you've addressed all my comments, please merge this -- it does not require a second-pass review, and I could use the `GenomeLoc` refactoring as part of cleanup in my HC branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1442#issuecomment-188432489:141,refactor,refactoring,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1442#issuecomment-188432489,1,['refactor'],['refactoring']
Modifiability,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:131,adapt,adapter,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,2,['adapt'],['adapter']
Modifiability,"@akiezun It has to do with the code, one part laziness in avoiding to implement a bona fide `GATKRead` adapter for `AlignmentRecord`, and one part due to the interval filter operating only on `SAMRecord`s (`samRecordOverlaps`). Assuming ADAM does become a more important use case to support, we can refactor to improve this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152358785:103,adapt,adapter,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1067#issuecomment-152358785,2,"['adapt', 'refactor']","['adapter', 'refactor']"
Modifiability,"@akiezun Yes, it's true that there's no way to prevent that, short of doing deep copies every time a read is wrapped in an adapter. But we can make it clear in the GATKRead contract that the backing reads should not be directly modified after wrapping within an adapter, and if they are they need to be re-wrapped in a new adapter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718:123,adapt,adapter,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235291718,3,['adapt'],['adapter']
Modifiability,"@akiezun Yes, the tool is written and waiting for review. Please see #1566. It's not a complete rewrite (some filters not included yet, but adding them will be easy).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1012#issuecomment-202906769:96,rewrite,rewrite,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1012#issuecomment-202906769,1,['rewrite'],['rewrite']
Modifiability,"@akiezun thanks for trying this out. Regarding the wrong codec being used - this is because it wasn't wired up - I've fixed that now. The `UnknownHostException` is a limitation in the jsr203-hadoop library. It uses the host from the URI to determine the HDFS namenode, and doesn't fall back to the configuration if the URI doesn't have a host. You can work around this by specifying the host in the reference paths. I'll open an issue to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1936#issuecomment-229668510:298,config,configuration,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1936#issuecomment-229668510,1,['config'],['configuration']
Modifiability,"@ambarishK Could you provide your WDL script and point us to which tasks were configured with a GATK docker, which steps run, and which steps fail?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5906#issuecomment-583802837:78,config,configured,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906#issuecomment-583802837,1,['config'],['configured']
Modifiability,"@apete Thanks for the PR! That's really helpful to update and any svd improvements are definitely something we want. . It's failing to build though, because it can't locate `ojalgo-extensions-1.0.0`. I get the following error:; ```; Build file '/Users/louisb/Workspace/gatk/build.gradle' line: 511. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all dependencies for configuration ':runtime'.; > Could not resolve org.ojalgo:ojalgo-commons-math3:1.0.0.; Required by:; project :; > Could not resolve org.ojalgo:ojalgo-commons-math3:1.0.0.; > Could not parse POM https://repo1.maven.org/maven2/org/ojalgo/ojalgo-commons-math3/1.0.0/ojalgo-commons-math3-1.0.0.pom; > Could not find org.ojalgo:ojalgo-extensions:1.0.0.; Searched in the following locations:; https://repo1.maven.org/maven2/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.pom; https://repo1.maven.org/maven2/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.jar; https://jcenter.bintray.com/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.pom; https://jcenter.bintray.com/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.jar; https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.pom; https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.jar; file:/Users/louisb/.m2/repository/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.pom; file:/Users/louisb/.m2/repository/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.jar; > Could not resolve org.ojalgo:ojalgo-commons-math3:1.0.0.; > Could not parse POM https://jcenter.bintray.com/org/ojalgo/ojalgo-commons-math3/1.0.0/ojalgo-commons-math3-1.0.0.pom; > Could not find org.ojalgo:ojalgo-extensions:1.0.0.; > Could not resolve org.ojalgo:ojalgo-commons-math3:1.0.0.; > Could not parse POM https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3970#issuecomment-351825206:412,config,configuration,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3970#issuecomment-351825206,1,['config'],['configuration']
Modifiability,"@asmirnov239 I think that some of the optimizations that @vruano made to the postprocessing step concern the config JSONs, gCNV version, and interval list output added in #5176. These take a lot of time to localize when the number of shards is large but, aside from the interval list, aren't really used for anything, correct?. Were these just added for debugging purposes, or for reproducibility/provenance? Let's revisit whether it's necessary to pass these files on when we merge @vruano's changes into the canonical WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393:109,config,config,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393,1,['config'],['config']
Modifiability,"@asmirnov239 I'm getting the same error on Terra when bam files and their indices are not in the same location. I'm running https://github.com/broadinstitute/gatk/blob/4.1.7.0/scripts/mutect2_wdl/mutect2_pon.wdl. The workflow input for bam index seems to be just a placeholder, if you follow the variables along the wdl it's never actually used.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7487#issuecomment-1697767879:296,variab,variables,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487#issuecomment-1697767879,1,['variab'],['variables']
Modifiability,@asmirnov239 and Jack Fu are currently developing tests using Talkowski-SV truth that will ultimately cover #5633. Should be adapted to fit into whatever framework arises from #4630.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-459834130:125,adapt,adapted,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-459834130,1,['adapt'],['adapted']
Modifiability,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3002:1129,extend,extend,1129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002,1,['extend'],['extend']
Modifiability,"@bbimber - using the `FeatureContext` to get the overlapping BED/VCF records should do the job (for the first message, second part). Regarding the `PluginManager`, I am not familiar with the GATK3's concept. If you are talking about the plugins like `ReadFilter`s, you can retrieve them by using: `getCommandLineParser().getPluginDescriptor(RequiredStratification.class)`. This will get the plugin descriptor already parsed by the command line. But for that, you need to have a `RequiredStratification` plugin...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343678198:148,Plugin,PluginManager,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343678198,4,"['Plugin', 'plugin']","['PluginManager', 'plugin', 'plugins']"
Modifiability,"@bbimber I did start to take a look at this today. We'd probably take many of the peripheral classes from GATK3 as they are (though there will be some exceptions, i.e., `VariantEvalUtils` calls `System.exit`, which we wouldn't want to do). And in some cases we may want to make tickets for places where we want to do refactoring, like we did for MendelianViolations. The code will need to conform to GATK4 in some areas (use hyphen-separated argument names, updated javadoc where it's GATK3-specific, etc). The easiest way to do this would be to add a commit with the raw GATK3 code as I mentioned above. We would then focus on reviewing the diffs, instead of line-by-line for all of the code. I can add that commit if you like - we'd just need to coordinate since I would have to push to your branch. Also, FYI, I'll be away all of next week so I won't be able to do any more on this until I return.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-413988997:317,refactor,refactoring,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-413988997,1,['refactor'],['refactoring']
Modifiability,"@bbimber Nice to virtually meet you too. I think we've probably interacted once or twice in the past as well. I'm glad to hear you're planning on using and extending Funcotator! It would be great if you could push back the useful changes you make. FYI - come the new year, I'm going to be refactoring several things in the Funcotator core that will make it much more extensible (mostly around GTF parsing). I'm also going to fix a few bugs involving variants that span transcript and contig boundaries. It's been a long time coming - I've just been working on a set of high-priority tasks that needed my attention. Also - Funcotator is production-ready for human data, so it should basically be ""guaranteed stable"" (modulo certain known issues like the ones I referenced above).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8124#issuecomment-1355036390:156,extend,extending,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124#issuecomment-1355036390,2,"['extend', 'refactor']","['extending', 'refactoring']"
Modifiability,"@bbimber The `GATKAnnotationPluginDescriptor` uses the GATK config file to find packages that contain annotation classes, and will automatically discover annotation classes that are in packages listed there (see [this](https://github.com/broadinstitute/gatk/blob/f4996564f0abe02b6a202eaff402c2f3fa044633/src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKAnnotationPluginDescriptor.java#L46) and [this](https://github.com/broadinstitute/gatk/blob/master/src/main/resources/org/broadinstitute/hellbender/utils/config/GATKConfig.properties#L77)).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-720551282:60,config,config,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-720551282,2,['config'],['config']
Modifiability,"@bbimber This looks like a good change, but I don't think it'll solve the problem. The `XsvLocatableTableCodec` works differently than other codecs. It was essentially created for Funcotator datasources, and it expects to be given a `.config` file rather than the XSV file itself. For example, if you wanted to index the Oreganno data source file, you'd need to first create a funcotator configuration file adjacent to it, and then use `IndexFeatureFile` to index the config file rather than the tsv. This is not a good design (my fault), but it's how the tool operates as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591483042:235,config,config,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591483042,3,['config'],"['config', 'configuration']"
Modifiability,"@bbimber This this looks pretty good now - I don't have any new substantive comments. We just need to get the other PRs in and the tests passing and this should be good to go. There is one outstanding thing that would be nice, but I'll leave it up to you as to whether you want to address it. `VariantStratifier` and it's subclasses still have `initialize` methods that take no arguments. As far as I can tell, this is unnecessary now that they are required to have a constructor that take a `VariantEvalEngine`. If you feel so inclined, the `initialize` content can be moved into the constructors, and the initialize method, and call to it, can be eliminated (BTW, the constructors in those classes should be moved to be after the variable declarations). Overall, though, I think this PR makes a pretty big improvement to the state of things. Thanks for doing it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-775474714:732,variab,variable,732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-775474714,1,['variab'],['variable']
Modifiability,@bbimber You also have the option of bundling your custom config file inside your DISCVRseq jar -- that way you don't need to supply it on the command line,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-720615025:58,config,config,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-720615025,1,['config'],['config']
Modifiability,"@bbimber, you can configure the `LocusWalker` to traverse every locus even if it is empty and thus the `getSkippedBases()` is unnecessary. Another option is to track the previous `AlignmentContext` and compare the location with the current one, to check how many bases have been skipped....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-337849475:18,config,configure,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-337849475,1,['config'],['configure']
Modifiability,"@bhanugandham As a side note, you shouldn't be running GATK4 using `java -jar` directly. You should use the included `gatk` launcher script, which sets a lot of important configuration settings, some of which have a major effect on tool performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403:171,config,configuration,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403,1,['config'],['configuration']
Modifiability,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:154,variab,variable,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139,1,['variab'],['variable']
Modifiability,"@byoo Thanks, we will fix that one ASAP. By the way, we are doing a big refactoring of Mutect2 filtering, which won't change the outputs but will improve the internal software engineering so that we can write better unit tests to catch these sorts of things.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5563#issuecomment-452851947:72,refactor,refactoring,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5563#issuecomment-452851947,1,['refactor'],['refactoring']
Modifiability,"@byoo We run the M2 wdl without modification on a Broad SGE cluster for our internal evaluations all the time. A cromwell config file for SGE is required, but that's the case for every backend. You *do* need to supply a gatk docker image string as an input, but it is ignored. Please feel free to share your input json and config file if it doesn't work for you. I can't say I'm skilled enough to catch anything subtle, but I can check for obvious differences.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358960833:122,config,config,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358960833,2,['config'],['config']
Modifiability,"@ccastane9, looks like a memory issue. Some questions -. 1. What are the sizes of the book-keeping files in your GenomicsDB workspace? Try running `find /ECA3_GenomicsDB_260 -name __book_keeping.tdb.gz -ls`.; 2. Is /ECA3_GenomicsDB_260 on NFS or another shared Posix FS? Can you try running GenotypeGVCFs with `--genomicsdb-shared-posixfs-optimizations` turned on?; 3. What does your hardware configuration look like, memory wise?; 3. What are your `-Xmx` and `-Xms` java options?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432:393,config,configuration,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432,1,['config'],['configuration']
Modifiability,"@chandrans @chapmanb @ldgauthier The `-ploidy` argument is one of several arguments inherited from `AssemblyRegionWalker` that apply only to `HaplotypeCaller` and do nothing in `Mutect2`. (We should refactor this once the engine team's workload lightens enough to review lower-priority things like this.) The `GT` field emitted by Mutect is just the concatenation of all called alleles -- 0/1, 0/1/2, 0/1/2/3 etc -- and doesn't imply anything about the ploidy. Maybe we should get rid of it entirely since `AF` is so much more informative. I like the idea of splitting multiallelics into multiple lines. It would make filtering a lot easier. @LeeTL1220 do you have an opinion?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330401348:84,inherit,inherited,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330401348,2,"['inherit', 'refactor']","['inherited', 'refactor']"
Modifiability,@chapmanb As far as I know Spark (and Hadoop) needs the user name to submit a job. Can you set the `SPARK_USER` environment variable to the user you would like to use (even if it is not in /etc/passwd)? You should be able to pass it to Docker with the `-e` option. (I haven't tried this to see if it works.),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379226226:124,variab,variable,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379226226,1,['variab'],['variable']
Modifiability,"@chapmanb Do you have any control over the `docker run` command? If you do, you could mount `/etc/passwd`/`/etc/shadow` as external resources into the container (as described here, for example: `https://stackoverflow.com/questions/33013444/can-not-add-new-user-in-docker-container-with-mounted-etc-passwd-and-etc-shado`) . This seems slightly less awful than the workarounds you linked to above. It seems to me, however, that running as a user not present in `/etc/passwd` could cause quite a few things to fail, not just Spark. Perhaps you should file a bug report against these CWL runners? They should really be configuring the runtime environment in the container properly for the username they force you to run under! If they are trying to mirror the external user within the container, they should probably mount the external `passwd` file into the container on your behalf when executing `docker run`. @tomwhite Do you happen to know of a workaround on the Spark side to prevent it from doing a username lookup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379060749:615,config,configuring,615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379060749,1,['config'],['configuring']
Modifiability,"@chapmanb Singularity's default configuration has a line ""config passwd = yes"" and that will create a user entry in the /etc/passwd automatically. So it I understand the issue, spark would automatically find the user running the container in the /etc/passwd file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335:32,config,configuration,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335,2,['config'],"['config', 'configuration']"
Modifiability,"@chatchawit Thanks. From the stack trace it looks like another issue that I'm actually currently working on. I'm not sure when it will be done, but I'm going through it as part of fixing #4739 (it's an aggravating bug that will require some refactoring).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391468128:241,refactor,refactoring,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391468128,1,['refactor'],['refactoring']
Modifiability,"@chutianwen Thanks for reporting this. I'm pretty sure the issue is with your hdfs paths. You say that you're not running on a cluster, which makes me suspect you also don't have hdfs configured. Let me know if that isn't the case and your files are on hdfs. Try using file paths instead of the hdfs ones you're using. If you checked out the git-lfs files then the following command should work if you're in the gatk directory. ```; /bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam \; -R src/test/resources/large/human_g1k_v37.20.21.2bit \; --knownSites src/test/resources/large/dbsnp_138.b37.20.21.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1876#issuecomment-224326591:184,config,configured,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876#issuecomment-224326591,1,['config'],['configured']
Modifiability,"@cmnbroad , I've done the suggested refactoring and documentation changes.; Can you take a look again please? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4981#issuecomment-405582729:36,refactor,refactoring,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4981#issuecomment-405582729,1,['refactor'],['refactoring']
Modifiability,"@cmnbroad - I agree that we can add it as an `@Advanced` agument/filter, and clarify in the documentaion (javadoc for barclay-generated pages and in the argument itself) the purpose of this filter, but I would like to have this as a general purpose filter in `ReadTools` and thus picked by the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434579278:294,plugin,plugin,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434579278,1,['plugin'],['plugin']
Modifiability,"@cmnbroad - I am not concern about the methods, because they are perfectly configurable, and actually this issue is not that important for some of my projects (e.g., `ReadTools`), which does not use the base walker clases from GATK. Nevertheless, I guess that what @droazen is suggesting is quite important and I appreciate the interest for making better the downstream toolkits integration. Here, a real use case: I've just started to write a new toolkit that will use some walker classes from GATK (by now, `LocusWalker` or the `ReadSliderWalker` from #4682). With the current way of configuring the output, I will need to implement a layer for both walker classes (e.g., `MyLocusWalker` and `MyReadSliderWalker`) and use them in my implemented tools. In addition, I would like to bundle some tools from the GATK/Picard (`IndexFeatureFile `), but they would print inconsistent logs with the rest of my toolkits and they aren't overridable because the classes are final; thus, I would use a decorator over this tools to print the proper startup messages. After a while, I might implement a `VariantWalker`, which will require that I implement another layer (`MyVariantWalker`). Thus, I end up with a lot of naive classes implemented on top of the base walkers and wrappers around bundled GATK/Picard tools. This is very difficult to maintain, because if a change is done at the `CommandLineProgram` abstract class for the logging output (a new method, for example), I will need to update every naive class and wrapper if I bump the GATK version. In addition, extensions of my own toolkit (if any) would need to do the same, making the class-dependency tree so deep that it is difficult to follow (with GATK3, this problem was really driving me crazy when I tried to implement custom tools). On the other hand, there is another use case for the GATK itself: once barclay has a common class for CLP, GATK would be able to run directly Picard tools without the decorator; nevertheless, they will still n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646:75,config,configurable,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646,2,['config'],"['configurable', 'configuring']"
Modifiability,"@cmnbroad - it is not enough for me to make it `@Hidden`. In my downstream toolkits , I don't want it to appear wherever command line is printed (headers, logs, etc), because I am not supporting custom configuration files. In the case that `Main` is not accounting for the configuration file and an user provide their own, then the command line might indicate wrongly that the configuration was set, but it wasn't. I really need a way to remove completely the argument. If someone print the help with hidden arguments, then they will misunderstand that the configuration can be overwrite, even more if they know the configuration system of GATK. The only solution for my use case is to being able to remove completely the argument, not just reducing visibility.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371730828:202,config,configuration,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371730828,5,['config'],['configuration']
Modifiability,"@cmnbroad - what is the timeframe for a common CLP class? Because in the meantime it would be nice to be able to add picard tools, and anyway the common CLP would require refactoring in `Main`. So is it too much problem if I add a temporary method to add single picard tools?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4660#issuecomment-384224234:171,refactor,refactoring,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660#issuecomment-384224234,1,['refactor'],['refactoring']
Modifiability,"@cmnbroad : first - would it be possible to kick off travis tests? i refactored this and dont seem to be able to do that. Second, yes, I was trying to reorder and condense the commits but clearly didnt work. I think the problem was trying to put your GATK3 commit first (which would seem to make sense). in any case, I just recreated this, putting a pristine GATK3 first, following a consolidated set of my commits with 1) the limited core changes, 2) the meat of the VariantEval port, and 3) A separate commit with a port of GATK3 VariantEvalIntegrationTest which is useful for validation but should not be merged. To your points:. 1) I substantially cut down the incoming large files, mostly by limiting the intervals of new large VCFs. 2) On the plugin: this was discussed above, and I initially also pointed out this should ultimately go into Barclay. You are actually the one who proposed staging it in GATK. I am not entirely sure I understand the reticence on plugins; however, my goal is to get VariantEval ported by touching as little of it as possible. This is already sucking up a ton of time. I flipped VariantEvalUtils to gather a list of classes from the appropriate package instead of a full-on plugin. That should satisfy that concern?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735:69,refactor,refactored,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431913735,4,"['plugin', 'refactor']","['plugin', 'plugins', 'refactored']"
Modifiability,"@cmnbroad : if getStratifierClasses() is the only sticking point, we can drop that. . Stepping back: as you probably know we have a tool, VariantQC, which basically sets up a number of instances of VariantEval and uses them to aggregate data as it iterates a VCF. this allows the tool to capture data aggregated/stratified at multiple levels with one pass through the VCF. . There are two related aims:. 1) my tool needs to know the allowable stratification classes. Instead of copy/paste the reflection code to find classes, this PR was exposing that getter as a public method. I'm not sure I understand why this is a sticking point, but we can remove this without that much problem for me. Like we discussed earlier, VariantEval should get refactored into a walker class and some kind of VariantEvalEngine class, and this refactor might be a time to address my concern here. This is not actively blocking us. I could also make this a protected getter on VariantEval if the public aspect is what you dont like. 2) The remaining changes serve a different purpose. Most VariantEvaluator classes are 'dumb' in that they are instantiated with no configuration and always aggregate the same fields. In our tool, we wanted to let the user specify a list of INFO fields to aggregate (i.e. we dont know the target until runtime). We wrote an InfoFieldAggregator class, which is instantiated with the name of an INFO field. This lets our code create multiple instances of that VariantEvaluator, potentially summarizing different fields. The remaining changes are designed to enable this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502946807:742,refactor,refactored,742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502946807,3,"['config', 'refactor']","['configuration', 'refactor', 'refactored']"
Modifiability,"@cmnbroad @lbergelson The cram index looks like it has all the info required to generate the splits without using the CramContainerIterator to look at the cram file directly. . Could using the crai index for splits be a potential solution to the glacially slow cram split generation? ; ; CRAM index. A CRAM index is a gzipped tab delimited file containing the following columns:; 1. Sequence id; 2. Alignment start; 3. Alignment span; 4. **Container start byte offset in the file**; 5. Slice start byte offset in the container data (‘blocks’); 6. Slice bytes; Each line represents a slice in the CRAM file. Please note that all slices must be listed in index file. In Hadoop-bam this code could read the crai instead of the cram to find the container boundaries. public List<InputSplit> getSplits(List<InputSplit> splits, Configuration conf); throws IOException {; // update splits to align with CRAM container boundaries; List<InputSplit> newSplits = new ArrayList<InputSplit>();; Map<Path, List<Long>> fileToOffsets = new HashMap<Path, List<Long>>();; for (InputSplit split : splits) {; FileSplit fileSplit = (FileSplit) split;; Path path = fileSplit.getPath();; List<Long> containerOffsets = fileToOffsets.get(path);; if (containerOffsets == null) {; containerOffsets = getContainerOffsets(conf, path);; fileToOffsets.put(path, containerOffsets);; }; long newStart = nextContainerOffset(containerOffsets, fileSplit.getStart());; long newEnd = nextContainerOffset(containerOffsets, fileSplit.getStart() +; fileSplit.getLength());; long newLength = newEnd - newStart;; if (newLength == 0) { // split is wholly within a container; continue;; }; FileSplit newSplit = new FileSplit(fileSplit.getPath(), newStart, newLength,; fileSplit.getLocations());; newSplits.add(newSplit);; }; return newSplits;; }",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-373078699:822,Config,Configuration,822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-373078699,1,['Config'],['Configuration']
Modifiability,"@cmnbroad Back to you with a few nitpicks. Ready to merge when those are addressed. . I think there might be a few places that the metrics could be profitably refactored, but maybe we should wait until more exist to do so. The finish(..., ..., ...) methods seem like they should maybe be incorporated in to the base class somehow. Not necessarily as part of this PR though. Also, I noticed that MetricsCollectorSpark.saveMetrics() has a no-op default implementation. Why is that? It seems like it should be abstract.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2049#issuecomment-236974957:159,refactor,refactored,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2049#issuecomment-236974957,1,['refactor'],['refactored']
Modifiability,@cmnbroad Back to you. Some minor cosmetic comments about printing the help output. I think it looks good though. This version with the plugin descriptors is so much nicer than the early version.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-239237933:136,plugin,plugin,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-239237933,1,['plugin'],['plugin']
Modifiability,"@cmnbroad Back you with a minor refactoring request. It seems to me that GATKTool should try and close it's own writers, but maybe there's a reason not to that I'm missing?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2046#issuecomment-237007660:32,refactor,refactoring,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2046#issuecomment-237007660,1,['refactor'],['refactoring']
Modifiability,"@cmnbroad Chris, could you update `Miniconda2` to `Mininconda3` in the docker config? here's the sh bundle + MD5:; ```; ENV CONDA_URL https://repo.continuum.io/miniconda/Miniconda3-4.3.30-Linux-x86_64.sh; ENV CONDA_MD5 = ""0b80a152332a4ce5250f3c09589c7a81""```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350194001:78,config,config,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350194001,1,['config'],['config']
Modifiability,"@cmnbroad Hope you had a good break. As you might have seen above, I refactored out VariantEvalEngine, which I think will address some of the problems, like passing the walker around, which you didnt like in the PR. . I might not have as good an eye over tests as you, but I believe the failures mostly relate to the change to set 'source' on the VCs. As you probably know, that means it's passed into the resulting VCF, and will change test expectations in some cases. You said someone at GATK was looking into that, but I would be happy to take a stab if your team has ideas on how to address this. If I'm missing test failures beyond that I can take a look.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754015497:69,refactor,refactored,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754015497,1,['refactor'],['refactored']
Modifiability,"@cmnbroad I agree disabling some of the functionality should probably be done, I'll consider that a review comment. As to generalizing this disabling of the founderIDs, I would ere on the side of not doing it, since at the very least right now all the pedigree arguments are being populated by the plugin manager and not the annotations. I wouldn't know what to do in the case where the user asked for a pedigree annotation that did take founderIDs and one that didn't, how should that be resolved? As of right now the pedigree file and founderIDs get merged which seems sensible for other tools. Perhaps emit a warning for PossibleDenovo?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463278863:298,plugin,plugin,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463278863,1,['plugin'],['plugin']
Modifiability,@cmnbroad I left some of the AnnoationManager code in VariantAnnotatorEngine because there were tests for VariantAnnotatorEngine which required `ofSelectedMinusExcluded` and it seemed clunky to achieve the same thing through the creation and execution of an abstract plugin. I can change it if you would like. Fixes #3287,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4674:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674,1,['plugin'],['plugin']
Modifiability,@cmnbroad I moved this message so it runs during the actual task execution rather than at configuration time.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4147#issuecomment-357371881:90,config,configuration,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4147#issuecomment-357371881,1,['config'],['configuration']
Modifiability,"@cmnbroad I refactored the training java wrapper into separate wrappers to write tensors (CNNVariantWriteTensors.java) and to train (CNNVariantTrain.java) I think this simplified the meaning/necessity of many of the arguments, which was unclear when all those tools were rolled together. . I'm working on a release-style integration test that chains all the tools together, like @droazen discussed a few meetings ago, but for this PR I think I will have to do something simpler. Because of some issues with the GSA5 environment and GPU, I still have to write in a Python2/3 agnostic way, which precludes the use of type hints. I would like to update, but I'm blocked by BITs in the short term.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-367449432:12,refactor,refactored,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-367449432,1,['refactor'],['refactored']
Modifiability,"@cmnbroad I responded to your comments, what do you want to about names? Do you think we should refactor the haplotype caller tests to run with all the types of I also bumped to a more stable looking version of native bindings while I was at it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3600#issuecomment-331546971:96,refactor,refactor,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3600#issuecomment-331546971,1,['refactor'],['refactor']
Modifiability,"@cmnbroad I see. The ""CI"" variable does seem brittle, especially since I'm not strictly sure where it is set. I think a somewhat safer place would be to add some global test flag to the docker image would be to add it to the run_unit_tests.sh script. That way we know it is getting triggered exactly before we run the tests in just the docker environment. Is there some way of detecting what conda environment is active outside of conda.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5819#issuecomment-474871354:26,variab,variable,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819#issuecomment-474871354,1,['variab'],['variable']
Modifiability,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238:393,Config,Config,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238,1,['Config'],['Config']
Modifiability,"@cmnbroad OK - I think I addressed your review and also added a sizable amount of 'final', including to pre-existing code. . I did not do the refactor of eval/comp/gold-standard from VariantEvalArgumentCollection back into VariantEval. I see your point and am not totally opposed to it, but put a question on that thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-749844101:142,refactor,refactor,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-749844101,1,['refactor'],['refactor']
Modifiability,"@cmnbroad OK, considerable progress here. I was able to adjust behavior such that only two tests have changed behavior from GATK4/master. I think this is now correct. One instance of changed behavior is the Snpeff/overlap one we discussed above. The second is the one where we now provide the full genome as REF, not the truncated genome. I think this difference is justified since the tool now requires a reference, and the prior version was arguably too lenient on validation of contigs. Anyway, this branch now also removes by debugging code and comments. I think it is ready for a review. To some other questions you had above:. 1) The HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>> can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place. I realize that's non-optimal, but this isnt anything I introduced here. I would really like to keep this PR as limited as we can, and address some larger refactoring in a different PR, once we've migrated to MultiVariantWalkerGroupedOnStart. 2) I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round. As noted above, I'd like to propose this as iterative, with a second PR coming soon. I did this b/c it moved us toward not needing to pass around the walker. It minimizes the code that has access to the walker (as opposed to setting it after creating the instance of the Evaluator, etc. Yes, it exposes it for two methods, but those classes no longer hang on to it. I would like to ultimately remove this entirely. 3) To re-iterate testEvalTrackWithoutGenotypesWithSampleFields: the input file, noGenotypes.vcf, has a header dictionary with the full set of contigs, and ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130:998,refactor,refactoring,998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-747619130,1,['refactor'],['refactoring']
Modifiability,"@cmnbroad OK, so I see that I can supply a custom --gatk-config file on the command line, and I suppose I would also provide my other JAR on the classpath with my additional VariantAnnotation classes? That is useful, but also a little unclean given I am already building our DISCVRseq JAR, which includes the GATK4 dependency. I'm still inclined to make our tool that extends GATK's VariantAnnotation, and override makeVariantAnnotations(). I could either manipulate GATKAnnotationPluginDescriptor, or make my own to scan the expected package(s). It seems pretty surgical and less would be required of the user to run it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-720610784:57,config,config,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-720610784,2,"['config', 'extend']","['config', 'extends']"
Modifiability,"@cmnbroad OK, thanks. I need to do more investigation, but my initial thoughts/investigation were two-fold:. 1) Our main use-case if our VariantQC tool, which makes several instances of VariantEval in kind of a hacky way and calls apply() on them. Therefore refactoring a VariantEvalEngine out of VariantEval has value on this front, even if tricky. 2) From what I could tell, the worst perf is the iteration pattern. Using something like MultiVariantWalkerGroupedOnStart would be a big savings and avoid re-querying the component VCFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720617033:258,refactor,refactoring,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720617033,1,['refactor'],['refactoring']
Modifiability,"@cmnbroad OK, that's what I was afraid of. Has your group given thought to how barclay might attempt to de-convolute ""identical"" arguments defined across diverse plugins like this? . Anyway, I can try to follow the pattern of pedigree. I was trying to avoid special-casing these arguments, but I am already making my own PluginDescriptor anyway. My use-case is effectively to have a VariantAnnotator that supports more annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823493118:162,plugin,plugins,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823493118,2,"['Plugin', 'plugin']","['PluginDescriptor', 'plugins']"
Modifiability,@cmnbroad So I checked the out this issue after the recent refactor and it appears that it might have already been fixed. When I add a non-serializable object to the BwaSpark class it still seems to pass the BwaSparkIntegrationTest. Let me know if this still seems to be an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1926#issuecomment-228434693:59,refactor,refactor,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1926#issuecomment-228434693,1,['refactor'],['refactor']
Modifiability,@cmnbroad Thank you for your help with this branch. I will be happy to give a speedy review to your proposed refactoring branch once this gets in.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-472576463:109,refactor,refactoring,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-472576463,1,['refactor'],['refactoring']
Modifiability,@cmnbroad Thank you! I confirmed the speedup is not contigent on the environment variables.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476850732:81,variab,variables,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476850732,1,['variab'],['variables']
Modifiability,"@cmnbroad The path to the c++ compiler can be specifically provided to theano by setting `theano.config.cxx` in python scripts, or by creating a `.theanorc` in the home directory, or by setting the environmental variable `THEANO_FLAGS=cxx=<path_to_g++>,...`. If a working c++ compiler exists and provided to theano, it is fair to assume that the graph _will_ compile. If the c++ compiler is not explicitly specified, theano will try to discover it. It first tries to execute `g++ -v` in the present environment and if it succeeds, it resolves the absolute path to the executable. On darwin, it further searches for `clang++` and on Win32, it looks for a working mingw gcc setup. We could _enforce_ the presence of a c++ compiler at the beginning of all python scripts and throw an exception and an informative message instead of numpy/python fallback. If we do so, the integration tests (and all gCNV CLI tools) will fail and will force the user to install a c++ compiler. In your opinion, is this fail-fast strategy a better approach, given that python fallback runs 2~3 orders of magnitude slower?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350193484:97,config,config,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350193484,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"@cmnbroad This is related to discussion on issue 5439. This is not a final product yet. I'm opening the PR to see how it works on travis and to push discussion here. This PR is not trying to fix all issues with VariantEval. It's trying to address these:. 1) Switch to MultiVariantWalkerGroupedOnStart, primarily to avoid the constant re-querying of variants per site that took place in VariantEvalUtils.bindVariantContexts(). I believe this will substantially reduce the number of instance in which featureContext.getValues() is called. 2) I tried to move, but not full fix, some of the tight linkage between the VariantEval Walker class and the plugin classes. I also made a VariantEvalArgumentCollection to start separating these. Toward this objective, this PR does: a) makes a VariantEvalContext class, which is what gets passed to the VariantStratifier classes, and b) I try to reduce exposing the walker class directly to VariantStratifier and VariantEvaluator. The latter is not completely done, but I think this is moving it in that direction. At several points I stopped for the sake of keeping changes in one PR manageable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973:646,plugin,plugin,646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973,1,['plugin'],['plugin']
Modifiability,@cmnbroad We've merged the htsjdk changes! Could you refactor the progress logger by extending the new `AbstractProgressLogger`. Thanks and sorry for the delay.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/612#issuecomment-125967537:53,refactor,refactor,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/612#issuecomment-125967537,2,"['extend', 'refactor']","['extending', 'refactor']"
Modifiability,"@cmnbroad Worth a try. Do we understand the underlying issue, though? If it's just the static initializer in `BaseTest`, perhaps we could refactor that into a `@BeforeSuite`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330888023:138,refactor,refactor,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330888023,1,['refactor'],['refactor']
Modifiability,"@cmnbroad and @jonn-smith Thanks! It now says ""First-time contributors need a maintainer to approve running workflows"" - does something else need to be approved? I'm not actually a first-time GATK contributor, but I'm not sure how that logic is determined/enforced. I dont have merge privileges' either. @jonn-smith: good to virtually meet you. As I alluded to above, my goal is to extend funcotator to make a new output format that more or less takes the annotations and writes them as discrete INFO fields, rather than the concatenated string. This PR will hopefully largely unblock us. As this develops, do you have any interest in us trying to contribute features back into GATK, or should be just keep separate. There may also be a handful of more minor extensions as we use this more, such as adding a config option to include/exclude specific VCF INFO fields, rather than always transfer 100% of them (which appears to be behavior today, but I just reviewed briefly). Obviously these would be case-by-case, but I'm happy to try to push features back here if you have interest/time in reviewing them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8124#issuecomment-1353346614:382,extend,extend,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124#issuecomment-1353346614,2,"['config', 'extend']","['config', 'extend']"
Modifiability,"@cmnbroad could the failing WDL test simply be due to some Spark configuration issue, rather than memory? Locally, for both 1) the WDL test within the Docker and 2) CreateReadCountPanelOfNormalsIntegrationTest using 17.0.3 without the Docker, I seem to hit the exception discussed here: https://stackoverflow.com/questions/72724816/running-unit-tests-with-spark-3-3-0-on-java-17-fails-with-illegalaccesserror-cl. Not sure why CreateReadCountPanelOfNormalsIntegrationTest seems to pass in the CI environments, but perhaps it'll be more obvious to you?. Just for context, note that this tool relies on the Spark MLlib implementation of PCA.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1409180990:65,config,configuration,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1409180990,1,['config'],['configuration']
Modifiability,"@cmnbroad hi chris - sorry to ping you directly here, but does GATK have someone watching PRs? we're hoping to extend funcotator and this PR has some very minor changes from private->protected to enable that. is this something GATK would consider?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8124#issuecomment-1352245629:111,extend,extend,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124#issuecomment-1352245629,1,['extend'],['extend']
Modifiability,@cmnbroad rebased and reverted the dockerfile. Tests pass locally. I'm also running a test in the cloud to see the impact of environment variables on speedup.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476752829:137,variab,variables,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476752829,1,['variab'],['variables']
Modifiability,"@cmnbroad thanks for the review. I think addressed all comments except the arguments/weights simplification, which I would prefer to save for the PEP8 refactor we discussed. Back to you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-426366833:151,refactor,refactor,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175#issuecomment-426366833,1,['refactor'],['refactor']
Modifiability,"@cmnbroad 👍 to adding an advanced command line option for it. . @magicDGS Our goal is to make it unnecessary for normal users to ever need to see a stacktrace. We're definitely not at the point yet where every UserException produces either a) the complete information necessary to debug, or even b) the correct information. We're trying to fix all those cases, but there's a lot of possible failure modes between cloud access, spark, filesystem plugins, etc, so it's going to be an issue for a while. . I don't think it will hurt the user experience to have an extra commandline argument for it. Printing the stacktrace when debug is on isn't a bad idea, but I think it's better to have finer grained control over it. . The other issue is that it's easier to explain to an unsophisticated user how to set an extra commmand line argument rather than trying to get them to set the environment variable which well be helpful for our support team when they're trying to debug someone's problem. (especially since setting the environment variables may be different on a spark cluster than on a local run).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394:445,plugin,plugins,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285390394,3,"['plugin', 'variab']","['plugins', 'variable', 'variables']"
Modifiability,"@cmnbroad, that's not wholly unreasonable, but i'd like to push back on a number of these points. . 1) First - would GATK consider simply letting us take over VariantEval and maintain as a GATK4-based tool in another repo? My understanding from GATK4 issues is that plan was to never migrate VariantEval (i think in favor of other picard/gatk QC tools). There is a bit of a conflict between keeping a lean core engine and having all these tools built off it. I would think there's an argument for keeping your core engine and the many tools built off it separated (GATK3 seemed to include some dead tools, for example). I appreciate we're the ones pushing this migration, but I hope on the other side you can appreciate the bar is pretty significant on our time. . 2) What new plugins are you talking about? VariantStratification and VariantEvaluator are part of GATK3's VariantEval? Yeah, I wrote a base PluginDescriptor class patterned on how ReadFilters work. It probably should exist in a more core position in code. While there's some good ideas in the argument-parsing/plugin code of GATK/Barlcay, frankly seems like much of it isnt fully developed yet, which is why I kept this separated at the moment. . 3) Be aware, the GATK3 tests depend on ~30GB of files. I dont know the limits of git lfs, but I did not currently have plans to check those in. I assumed I would convert these to use GATK4 chr20/21 data for a final commit, but felt there was a lot of value in using unaltered GATK3 data to confirm parity (and it was during the migration).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407123968:777,plugin,plugins,777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407123968,3,"['Plugin', 'plugin']","['PluginDescriptor', 'plugin', 'plugins']"
Modifiability,"@cwhelan , I've addressed the comments in this commit. It's not very small due to the refactoring of `SVTYPE` and dealing with the new edge case cigars like `10S10I10M`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2376#issuecomment-278405353:86,refactor,refactoring,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2376#issuecomment-278405353,1,['refactor'],['refactoring']
Modifiability,"@cwhelan @tedsharpe @vruano @mwalker174 , I've organized the scripts for running the whole sv pipeline as it exists right now. There used to be a PR (if I recall correctly) but since that's outdated, why not an up-to-date one. Here's how to run it. 1. To create the cluster. ```; ./create_cluster.sh broad-dsde-methods sv-methods-1 broad-dsde-methods/sv; ```; where the 1st argument is the project name, 2nd argument is the cluster name, and the 3rd name is the place where input data lives. 2. To run the whole pipeline. ```; ./svCall.sh /Users/shuang/GATK/gatk sv-methods-1 /user/shuang/NA12878_PCR-_30X; ```; where the 1st argument is the location of my GATK directory and the 3rd argument is the location of all outputs on the cluster. So change them as necessary. The different stages called by the master script `svCall.sh` are (in order) `scanBam.sh` -> `assembly.sh` -> `alignAssembly.sh` -> `callVariants.sh`, which all take the same arguments. 3. To delete the whole cluster. ```; ./delete_cluster.sh sv-methods-1; ```; This avoids having to wait for the web-based Console's confirmation. One thing to note though, is that I've copied everything to a bucket at; ```; gs://broad-dsde-methods/sv/; ```; under a different project. We used to be developing under the project ""broad-dsde-dev"", but we are asked to move to project ""broad-dsde-methods"". So to run these scripts, you might need to switch to a different project via. ```; gcloud config set project broad-dsde-methods; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435:1447,config,config,1447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435,1,['config'],['config']
Modifiability,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2729:125,adapt,adapted,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729,1,['adapt'],['adapted']
Modifiability,@cwhelan I've made the suggested refactoring for breakpoint adjustment code.; Do you want to take a look again?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3571#issuecomment-338003549:33,refactor,refactoring,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3571#issuecomment-338003549,1,['refactor'],['refactoring']
Modifiability,"@cwhelan Thanks for the review! And I apologize for not clearly stating what problem is getting fixed here. I've addressed the comments in separate commit, changed the implementation, made more improvements that were discovered while reviewing the variants ; (https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest/tree/master/Evaluation/Analysis/masterVSfeature/notes.xlsx); The implemented fixes are:; * for removing the hard-coded/explicit mentioning of ""chr"" in non-canonical versions, it is now fixed in 5eff782e4d582d516004fba2cee7535d984b1540; * for contigs whose alignments paint ambiguous picture, i.e. multiple alignment configurations offer equally good explanation:; 	1. if only one configuration has all alignment with MQ above a specified threshold, it is favored; this is implemented in ecc31f5fbec4e524b401fc9474a3a1b7ab08c561; 	2. if one configuration has alignment to non-canonical chromosome that explains the contig better than would-be-event-inducing mappings to canonical chromosomes, the canonical mappings are saved but the better non-canonical mappings are saved as SA tag as in SAM spec, and the VCF record produced is annotated accordingly; this is implemented in 65cdb523a2f9fa2026334713fed45381d76ffc82; * fixed a bug where sometimes an assembly contig as several alignments, only one of which has non-mediocre MQ but at the sametime this alignment contains a large gap, such contigs were previously incorrectly filtered away, they are now salvaged by commit b6b2f197b112981e00efd9d415f010c024d31b36. So, for the FN variants (FN in the sense that they are captured in the stable version of our interpretation tool but now goes missing in the experimental interpretation tool); that were curated in the above-mentioned review, only the following ones are not salvaged, with plans or comments attached. ```; asm012854:tig00000	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem as face ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522:639,config,configurations,639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522,2,['config'],"['configuration', 'configurations']"
Modifiability,"@david-wb @popboy126 I'm having trouble reproducing the shutdown issue on our own cluster, I sometimes get the message `Shutdown hook called before final status was reported.` but the job status is SUCCEEDED. This happens if I run with the System.exit(0) or not. Could one of you test with this branch and let me know if it solves your issue? I think my cluster configuration must be different then yours in some way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3400#issuecomment-319778125:362,config,configuration,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3400#issuecomment-319778125,1,['config'],['configuration']
Modifiability,@david-wb Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368:21,plugin,plugin,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368,2,['plugin'],['plugin']
Modifiability,@davidadamsphd 1 comment. Either address that now or in your next refactoring commits. Otherwise :+1: merge at will.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125620920:66,refactor,refactoring,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125620920,1,['refactor'],['refactoring']
Modifiability,"@davidadamsphd Sure, here is a quick guide to the code:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:524,refactor,refactoring,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['refactor'],['refactoring']
Modifiability,@davidadamsphd what should we do with this PR? migrate to https://github.com/broadinstitute/hellbender-dataflow? Adapt to spark? both?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/851#issuecomment-143915796:113,Adapt,Adapt,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/851#issuecomment-143915796,1,['Adapt'],['Adapt']
Modifiability,"@davidbenjamin ; This will make the WDL default to producing a MAF from Funcotator instead of a VCF. There is no flag to switch between the two, so if you know of people that still want VCF output, please speak up now... Can you review the WDL and autotest-WDL changes? This has been tested in FireCloud and looks good (minus an issue that I have already filed), though I had to manually review. The Method configuration in FireCloud still uses the GATK jar override for this. Just in case you wanted to run it. Otherwise, we should blank out that parameter. As a reminder, I tested:; - mutect2.wdl: manually on local backend and FireCloud; - mutect2_nio.wdl: manually on FireCloud. Please review both WDL files. @jonn-smith Could you review the rest? I.e. the bug fixes. Apologies that I did not split these into two PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788:407,config,configuration,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788,1,['config'],['configuration']
Modifiability,"@davidbenjamin @droazen unfortunately the new PON does not make up for the precision loss introduced in v4.1.9.0.; In v4.4.0.0 we get just 2 fewer FP SNVs in our performance evaluation, compared to the old PON.; Benchmarking results in WES tumor-normal mode, HCC1395 benchmark, and:. - v4.1.8.1 (last release with high SNV precision), v4.1.9.0 (first release affected by precision drop), v4.4.0.0 (current release); - oldPON: 1000g_pon.hg38.vcf.gz, newPON: mutect2-hg38-pon.vcf.gz; ![FD_TN_4181_FD_TN_4181_oldPON_FD_TN_4181_newPON_FD_TN_4190_FD_TN_4190_oldPON_FD_TN_4190_newPON_FD_TN_4400_FD_TN_4400_oldPON_FD_TN_4400_newPON](https://user-images.githubusercontent.com/15612230/236126940-9fc26627-260a-43c2-b409-69fbcec6ad47.png). Any chance to get this issue fixed? With Mutect3 not being available and v4.1.8.1 being affected by the log4j vulnerability, it is quite regrettable to be stuck with inferior precision. Extended methods, code, and data to reproduce the issue are here: ; [https://github.com/ddrichel/Mutect2_calling_performance_bug](https://github.com/ddrichel/Mutect2_calling_performance_bug)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043:916,Extend,Extended,916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043,1,['Extend'],['Extended']
Modifiability,"@davidbenjamin @jamesemery As discussed in person, we should consider the possibility that this behavior is by design, and the band pass filter is deliberately being centered on the base before the high-quality soft clip begins (since this code is inherited from GATK3). Let's talk about this before hitting merge on this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5767#issuecomment-470257519:248,inherit,inherited,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5767#issuecomment-470257519,1,['inherit'],['inherited']
Modifiability,"@davidbenjamin @takutosato this involves a bit of a refactor of the ConcordanceWalker classes, so would appreciate a review from one of you for that part of the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7328#issuecomment-867612076:52,refactor,refactor,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7328#issuecomment-867612076,1,['refactor'],['refactor']
Modifiability,@davidbenjamin Can I assume that you didn't change `NearbyKmerErrorCorrector` (other than renaming and refactoring),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6470#issuecomment-592679911:103,refactor,refactoring,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6470#issuecomment-592679911,1,['refactor'],['refactoring']
Modifiability,@davidbenjamin Have you had a chance to think about this one yet? Do you think it will be possible to refactor so that doubles are not used as keys? @frank-y-liu was asking about this today.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4290#issuecomment-434011333:102,refactor,refactor,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4290#issuecomment-434011333,1,['refactor'],['refactor']
Modifiability,@davidbenjamin I have refactored this branch to account for changes to the codebase adjacent to this code. In the interest of not possibly harming any of the old results I have made this a toggle and I have also made the setting apply symmetrically to tails and heads and added a few simple tests in the existing framework.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6113#issuecomment-640870830:22,refactor,refactored,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6113#issuecomment-640870830,1,['refactor'],['refactored']
Modifiability,"@davidbenjamin I looked into this more. From what I can tell, when a site is polymorphic, GenotypeGVCFs is going to trim to found alleles. Under normal conditions, when a site is not polymorphic, it doesnt get output. When output is forced (--force-output-interverals or output-all-sites), then the VC is just output as-is. This PR initially fixed the bug in removeNonRefAlleles(). I think GenotypeGVCFs should also prune unused alleles. If the site is not polymorphic, by definition I think that means removing all ALTs? I just checked in a few tweaks:. 1) I switched from the original test gVCF data I added to use an existing gVCF from GATK. This has the advantage of containing actual callable sites, not adding new data, and contains existing multi-allelic sites. . 2) I added code to GenotypeGVCFs so unused alleles will be trimmed as part of removeNonRefAlleles. I could understand if you want to change the name of that method now. This is currently always true; however, I could see a rationale for making --retain-unused-alleles-from-force-output-sites as a command line argument (i'd argue to default to false).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-577859097:77,polymorphi,polymorphic,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-577859097,3,['polymorphi'],['polymorphic']
Modifiability,"@davidbenjamin I think so, but I didn't test beyond the cases at the ends of chromosomes so it's possible that I didn't fix it completely. . I can come back to this after I review your adaptive pruning PR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3944#issuecomment-443735719:185,adapt,adaptive,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3944#issuecomment-443735719,1,['adapt'],['adaptive']
Modifiability,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394,1,['refactor'],['refactoring']
Modifiability,"@davidbenjamin I thought you had implemented something a little more sophisticated initially, but then reverted to the ReCapSeg caller for some reason?. Anything that is relatively simple to implement yet sufficiently more principled than the ReCapSeg caller would be reasonable for this rewrite. Thought you might've had something that fit the bill originally, but maybe I'm remembering wrong. If so, then we can try leaving it as is for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206:288,rewrite,rewrite,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324142206,1,['rewrite'],['rewrite']
Modifiability,@davidbenjamin I will review this today. Thanks for doing this refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5840#issuecomment-477185044:63,refactor,refactoring,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840#issuecomment-477185044,1,['refactor'],['refactoring']
Modifiability,"@davidbenjamin I've significantly refactored the production code, see the last commit. Most of this refactoring was to done make the code for the accounting of different modes (SNP/INDEL/both x BGMM/python x non/allele-specific) more minimal and straightforward. I've also combined the score/apply steps using the TwoPassVariantWalker. There's still lots of documentation, cleanup, and hardening/validation to be done, but most of the key methods and design choices have been documented, so I think it could be worth a quick review at this stage. Again, no need to nitpick code-style details, etc. (unless you really want to!) In the meantime, I'm going to do some more testing/tieout to make sure the refactor didn't break anything. This covers ~1800 LOC, which is roughly 50% of the equivalent VQSR code. Even modulo the remaining work just mentioned, which may add a few hundred LOC, I think this is a decent improvement---additional functionality, stability, etc. notwithstanding!. There's stubs for adding the truth-sensitivity conversion you proposed---should be pretty straightforward. I think it should also still be pretty easy for future pushes to add features like extraction/downsampling of unlabeled data, etc., but please do keep an eye out for design choices that may ultimately be constraining.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044836946:34,refactor,refactored,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044836946,3,['refactor'],"['refactor', 'refactored', 'refactoring']"
Modifiability,@davidbenjamin Interesting. In #6634 I added a fix similar to this as now HC is holding onto bases that were clipped by the various layers of low edge and low quality end removal code in the form of soft clips. I suspect your fix might be a little more elegant than mine. Could we hold off on merging this for a bbit to save ourselves the trouble when we try to rebase?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6792#issuecomment-692226336:132,layers,layers,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6792#issuecomment-692226336,1,['layers'],['layers']
Modifiability,"@davidbenjamin It's actually worth exploring, I think -- with a Spark implementation it is much easier for end-users running the GATK directly to run with multiple cores. Multi-process interval-based parallelism is only trivial when using a pipeline runner. . There is already a beta `HaplotypeCallerSpark` implementation -- it would probably be pretty easy to adapt it to call into a `Mutect2Engine` instead of a `HaplotypeCallerEngine`, I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-379885003:361,adapt,adapt,361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-379885003,1,['adapt'],['adapt']
Modifiability,"@davidbenjamin LODs are fixed, but I'm not super happy with them. They fluctuate a lot, making for a big GVCF. How easy would it be to modify the likelihood calculation to integrate over all AFs greater than some threshold of interest? I'm hoping that would produce more stability. Just to give you an idea of the fluctuation, below are some lines from my integration test VCF where I block anything less than -2 and between -2 and 0. I tried a few other thresholds, but it's just a lot of variability. I have a hunch it has to do with the minimum base quality in the pileup. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chrM 1 . G <NON_REF> . . END=4 GT:DP:MIN_DP:TLOD 0/0:67:48:-1.958e+00; chrM 5 . A <NON_REF> . . END=5 GT:DP:MIN_DP:TLOD 0/0:107:107:-2.033e+00; chrM 6 . C <NON_REF> . . END=6 GT:DP:MIN_DP:TLOD 0/0:123:123:-1.603e+00; chrM 7 . A <NON_REF> . . END=8 GT:DP:MIN_DP:TLOD 0/0:135:135:-2.138e+00; chrM 9 . G <NON_REF> . . END=9 GT:DP:MIN_DP:TLOD 0/0:138:138:-1.975e+00; chrM 10 . T <NON_REF> . . END=13 GT:DP:MIN_DP:TLOD 0/0:178:154:-2.226e+00; chrM 14 . T <NON_REF> . . END=15 GT:DP:MIN_DP:TLOD 0/0:208:205:-1.974e+00; chrM 16 . A <NON_REF> . . END=23 GT:DP:MIN_DP:TLOD 0/0:259:218:-2.424e+00; chrM 24 . A <NON_REF> . . END=25 GT:DP:MIN_DP:TLOD 0/0:312:310:-8.945e-01; chrM 26 . C <NON_REF> . . END=26 GT:DP:MIN_DP:TLOD 0/0:317:317:-2.509e+00; chrM 27 . C <NON_REF> . . END=27 GT:DP:MIN_DP:TLOD 0/0:335:335:-1.962e+00; chrM 28 . A <NON_REF> . . END=50 GT:DP:MIN_DP:TLOD 0/0:492:343:-2.821e+00; chrM 51 . T <NON_REF> . . END=51 GT:DP:MIN_DP:TLOD 0/0:700:700:-3.808e-01; chrM 52 . T <NON_REF> . . END=63 GT:DP:MIN_DP:TLOD 0/0:822:722:-2.943e+00; chrM 64 . C <NON_REF> . . END=64 GT:DP:MIN_DP:TLOD 0/0:909:909:-1.492e+00; chrM 65 . T <NON_REF> . . END=86 GT:DP:MIN_DP:TLOD 0/0:1064:938:-3.065e+00; chrM 87 . A C,<NON_REF> . . DP=942;ECNT=8;POP_AF=5.000e-08,5.000e-08;TLOD=-2.463e+00,-2.668e+00 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_PO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-437501517:490,variab,variability,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-437501517,1,['variab'],['variability']
Modifiability,"@davidbenjamin Looks like the variable for this arg is `final` and `static`. It probably should be neither, but making it non-static looks it it would require retaining a reference to `M2FiltersArgumentCollection` in `Mutect2FilteringEngine`. I'll leave it to you to figure out the right fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5978#issuecomment-498667359:30,variab,variable,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978#issuecomment-498667359,1,['variab'],['variable']
Modifiability,@davidbenjamin My review is complete. Lots of tiny documentation issues. I'd refactor that unit test with a data provider as well.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/767#issuecomment-126069170:77,refactor,refactor,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/767#issuecomment-126069170,1,['refactor'],['refactor']
Modifiability,"@davidbenjamin Since this issue was related to changes from one of your refactoring PRs, would you mind reviewing this branch and adding a quick unit test? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8445#issuecomment-1658659432:72,refactor,refactoring,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8445#issuecomment-1658659432,1,['refactor'],['refactoring']
Modifiability,"@davidbenjamin This branch seems to have a lot of unrelated refactoring bundled in with the substantive fix for `Mutect2`, and some of this refactoring seems problematic to me (eg., deletion of some downsampler API methods that will be needed when `ReadWalker` downsampling is ported). . Since mixing refactoring into the same PR as behavioral changes is asking for trouble as a general rule, could I ask you to strip out everything except the substantive change to `PositionalDownsampler` from this PR? Happy to do a quick review once this is done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-314545787:60,refactor,refactoring,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-314545787,3,['refactor'],['refactoring']
Modifiability,@davidbenjamin This is the issue I was talking about -- the relevant code is in `MannWhitneyU`:. ```; double sumOfAllSmallerBins = histo.get(testStatU).getValue() / 2.0;. for (final Histogram.Bin<Double> bin : histo.values()) {; if (bin.getId() < testStatU) sumOfAllSmallerBins += bin.getValue();; }. return sumOfAllSmallerBins / histo.getCount();; ```. Where `testStatU` is a double. This has caused issues like the one reported in https://github.com/broadinstitute/gatk/pull/5190. I'm wondering whether the class can be refactored to not use a double as a key.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4290#issuecomment-422126933:522,refactor,refactored,522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4290#issuecomment-422126933,1,['refactor'],['refactored']
Modifiability,"@davidbenjamin This seems to now be a problem for `PalindromeArtifactClipReadTransformer` as well. In cases where there are soft clips you can miscalculate the `adaptorBoundary`. For mitochondria this causes edge case problems if you're on the end of the contig. I suppose for that specific case it could be fixed in `PalindromeArtifactClipReadTransformer` directly, but it makes more sense to happen in `getAdaptorBoundary` (if cost was no issue).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-413975723:161,adapt,adaptorBoundary,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-413975723,1,['adapt'],['adaptorBoundary']
Modifiability,"@davidbenjamin commented on [Sat Dec 19 2015](https://github.com/broadinstitute/gatk-protected/issues/260). We use (linear) PCA to map the PoN, each datum of which is a high-dimensional vector over targets, to a low-dimensional manifold. Do we really believe that this manifold is simply a hyperplane?. Concretely, suppose the data really lives on a 4-dimensional curved manifold. Due to the curvature, we might require many more, say 20, flat dimensions to encompass a significant amount of the PoN's variance. What this means is that we lump a huge amount of noise in with the true signal. Non-linear alternatives worth investigating include kernel PCA -- nice because like all machine learning things involving the kernel trick you get to recycle almost all of your mathematical and algorithmic machinery, denoising autoencoders, and Gaussian process latent variable models. ---. @davidbenjamin commented on [Wed Dec 23 2015](https://github.com/broadinstitute/gatk-protected/issues/260#issuecomment-166804219). Linear PCA could be sufficient if the PoN samples are tightly clustered about their mean so that variance is a small perturbation that can be treated linearly. I don't think we know enough about PoNs to judge what actually occurs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2830:861,variab,variable,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2830,1,['variab'],['variable']
Modifiability,"@davidbenjamin comparing [better error bars for samples with small contamination](https://github.com/broadinstitute/gatk/pull/7003) version to master 4.2.0.0. - running on large set of probably considered as a small panel with high read depth.; percent of observations with all compared variables equal 88%, values unequal 12%. We can conclude that over all the Zero contamination remains Zero contamination but with non zero stdError (accepted after the binary searched). . **only 0.5%** of samples which were reported zero on version 4.2.0.0 are now reported as non zero contamination when the stdError was not accepted and the min maf iteration continued. ### Need to distinguish between **true zero contamination** ie was not found after running all maf iterations over all strategies (HOMO ALT,REF,UNSCRUPULOUS_HOM_REF) to a zero contamination after accepting stdError of Zero contamination of a single strategy. it is about **15%** of samples which will be reported non zero if continuing the maf iterations . @itaibeno to test on larger panel and report here. @davidbenjamin - could you consider running all maf iterations and report contamination per strategy,minMaf and loci if exists?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7177#issuecomment-856187340:287,variab,variables,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177#issuecomment-856187340,1,['variab'],['variables']
Modifiability,"@davidbenjamin, @fleharty agree with the implementation of PR #7003. ; I'm running that version comparing to samples previously reported 0 contamination and 0 std error.; will update here. One concern on the math Eq37 - Eq.42 calculate std (chi).; the math not taking into account the sample size (n) i.e number of homs.; note that we are starting with GetPileupSummaries output ~5k loci , ; filterSitesByCoverage keep ~300 loci ; filter segments + MAF keep variable number of loci depend on panel size etc. this number can vary and the question should the implementation will take it into account? i.e. reject the sample if n<NUM_LOCI (5,10,50???). **Thinking on the end user observing the Pair( contamination,stdError) and his interpretation on that...** . Probably adding the number of homs sites + strategy to final output as well as listing the pileups for the homsites will let the end user better understand the sample contamination output or even to find contaminant of a batch. **suggesting the following update to output file:**; sample	contamination	error; TUMOR	0.019245855721094312	0.0036809520099731763. sample, **strategy, n_loci,** contamination, error; TUMOR,HOM_ALT,M. list of homosites used; **contig	position	ref_count	alt_count	other_alt_count	allele_frequency**; PileupSummary1; PileupSummary2; ...; PileupSummaryM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7177#issuecomment-833821730:458,variab,variable,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177#issuecomment-833821730,1,['variab'],['variable']
Modifiability,"@davidbenjamin, looks like you may be able to use `AF` when `AD` is missing at least for `VCFCodec`. Check the output of SelectVariants in the [comment](https://github.com/broadinstitute/gatk/issues/6744#issuecomment-674291742) above, looks like `AF` is being correctly handled by `VCFCodec`, whereas `BCFCodec` still suffers from all elements getting dropped after encountering the missing value `.`. As for using `GERMQ` annotation, no combination is specified to GenomicsDB, so it gets dropped from being included in the `INFO` fields currently. If you want to specify the combination(choose from `none`, `sum`, `mean`, `median`, `element_wise_sum`, `concatenate` or `histogram_sum`), see [examples in GenomicsDBUtils.java](https://github.com/broadinstitute/gatk/blob/356a9ddb50f1467717e87ee6c5ef1c2f084694d3/src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java#L51) to extend this to `GERMQ`. Also see outstanding PR #6514 for adding reasonable combination defaults for known info fields.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-680149610:902,extend,extend,902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-680149610,1,['extend'],['extend']
Modifiability,@davidbernick Can we inherit permissions to launch things from github like our jenkins edit permissions do?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-233035843:21,inherit,inherit,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-233035843,1,['inherit'],['inherit']
Modifiability,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2977:123,enhance,enhancement,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977,1,['enhance'],['enhancement']
Modifiability,"@droazen , I got a genomicsdb.jar from @kgururaj and just tried out the GenomicsDB cloud tests. The call stack that I got from my test run in nalini_new_genomicsdb_jar branch mentions that we do need the **fs.gs.project.id** hadoop configuration set. The google service json I use for our internal testing has this key, but the Hellbender service json does not. . Any ideas on how to get this key for the tests? Would this value be HELLBENDER_TEST_PROJECT? How is it being made available to the spark cloud tests for example? I do see it being configured in src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java. ```; hdfsBuilderConnect(forceNewInstance=1, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: **fs.gs.project.id**; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-427509641:232,config,configuration,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-427509641,6,"['Config', 'config']","['ConfigurationUtil', 'configuration', 'configure', 'configured']"
Modifiability,"@droazen - a proposal for that, which will be great for my toolkit too, is to make `GATKTool` params an argument collection which defaults to the ones in the tool now, but can be change in a tool-basis. For example, if I have a `VariantWalker` which does not use any read-source, disabling all params for reads will be nice for re-use the `VariantWalker` interface without allowing the user to pass something that it is not used at all. It is not enough to provide a way to require or not a source, but to completely remove from the command line the ability to get that argument. I guess that's what it is required also for the CNV tools (correct me if I am wrong, @samuelkle), to be able to change that behaviour and to being able to provide custom documentation/arguments (re-factor the reads input `-I` to be other kind of input). I did something similar for the `ReadFilter` plugin to change the documentation and hide some arguments in my tools using it. Let me know if I can help with something in this direction, because it will be useful for me too...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358254662:879,plugin,plugin,879,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358254662,1,['plugin'],['plugin']
Modifiability,"@droazen - any news about thi one? I would like to have some control over the CLP class arguments, such as the config-file one. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-365624479:111,config,config-file,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-365624479,1,['config'],['config-file']
Modifiability,@droazen - maybe for the patch of the upplication implementation it might be worthy to have a look to [epam/htsjdk-s3-plugin `S3SeekableStream`](https://github.com/epam/htsjdk-s3-plugin/blob/master/S3HtsjdkPlugin/src/main/java/com/epam/cmbi/s3/S3SeekableStream.java). I will suggest that at the original issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-375368881:118,plugin,plugin,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-375368881,2,['plugin'],['plugin']
Modifiability,@droazen :man_facepalming: I wish we had thought of this ahead of time so we could have done the appropriate commit message rewrites...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2814#issuecomment-306012234:124,rewrite,rewrites,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2814#issuecomment-306012234,1,['rewrite'],['rewrites']
Modifiability,"@droazen @cmnbroad Thanks this is helpful. Having @Arguments in the plugin helps. There is one blocker I'm not seeing a way around. In GATK3 the VariantAnnotation and GenotypeAnnotation classes were passed the GATK3 equivalent of FeatureContext. My use case is annotating genoype concordance between the VCF being annotated and another VCF. Within annotate(), I think I would like to call something like featureContext.getValues() in order to query any variants in my reference VCF for this site. Is there another solution to accomplish this kind of thing from within VariantAnnotation.annotate()? . For example:. ```. public class GenotypeConcordance extends GenotypeAnnotation {; public static final String KEY = ""GTD"";; public static final String D_KEY = ""REF_GT"";. @Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); public FeatureInput<VariantContext> referenceVcf = null;. @Override; public List<String> getKeyNames() {; return Arrays.asList(KEY, D_KEY);; }. @Override; public void annotate(ReferenceContext ref, VariantContext vc, Genotype g, GenotypeBuilder gb, AlleleLikelihoods<GATKRead, Allele> likelihoods) {; if (referenceVcf == null) {; throw new IllegalArgumentException(""Must provide a VCF with reference genotypes!"");; }. if (g.isFiltered() || g.isNoCall()) {; return;; }; ; //TODO: how to accomplish this?; List<VariantContext> list = featureContext.getValues(referenceVcf);; if (list == null || list.isEmpty()){; return;; }. for (VariantContext c : list){; Genotype refGenotype = c.getGenotype(g.getSampleName());; if (refGenotype != null && !refGenotype.isFiltered() && !refGenotype.isNoCall()) {; if (!refGenotype.sameGenotype(g)) {; gb.attribute(KEY, ""1"");; gb.attribute(D_KEY, refGenotype.getGenotypeString());; }; else {; gb.attribute(KEY, ""0"");; }. }; }; }. @Override; public List<VCFFormatHeaderLine> getDescriptions() {; return Arrays.asList(; new VCFFormatHeaderLine(KEY, 1, VCFHeaderLineType.Integer, ""Flags g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-749243236:68,plugin,plugin,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-749243236,2,"['extend', 'plugin']","['extends', 'plugin']"
Modifiability,"@droazen @sooheelee Anything involving adaptors is not my forte but here goes my best shot. The basic idea of `getAdaptorBoundary` is to find the end of the insert, that is, where the original fragment ends and the adaptor begins. Since the 5' ends of the paired reads define the bounds of a fragment, its approach is to look for the start of the forward strand mate if our read is on the reverse strand, or the end of a reverse strand mate if our read is on the forward strand. This protects us from the possibility of a read longer than its insert. So far, so good, I think. One possible source of error is that these boundaries are determined by the *alignment* starts of the paired reads. This might clip too much if a read has a soft-clipped end and it turns out that these soft-clipped bases were e.g. a real insertion. Then the soft-clipped bases would be considered part of the adaptor. However, this can't do the more harmful thing of failing to clip an adaptor as far as I can tell. So it looks to me like the only issue is the edge case of falsely soft-clipped bases in very short inserts, in which case we hard clip the soft clips and in theory could lose some sensitivity. . That said, @sooheelee may have something else in mind, and @yfarjoun will have a better-informed opinion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-311548963:39,adapt,adaptors,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-311548963,4,['adapt'],"['adaptor', 'adaptors']"
Modifiability,"@droazen Changes to the native code include removing SSE code, ifdef'ed code for profiling, and unused sandbox code. The goal is to reduce the amount of C code we need to maintain. Let me know if you have any concerns. We can have a code review after integrating into HaplotypeCaller. @lbergelson Thanks for helping debug the build. I doubt the code will build as is with clang, it currently uses gcc intrinsics to test machine capabilities. I added support for GATK_SKIP_NATIVE_BUILD env variable. Submitting changes to get debug info from Travis soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185850134:103,sandbox,sandbox,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185850134,2,"['sandbox', 'variab']","['sandbox', 'variable']"
Modifiability,"@droazen How hard would it be to have a plug-in framework for fragment-based likelihoods parallel to what we already have for read likelihoods? It would be nice to specify all annotations with `-A`. Barring that, it won't be hard to write some fragment-based likelihoods and hard-code them into M2 and HC, but it seems clumsy to do so.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6096#issuecomment-576800907:40,plug-in,plug-in,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096#issuecomment-576800907,1,['plug-in'],['plug-in']
Modifiability,@droazen I can look into this. I suspect the situation may have evolved since we last looked at it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-337352568:64,evolve,evolved,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873#issuecomment-337352568,1,['evolve'],['evolved']
Modifiability,"@droazen I had put in https://github.com/broadinstitute/gatk/issues/3899 over a week ago and just now assigned you to decide if we want to consider instituting a symlink/environmental variable `gatk` that is callable from anywhere in the Docker. . As someone with a newbie perspective, it is easier for me to grasp `gatk` represents the script to which I must provide the path to (e.g. ~/Downloads/gatk/gatk) than to understand that `./gatk` must be run in a particular folder.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3911#issuecomment-350787427:184,variab,variable,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3911#issuecomment-350787427,1,['variab'],['variable']
Modifiability,@droazen I responded to your comments. I've additionally some changes to the readme to include information about the test environment variables. Let me know if there are horrible spelling errors that I somehow missed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278802390:134,variab,variables,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399#issuecomment-278802390,1,['variab'],['variables']
Modifiability,@droazen I think the BCF code is broken here too. The problem is fundamental to htsjdk. CombineVariants almost certainly has the same or similar problems because it's fundamental to combining vcfs and the fact that htsjdk doesn't handle partially empty lists. Bcftools likely has similar issues. Or loading the correct output from bcftools will recreate the issuue. What about fixing the combine operation so it can substitute default missing values with a per attribute configuration for what value to substitute?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646:471,config,configuration,471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646,1,['config'],['configuration']
Modifiability,"@droazen I think we should try out codecov. I've played with it a little bit and it seems like it would be an improvement over the abysmal coveralls at the very least. At the most it seems like it might be actually useful during pull requests because it seems like it can produce a useful view of newly covered and uncovered lines. It might need some configuration to be maximally useful with minimal spam, and it's hard to know exactly what configuration it should be set with until we have it set up in master. It reports lower overall code coverage than coveralls did because it includes a notion of ""partial coverage""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2167#issuecomment-247643957:351,config,configuration,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2167#issuecomment-247643957,2,['config'],['configuration']
Modifiability,"@droazen I've finally been able to move ahead on this and have a question on what to expect for plugins that define arguments. I have two plugins that each share an argument. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. ```. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ​; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. ​. .etc......; }. Gpublic class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; ​@Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); ​public FeatureInput<VariantContext> referenceVcf = null;; }. ```. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. . Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-794444380:96,plugin,plugins,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6929#issuecomment-794444380,8,"['extend', 'plugin']","['extends', 'plugin', 'plugins']"
Modifiability,"@droazen I've rebased this onto the current master. Unfortunately, that seems to have totally blown away the comments that were on one of the existing commits... . As best as I know I've addressed all of your comments. I did a major rewrite on `HomRefBlock` since you last looked at it though, so you may have new comments. . It also hasn't done a good job understanding that the files that moved from engine to utils are the same ones, so it's producing garbage diffs of them. I can try to move them in the history if that would be helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/859#issuecomment-145652124:233,rewrite,rewrite,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/859#issuecomment-145652124,1,['rewrite'],['rewrite']
Modifiability,@droazen OK - the tests in `DataSourceUtilsUnitTest` pass for me locally. I had to substantially refactor the tests and I added some log messages and some `final` modifiers to clean up some other Intellij warnings.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6807#issuecomment-694956600:97,refactor,refactor,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6807#issuecomment-694956600,1,['refactor'],['refactor']
Modifiability,@droazen Refactored. Any additional comments or are we ok?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/685#issuecomment-123842259:9,Refactor,Refactored,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/685#issuecomment-123842259,1,['Refactor'],['Refactored']
Modifiability,"@droazen Thank you for the confirmation that HaplotypeCaller performs separate filtering passes on the read mapping qualities, and that the code on line 729 of HaplotypeCallerEngine.java (method ```filterNonPassingReads()``` ) is indeed executing subsequent to the ```MappingQualityReadFilter```. May I suggest, however, that MAPQ values less than 20 might not necessarily lead to an increase in FP variant calls? My understanding is that HaplotypeCaller uses MAPQ values only in a nonparametric rank sum test, in which case MAPQ is treated as an ordinal. This seems appropriate since the magnitude of a MAPQ value depends both on the data and on the computational model the read aligner uses to calculate it. With this in mind, a set of mappings with MAPQ in a lower range (e.g., ```--minimum-mapping-quality 10``` and a correspondingly lower ```--maximum-mapping-quality``` as well) might very well be appropriate for variant calling. So changing the semantics of ```MappingQualityReadFilter``` or parameterizing the currently-hardwired MAPQ range would enable additional control without affecting performance. @jamesemery I will watch for the HaplotypeCaller update that implements that functionality. And if you have a moment, could you please point me to the code that might be adversely affected by decreasing the low-end MAPQ threshold? I might have some ideas about that (or not!)... Thanks again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278:1000,parameteriz,parameterizing,1000,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278,1,['parameteriz'],['parameterizing']
Modifiability,"@droazen This should be ready for a second review. I refactored it. I don't have direct tests for tiledb in FeatureDataSource tests since I included them in GenomicsDBIntegration test, let me know if you think we need them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-235097132:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-235097132,1,['refactor'],['refactored']
Modifiability,"@droazen Yes, this is because the native PairHMM is using single precision floating point and Flush To Zero (FTZ), while the Java PairHMM is using double precision and not using FTZ. I planned to address this when we integrate native PairHMM into HaplotypeCaller. It looks like the time is here. For now, you can configure native PairHMM to use double precision and not use FTZ. With the diff below, the VCFs from native and Java PairHMM are exactly the same. In the future, we need to enable FTZ in the Java PairHMM and provide the option to use single precision or double precision in native PairHMM. ``` diff; --- i/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/LoadTimeInitializer.cc; @@ -23,7 +23,7 @@ LoadTimeInitializer::LoadTimeInitializer() //will be called when library is loa; //Very important to get good performance on Intel processors; //Function: enabling FTZ converts denormals to 0 in hardware; //Denormals cause microcode to insert uops into the core causing big slowdown; - _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);; + // _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);. //Profiling: times for compute and transfer (either bytes copied or pointers copied); m_compute_time = 0;; diff --git i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc w/src/main/cpp/VectorLoglessPairH; index f45153e..70cf54f 100644; --- i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; @@ -6,7 +6,7 @@. using namespace std;. -bool use_double = false;; +bool use_double = true;. //Should be called only once for the whole Java process - initializes field ids for the classes JNIReadDataHolderClass; //and JNIHaplotypeDataHolderClass; diff --git i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java w/src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083:313,config,configure,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083,1,['config'],['configure']
Modifiability,"@droazen a few minor tests I missed, but this should be good to go otherwise. I did the refactor for Evoquer and I think everything else has been addressed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-514690456:88,refactor,refactor,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-514690456,1,['refactor'],['refactor']
Modifiability,"@droazen and @cmnbroad: i completely understand that this is outside the main GATK dev cycle and priorities; however, do you have any guess as to when you might be able to review? I dont know how active development is, but I'd especially like to get that change in VariantWalker and MultiVariantWalker (which are currently really simple refactors) in before other development on them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-378318089:337,refactor,refactors,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-378318089,1,['refactor'],['refactors']
Modifiability,"@droazen here you go. It turns out that this variable had no business having class scope, so I made it local to the one method that uses it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2547:45,variab,variable,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2547,1,['variab'],['variable']
Modifiability,"@droazen sure. This PR updates both `BucketUtils.getPathOnGcs` and `IOUtils.getPath` (the latter indirectly) to create a Google Cloud Storage filesystem with a default reopen set to 3. Anyone opening the given `Path` (or even a `Path` derived from it, e.g. via `subpath`) will have the retries enabled. `addPrefetcher` wraps an existing `Path`, so it inherits the underlying `Path`'s retry behavior. Classes within htsjdk that create a `Path` from a String, without using our utility code, would indeed not set retries and errors on those files would not get the benefit of our extra retry. The exception to that is the Spark code path with `NioBam`: this goes via `ReadsIterable`, which also sets the retries. My understanding is that the normal way to open BAMs for tools always creates the `Path` object when parsing the command line, as in e.g. `ReadInputArgumentCollection::GetReadIndexPaths()`. That code path that calls `IOUtils.getPath` (and thus sets the retries). I would expect that the added support for VCF follows the same style, though it looks like it doesn't. If there's a code path that I missed where you think we need retries then please let me know so I can add it!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-289585791:351,inherit,inherits,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-289585791,1,['inherit'],['inherits']
Modifiability,"@droazen, I have introduced --genomicsdb-use-vcf-codec argument defaulting to false. This will allow the user to choose between VCFCodec and BCF2Codec for GenomicsDB streams. ```; --genomicsdb-use-vcf-codec:Boolean; Use VCF Codec Streaming for data from GenomicsDB Default value: true. Possible values:; {true, false} ; ```. Also, hooked --max-genotype-count arg from GenotypeGVCFs/GnarlyGenotyper to GenomicsDB Export Configuration and introduced --genomicsdb-max-genotype-count to be used by tools like SelectVariants that do not use the Genotype ArgumentCollection. This will help with Issue #6275. ```; --genomicsdb-max-genotype-count:Integer; Maximum number of genotypes to consider at any site Default value: 1024.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6305#issuecomment-583536862:419,Config,Configuration,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6305#issuecomment-583536862,1,['Config'],['Configuration']
Modifiability,"@droazen, if it's true we can merge the headers then I suppose that may be a possibility. I'm a little bit dubious, though, because one of the pieces of information is the reference index to reference name mapping, and I didn't think we required those to match. Or do we?. Keep in mind however that static variables aren't serialized, so we'd still have to include some fancy code to make sure that every worker has the header. And since workers may be added dynamically by Dataflow's auto-tuning, we may have to call that code at the beginning of every transform... just like addHeaders. . So we would have some ""action-at-a-distance"" type of code, with statics that may or may not have different values depending on which computer you're on. This sounds like it may be just as error-prone and headache-causing as the other approach. At this point I'm still leaning towards manually putting the headers back. I think this makes the code less surprising than using globals (statics are a form of global variable after all) and so less likely to confuse someone new to the codebase.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141155796:306,variab,variables,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141155796,2,['variab'],"['variable', 'variables']"
Modifiability,"@droazen, we have a free GCS account, so it is possible that Hadoop requires extra configuration for authenticating/connecting with the HELLBENDER travis service account. Can anyone help here? This the code we have for connecting to GCS via Hadoop. ```; hdfsFS gcs_connect(struct hdfsBuilder *builder, const std::string& working_dir) {; char *gcs_creds = getenv(""GOOGLE_APPLICATION_CREDENTIALS"");; if (gcs_creds) {; value = parse_json(gcs_creds, ""project_id""); // free value after hdfsBuilderConnect as it is shallow copied.; if (value) {; hdfsBuilderConfSetStr(builder, ""google.cloud.auth.service.account.enable"", ""true"");; hdfsBuilderConfSetStr(builder, ""google.cloud.auth.service.account.json.keyfile"", gcs_creds);; hdfsBuilderConfSetStr(builder, ""fs.gs.project.id"", value);; }; }. if (working_dir.empty()) {; hdfsBuilderConfSetStr(builder, ""fs.gs.working.dir"", ""/"");; } else {; hdfsBuilderConfSetStr(builder, ""fs.gs.working.dir"", working_dir.c_str());; }. // Default buffer sizes are huge in the GCS connector. GenomicsDB reads/writes in smaller chunks,; // so the buffer size can be made a little smaller.; hdfsBuilderConfSetStr(builder, ""fs.gs.io.buffersize.write"", ""262144"");. hdfsFS hdfs_handle = hdfsBuilderConnect(builder);; free(value);; return hdfs_handle;; }; ```. This is the error from Travis logs-; ```; Running Test: Test method testWriteToAndQueryFromGCS(org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest); hdfsBuilderConnect(forceNewInstance=1, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:210); at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:75); at com.google.cloud.hadoop.fs.gcs.GoogleHadoo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422915888:83,config,configuration,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-422915888,1,['config'],['configuration']
Modifiability,"@droazon Yes, I think we should keep this open. The VCF header refactoring will be step one on the road to fixing this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1116#issuecomment-288714998:63,refactor,refactoring,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1116#issuecomment-288714998,1,['refactor'],['refactoring']
Modifiability,"@felixm3 The bioconda environment doesn't actually configure the gatk conda environment (it installs gatk, but not the python dependencies required for CNNScoreVariants). You need to set up the gatk conda environment, as described in the Python Dependencies section in the README.md file: https://github.com/broadinstitute/gatk#readme.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1275000632:51,config,configure,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1275000632,1,['config'],['configure']
Modifiability,"@fleharty this is a rebased version of the 17cadfa399643877c70ba830d0b4abf9e5b159a9 branch used to generate the Pf7 CNV call set. There are two minor changes: a) one to remove spurious negative dCR estimates reported by gCNV, which were negatively affecting genotyping of HRP2/3 deletions, and b) updating sklearn to the version used for clustering, so that we can reproduce everything exactly using just the GATK Docker. The latter change probably isn't absolutely necessary, but it doesn't seem to break anything so I'm going to go ahead with it. We might want to update to an even more recent version later on (especially if we make any breaking/non-refactoring improvements to the malaria genotyping code after the initial PR), but unfortunately this slightly changes the clustering assignment for a few samples. @mwalker174 @asmirnov239 we discussed the first change some time ago, but just a heads up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261:653,refactor,refactoring,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261,1,['refactor'],['refactoring']
Modifiability,"@frank-y-liu As discussed in person, I think this is actually a symptom of https://github.com/broadinstitute/gatk/issues/4290 (using a double value as a key in a map get operation), in which case always returning 0.0 when a `get()` fails is not the right solution. I think we should instead refactor `MannWhitneyU` to not use doubles as keys -- I'll see if I can recruit a volunteer for that task.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5190#issuecomment-422108543:291,refactor,refactor,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5190#issuecomment-422108543,1,['refactor'],['refactor']
Modifiability,"@frank-y-liu I ran using the scripts in https://github.com/broadinstitute/gatk/tree/tw_spark_eval/q4_spark_eval, so no extra ENV variables. What happens if you try to run a small job using a GATK Spark JAR built from master? Do you get the same error?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299503397:129,variab,variables,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299503397,1,['variab'],['variables']
Modifiability,"@frank-y-liu Yes, I think it's not only fragile, but the direct cause of the error you're seeing. @davidbenjamin has agreed to attempt a refactor of the class when he has some free time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4290#issuecomment-422444261:137,refactor,refactor,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4290#issuecomment-422444261,1,['refactor'],['refactor']
Modifiability,"@freeseek No option currently, but after #5831 goes in and Mutect2 groups paired reads together I intend to refactor our annotation engine to account for this. That way we can have both read-based annotations and pair-based annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-523617830:108,refactor,refactor,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-523617830,1,['refactor'],['refactor']
Modifiability,"@freeseek This is a regrettable but temporary regression done for the sake of making Mutect2 much more principled ultimately. Let me try to explain with a timeline. * ~6 months ago: Mutect2 throws away one read whenever mates overlap. This reduces sensitivity unnecessarily, especially for indels, and messes up several annotations, although it does make the ADs come out right.; * ~3 months ago: we no longer throw out reads, and instead modify base and indel quals of overlapping mates to account for the possibility of PCR error. This improves sensitivity and strand, orientation, and position annotations, but it *is* a genuine regression in AD.; * [in 2nd round of code review, probably merged in a week]: Mutect2's `ReadLikelihoods` matrix forces mates to support the same haplotype and the entire likelihood framework is rewritten to allow pairs (or indeed, arbitrary groups of reads) as the atomic unit of data.; * [next step, 1 - 2 months?]: rewrite the annotations engine to accept read likelihoods for some annotations and pair likelihoods for others (such as AD).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6096#issuecomment-524138917:951,rewrite,rewrite,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096#issuecomment-524138917,1,['rewrite'],['rewrite']
Modifiability,"@gbrandt6 The online docs have a `meta` tag with a `description` attribute that contains the php code:. `<meta name=""description"" content=""include '../../../../common/include/common.php'; include_once '../../../config.php'; $module = modules::GATK; $name =..."" /><meta property=""og:image"" content=""https://theme.zdassets.com/theme_assets/2378360/ceb967563bfc35b9ab52ba13c0f5c4d870dff930.png"" />; `. but that `meta` tag isn't generated by the doc build process (the raw doc files produced by the build have no `meta` tags). So these must be introduced by whatever transformation is done as part of publishing. The raw templates used by the build do contain a `php` tag that contains the php code that winds up in the meta tag. It may be that the solution involves adding meta tags to the raw templates used by the build, but determining what changes need to be made requires someone with knowledge of the publish process. Ideally we would nominate someone from comms who understands that process, and have them also be the steward of the templates used by the build (I'm happy to give a tutorial on the doc build if that helps). It would also help with other questions such as whether we even need that php code in the templates...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7024#issuecomment-757975499:211,config,config,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7024#issuecomment-757975499,1,['config'],['config']
Modifiability,@gspowley Could you add the option to skip building the c library if an environment variable is set? maybe `GATK_SKIP_NATIVE_BUILD=true`. I think it's important that we have an opt in mechanism for avoiding the C compilation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185810475:84,variab,variable,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185810475,1,['variab'],['variable']
Modifiability,@gspowley I think we should probably move it to `src/main/cpp`. We can revisit it when they update the java plugin to use the new software model. If it's not easy to avoid that hardcoded path then don't worry about it. I just figure the fewer hardcoded paths the better. I think it's OK to use sudo. It looks like the travis infrastructure has improved since we last looked into that. I just wanted to explain why we didn't just use sudo apt get and had that weird travis apt block.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186443336:108,plugin,plugin,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-186443336,1,['plugin'],['plugin']
Modifiability,"@gudeqing I think you are referrring to the calls to `GetPileupSummaries`, where we have both `-L` and `-V` arguments with the same variable. This is actually not redundant, though I admit it is clumsy. This is a consequence of `GetPileupSummaries` being written as a GATK `LocusWalker`, which is necessary for optimal performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085:132,variab,variable,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085,1,['variab'],['variable']
Modifiability,"@igordot It used to exist because `AssemblyBasedCallerArgumentCollection` used to extend `StandardCallerArgumentCollection`, causing `Mutect2` to have a bunch of `HaplotypeCaller` arguments that it didn't use. This was fixed in PR #5758. `FilterMutectCalls` also lost a few arguments as part of a huge change to the entire filtering model in PR #5688. I'm working on a blog post about this but for now the Mutect2 docs at https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf are up-to-date and more user-friendly than they used to be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-478008792:82,extend,extend,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-478008792,1,['extend'],['extend']
Modifiability,"@jacobrh91 Thanks for the report! I believe this is the result of our (still-incomplete) transition to using a GATK configuration file for certain toolkit-wide settings. Could you please try making a copy of `src/main/resources/org/broadinstitute/hellbender/utils/config/GATKConfig.properties`, editing the `samjdk.use_async_io_read_samtools` line in that file to have a value of `true` rather than `false`, pass in the edited config file to GATK via the `--gatk-config-file` argument, and see if the setting gets changed at runtime?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367411571:116,config,configuration,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367411571,4,['config'],"['config', 'config-file', 'configuration']"
Modifiability,"@jamesemery - I think that the rebase is done. I'd like to have this in as soon as it can be, to avoid the extra-work of rebasing due to new tests or refactoring of them.... Thank you in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-338990541:150,refactor,refactoring,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-338990541,1,['refactor'],['refactoring']
Modifiability,"@jamesemery - we should get this merge as soon as possible to avoid conflicts that pop up in every round of comments. Once this is in, I can go to the open PRs to point out the conflicts and the new structure (e.g., change the new tests to extend `GATKBaseTest`). I added a new commit addressing the issues and I will rebase to resolve conflicts again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-340724214:240,extend,extend,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-340724214,1,['extend'],['extend']
Modifiability,"@jamesemery Back to you, at long last. I adopted your suggestion of a proper search that doesn't revisit already-seen vertices and came up with a better way of seeding the ""good"" subgraph that is safe from your STR concern. As far as code is concerned it's a total rewrite — you can pretend the first PR commit doesn't exist. The new criterion for seeding the search is chains with good log odds on both ends and which are incident on a vertex with multiple good out-edges or multiple good in-edges. The rationale is that the adjacency of two bad edges may have good log odds (Suppose a bad edge comes in and two bad edges come out. One is a new error on top of the original error and one is the continuation of the original error) but two have two outgoing edges with good log odds requires an actual real variant. On our M2 validations this essentially no effect on sensitivity and a mild reduction in false positives. I will leave it to you (or to me when I don't have to work like a vampire) to investigate how well it interacts with junction trees. As a first step I wrote a basic unit test for the basic pathology of the old method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441:265,rewrite,rewrite,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441,1,['rewrite'],['rewrite']
Modifiability,@jamesemery Back to you. Looks good! A had a bunch of minor comments and refactoring requests to make things a little cleaner and more foolproof for later readers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2021#issuecomment-239935022:73,refactor,refactoring,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2021#issuecomment-239935022,1,['refactor'],['refactoring']
Modifiability,"@jamesemery Could you review this? I think you may appreciate it. It took several tries, but I was finally able to write a stripped-down version of the code that actually slightly outperforms the old version. What I realized after a lot of profiling the old code and various failed rewrites was that cache-friendliness is the critical thing here. It turns out that this can be achieved without too many buffers, without precomputing the log frequencies, and without storing 2D and 3D arrays as flattened 1D arrays.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351:282,rewrite,rewrites,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351,1,['rewrite'],['rewrites']
Modifiability,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:163,config,config,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188,2,['config'],['config']
Modifiability,"@jamesemery Here's another fun one, again no change in output but significant refactoring of `constructHaplotypeFromVariants` and `createNewPDHaplotypeFromEvents`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8367:78,refactor,refactoring,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8367,1,['refactor'],['refactoring']
Modifiability,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:94,variab,variables,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,1,['variab'],['variables']
Modifiability,"@jamesemery I see how the returned `SortedSet` in the existing code is not sorted with respect to the resulting trimming bounds, but I don't see how it leads to a bug. Here's my commented version of the code in `AssemblyRegionTrimmer::trim` that the `SortedSet` of all variation events gets passed to:. ```; for (final VariantContext vc : variantsInRegion) {; int padding = assemblyRegionArgs.snpPaddingForGenotyping;; if (vc.isIndel()) {; padding = assemblyRegionArgs.indelPaddingForGenotyping;; // omitting a few lines where STRs get additional padding; }. // seems to me like if this is the second event and it's an indel then minStart could extend; // past the padding of a previous SNP event; minStart = Math.min(minStart, Math.max(vc.getStart() - padding,1));; maxEnd = Math.max(maxEnd, vc.getEnd() + padding);; }. final SimpleInterval paddedVariantSpan = new SimpleInterval(region.getContig(), minStart, maxEnd).intersect(region.getPaddedSpan());; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645509421:645,extend,extend,645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645509421,1,['extend'],['extend']
Modifiability,"@jamesemery I think this is because the annotation plugin, which has the pedigree arg, hasn't been integrated with the tools yet (second part of https://github.com/broadinstitute/gatk/issues/3287) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093:51,plugin,plugin,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093,1,['plugin'],['plugin']
Modifiability,@jamesemery I'm not surprised - I thought the refactoring might solve this when I saw the changes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1926#issuecomment-228742273:46,refactor,refactoring,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1926#issuecomment-228742273,1,['refactor'],['refactoring']
Modifiability,"@jamesemery Oh I see ! My bad , sorry, I messed-up something when the boundaries of the exons where extended by 100bp. Thank you for your time !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7289#issuecomment-856179216:100,extend,extended,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289#issuecomment-856179216,1,['extend'],['extended']
Modifiability,"@jamesemery This fixes the case you found, hopefully bringing us closer to turning on linked de Bruijn graphs. I will start testing M2. If you test in HC, continue to keep in mind that adaptive pruning is not default. This change will be most important for rare complex graphs and in combination with junction trees but I did see modest improvements to indel sensitivity even with the old assembly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520:185,adapt,adaptive,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520,1,['adapt'],['adaptive']
Modifiability,"@jamesemery This is related to #6617. We've been using GATK4 DepthOfCoverage, and noticed that since it inherits from LocusWalkerByInterval, -L is now required. To this point:. 1) the usage examples still say -L is optional, at minimum this should be updated. 2) It would be nice if it was not required. Perhaps if omitted, all intervals (inferred from genome) would be used?. 3) Alternately, perhaps there could be a shortcut way to pass ""all intervals in the genome"" to GATK in the -L argument? While one can convert a .dict file to intervals manually, it would be convenient if this were more seamless. Thanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6648:104,inherit,inherits,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6648,1,['inherit'],['inherits']
Modifiability,"@jamesemery This is related to #6930 . The background is that PedigreeAnnotation is special-cased in GATK, which provides better command-line argument validation, and it will also be used to inject the PedigreeFile, create the SampleDB, etc. This is currently a subclass of InfoFieldAnnotation, and therefore cant be used for GenotypeFieldAnnotation. There shouldnt be this limitation, and this PR tried to address that. The way I propose to do this is to make InfoFieldAnnotation and GenotypeAnnotation into interfaces, with default methods where possible. The existing subclasses all switch from extending them to implementing them. This is generally a trivial difference, but it touches a lot of classes. . All existing classes that previously extended PedigreeAnnotation (formerly a subclass of InfoFieldAnnotation), now extend PedigreeAnnotation and implement InfoFieldAnnotation. This is a minimal difference, but it makes it possible for future classes to extend PedigreeAnnotation, and then implement GenotypeAnnotation. The only part this includes that I didnt like was the fact that the existing InfoFieldAnnotation overrides toString(), which I cant do in an interface. So I created AbstractInfoFieldAnnotation, and all existing InfoFieldAnnotation classes extend that. It's not currently clear to me how critical that override of toString() is. The weakness of this PR is that classes outside the GATK project that currently extend InfoFieldAnnotation would not inherit this. I could keep InfoFieldAnnotation a class as-is, and make a differently named interface behind it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7041:598,extend,extending,598,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7041,7,"['extend', 'inherit']","['extend', 'extended', 'extending', 'inherit']"
Modifiability,"@jamesemery and now the overview of the more complex changes:. - `AssemblyResultSet`: the code for adding and removing haplotypes based on pileup alleles has become a `void` method of this class, where it belongs. Here and elsewhere I introduce snappy variable and function named referring to ""good"" and ""bad"" alleles, which I find visually much clearer. The code is basically the same as before but somewhat streamified. I extracted a `makeHaplotypeWithInsertedEvent` method to eliminate some code duplication between GGA and pileup force-calling.; - `HaplotypeCallerEngine` and `Mutect2Engine`: Force-calling alleles are split into biallelic `Events`. Duplicated code for finding all pileup events, then sifting them into good event to force-call and bad events to remove is extracted as `PileupBasedAlleles.goodAndBadPileupEvents`. Computing `allVariationEvents` is much simpler because 1) it now uses `Event` instead of `VariantContext` and 2) `Event` overrides `equals` and `hashCode`.; - `PileupBasedAlleles`: `getPileupVariantContexts` and sorting into good and bad pileup variants has been unified into `goodAndBadPileupEvents()`. It has additionally been somewhat rewritten for conciseness. Also, instead of the somewhat kludgy method of making `VariantContext` with four temporary attributes, then filtering based on those attributes, it calculates the filtering status immediately and uses `Events`. Also fixed the somewhat-misleading use of the word `alt` to mean `SNP`.; - `AssemblyBasedCallerUtils`: `applyPileupEventsAsForcedAlleles`, along with several helper methods that it calls, has been moved into `AssemblyResultResult`, where it is now a void member method.; - `GATKVariantContextUtils` mainly just using `Event` instead of `VariantContext`, which simplifies the code for splitting a `VariantContext` into biallelics. After going through this exercise I realize that it's not actually so much. The diff's bark is worse than its bite. The overwhelming majority of changes are eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702:252,variab,variable,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574175702,1,['variab'],['variable']
Modifiability,"@jamesemery sorry to bug on this topic, but I'm hoping to make a push early this year to fully migrate my lab off GATK3 . I looked more closely at the specific annotations we need to migrate. I decided that I will implement our walker, 'DiscvrVariantAnnotator', which is basically a light wrapper around VariantAnnotation. This will make it easier to spike in custom annotations. In that walker, I will override makeVariantAnnotations(). I will make a new marker interface for EngineAwareAnnotation, and test that on all the Annotation classes, and use this to inject FeatureManager. So no core GATK changes needed. I did find one thing I'd like to propose. You probably know PedigreeAnnotation is special-cased in GATK. Annotations that use it have automatic argument validation and have the SampleDB injected. Currently, PedigreeAnnotation is a subclass of InfoFieldAnnotation, so isnt available to GenotypeAnnotations. There doesnt appear to be a solid reason why. I tried to fix that and my best idea is the proposal here: #7041 . The core idea is to convert InfoFieldAnnotation and GenotypeAnnotation to interfaces. This is generally a trivial switch in existing code. With that, it becomes possible for classes that currently extend PedigreeAnnotation (which I switched to no longer extend InfoFieldAnnotation) to simply PedigreeAnnotation and implement InfoFieldAnnotation. This makes it possible for future classes to extend PedigreeAnnotation and implement GenotypeAnnotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063:1232,extend,extend,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-760424063,3,['extend'],['extend']
Modifiability,"@jamesemery the use of padding is a little confusing. One use of padding is for providing extra reference bases around the reads. This is the case in `ReadWalkerSpark`, for example. When the whole reference is available via the Spark files mechanism (which is what this PR is about) then there is no need for padding (and in the case of `ReadWalkerSpark`, no need for sharding at all). Therefore it makes sense to remove the use of padding in these cases. Another use of padding was for providing more reads around an area of interest in assembly region/haplotype caller tools. This is no longer the case though, since the refactoring that @droazen did in `HaplotypeCaller` in #4031. However, `HaplotypeCallerSpark` has not been updated to reflect the changes from that refactoring, so it's probably best not to change the padding in that class. I've reverted the change there for this reason. The locus walkers never had any need for padding, except for padding reference bases in `LocusWalkerSpark`, mentioned above. I've added a comment in the code to clarify this. Hopefully that makes more sense now. Please let me know what you think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5221#issuecomment-426274932:623,refactor,refactoring,623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5221#issuecomment-426274932,2,['refactor'],['refactoring']
Modifiability,@jason-weirather I see the error in the config file - this is definitely a bug in the data source release. There will need to be another data sources release to fix this and another data sources bug. I expect that this will be released by end of week. I will run through some additional tests on hg38 data to see if I can duplicate the null pointer exception.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-372345798:40,config,config,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-372345798,1,['config'],['config']
Modifiability,"@jean-philippe-martin Ack! I pointed to the wrong thing, sorry about that. Naming confusion. You're absolutely right that that one goes the other way. What I meant to point at was `com.google.cloud.genomics.gatk.common.GenomicsConverter` . It's another Read-> SAMRecord converter. . I'm asking about the serialization issue, because I thought the purpose of extending the SAMRecord was so that they could be directly serialized and passed around instead of doing a conversion at every step. I doesn't look like these are directly serializable (and probably can't be without work since SAMRecord doesn't offer a default constructor.) . I don't understand how AVRO coders work exactly, so maybe I'm missing something important about how to think about serialization in dataflow. I thought you needed some sort of AVRO schema defining your object. Can you generate one automatically from any java object?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100265021:358,extend,extending,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100265021,1,['extend'],['extending']
Modifiability,"@jean-philippe-martin Agree that we probably shouldn't refactor `IntegrationTest` as part of this PR, but it looks like some other tests are failing now. The PR build failures are [here](https://travis-ci.com/broadinstitute/gatk/builds/97887212). There are some CRAN mirror problems that are affecting all builds at the moment, but there are also some failures that are fallout from the `IntegrationTest` changes. See [this](https://travis-ci.com/broadinstitute/gatk/jobs/171535202). The previous (`XReadLines`) code was gzip aware, but the new code is not, which is causing the test failures.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-456645889:55,refactor,refactor,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-456645889,1,['refactor'],['refactor']
Modifiability,@jean-philippe-martin If the connector could configure itself with default credentials that would be amazing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-386388889:45,config,configure,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-386388889,1,['config'],['configure']
Modifiability,@jean-philippe-martin It looks like `ReferenceDataSourceUnitTest` is what's covering the `engine.ReferenceFileSource` functionality. It should be easy to adapt one of those tests to run in jmfs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3921#issuecomment-349752680:154,adapt,adapt,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3921#issuecomment-349752680,1,['adapt'],['adapt']
Modifiability,"@jean-philippe-martin Possibly we were using the GCS<->HDFS adapter previously, and something changed in the code to make us use NIO here instead? (possibly https://github.com/HadoopGenomics/Hadoop-BAM/pull/111?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265002735:60,adapt,adapter,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265002735,1,['adapt'],['adapter']
Modifiability,@jean-philippe-martin Review complete. Just minor refactorings for clarity.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114991724:50,refactor,refactorings,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114991724,1,['refactor'],['refactorings']
Modifiability,"@jean-philippe-martin Sorry, the baby was not very asleep last night so I may be slightly less coherent than usual... . I see how you heard that, but it wasn't what I meant. What I mean was that we should eventually move the information about how to set -DSTACK_TRACE_ON_USEREXCEPTION into the top level UserException message to make it discoverable, and remove it from the comment it's in now. Lets do that in a different PR though since it's sort of orthogonal and the best thing to do might be to integrate it as a regular commandline option instead of an environment variable. Separately from that, I was wondering if we should catch StorageExceptions at the top level and handle them specially. If we're going to do that I think we could just add them to the UserException catch block and have them be treated the same way, no need for special handling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-285152468:571,variab,variable,571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417#issuecomment-285152468,1,['variab'],['variable']
Modifiability,"@jean-philippe-martin Tests fail in my branch, but they all look to be legitimate failures, missing environment variables and such. https://storage.googleapis.com/hellbender/test/build_reports/5237.2/tests/index.html",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1662#issuecomment-205463141:112,variab,variables,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1662#issuecomment-205463141,1,['variab'],['variables']
Modifiability,"@jean-philippe-martin Travis runs the cloud tests using a service account on a non gcs machine. (at least I assume it's not a gcs vm, I think they use amazon cloud although that could have changed...) All we do to log in is:. ```; gcloud config set project broad-dsde-dev;; gcloud auth activate-service-account --key-file servicekey.json; ```. @kcibul Where you connecting from the broad network? I've had problems connecting to gcs from home because of IP restrictions on the broad projects. Maybe your gsutil has some configuration setup to do tunneling but gatk doesn't? Sort of a long shot since I would expect both to not work if either doesn't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282406671:238,config,config,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2425#issuecomment-282406671,2,['config'],"['config', 'configuration']"
Modifiability,@jean-philippe-martin Would it make sense to change the default `maxChannelReopens` to 3? There will inevitably be code paths that construct their own Path objects. Would it be easy to add the ability to configure these settings globally rather than per-Path? Should we create a ticket for that?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-308756943:204,config,configure,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-308756943,1,['config'],['configure']
Modifiability,"@jean-philippe-martin Yeah, I'm seeing that. I suspect it's because we aren't passing the environment variables correctly to the docker tests so it's exploding with an unhelpful error during test initialization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314237740:102,variab,variables,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314237740,1,['variab'],['variables']
Modifiability,"@jfarrell Do you recognize ""scc"" as a local host name ? ""hdfs:///project/casa/gcad/adsp.cc/sv"" looks reasonable enough as a file URI, except that the hadoop file system provider requires an authority component (the part of the uri between the second and third slash: ""hdfs://authority-component/..."") be provided in such URIs. Since you didn't include one as part of the hdfs path on the command line, it looks like transform along the way resulted in one being added (the authority component looks like ""host:port""), resulting in the port number -1. So I'm not clear if its a configuration issue, or a bad code code path, or both. But I would suggest trying an hdfs path with a valid authority component (one that works with the hadoop shell). @SHuang-Broad I do see some code paths in `StructuralVariationDiscoveryPipelineSpark` that call `Paths.get directly`, rather than `IOUtils.getPath()`. I would also suggest replacing the direct calls to `makeSAMOrBAMWriter` in `SVFileUtils` with the GATK wrapper code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-493980166:577,config,configuration,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-493980166,1,['config'],['configuration']
Modifiability,"@jkobject Here's an update for you on this issue, as promised: we've tested the patch in https://github.com/broadinstitute/gatk/pull/7730 extensively on both our local machines and on a clean Google Cloud VM, and found it to work perfectly with all kinds of requester pays inputs to GATK (fastas, bams, vcfs, interval_lists, etc.). We now believe that the test failures in the PR are artifacts of some configuration issue in our Travis CI test environment, and that the PR does actually fix requester pays support in GATK. We are considering merging and releasing the branch as-is, and dealing with the issues in our test suite post-release. It would make us more confident doing this if you could test the branch out as well on your end and confirm whether it works for you. You can do this using the following commands:. ```; git clone https://github.com/broadinstitute/gatk.git gatk; cd gatk; git fetch; git checkout -b lb_refix_requester_pays origin/lb_refix_requester_pays; ./gradlew clean installDist; ./gatk <a GATK command that failed for you previously>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7716#issuecomment-1079421576:402,config,configuration,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7716#issuecomment-1079421576,1,['config'],['configuration']
Modifiability,"@jkobject This problem has to do with indels and predicted protein change sequences. I'm starting a refactor of how the predicted protein changes get created. When that's complete, this issue will be fixed. In the meantime, can you post the stack trace and share the example workspace you mention in #6289 ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1181815133:100,refactor,refactor,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1181815133,1,['refactor'],['refactor']
Modifiability,"@jonn-smith Can you be more specific? If we come back to this ticket in 3 months, we won't remember exactly how it needs to be refactored.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4550#issuecomment-375061877:127,refactor,refactored,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4550#issuecomment-375061877,1,['refactor'],['refactored']
Modifiability,"@jonn-smith Can you please chime in here as well to tell us whether it's currently possible to override a single system property setting in Owner on the command line, or is `--gatk-config-file` currently the only way?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367412275:181,config,config-file,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367412275,1,['config'],['config-file']
Modifiability,"@jonn-smith Quick one when you get a chance - this fixes some things I noticed on my GATK branch when testing my new htsjdk `VCFHeader` code. The `Funcotator.checkIfAlreadyAnnotated `code was checking for a header line that was never generated by `Funcotator` AFAICT, and this also ties together the `Funcotator` engine and test code to use the same constants. More could probably be done here but it would require a bigger refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7555:424,refactor,refactoring,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7555,1,['refactor'],['refactoring']
Modifiability,"@jonn-smith, I did see XsvLocatableTableCodec and the .config file path, but this does not appear to work. To be clear this is something like:; ```; gatk IndexFeatureFile -I ./hg19/testTextSource.config; ```; In IndexFeatureFile (https://github.com/broadinstitute/gatk/blob/abe8148bda234edf6bd00fa51df44d456e8e2641/src/main/java/org/broadinstitute/hellbender/tools/IndexFeatureFile.java#L118), it does identify the correct codec; however, it then calls:. IndexFactory.createDynamicIndex(featurePath.toPath(), ...). where featurePath is the config file. This calls IndexFactory to open a lineReader on the config file (not the backing data source): https://github.com/samtools/htsjdk/blob/6d3fc7bc1f613ecfce1c22d368f3ae17cb86823d/src/main/java/htsjdk/tribble/index/IndexFactory.java#L598. . This then fails during XsvLocatableTableCodec.readActualHeader(), since this is trying to read the config file, not the TXT file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591678472:55,config,config,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1591678472,5,['config'],['config']
Modifiability,@kcibul @cmnbroad This should hopefully fix the problems you've both been seeing. . It turned out we weren't logging into docker on the wdl test branches because they don't set DOCKER_TEST but they still build docker images. I was confused about how that variable was used.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7256#issuecomment-841391272:255,variab,variable,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7256#issuecomment-841391272,1,['variab'],['variable']
Modifiability,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2636:161,config,configuration,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636,1,['config'],['configuration']
Modifiability,@kdatta It shouldn't cause any problems for you. It was a local configuration issue. If your already set up to download the lfs files these should just work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2510#issuecomment-288540859:64,config,configuration,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2510#issuecomment-288540859,1,['config'],['configuration']
Modifiability,"@kdatta Right, you shouldn't assume that all the input vcfs have the same header. Ideally, you'd have your tool extend `GATKTool`, take as an `@Argument` a `List<FeatureInput<VariantContext>>`, and then call the inherited method `getHeaderForFeatures( final FeatureInput<T> featureDescriptor )` to get at each input's header.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-280412809:112,extend,extend,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-280412809,2,"['extend', 'inherit']","['extend', 'inherited']"
Modifiability,"@koncheto-broad this is one of the VQSR-lite PRs you will want to eventually rebase on. It's still awaiting review (I was waiting until the dust from updating to Java 17 in #8035 settles), but if anyone from your team wants to take a first crack, feel free! Not too many code changes, so hopefully it should be pretty manageable. Just so it's all in one place: your #8157 GVS branch is currently rebased on #7954, which contains the ""serial SNP-then-indel"" version of the Joint Genotyping WDL (written by Megan for Ultima) and the Java code for the tools. Some minor updates were made to the Java code in #8049 and the WDL was rewritten by me to do SNPs and indels in a single pass in #8074. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially—thanks to George for correcting me here.). The PR here makes relatively minor updates to both the Java code and the WDL and might require very minor updates to GVS code or JSON configurations. And finally, the larger PR at #8132 adds a Pure Java BGMM backend. As we discussed during my mobbing presentation, this is provided merely as a convenience for those users that might not be able to control their python environment (hopefully a small number, these days!), so getting it merged is probably less urgent and should not affect any GVS work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414056344:1005,config,configurations,1005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414056344,1,['config'],['configurations']
Modifiability,@laserson ; 1. How do you handle situations where you need different split sizes for different file types? ; 2. How do you handle setting reasonable defaults so it works right out of the box without people having to set their spark configs explicitly?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158224412:232,config,configs,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158224412,1,['config'],['configs']
Modifiability,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:159,refactor,refactored,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025,1,['refactor'],['refactored']
Modifiability,"@lbergelson & I think @tedsharpe 's proposal above re: attributes sounds reasonable (including a whitelist of attributes to copy, optionally configurable via the command line). The rest of the proposal sounds reasonable as well, modulo one or two comments which @lbergelson will add below.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3666#issuecomment-337006792:141,config,configurable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3666#issuecomment-337006792,1,['config'],['configurable']
Modifiability,"@lbergelson - In gradle, I first resolve with maven central and then with your artifactory:. ```gradle; repositories {; mavenCentral(); maven {; url ""https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/""; }; }; ```. In the case of maven, for several repositories this should be done following [this](https://maven.apache.org/guides/mini/guide-multiple-repositories.html). I think that the configuration for the repositories should look like this (if I remember correctly):. ```xml; <repositories>; <repository>; <id>central</id>; <name>Maven Repository Switchboard</name>; <layout>default</layout>; <url>http://repo1.maven.org/maven2</url>; <snapshots>; <enabled>false</enabled>; </snapshots>; </repository>; <repository>; <id>snapshots</id>; <snapshots>; <enabled>true</enabled>; </snapshots>; <name>libs-snapshot</name>; <url>https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot</url>; </repository>; </repositories>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340482852:398,config,configuration,398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340482852,1,['config'],['configuration']
Modifiability,"@lbergelson ; now: 3.9GB (without ~0.3-5 GB extra off from the base image changes currently in the branch); then: 5.22GB; A moderate but not irrelevant reduction in size. With these changes some more drastic changes will be easier, namely trying to slice down the python packages or refactoring the build jars should be easier. I intend to open another PR to upgrade the base image shortly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796:283,refactor,refactoring,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796,1,['refactor'],['refactoring']
Modifiability,"@lbergelson @droazen @kgururaj ; 1. I was playing around with the test codes in GATK and did not push GenomicsDB tests in this PR. Will push it in the next commit.; 2. This is at the top of our discussion list for next week. GenomicsDB interfaces use these JSON files today which contain input configuration, list of samples, mapping between sample IDs and TileDB row indexes and stream ids for the input VCFs. If this tool takes the list of VCFs and intervals as input, we'd have to recreate JSON files internally and pass it to GenomicsDB. I wanted to avoid this for now as we are thinking about overhauling the input methodology completely in GenomicsDB with protocol buffers, but this is going to take a while. Also, we need to decide what's the best way to maintain the callset mappings.; 3. Will let you know asap. -Kushal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277320579:294,config,configuration,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277320579,1,['config'],['configuration']
Modifiability,"@lbergelson @droazen Both of you committed changes to the Dockerfile recently, but as far as I can tell they are not security related. Should I keep this PR at [4.2.4.1](https://hub.docker.com/layers/broadinstitute/gatk/4.2.4.1/images/sha256-421d2fb2cc869249cef3f4d7a77289256d295b04ba623096228e0e5fd42939e9?context=explore)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-1048281865:193,layers,layers,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-1048281865,1,['layers'],['layers']
Modifiability,"@lbergelson @droazen Just noticed the M2 WDL test is failing here---is this similar to the CNV WDL failures you were seeing just before release?. ````; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received; The build has been terminated; ````. If so, this is definitely something I've seen more frequently lately. Not sure what the remedy is, but would probably be worth looking into soon so we don't lose too much time to intermittent failures.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4118#issuecomment-356754166:331,config,configuration,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4118#issuecomment-356754166,1,['config'],['configuration']
Modifiability,"@lbergelson @droazen While I was running gradle test, it occurred to me that spark tests are hogging a lot of resources. Each Power8 core can support 8 threads. On a big SMP machine, the statement ""DEFAULT_SPARK_MASTER=local[*]"" means a LOT of spark workers. Since we are running the test on a shared machine, the resource-hungry test is giving other applications some headaches. Is it possible to specify the number of local spark workers via an environmental variable? If the variable is not specified, then we can fall back to default ""local[*]"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1761#issuecomment-213613328:461,variab,variable,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761#issuecomment-213613328,2,['variab'],['variable']
Modifiability,"@lbergelson @droazen, what's about a `GATKConf` class containing settable values? Defaults are the current ones and it's up to the API user if a configuration file or hardcoded solution is implemented, or even command line if they need it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2156#issuecomment-265244675:145,config,configuration,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2156#issuecomment-265244675,1,['config'],['configuration']
Modifiability,"@lbergelson According to my reading of the code, the variable you changed governs the maximum number of elements in the generated cigars. I think that 10 was chosen so that each of the 5 elements S, H, I, D, and M could appear twice. I support this change if the speedup is as great as you say, but could you rename `cigarStringLength` to something like `maxCigarElements`? Feel free to merge yourself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/531#issuecomment-105998993:53,variab,variable,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/531#issuecomment-105998993,1,['variab'],['variable']
Modifiability,@lbergelson Are you happy with this branch as-is or do you want further refactoring as @takutosato hinted at above?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-607394558:72,refactor,refactoring,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-607394558,1,['refactor'],['refactoring']
Modifiability,"@lbergelson For the `isinf` error, let's try this change in common_data_structure.h. ```; - if (isinf(small) == -1 || isinf(big) == -1); + if (std::isinf(small) == -1 || std::isinf(big) == -1); ```. The gettime related errors are because mac doesn't support clock_gettime ([discussion here](http://stackoverflow.com/questions/5167269/clock-gettime-alternative-in-mac-os-x)). The code in util.cc is part of some unused sandbox code, which can be removed. Try this command to check for AVX support on mac. ```; sysctl -a | grep machdep.cpu.features; ```. I'll try to get access to a mac to help the build debug go faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187840330:418,sandbox,sandbox,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-187840330,1,['sandbox'],['sandbox']
Modifiability,"@lbergelson Hi, I want to inform you that I have solved the problem. It is because the JAVA version that the master and the worker used are different. It is solved by adding the JAVA_HOME path to the spark configuration file. It is not really a GATK problem, sorry for that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050#issuecomment-307204194:206,config,configuration,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050#issuecomment-307204194,1,['config'],['configuration']
Modifiability,"@lbergelson I moved the ""disable"" toggle to the `ProgressMeter` constructors, and made it `final` so that it won't change over the lifetime of the object. I agree that extracting an interface, etc., would be nicer, but I think such a larger refactor can wait for a future PR. For now, this fixes the currently-broken `disableProgressMeter()` method in a way that involves the least-invasive changes to the tools / traversals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7354#issuecomment-881711125:241,refactor,refactor,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7354#issuecomment-881711125,1,['refactor'],['refactor']
Modifiability,@lbergelson I refactored the common logic into a utility class. Please take a look.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-243942141:14,refactor,refactored,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-243942141,1,['refactor'],['refactored']
Modifiability,"@lbergelson I've refactored this PR to just keep the switch to using non-interactive mode in the gcloud installer, which I think is worth merging to guard against future issues. Can you re-review?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6974#issuecomment-733884778:17,refactor,refactored,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6974#issuecomment-733884778,1,['refactor'],['refactored']
Modifiability,"@lbergelson If you have time to do the refactoring we talked about (moving the reader creation back into `FeatureDataSource`, and having `FeatureInput` handle only the tagging), it would save me some future pain. If there isn't time to do it now, though, it's ok.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-233734236:39,refactor,refactoring,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975#issuecomment-233734236,1,['refactor'],['refactoring']
Modifiability,"@lbergelson Is the existing environment variable method of doing this sufficient, or should we work on a nicer way?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1449#issuecomment-202534494:40,variab,variable,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1449#issuecomment-202534494,1,['variab'],['variable']
Modifiability,@lbergelson Looks like we have a failure in `BigQueryUtilsUnitTest`:. ```; testQueryWithStorageAPI; java.lang.IllegalStateException: getTransportChannel() called when needsExecutor() is true; 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:194); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:241); 	at com.google.cloud.bigquery.storage.v1beta1.stub.EnhancedBigQueryStorageStub.create(EnhancedBigQueryStorageStub.java:108); 	at com.google.cloud.bigquery.storage.v1beta1.BigQueryStorageClient.<init>(BigQueryStorageClient.java:144); 	at com.google.cloud.bigquery.storage.v1beta1.BigQueryStorageClient.create(BigQueryStorageClient.java:125); 	at com.google.cloud.bigquery.storage.v1beta1.BigQueryStorageClient.create(BigQueryStorageClient.java:116); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:51); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:45); 	at org.broadinstitute.hellbender.utils.bigquery.BigQueryUtils.executeQueryWithStorageAPI(BigQueryUtils.java:394); 	at org.broadinstitute.hellbender.utils.bigquery.BigQueryUtils.executeQueryWithStorageAPI(BigQueryUtils.java:370); 	at org.broadinstitute.hellbender.utils.bigquery.BigQueryUtilsUnitTest.testQueryWithStorageAPI(BigQueryUtilsUnitTest.java:74); ```. I suspect we need to bump our BigQuery dependency in this PR as well -- I'll attempt it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1051025679:441,Enhance,EnhancedBigQueryStorageStub,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1051025679,2,['Enhance'],['EnhancedBigQueryStorageStub']
Modifiability,"@lbergelson Sorry to be unclear---this isn't a GATK issue. For Cromwell, you can configure various options for each backend. For example, if you are running on a local backend with Docker, you can set a `submit-docker` attribute to specify the string that runs the Docker container; so to solve the above problem, you'd set this to include `--shm-size` and set it accordingly. However, according to @jsotobroad, you're not allowed such an attribute when submitting to Google cloud. If that's the case, then this is more of an issue with the Cromwell/Google Pipelines interface than the data.table package (although, as the discussion in the GitHub issue above shows, it'd be a simple fix on the data.table end, so I'm not sure why it's not addressed yet...) Changing the R script to get around the issue in this particular case is not unacceptably ugly, but you could imagine we might run into a similar problem in the future if anything else exceeds the 64MB /dev/shm limit and also cannot specify tmpfs. So perhaps we should take a look at the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357375691:81,config,configure,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357375691,1,['config'],['configure']
Modifiability,"@lbergelson There aren't too many incompatible changes between Spark 1.x and Spark 2.x, but there are some, and that required us to further parameterize our build and release. Feel free to borrow from our scripts if you go down that route. I'll keep an eye out for your new patch to review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2264#issuecomment-261284201:140,parameteriz,parameterize,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2264#issuecomment-261284201,1,['parameteriz'],['parameterize']
Modifiability,"@lbergelson When you get a chance. I think it was your last review that pushed me over the edge to refactor the barclay docgen code. So this now has additional commits that reflect that. One has the changes based on your general code review comments; one has the substantial changes due to refactoring (small changes to just 3 files I think); one has the annotation changes based on that; and one has the change to build.gradle for the barclay snapshot, which I need on this branch but will remove once I rebase. My apologies if its confusing. Le me know if you have questions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-278820155:99,refactor,refactor,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-278820155,2,['refactor'],"['refactor', 'refactoring']"
Modifiability,@lbergelson With that assuming that last commit didn't break everything I think thats all the comments at least explicitly punted on. Its running all the tests. I intend to open issues on merge of this for two lingering problems: ; 1. figuring out how to get fork-pr branches to work without crashing ; 2. refactoring the wdl tests to use the docker image that is being built for docker images (note this is probably contingent on our answer for 1 since currently the docker upload and pull path breaks without secrets...),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7754#issuecomment-1098463662:306,refactor,refactoring,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7754#issuecomment-1098463662,1,['refactor'],['refactoring']
Modifiability,"@lbergelson You're right, it would be easier to read that way, but it leaves a dangling ""Optional Arguments"" string in the output even when there are no optional arguments. If you're still not sold I can rewrite it to iterate once to count the optional args, but this is a cheap, simple way to get the right output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579:204,rewrite,rewrite,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/566#issuecomment-112561579,1,['rewrite'],['rewrite']
Modifiability,"@lbergelson and @droazen - back to you and thank you for considering this when I know that you are about to release. Please, feel free to modify the PR, rebase to test if there is any incompatibility and/or revert commits. I would like to have this for the released version to be able to remove the configuration argument, which might be confusing for my users, and to been able to use `java.nio.Path` as a temp directory. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-355948112:299,config,configuration,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-355948112,1,['config'],['configuration']
Modifiability,"@lbergelson and @ldgauthier have confirmed that GATK CombineGVCFs (the predecessor to GenomicsDB) also had this same limit, so GenomicsDB is not doing anything radically new here. This ticket is just to ensure that the limit is configurable if it already isn't",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300271189:228,config,configurable,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300271189,1,['config'],['configurable']
Modifiability,@lbergelson can you have a look why the LFS files are working on one of the test configs and not the other? The test is CollectWgsMetrics and uses the big bam file + fasta,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/844#issuecomment-132996288:81,config,configs,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/844#issuecomment-132996288,1,['config'],['configs']
Modifiability,@lbergelson can you review - it's a simple enhancement to the CompareBaseQualities tool,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716:43,enhance,enhancement,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1773#issuecomment-214757716,1,['enhance'],['enhancement']
Modifiability,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:294,config,configuration,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542,3,['config'],"['config', 'configuration']"
Modifiability,"@lbergelson this minor refactoring is needed for AllelicCapSeg in hellbender-protected. @vruano if you disapprove of anything, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/767#issuecomment-126058775:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/767#issuecomment-126058775,1,['refactor'],['refactoring']
Modifiability,"@lbergelson we've chatted about this about a month ago. I recall you wanted me to write a ""ReadWalkerBase,"" which would be the parent class of ReadWalker and DuplicateSetWalker. I haven't done that just yet—in this code DuplicateSetWalker is a subclass of ReadWalker. I wanted to show you this code first before embarking on further refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6512:333,refactor,refactoring,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6512,1,['refactor'],['refactoring']
Modifiability,"@lbergelson, I don't think that this solution will help in this case, because another error when trying to use `CommandLineProgramTest`is that it extends `BaseTest`, which loads directly a `GenomeLocParser` for a reference that is not present and it blows up in every test. Regarding the `Main` class, because you point it out here, I would like to have some control over `Main` and how it manages things like errors or logging header. Basically all the things that I'm facing at the moment are, apart of this error using the testing framework, is that the framework have tons of mentions to the GATK itself (error messages pointing to the GATK manual page or bundle tools), and little control over which of them should be expose to the final user. Only as an example, I would like to output a line with the name and version of my software and a short notice about the usage of the GATK framework and which version I'm using (for easier maintenance, and contribution if a bug is found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278:146,extend,extends,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278,1,['extend'],['extends']
Modifiability,"@lbergelson. Our simple s3 nio library is not currently open source, but we would be willing to make it so after testing it properly. We found that to get the s3 nio library work with GATK, a few minor changes needed to be made in the GATK source. This is especially true for the spark tools because, on AWS EMR Spark clusters, s3 uris can be treated exactly as if they are HDFS uris. Therefore, it was not quite as simple for us to just add the s3 nio library to the classpath and have everything work as expected. For that reason we put the project on hold until GATK is closer to release. Thanks,; David. ________________________________; From: Louis Bergelson <notifications@github.com>; Sent: Monday, July 31, 2017 11:57 AM; To: broadinstitute/gatk; Cc: David Brown; Mention; Subject: Re: [broadinstitute/gatk] update com.google.guava version (#3102). @david-wb<https://github.com/david-wb> Is your s3 plugin available as an open source plugin that others could use? We had another question about s3 support in gatk and I thought you might have some insight about it. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319146368>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABxO-d5XTtUyeAI0GzCFLP5eVGYiyQJEks5sThWegaJpZM4N31U->.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834:907,plugin,plugin,907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319185834,2,['plugin'],['plugin']
Modifiability,"@ldgauthier 's error sounds like what I saw before when trying to run the joint genotyping pipeline. When I spoke about it with @ruchim she said that based on some experiments she did and conversations with the production team she thought it was a symptom VM's running under PAPI; Slack excerpt:. ```; rmunshi [9:59 AM]; Last night I reached out to the Pipelines API folks; I ran a script that outputs the external IP address of the VM every 5 mins; and I see that after about ~17 hours, the request fails with. `curl: (6) Could not resolve host: metadata.google.internal` (edited); ```. So we decided that there's an effective limit of about 17 hours for VMs managed by PAPI at which point either the network configuration or metadata process server on the VMs changes, causing these failures. I worked around the issue by increasing my scatter interval count such that no tasks took longer than the ~17 hours that seems to be the critical point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412938932:710,config,configuration,710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412938932,1,['config'],['configuration']
Modifiability,"@ldgauthier ; My thinking on not doing the larger tests was that the native code hasn't changed for this support, so performance and functionality shouldn't see anything unexpected. Additionally, we technically do ""incremental import"" whenever the import is batched currently. We're just extending that same paradigm to extend beyond the case where the initial GenomicsDBImport command is used. Of course, all of this is not to say I don't want to do the larger tests...just wondering if we could capture that in a separate issue? @droazen mentioned that there's a tentative plan for a new GATK release this week and we would like to have this feature in there, if you agree. We'll work in parallel on the performance testing you requested. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043:288,extend,extending,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043,2,['extend'],"['extend', 'extending']"
Modifiability,@ldgauthier @davidbenjamin please take a look. . Don't allow the number of lines changed intimidate you... (most are in test resource files). The first commit contains the actual main code changes. . The second and third commits update the test resources (where most of the changed lines come from) and test code. . The very last commit changes the default radius to 2... I was planning to set it to 0 since it is more parsimonious (less complex configuration) but it may well affect sensitivity and certainly changes the PL/QUAL values so I guess set the value two the current 2 (for PLs) is a safer and more conservative approach until we evaluate what is the optimal value for this parameter. . Perhaps @davidbenjamin would like to have a different default for Mutec. This is last minute change and may break some of the integration test so bear with me if that is the case. However I think you can start reviewing the code at this point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042:446,config,configuration,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042,1,['config'],['configuration']
Modifiability,"@ldgauthier @jamesemery If `PossibleDeNovo` can't work with just founderIDs, then the constructor that takes a set of founderIDs should be removed (??) Or better yet, should the base `Pedigree` class be refactored to make more explicit the distinction between annotation subclasses that require the full pedigree and those that just require the founderIDs (perhaps as a separate PR) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463263757:203,refactor,refactored,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463263757,1,['refactor'],['refactored']
Modifiability,"@ldgauthier Also, try running with the prebuilt jars (https://github.com/broadinstitute/gatk/releases/download/4.1.8.1/gatk-4.1.8.1.zip) and/or the latest docker image to eliminate your local build environment as a variable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5568#issuecomment-663152910:215,variab,variable,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5568#issuecomment-663152910,1,['variab'],['variable']
Modifiability,"@ldgauthier As discussed in person, could you please pull out a `GnarlyGenotyperEngine` with a `VariantContext finalizeGenotypes(VariantContext)` public entry point, so that we can do gnarly genotyping in BigQuery? You can see the version @jonn-smith wrote here: . https://github.com/broadinstitute/gatk/blob/jts_bigquery_spark_example/src/main/java/org/broadinstitute/hellbender/tools/evoquer/GnarlyGenotyperEngine.java. (But note that the version above does not exactly match the latest version of your code -- it's just an example of the refactoring we'll need in this branch). I think once this is done, and the few comments I add just now are addressed, and this branch is updated to the latest and greatest version of your tool, this can be merged",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843:541,refactor,refactoring,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843,1,['refactor'],['refactoring']
Modifiability,"@ldgauthier I think at some point we removed example JSONs in both the CNV and M2 WDL directories. I believe the reasoning was that those JSONs were mostly non-informative templates that could just as easily be generated with `womtool inputs`; since they were also not tested (in contrast to the JSONs used by the Travis WDL tests), they had to be kept in sync manually. @davidbenjamin @LeeTL1220 can correct me if I'm wrong. In contrast, providing Jack's hyperparameters for WES via JSONs will actually be informative! However, we will inevitably run into some issues touched upon in https://github.com/broadinstitute/gatk/issues/4719. I agree that it would be desirable to set some default WES/WGS hyperparameters in the featured workspace. However, I hope this wouldn't require two separate workspaces for WES/WGS or any shenanigans like that. Ideally, this sort of thing could be covered at the tool level with argsets, as mentioned in that issue. @droazen any updates there?. In any case, I'm not sure having the JSON in this repo and not covered by any tests is what we want. ; Maybe @bshifaw can chime in? Are the featured workspaces covered by tests elsewhere? What is the current SOP for taking workflows from this repo, turning them into featured workspaces, and populating their configurations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-505971851:1290,config,configurations,1290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-505971851,1,['config'],['configurations']
Modifiability,"@ldgauthier Let me know once you've had the chance to do that `GnarlyGenotyperEngine` refactoring discussed above, and I'd be happy to give this a (hopefully) final look.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-499240904:86,refactor,refactoring,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-499240904,1,['refactor'],['refactoring']
Modifiability,"@ldgauthier Some parts of taking splitting MNPs at the end of HaplotypeCaller are easy: breaking eg one DNP at position n into a SNP at n and a SNP at n + 1, letting the SNPs inherit the PLs, AF, and AD (okay, this isn't quite right because a read might end in the middle of the MNP, but close enough) of the parent MNP. . . but the general problem of splitting annotations seems like it might be too tricky. I'm leaning toward instead just modifying `AssemblyBasedCallerGenotypingEngine.phaseCalls()`. It seems that this phasing relies very heavily on perfect phasing or anti-phasing and that even one questionable haplotype with incorrect phasing can spoil things. I would guess that we could improve the phasing by making some simple guess as to which haplotypes are real. Basically, the problem is that while HaplotypeCaller imposes ploidy on alleles, it does not do so on haplotypes, and so phasing information is diluted. With your permission I would like to merge this PR and open a new issue for improving `phaseCalls`. After all, the issue is fixed in M2, and HC now has a perfectly good MNP mode, with the caveat that it doesn't interact nicely with GVCF mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262:175,inherit,inherit,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262,1,['inherit'],['inherit']
Modifiability,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:474,extend,extended,474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806,1,['extend'],['extended']
Modifiability,"@ldgauthier Well, there is always the `@ExperimentalFeature` tag if you think it's appropriate in this case. But if @meganshand did a pass to clean up / refactor the old code, `@BetaFeature` might be the right label....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8933#issuecomment-2261126897:153,refactor,refactor,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8933#issuecomment-2261126897,1,['refactor'],['refactor']
Modifiability,"@ldgauthier defer to you on this, but agree that it seems confusing/misleading, especially in the case of large ref blocks with highly variable depth",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7185#issuecomment-824151659:135,variab,variable,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7185#issuecomment-824151659,1,['variab'],['variable']
Modifiability,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5359:565,refactor,refactoring,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359,1,['refactor'],['refactoring']
Modifiability,"@lindenb commented on [Mon Feb 06 2017](https://github.com/broadinstitute/gatk-protected/issues/891). Hi GATK team,. In my VCF I've got a variant that was called but it's only the consequence of a set of soft-clipped reads (it's a Haloplex assay, that's why I've got the 'bars' / high depth below). ![jeter](https://cloud.githubusercontent.com/assets/33838/22643385/885096d0-ec5e-11e6-91ae-7331affafc36.png). I was playing with the GATK 3.7 API to find the soft clipped reads overlapping my variation. ```java. @Downsample(by= DownsampleType.BY_SAMPLE, toCoverage=1000000); public class MyWalker ; 	extends RodWalker<Integer, Integer> implements TreeReducible<Integer> {; (...); public Integer map(RefMetaDataTracker tracker, ReferenceContext ref, AlignmentContext context) {; {; (...); final Genotype g=variantContext.getGenotype(i);; 		final ReadBackedPileup sampleReadBackedPileup =rbp.getPileupForSample(g.getSampleName());; 		 final List<GATKSAMRecord> recs= sampleReadBackedPileup.getReads();. (...); }. (...); }; ```. unfortunately **, I cannot find any GATKSAMRecord containing the soft clipping segment**. So my question is: does GATK cannot find my reads because it only considers **start/end** but not **unclippedStart/unclippedEnd** ? Is there any parameter to get those clipped reads ?. Many thanks in advance,. Pierre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2938:599,extend,extends,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2938,1,['extend'],['extends']
Modifiability,"@lucidtronix . Thanks for the explanation. Here are a few lines with the FILTER and INFO fields. Let me know if you need a larger subset. I would guess that since DP is the total depth for 5000 subjects instead of just 1 that would likely be the culprit. Maybe average depth across samples could be used for a replacement variable when there are multiple samples. . ```; VQSRTrancheSNP99.70to99.80 AC=8;AF=0.0008391;AN=9532;BaseQRankSum=-0.915;CNN_1D=-16.118;ClippingRankSum=0;DP=177452;ExcessHet=3.0267;FS=6.543;InbreedingCoeff=-0.002;MLEAC=8;MLEAF=0.0008391;MQ=71.41;MQRankSum=0.005;NEGATIVE_TRAIN_SITE;QD=10.23;ReadPosRankSum=0.483;SOR=1.039;VQSLOD=-3.08;culprit=MQRankSum; VQSRTrancheSNP99.70to99.80 AC=62,81;AF=0.006477,0.008462;AN=9570;BaseQRankSum=0.398;CNN_1D=-16.118;ClippingRankSum=0;DP=196764;ExcessHet=2.802;FS=0;InbreedingCoeff=-0.0026;MLEAC=61,67;MLEAF=0.006373,0.007;MQ=68.23;MQRankSum=-0.961;NEGATIVE_TRAIN_SITE;QD=3.96;ReadPosRankSum=-0.318;SOR=0.666;VQSLOD=-5.206;culprit=MQRankSum; VQSRTrancheSNP99.70to99.80 AC=117;AF=0.012;AN=9574;BaseQRankSum=0;CNN_1D=-16.118;ClippingRankSum=0;DP=203481;ExcessHet=1.8042;FS=0.593;InbreedingCoeff=0.0034;MLEAC=117;MLEAF=0.012;MQ=55.51;MQRankSum=1.32;NEGATIVE_TRAIN_SITE;QD=6.25;ReadPosRankSum=0.087;SOR=0.642;VQSLOD=-5.629;culprit=MQRankSum; VQSRTrancheSNP99.70to99.80 AC=21;AF=0.002193;AN=9574;BaseQRankSum=0;CNN_1D=-16.118;ClippingRankSum=0;DP=209533;ExcessHet=3.1253;FS=0;InbreedingCoeff=-0.0027;MLEAC=21;MLEAF=0.002193;MQ=57.8;MQRankSum=1.19;NEGATIVE_TRAIN_SITE;QD=4.35;ReadPosRankSum=-0.075;SOR=0.695;VQSLOD=-5.57;culprit=DP; VQSRTrancheSNP99.80to99.90 AC=1;AF=0.0001044;AN=9572;BaseQRankSum=-2.379;CNN_1D=-16.118;ClippingRankSum=0;DP=212253;ExcessHet=3.0108;FS=0;InbreedingCoeff=-0.0003;MLEAC=1;MLEAF=0.0001044;MQ=186.25;MQRankSum=0.084;QD=0.28;ReadPosRankSum=-0.743;SOR=0.798;VQSLOD=-17.7;culprit=MQ; VQSRTrancheSNP99.80to99.90 AC=5214;AF=0.545;AN=9570;BaseQRankSum=-1.071;CNN_1D=-16.118;ClippingRankSum=0;DP=276543;ExcessHet=160;FS=10.66;",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5101#issuecomment-412996579:322,variab,variable,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101#issuecomment-412996579,1,['variab'],['variable']
Modifiability,"@lucidtronix Are the environment variables that you added to the Docker env essential to realize the speed 2x improvement ? I'm reluctant to just add them to the Docker env without understanding what they're doing and whether/how they impact other components. i.e., changing OPEN_MP thread affinity/pinning params etc. might impact the native Intel PairHMM implementation (also @samuelklee will these impact CNV) ? Another option is reduce the scope of them and set them only for the specific tool(s), possibly exposed as command line arguments. The ScriptExecutor has control over the python process' environment and could easily propagate them to the so they only affect the particular Python process. But the values would have to be provided somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790:33,variab,variables,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-475614790,1,['variab'],['variables']
Modifiability,"@lucidtronix Could you try this branch out by running your CNN tool and confirming that it runs to completion, produces correct output, and doesn't time out? It would be helpful if you could run it in a configuration that prior to this PR would reliably time out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208:203,config,configuration,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208,1,['config'],['configuration']
Modifiability,"@lucidtronix It would be great to get this into the upcoming release later this week, but see my questions above about the intent of the environment variable settings, and whether they're essential to achieve the speedup. Setting OPEN_MP_NUM_THREADS for the entire docker will potentially impact the native PairHMM code, so we'll either need to remove these, narrow the scope to tool/WDL, or better understand the intent/impact of these on the native code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476605177:149,variab,variable,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725#issuecomment-476605177,1,['variab'],['variable']
Modifiability,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:45,config,configuration,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177,3,"['Config', 'config']","['Configured', 'configuration', 'configure']"
Modifiability,"@lydiarck - Not yet our priorities shifted and I haven't had time to address this. . @jnktsj That's great to hear! I'm surprised because this is the first I've heard of it. If you have some time, I'd love to discuss it with you (and / or Niall and / or Carrie). I'm particularly interested in how you will incorporate new versions of the software as updates are made. Things are starting to slow down, so I should *actually* be able to start taking a look next or the following week. It's going to require refactoring several things deep in the Funcotator Engine, which is why it hasn't happened yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-913047919:506,refactor,refactoring,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-913047919,1,['refactor'],['refactoring']
Modifiability,"@magicDGS - thanks again for the help on this. yes, GATKReadFilterPluginDescriptor seems somewhat like the same concept as GATK3's PluginManager. Some of the not-yet-ported tools like VariantAnnotator used the plugin idea more heavily as well, so perhaps that'll get built out more when those are ported? I will work with that and perhaps try to pull out an AbstractPluginDescriptor. @droazen - in the earlier post you mentioned support on porting this. i think we're almost done, but it would be really helpful to test this using the same test data as GATK3 to ensure identical behavior. I dont know if there's any issues preventing sharing those files, but it seems like sharing those, at least privately wit us, should be a relatively straightforward thing. We're happy to do all that testing, if we could just get that test data. Sorry to keep bugging you on this point, but it'd be helpful if someone from GATK could reply on whether this is a possibility. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-344022553:131,Plugin,PluginManager,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-344022553,2,"['Plugin', 'plugin']","['PluginManager', 'plugin']"
Modifiability,"@magicDGS Args like the config file that are truly optional (have no default value at all) do not show up in the command line or headers unless they're populated with some value. It should be pretty easy for ReadTools (which I think already has a common base class for its tools), to ensure a config file is never accepted by just precluding it via custom command line validation, or arg preprocessing. BTW, all tools built with GATK already have numerous common args that may or may not apply in a given tool context. For example, all of the ReadWalkers have a `--lenientVCFProcessing` arg. So I'm not even sure we need to make this hidden, since it will hide it from gatk users. My 2 cents. Others may feel differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413:24,config,config,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371819413,2,['config'],['config']
Modifiability,"@magicDGS Can you move the make*Transformer methods up to GATKTool (at some point we'll have a plugin at that level), and then also integrate these with AssemblyRegionWalker ? We'll want to do the Spark tools as well, but we leave that for a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-289924338:95,plugin,plugin,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-289924338,1,['plugin'],['plugin']
Modifiability,@magicDGS Do you have an example of something you need to configure that doesn't work well with java properties?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307798804:58,config,configure,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307798804,1,['config'],['configure']
Modifiability,"@magicDGS Esotericsoftware's minLog is used by Esotericsoftware's kryo. So, they would have to extend the class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315767184:95,extend,extend,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315767184,1,['extend'],['extend']
Modifiability,"@magicDGS I agree that a fully implemented AbstractPluginDescritor class is a good idea. Why not do this in a separate PR, and would this be most appropriate in Barclay? I would be happy to take a stab or let you. We should look at the usage pattern of ReadFilters, VariantEval's two plugins, VariantAnnotation and perhaps others I'm not thinking about.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407469692:284,plugin,plugins,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407469692,1,['plugin'],['plugins']
Modifiability,"@magicDGS I like your idea of making `TwoPassReadWalker` more configurable, with the ability to specify different filters/transformers/intervals per-pass, provided that tools that don't need this level of configuration don't have to override any additional methods.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988:62,config,configurable,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988,2,['config'],"['configurable', 'configuration']"
Modifiability,"@magicDGS I like your suggestion of factoring out a CountingFilter class that can be reused for both filter types. Ideally, I think we should first get https://github.com/broadinstitute/gatk/pull/2218/files in first (it has some minor changes to CountingReadFilter). Hopefully that will be soon. In the meantime, if you're so inclined, you might try doing the CountingFilter refactoring as part of this PR (which still needs tests), since you'll have two clients for it. That will definitely require some careful refactoring of the existing classes in order to retain the current CountingFilter behavior).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-261546961:375,refactor,refactoring,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-261546961,2,['refactor'],['refactoring']
Modifiability,"@magicDGS I made the return type `List<Object>` because the existing consumers that I know of (clp and docgen) have no explicit knowledge of the actual type used for any given plugin descriptor. The proper way to handle this would be to use the bounded type `List<? super T>`; if you just make it `List<T>`, then casual consumers like the clp would get a compile error for this:. List<Object> plugins = descriptor.getDefaultInstances();. However, other similar methods in the descriptor class aren't bounded (a couple of them should be); and I didn't want to include that change in the docgen PR so I stayed consistent with the existing methods. I really should fix this in a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-276224076:176,plugin,plugin,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-276224076,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"@magicDGS I'd strongly prefer not to introduce a read filter descriptor hierarchy if we can avoid it, as it will be tricky to get right, and add complexity. We definitely need to be able to extend the package list used by the descriptor to find plugins, but as you point out we'll be able to use the configuration mechanism for that. For before/after-analysis filters, I expect that we'll just add that directly to the existing plugin once we resolve https://github.com/broadinstitute/gatk/pull/2085 (which I hope to get to this week). I think the rest of the cases can be addressed by overriding makeReadFilter and providing custom behavior of filter merging. If this turns out to be something truly common, we could consider allowing the tool to inject an argument collection into the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451:190,extend,extend,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451,5,"['config', 'extend', 'plugin']","['configuration', 'extend', 'plugin', 'plugins']"
Modifiability,"@magicDGS I'm still missing why this wouldn't work for downstream projects (as long as they load Main or some Main-derived class). I think the owner config issue is different; for locale, we need to always force US. Can you verify this, or maybe provide more details about what case doesn't work ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945:149,config,config,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945,1,['config'],['config']
Modifiability,"@magicDGS It looks like you have triggered a few new compiler errors in the last branch, namely in the following places:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/SVDiscoveryTestDataProvider.java:33: error: cannot find symbol; BaseTest.b38_reference_20_21, ReferenceWindowFunctions.IDENTITY_FUNCTION);; ^; symbol: variable BaseTest; location: class SVDiscoveryTestDataProvider; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/formats/SampleLocatableCollectionUnitTest.java:30: error: cannot find symbol; private static final String TEST_SUB_DIR = toolsTestDir + ""copynumber/formats"";; ^; symbol: variable toolsTestDir; location: class SampleLocatableCollectionUnitTest; /gatk/src/test/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedregion/SimpleAnnotatedGenomicRegionUnitTest.java:18: error: cannot find symbol; private static final String TEST_FILE = publicTestDir + ""org/broadinstitute/hellbender/tools/copynumber/utils/combine-segment-breakpoints-with-legacy-header-learning-combined-copy-number.tsv"";; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259:353,variab,variable,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-341143259,2,['variab'],['variable']
Modifiability,"@magicDGS My take on these: I think the read filter plugin descriptor shouldn't be removed, since it also does filter merging, header propagation, etc. which need to be done even if you want to disable user command line control. (Also, if you remove it I would expect you'd get an NPE). So I would say that we should update the doc to say that you shouldn't remove it from the list, and should override `makeReadFilter` in the case where you want finer control. I know we talked a lot about the transformer issue a while back; I think the intention was that we would do the integration with the rest of the tool types as part of https://github.com/broadinstitute/gatk/issues/2160, but @droazen may recall differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453:52,plugin,plugin,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453,1,['plugin'],['plugin']
Modifiability,@magicDGS Perhaps a better solution to this problem would be to refactor the -L code to be able to handle queryname sorted files (we could produce a very similar filter to your filter and add it to the traversal early),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-435175872:64,refactor,refactor,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-435175872,1,['refactor'],['refactor']
Modifiability,"@magicDGS Rather than have a special case for doc-only args, I think we should close this PR and include the config file arg as part of the arg collection you added in https://github.com/broadinstitute/gatk/pull/3998.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-377341373:109,config,config,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-377341373,1,['config'],['config']
Modifiability,"@magicDGS Sorry for the delay on these AssemblyRegion-related PRs. There is an effort at the Broad right now to validate the GATK4 `HaplotypeCaller` against the GATK3 version. Until this is complete, we're not accepting even minor changes to code on the critical path for the `HaplotypeCaller`, except for bug fixes that arise from the validation work. It's still possible that as a result of this validation work `AssemblyRegionWalker` may get refactored/altered to address problems discovered, so until we have a final version that produces acceptable results for `HaplotypeCaller` (and we're not quite there yet) other changes to that part of the codebase will have to wait. Sorry for the inconvenience -- once GATK4's `HaplotypeCaller` gets the official stamp of approval we will certainly find a way to get all of your changes in. In the mean time we have to ask you to be patient a little longer!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471:445,refactor,refactored,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-287447471,1,['refactor'],['refactored']
Modifiability,"@magicDGS The GATK versioning scheme is not related to the API -- it is targeted at end users rather than projects using GATK as a library. Here's a slide that explains it:. <img width=""824"" alt=""gatk_versioning"" src=""https://user-images.githubusercontent.com/798637/38042254-e5bb85a4-3281-11e8-8d83-017bb6b73fda.png"">. As the slide mentions, we have given some thought to supplementing the main version number with an ""API version number"", but we'd have to more clearly define what constitutes the official public API for the GATK before doing so. On a side note, now that we're in general release it may be easier for you to get PRs for things like new walker types merged into the GATK proper, particularly if they are fairly self-contained and don't involve refactoring lots of engine classes. I was planning to ask whether you wanted to resurrect your `SlidingWindowWalker` PR at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968:762,refactor,refactoring,762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968,1,['refactor'],['refactoring']
Modifiability,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:373,extend,extend,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,3,"['Inherit', 'adapt', 'extend']","['Inheriting', 'adapted', 'extend']"
Modifiability,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:714,extend,extend,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305,1,['extend'],['extend']
Modifiability,"@magicDGS This seems like a good idea. I have a few comments.; 1. It seems like there should be an analogous function to `getPackageList()` where you set the list of individual classes, maybe `getNamedToolList()`.; 2. It seems like there's a design flaw in the `Main` class. I thought that the intent was that people could inherit from `Main` and override `getPackageList()` in order to substitute their own packages. But for some reason `getPackageList()` is static which is preventing that from happening. I think we should make it non-static which would allow a subclass of main to just override `getPackageList` and `getNamedToolList()`. What does your derived main class look like now?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255832583:323,inherit,inherit,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255832583,1,['inherit'],['inherit']
Modifiability,"@magicDGS We like the idea of making this configurable. We're not sure that this is the best mechanism for doing so though. We've been thinking about possibly including a standardized mechanism for configuring properties, i.e. some property file in the jar that could then be overridden by downstream projects. We don't know a great mechanism for doing that though. . Do you have any thoughts? Maybe using something like [commons-configuration](https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html). It seems like we have a number of places that need this sort of configuration, and it would be good if we had a standard way of doing so instead of relying on little hacks for each instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2156#issuecomment-265240247:42,config,configurable,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2156#issuecomment-265240247,5,['config'],"['configurable', 'configuration', 'configuring']"
Modifiability,"@magicDGS We talked about this a bit today. I think we should get https://github.com/broadinstitute/gatk/pull/4469 finished and merged, and then rebase this on top of that. Then we should put the remaining arguments, including the ""doc-only"" config file arg, as well as the special arguments collection, into the interface and default implementation class that you've defined here. Then we can do a detailed review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-377344569:242,config,config,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-377344569,1,['config'],['config']
Modifiability,"@magicDGS We talked with our ops people and it looks like we periodically purge anything older than 60 days, so this snapshot is gone. We're going to try to streamline what we upload, which will then allow us to extend the retention time for these things. Hopefully in the meantime you can rebuild what you need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4565#issuecomment-375725088:212,extend,extend,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4565#issuecomment-375725088,1,['extend'],['extend']
Modifiability,"@magicDGS We will definitely be keeping the `GATKRead` interface around for the foreseeable future. When the HTSJDK 3.0 interfaces materialize we'll re-evaluate, but it's possible that we would continue to code against `GATKRead` even then, as our `SAMRecord` -> `GATKRead` adapter layer does some useful caching, and having our own interface has certain advantages (as well as disadvantages).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166#issuecomment-358325854:274,adapt,adapter,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166#issuecomment-358325854,1,['adapt'],['adapter']
Modifiability,"@magicDGS We've inherited customCommandLineValidation() from picard, but it never really caught on as a way to do things in GATK. The fact that it's output is inconsistent with the rest of the command line parsing is an accident and should be fixed. . I think the right thing to do here would be to make `customCommandLineValidation()` into a `void` method that either throws a `CommandLineException` or doesn't. It will take some changes to a few tools but it will make things less confusing in the long run. Then you can move the call to customCommandLineValidation to just after the call to `parseArgs` like you've done, but we can remove the custom code to print error messages since it will just be handled by the regular exception handling.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255845846:16,inherit,inherited,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255845846,1,['inherit'],['inherited']
Modifiability,"@magicDGS Yes, I think this should be closed in favor of a PR to make `ReadTransformers` into plugins after the merge of https://github.com/broadinstitute/gatk/pull/2085. The plugin framework implemented by @cmnbroad allows for plugin classes to themselves contain command-line arguments, so you could add an argument directly to the `MisencodedBaseQualityReadTransformer` to control whether misencoded quals should be fixed or generate an error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245992067:94,plugin,plugins,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245992067,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"@magicDGS after I did some refactoring for Mutect2 mitochondria GVCF output, this should be pretty easily doable by overriding the GVCFBlockCombiner. Can we close this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1883#issuecomment-590349072:27,refactor,refactoring,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1883#issuecomment-590349072,1,['refactor'],['refactoring']
Modifiability,"@magicDGS is correct that no annotations are tagged as `@DocumentedFeature` yet, but thats just because nobody has done it. The plugin (which I think @jamesemery is planning to implement soon), is a completely separate issue - the `@DocumentedFeature` annotations can be added either way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823633:128,plugin,plugin,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823633,1,['plugin'],['plugin']
Modifiability,"@magicDGS, is this the same Mac as the one in #1985? Assuming yes, this crash happens because AVX is not supported. We had a check for AVX in GKL 0.2.0, but the call was removed by mistake in 0.3.0. . Sorry about that, we'll fix it in the next GKL release. In the meantime, you can use the workaround from #2302 and define this environment variable: `export GKL_USE_LIB_PATH=1`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2315#issuecomment-267115817:340,variab,variable,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315#issuecomment-267115817,1,['variab'],['variable']
Modifiability,@magicDGS. It looks like there's some sort of compilation error here:. ```; /gatk/src/test/java/org/broadinstitute/hellbender/tools/walkers/markduplicates/MarkDuplicatesGATKIntegrationTest.java:180: error: incompatible types: inference variable T has incompatible bounds; ((GATKDefaultCLPConfigurationArgumentCollection) markDuplicatesGATK.configArgs).TMP_DIR = CollectionUtil.makeList(outputDir);; ^; equality constraints: String; lower bounds: File; where T is a type-variable:; T extends Object declared in method <T>makeList(T...); /gatk/src/test/java/org/broadinstitute/hellbender/tools/walkers/markduplicates/MarkDuplicatesGATKIntegrationTest.java:256: error: incompatible types: inference variable T has incompatible bounds; ((GATKDefaultCLPConfigurationArgumentCollection) markDuplicatesGATK.configArgs).TMP_DIR = CollectionUtil.makeList(outputDir);; ^; equality constraints: String; lower bounds: File; where T is a type-variable:; T extends Object declared in method <T>makeList(T...); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-355606477:236,variab,variable,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-355606477,8,"['config', 'extend', 'variab']","['configArgs', 'extends', 'variable']"
Modifiability,"@magicdgs Do you get all your dependencies from there, or only gatk? I assumed you had multiple repos configured so that it first resolves from central and then from artifactory. I'm not sure how to configure that sanely in maven, although I'm sure it's doable to someone who is less ignorant of maven then me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340474332:102,config,configured,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-340474332,2,['config'],"['configure', 'configured']"
Modifiability,"@magicdgs You could include this filter in ReadTools and the plugin would discover it - after all, thats part of the purpose of plugins ;-). Anyway, at a minimum we should make sure the doc clearly explains when/how to use this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434681393:61,plugin,plugin,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5367#issuecomment-434681393,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"@mattsooknah Are there any places you'd like us to look more carefully? I'm tempted to just inhale this as is and deal with refactoring later. But if there are places you modified more heavily, we should look a bit at them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89308937:124,refactor,refactoring,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89308937,1,['refactor'],['refactoring']
Modifiability,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:180,config,configuration,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,2,['config'],"['configs', 'configuration']"
Modifiability,"@mbabadi You can directly register more classes by adding them in GATKRegistrator. It doesn't fix the problem of needing custom configuration, but that's how we've been dealing with custom serializers, just put them all in all the time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272290322:128,config,configuration,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272290322,1,['config'],['configuration']
Modifiability,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3004:565,adapt,adaptive,565,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004,2,['adapt'],"['adaptive', 'adaptively']"
Modifiability,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1062). - [ ] the ability to _override_ global transition priors in polymorphic regions, etc.; - [ ] the ability to ingest XHMM-like priors (i.e. parametrized models); - [ ] (bonus) different max copy number states for different genomic loci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2998:163,polymorphi,polymorphic,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2998,1,['polymorphi'],['polymorphic']
Modifiability,@mbabadi commented on [Tue May 02 2017](https://github.com/broadinstitute/gatk-protected/issues/1021). - [ ] factor I/O methods out of `CoverageModelEMWorkspace` and to a new class; - [ ] shrink the exposed API; - [ ] rename/refactor `CopyRatioCallingMetadata` appropriately; - [ ] rename/move `MathObjectAsserts` to test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2979:225,refactor,refactor,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979,1,['refactor'],['refactor']
Modifiability,@mbabadi commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/990). `RobustBrentSolver` is a univariate solver developed as a part of GATK coverage model. It has a Brent solver at the core but tries to avoid spurious non-bracketing conditions by creating a collection of refined sub-brackets. The implementation needs to be made more flexible:; - Allow the user to specify how sub-brackets are generated. The default grid is a logarithmic grid concentrated about the leftmost endpoint followed by uniform refinement of each grid element.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2971:368,flexible,flexible,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971,1,['flexible'],['flexible']
Modifiability,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/747). We use Genome STRiP TCGA/GPC2 call sets as ground truth. It is desirable to evaluate:. (1) XHMM and CODEX vs. GATK (almost done @asmirnov239); (2) GATK ROC curves as a function of bias latent space dimension D; (3) GATK ROC curves for a fixed D, and for different unexplained variance models: (isotropic, target-resolved, adaptive), w/ and wo/ sample-specific unexplained variance calling during PoN creations, and calling. That is, 3 x 2 x 2 = 12 cases. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/747#issuecomment-302465336). This was done a while ago. Keeping open for upcoming evaluations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2907:424,adapt,adaptive,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2907,1,['adapt'],['adaptive']
Modifiability,"@mcovarr Follow-up on the codecov thing. Apparently codecov is just not configured correctly? I asked at an Engine Team meeting and it looks like we're just not excluding test files for some reason, so that explains why it's calling out my test file as not covered. We could change it to ignore that directory, since it makes sense to not check for test coverage on test files, but I'm hesitant to include it here because it's sort of outside the scope of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2000123077:72,config,configured,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2000123077,1,['config'],['configured']
Modifiability,"@mcovarr added two tests (and refactored a dataprovider, because I'm a good person) -- take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7864#issuecomment-1152529144:30,refactor,refactored,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7864#issuecomment-1152529144,1,['refactor'],['refactored']
Modifiability,"@mdayii I'd suggest that you search the gatk [forum](https://gatk.broadinstitute.org/hc/en-us/community/topics) for similar topics, or post this issue there, to see if anyone there can help with your configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-896003059:200,config,configuration,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-896003059,1,['config'],['configuration']
Modifiability,"@meganshand Latest build failed with:. ```; org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.testWithAllAnnotations FAILED; java.lang.AssertionError: Iterators differ at element [9]: FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification""> != FORMAT=<ID=NUMT,Number=1,Type=String,Description=""Potentially a polymorphic NuMT false positive rather than a real mitochondrial variant.""> expected [FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">] but found [FORMAT=<ID=NUMT,Number=1,Type=String,Description=""Potentially a polymorphic NuMT false positive rather than a real mitochondrial variant."">]; at org.testng.Assert.fail(Assert.java:93); at org.testng.Assert.failNotEquals(Assert.java:512); at org.testng.Assert.assertEqualsImpl(Assert.java:134); at org.testng.Assert.assertEquals(Assert.java:610); at org.testng.Assert.assertEquals(Assert.java:578); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.assertHeadersMatch(VariantAnnotatorIntegrationTest.java:66); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.runVariantAnnotatorAndAssertSomething(VariantAnnotatorIntegrationTest.java:92); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.assertVariantContextsMatch(VariantAnnotatorIntegrationTest.java:55); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.assertVariantContextsMatch(VariantAnnotatorIntegrationTest.java:49); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorIntegrationTest.testWithAllAnnotations(VariantAnnotatorIntegrationTest.java:224); Exception in thread ""Thread-5"" java.nio.file.ClosedFileSystemException; 	at com.google.common.jimfs.FileSystemState.checkOpen(FileSystemState.java:64); ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477769414:425,polymorphi,polymorphic,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477769414,2,['polymorphi'],['polymorphic']
Modifiability,@mr-c I'm sorry this PR was forgotten about. Could you explain what it does? None of us are familiar with the `JAVAPATH` environment variable and googling didn't reveal anything useful.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3866#issuecomment-453586187:133,variab,variable,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3866#issuecomment-453586187,1,['variab'],['variable']
Modifiability,"@munrosa @ldgauthier Possible breakthrough. . First, what's definitely true about the het at 169510380 in 55_55003_F5region.bam when I reproduce the bug with `-L chr1:169510380 -ip 100`:. * The variant is considered active and triggers assembly, as it should.; * For every kmer size there are non-unique kmers in the reference, so it increases up to k = 85, the last attempt at which the engine relaxes the unique kmers requirement. (See `ReadThreadingAssembler` line 425).; * Once it reaches this kmer size, there are cycles in the graph and so no assembly is returned. (See `ReadThreadingAssembler` line 464). Thus no alt haplotype is discovered and the variant is missed. I believe there are two possible solutions.; * The assembly engine looks for cycles before pruning, but this order could be switched with no ill effects. In the case of this het there are no cycles after pruning because the apparent cycle was a poorly-supported path due to sequencing error. Here regular pruning works but the new `--adaptive-pruning` option would give a bit more security against false cycles.; * We don't actually have to check for cycles, especially in the last, desperate kmer attempt. Well, we do with the current recursive implementation of `KBestHaplotypeFinder`, but we *don't* in the Dijkstra's algorithm implementation currently under review: #5462. (Technical note: @ldgauthier I know I promised that this PR gives entirely equivalent results to the existing implementation, but technically this is only true if the existing implementation finishes in finite time. Due to the greedy -- but optimal -- nature of Dijkstra's algorithm, cycles do not cause issues). Personally, I am in favor of *both* solutions -- looking for cycles after pruning, and waiving the no-cycle requirement on the last attempt. They are complementary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913:1009,adapt,adaptive-pruning,1009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-446465913,1,['adapt'],['adaptive-pruning']
Modifiability,"@mwalker174 Hi Mark, I've refactored the code significantly, would you take a look again? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-386752928:26,refactor,refactored,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-386752928,1,['refactor'],['refactored']
Modifiability,"@mwalker174 Is encountering the same problem in the wild. He's reporting that it goes away if you specify the environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY. he's seeing the warning message:; ```; 16:55:09.480 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; ```; which *should* only appear during tests, so something is strange.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894:122,variab,variables,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894,3,"['config', 'variab']","['configured', 'variables']"
Modifiability,"@nalinigans: We now believe this is actually a GenomicsDB issue (or possibly an issue in the JNI layer.). @gokalpcelik was able to reproduce this problem on a set of 330 whole exomes. He found that if he ran GenotypeGenotypeGVCs from a GenomicsDB the memory usage climbed up to 10s of GB, but the java heap memory remained constant. He then tested firt extracting the combined GVCF from genomics db and then running GenotypeGVCFs and saw that memory usage for GenotypeGVCFs remained constant at 1 G. So we think this is probably a GenomicsDB issue. . GenomicsDBImport > GenotypeGVCFs ---- Memory ramps up immediately to 10s of gigabytes; GenomicsDBImport > SelectVariants to GVCF > GenotypeGVCFs ---- Memory is fixed at 1.1 GB. He can fill in more detail about the exact configuration if it helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2432001934:771,config,configuration,771,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8989#issuecomment-2432001934,1,['config'],['configuration']
Modifiability,"@olavurmortensen, looks like TILEDB_DISABLE_FILE_LOCKING=1 did not get passed to the tool. Did you use `export TILEDB_DISABLE_FILE_LOCKING=1` as the command to set the environment variable?; If you have and see the issue, please try the attached zip that contains a shared library with some debug/tracing messages, so we can pinpoint the issue a little more.; [libgenomicsdb.zip](https://github.com/broadinstitute/gatk/files/2922767/libgenomicsdb.zip); To use the zip, from a bash shell:. ```; %: tar zxf libgenomicsdb.zip; %: export LD_LIBRARY_PATH=`pwd`:$LD_LIBRARY_PATH; %: export TILEDB_DISABLE_FILE_LOCKING=1; %: gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport ...; ```; Please attach the log if you still see the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740#issuecomment-468989614:180,variab,variable,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740#issuecomment-468989614,1,['variab'],['variable']
Modifiability,"@olavurmortensen, yes please file a new issue. Please include your OS/platform version, your command, all the logs, any core dumps and steps to create a CIFS mount with your configuration in the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-468371003:174,config,configuration,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-468371003,1,['config'],['configuration']
Modifiability,"@olavurmortensen, yes, this has been tested on our systems and the fix will only mitigate NFS type of issues. However, there is improved logging of system errors now and we would like your inputs with logs and the exact NFS/CIFS configuration if you still run into issues to help us further debug what is happening. Please note that TILEDB_DISABLE_FILE_LOCKING env variable has to be set to 1 when doing the GenomicsDBImport.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-458665946:229,config,configuration,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608#issuecomment-458665946,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,@psfoley The branch is failing tests after your latest commit with the error:. ```; > Could not resolve all dependencies for configuration ':runtime'.; > Could not find com.intel:genomicsdb:0.9.2-proto-3.0.0-beta-1+uuid-static.; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4261#issuecomment-360870965:125,config,configuration,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4261#issuecomment-360870965,1,['config'],['configuration']
Modifiability,"@rdbremel This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-547962085:176,layers,layers,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-547962085,1,['layers'],['layers']
Modifiability,"@rickymagner I'd recommend postponing any further refactoring for future PRs post-release, in the interests of checkpointing this feature -- assuming that we're confident the feature works as intended and there are no side effects in other HaplotypeCaller codepaths. If the new argument is not already marked as Experimental, perhaps it should be until it's seen some more usage?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847920221:50,refactor,refactoring,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847920221,1,['refactor'],['refactoring']
Modifiability,@ronlevine @lbergelson Have you guys thought about the Spark case - I don't think this will capture logging output from Spark workers unless the settings get propagated through the config (we also don't propagate the verbosity IIRC). It might be a little misleading to allow specification of a file for Spark tools if it doesn't capture all of the output.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315897839:181,config,config,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315897839,1,['config'],['config']
Modifiability,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:70,refactor,refactoring,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049,1,['refactor'],['refactoring']
Modifiability,@ronlevine It looks to me that there could be some refactoring done here to separate out the AF update logic into a single function call that could be used in multiple places rather than having two sets of fairly complicated and highly similar logic. Could you see if that's doable?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-242121998:51,refactor,refactoring,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-242121998,1,['refactor'],['refactoring']
Modifiability,"@ronlevine Sorry for the slow response. I like it much more with the refactoring you did. I had a few minor comments. I want to ask @vdauwera about the names on monday, I find them confusing, but if it's standard nomenclature then we should keep them. So don't do any renaming until I get back to you about that I guess.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-246046362:69,refactor,refactoring,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871#issuecomment-246046362,1,['refactor'],['refactoring']
Modifiability,"@ronlevine, I think that the following should work for output the Esotericsoft's MinLog to write to a file:; * Implement in `LoggerUtils` a class extending `com.esotericsoftware.minlog.Log.Logger` that overrides the print method to append to the provided file.; * Use`Log.setLogger()` with an instance of that class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315706353:146,extend,extending,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-315706353,1,['extend'],['extending']
Modifiability,"@samuelklee ; My understanding: the code that **can** (and I think should) be borrowed from VCF is `CHROM`, `POS`, `ID`, `INFO`, with `END` from `INFO` extracted to be its own column. ; Then; * `FILTER` can be optional.; * `QUAL` can be optional but it is a nice-to-have feature as a quick-glance confidence measure, if that applies.; * `FORMAT` is going to be hard, because I understand the complaint that they can be wasting space, but I have seen VCF files that have rows with different numbers of fields in `FORMAT`, and that is spec-compliant. If this flexibility is allowed, i.e. allowing sample specific information to be missing on several rows, then the `FORMAT` column can be shared. Recap: only `REF`, `ALT` are missing, which is not much code I believe. I think VCF just happens to have a name that starts with V. Stripping out the `REF`, `ALT`, it is quite flexible for describing any annotated interval (OK, 0-length is up for debate) on a piecewise linear coordinate. And I just made myself sound like a VCF-lover. I simply think much of it can be reused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416:870,flexible,flexible,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416,1,['flexible'],['flexible']
Modifiability,"@samuelklee @asmirnov239 @mbabadi I tried to run a 30-sample cohort through gCNV on all canonical chromosomes with 250bp bins sharded in 10k-interval blocks, but PostprocessGermlineCNVCalls gave the following error:. ```...; 19:26:14.967 INFO PostprocessGermlineCNVCalls - Analyzing shard 223...; 19:26:15.107 INFO PostprocessGermlineCNVCalls - Analyzing shard 224...; 19:26:15.259 INFO PostprocessGermlineCNVCalls - Analyzing shard 225...; 19:26:15.260 INFO PostprocessGermlineCNVCalls - Shutting down engine; [May 29, 2018 7:26:15 PM UTC] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 3.34 minutes.; Runtime.totalMemory()=39753089024; ***********************************************************************. A USER ERROR has occurred: Bad input: Validation error occurred on line %d of the posterior file: Posterior probabilities for at at least one posterior record do not sum up to one.; ```. After inspecting the output from shard 225, it seems that the model starts producing nan values after ~1600 warmup iterations (looking at the ELBO log). This shard corresponds to a pericentromeric region chr3:91540501-94090250. . It would be nice to have the option to bypass this error in PostprocessGermlineCNVCalls. Here is the model config for the shard:. ```""p_alt"": 1e-06,; ""p_active"": 0.01,; ""cnv_coherence_length"": 10000.0,; ""class_coherence_length"": 10000.0,; ""max_copy_number"": 5,; ""num_calling_processes"": 1,; ""num_copy_number_states"": 6,; ""num_copy_number_classes"": 2; ""max_bias_factors"": 5,; ""mapping_error_rate"": 0.01,; ""psi_t_scale"": 0.001,; ""psi_s_scale"": 0.0001,; ""depth_correction_tau"": 10000.0,; ""log_mean_bias_std"": 0.1,; ""init_ard_rel_unexplained_variance"": 0.1,; ""num_gc_bins"": 20,; ""gc_curve_sd"": 1.0,; ""q_c_expectation_mode"": ""hybrid"",; ""active_class_padding_hybrid_mode"": 50000,; ""enable_bias_factors"": false,; ""enable_explicit_gc_bias_modeling"": false,; ""disable_bias_factors_in_active_class"": false; ""version"": ""0.7""; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4824:1283,config,config,1283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824,1,['config'],['config']
Modifiability,"@samuelklee @lucidtronix @asmirnov239 @davidbenjamin This is a good time for all of us to decide on GATK-style python coding (how much type hinting? how much documentation? etc.). I am adding you to this thread so that we can all have a say. **Code style**: The standard practice is to strictly follow [PEP-0008](https://www.python.org/dev/peps/pep-0008/). The python plugin for IntelliJ IDEA (=PyCharm) bark loudly when one deviates, which is pretty useful. **Docstring style**: I have written most of it in reST-style. In retrospect, perhaps [NumPyStyle](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt) would have been a better choice. If I have time now, I will switch. Definitely at some point. You may want to wait until I check the I tick off ""fill in missing docs"" and ""further code cleanup"" boxes before you take a look at the code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344508765:368,plugin,plugin,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344508765,1,['plugin'],['plugin']
Modifiability,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:1221,config,configure,1221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,1,['config'],['configure']
Modifiability,"@samuelklee I'm not making any changes to the CNV collection classes. I; think none of this PR affects those classes. On Thu, Mar 1, 2018 at 4:19 PM, samuelklee <notifications@github.com> wrote:. > Sorry @droazen <https://github.com/droazen> @LeeTL1220; > <https://github.com/leetl1220>, can you give me a bit more context?; > @LeeTL1220 <https://github.com/leetl1220> is no longer using any of the; > CNV-specific collections classes that I had hoped might be Tribble-ized in; > the future, so I'm OK with any decisions you guys make that is specific to; > his classes (does @jonn-smith <https://github.com/jonn-smith> have an; > opinion?) I think that moving towards storing the config in the header is a; > good thing, in general.; >; > If we need to make corresponding changes to the CNV-specific collections; > classes, then we should talk more. Not all of those collections describe; > locatables, so I'm not sure how we could fit them in the Tribble framework.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkzoWV1fcDEucTdcNZ_DggL0UW4M9ks5taGXhgaJpZM4Ru2it>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369735007:681,config,config,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369735007,1,['config'],['config']
Modifiability,"@samuelklee If there are CNV tools that can't comfortably extend `GATKTool` as things stand now, then I think that we should adjust `GATKTool` to be more flexible until they can do so. This would help with certain long-term goals that the engine team has (such as all tools supporting NIO for all inputs, consistent sequence dictionary validation, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921:58,extend,extend,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921,2,"['extend', 'flexible']","['extend', 'flexible']"
Modifiability,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:55,rewrite,rewrite,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,2,"['rewrite', 'variab']","['rewrite', 'variables']"
Modifiability,@samuelklee Removed references to union and replaced with combine; @droazen Undid the IDE formatting changes.; @droazen Refactored to use `IntervalUtils.compareLocatables(....)`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3713#issuecomment-338739173:120,Refactor,Refactored,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3713#issuecomment-338739173,1,['Refactor'],['Refactored']
Modifiability,"@samuelklee Yes, setting the `GATK_LOCAL_JAR` and/or `GATK_SPARK_JAR` environment variables will cause the gatk script to use that jar, instead of looking in its directory for a jar. The naming of the jar itself also doesn't matter if you use the environment variable method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3968#issuecomment-351734787:82,variab,variables,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3968#issuecomment-351734787,2,['variab'],"['variable', 'variables']"
Modifiability,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2836:315,extend,extends,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836,2,['extend'],['extends']
Modifiability,@samuelklee commented on [Mon Feb 06 2017](https://github.com/broadinstitute/gatk-protected/issues/892). ---. @droazen commented on [Tue Feb 14 2017](https://github.com/broadinstitute/gatk-protected/issues/892#issuecomment-279793685). Extend `GATKTool` to get the standard `-L`/`-XL` functionality for free.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2939:235,Extend,Extend,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2939,1,['Extend'],['Extend']
Modifiability,"@samuelklee commented on [Mon Oct 05 2015](https://github.com/broadinstitute/gatk-protected/issues/126). Some possible enhancements/improvements, in no particular order and of varying scope:. -Change SliceSampler to be able to handle multimodal univariate distributions. Should just be a matter of implementing the pseudocode in Neal 2003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the Slic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2824:119,enhance,enhancements,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824,2,"['enhance', 'flexible']","['enhancements', 'flexible']"
Modifiability,"@samuelklee commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/750). Tool for inferring mixture of CNV subclones from ACNV output.; - [x] Develop resources for simulating tumor phylogenies/mixtures; - Wrote python code for simulating phylogenies and generating corresponding truth tables for CN profiles, ACNV segment files (with varying noise---i.e., CR and MAF credible-interval sizes---and purity levels), and plots.; - [x] Design basic algorithm; - Gibbs sampling MCMC of Dirichlet mixture of CNV subclones, to start. Graphical model is written down.; - [x] Implement basic algorithm; - CLI roughly implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:701,refactor,refactoring,701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['refactor'],['refactoring']
Modifiability,@samuelklee correct me if I'm wrong - I should manually pass old default values to this set of commands?; ```; /soft/gatk-4.5.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.5.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.5.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.5.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. Or I can change some config file to skip this manual part?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1857396943:1870,config,config,1870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1857396943,1,['config'],['config']
Modifiability,"@samuelklee os.environ would probably work, though it might be easier to set `theano.config.base_compiledir` directly so you don't have to worry about clobbering any other flags.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390648848:85,config,config,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390648848,1,['config'],['config']
Modifiability,"@samuelklee, thanks for the update and suggestion. I moved CollectAllelicCounts to the `Coverage Analysis` category. CollectFragmentCounts isn't on the list currently so I added it to the same. I hope I'm not missing a bunch of other new tools given I missed this one. . @yfarjoun ; - You are now in charge of deciding whether we should include authorship in code. What the Comms team wants is for authorship to NOT show up in the gatkDoc/javaDoc. If you want to keep them, author lines should be at the bottom and formatted so they do not show up in the documentation. Geraldine is fine with completely removing them if you prefer that. There is a format trick that has javaDoc skip the author line and @vdauwera would know this or I can get you what I see in other docs. Let either of us know.; - I can help you test your changes. I think the categories are good to go now so I will need to put these into both Picard and GATK HelpConstants.java, with the latter being a placeholder until the new Picard release is incorporated into the next GATK release, with variables that then must be included in each tool doc. I will find an example in a bit. Which tool do you want to test? @cmnbroad can explain the engineering details in engineering lingo if you need more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645:1063,variab,variables,1063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645,1,['variab'],['variables']
Modifiability,"@samuelklee: @LeeTL1220 and I just had a discussion about the writer aspect of this branch, and we agreed on the following:. 1. Lee will introduce a new header type to encapsulate the information that's currently passed in individually to the `writeHeader()` method in `AnnotatedIntervalWriter`. This makes the interface cleaner and more future-proof, since the signature will just become `writeHeader(AnnotatedIntervalHeader)`. 2. Lee will start writing out 3 additional structured header lines (as comment lines) to every header, declaring the names of the chrom, start, and stop columns. These will not be respected on input yet (he will still be relying on a config file to get the names of these 3 columns), but it's the first step in the direction of storing all necessary schema information in the header of each file, rather than separately from each file. 3. Lee will file a github issue to eventually use these 3 header lines on input, when they are present, to get the names of the chrom/start/stop columns (possibly still with a fallback to a separate config file if they aren't, but that is a point we can debate in a future PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369709447:663,config,config,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369709447,2,['config'],['config']
Modifiability,@skwalker @madduran This puts in the variable depth bins that make the sensitivity part of M2 autoval viable for WGS. Would one of you care to review?. @LeeTL1220 Looping you in.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4339:37,variab,variable,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4339,1,['variab'],['variable']
Modifiability,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:1051,config,configdefaults,1051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833,1,['config'],['configdefaults']
Modifiability,"@sooheelee As an initial attempt to address this without too much refactoring, what about a two-step process where the user runs M2 with all samples eg `-I normal.bam -I tumor1.bam -I tumor2.bam -tumor sample1 -tumor sample2` (this would require a small code change to specify `-tumor` more than once) and then uses the common set of variants in GGA mode (PR #4601) on each sample individually?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4327#issuecomment-376960734:66,refactor,refactoring,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4327#issuecomment-376960734,1,['refactor'],['refactoring']
Modifiability,"@sooheelee It looks like split2_8.vcf.gz and split3_8.vcf.gz were generated with GATK 3 M2. In GATK 4 we only emit calls that are within the unpadded read shards, which I believe do not extend past the input intervals. Thus, as long as `SplitIntervals` returns disjoint subintervals (which it does), different scatters of M2 should produce disjoint calls. Could you try to reproduce the issue with GATK 4 M2?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-314866090:186,extend,extend,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-314866090,1,['extend'],['extend']
Modifiability,"@sooheelee We evaluated the S3 plugin, but found that it always localizes the entire file, which defeats the purpose of NIO. We are currently assessing how difficult it would be to patch the existing plugin. Issue is here: https://github.com/Upplication/Amazon-S3-FileSystem-NIO2/issues/103",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-374610893:31,plugin,plugin,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-374610893,2,['plugin'],['plugin']
Modifiability,"@sooheelee You are right, the python environment is not activated automatically. However, I manually activated the environment and then run my test.pbs, I still got the error. ```; [shengq2@cqs1 singularity]$ singularity shell gatk.simg; Singularity: Invoking an interactive shell within container...; Singularity gatk.simg:/scratch/cqs/softwares/singularity> source activate gatk; (gatk) Singularity gatk.simg:/scratch/cqs/softwares/singularity> sh test.pbs. ...; 21:27:03.205 INFO DetermineGermlineContigPloidy - Initializing engine; 21:27:03.210 DEBUG ScriptExecutor - Executing:; 21:27:03.210 DEBUG ScriptExecutor - python; 21:27:03.210 DEBUG ScriptExecutor - -c; 21:27:03.210 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346:825,config,configdefaults,825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346,1,['config'],['configdefaults']
Modifiability,"@sooheelee [Commenting on the forum discussion] `--alleles`, `--genotyping-mode`, and `--consensus` are inherited from a common parent class of Mutect2 and HaplotypeCaller and are inactive in GATK 4 Mutect2. I should fix this misleading situation. @cbao-bi's use case is important and the work-around that I gave on the forum is not satisfactory. I'm convinced that it's worth doing it right. Let me tentatively guess that I can put in a good force-calling mode within two months.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4555#issuecomment-375935335:104,inherit,inherited,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4555#issuecomment-375935335,1,['inherit'],['inherited']
Modifiability,"@sooheelee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1045). For MuTect2 developers, a feature to keep in mind -- allowing moving-variable-ploidy calling on highly diverse pooled microbiome samples. ---; Hi, I am interested in knowing how to apply gatk tools to microbiome data. Specifically, I would like to override the assumption of ploidy in the HaplotypeCaller and making it flexible, in that one sample could have a unknown number of haplotypes at the same time, I know somatic mutation caller Mutect2 does not share the assumption but then it's designed to specifically deal with normal - tumor sample pair which is not really applicable in the microbiome studies. Thanks. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/9594/applying-gatk-to-microbiome-data/p1. ---. @davidbenjamin commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303273671). @sooheelee Tumor-only calling is fully supported in GATK 4 M2, but I will need to understand more about microbiome variant calling to know if M2 could be used. ---. @vdauwera commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303284834). We've had users trying to use MuTect this way for a long time; we just tell them it's unsupported. Actually putting effort into figuring this out and potentially supporting it would probably be a quarterly-goals level decision. Given everything on everyone's plate I wouldn't expect this to get on the QGs in a long while, unless there's a high-profile project that demands it. Don't get me wrong, I'd love to see this done, and I will bring it up, but realistically I'm not going to hold my breath... . ---. @davidbenjamin commented on [Tue May 23 2017](https://github.com/broadinstitute/gatk-protected/issues/1045#issuecomment-303413211). This could be the sort of thing like mitochondrial calli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2987:174,variab,variable-ploidy,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2987,2,"['flexible', 'variab']","['flexible', 'variable-ploidy']"
Modifiability,"@sooheelee has some serious concerns about `ReadClipper.hardClipAdaptorSequence()`, which is called in `Mutect2` and `HaplotypeCaller` via `AssemblyBasedCallerUtils.finalizeRegion()`. She thinks that the method being used to find the adaptor boundary for clipping purposes is completely bogus!. This is some pretty old code that was also in the GATK3 versions of these tools, so if it's crazy, then we can at least take ""comfort"" in the fact that it's been like this for a very long time...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184:234,adapt,adaptor,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184,1,['adapt'],['adaptor']
Modifiability,"@srikarchamala We haven't heard from enough people to make this a priority. However, we are doing a big refactoring to make all of the Mutect2 annotations and filters inherently multiallelic so that splitting with external tools before or after FilterMutectCalls ought not to cause problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-570229469:104,refactor,refactoring,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-570229469,1,['refactor'],['refactoring']
Modifiability,"@t-ogasawara @frank-y-liu @gspowley @paolonarvaez @droazen @lbergelson please comment on the following proposal. The proposal is that we would spin off native PairHMM as a separate project/repo on github and host AVX code there and have alternative implementations extend that project/repo (by creating repos that depend on the AVX one). . In other words, now we have 1 repo, broadinstitute/gatk. After the proposed change we'll have 3 repos (all BSD licensed):; 1) broadinstitute/gatk; 2) broadinstitute/nativePairHMM-AVX; 2) broadinstitute/nativePairHMM-PPC. We will duplicate the native code (AVX and PPC will be separate copies of C++ files etc) to simplify the testing burden. The parties interested in working on a specific architecture will contribute code directly to the respective architecture-specific repo and gatk will take occasional updates of those repos. The gatk repo will depend on the other two. The PPC repo will depend on the AVX repo (and any other native repos will depend on the AVX one). The avx and ppc repos will have their own build systems and unit tests against the new interface. The AVX repo will expose something like the following Java API (to be worked out in detail). ```; //Used to copy references to byteArrays to JNI from reads; public final class JNIReadDataHolderClass {; public byte[] readBases = null;; public byte[] readQuals = null;; public byte[] insertionGOP = null;; public byte[] deletionGOP = null;; public byte[] overallGCP = null;; }. //Used to copy references to byteArrays to JNI from haplotypes; public final class JNIHaplotypeDataHolderClass {; public byte[] haplotypeBases = null;; }. public interface NativePairHMMKernel extends AutoCloseable { . /**; * Function to initialize the fields of JNIReadDataHolderClass and JNIHaplotypeDataHolderClass from JVM.; * C++ code gets FieldIDs for these classes once and re-uses these IDs for the remainder of the program. Field IDs do not; * change per JVM session; *; * @param readDataHolderClass class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864:265,extend,extend,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748#issuecomment-214914864,1,['extend'],['extend']
Modifiability,@takutosato Here's another bug fix. After the big filtering refactor M2 filters will be unit-testable and these things will be easier to prevent. This fixes the last edge case mentioned by @byoo in the discussion of #5563.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5578:60,refactor,refactor,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5578,1,['refactor'],['refactor']
Modifiability,"@takutosato Most of this PR is refactoring to make filtering work for multiple samples while leaving single-pair output unchanged. For example, moving FORMAT annotations to the INFO field, keeping track of the sample of orientation bias priors etc. I'm leaving the wdl unchanged for now. It will still work with the new release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5560:31,refactor,refactoring,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5560,1,['refactor'],['refactoring']
Modifiability,"@takutosato This is needed to rev gatk public, which in turn is needed for the Mutect2 refactoring I'm working on. Can you review?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257135814:87,refactor,refactoring,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239#issuecomment-257135814,1,['refactor'],['refactoring']
Modifiability,@tedsharpe @SHuang-Broad I've tried to address your comments -- want to have a another look? . Due to issues in the class I backed out my usage and refactoring of SATagAlignmentBuilder and SATagAlignment and just went with my own simple little parser.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060:148,refactor,refactoring,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684#issuecomment-301569060,1,['refactor'],['refactoring']
Modifiability,"@tedsharpe @cwhelan please review. - Adds MarkDuplicatesReadFilter (to replace MarkedOpticalDuplicateReadFilter). MarkedOpticalDuplicateReadFilter will be removed in a subsequent PR because the Filter tool currently uses it.; - Changed some types (short to int, float to double) in the DUST algorithm; - Adds HostAlignmentReadFilter for filtering sufficiently well-mapped host reads. The helper function is there to run the test on supplementary alignments. I chose not to expose this as a GATK filter because the definitions of coverage and identity used here could be different than what some users would expect. @lbergelson Addressed your comments from the other branch:; - Added docstring to AmbiguousBaseReadFilter argument; - Made filterOpticalOnly an argument; - Argument variables changed from uppercase to lowercase; - See above regarding the duplicates filters",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2665:779,variab,variables,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2665,1,['variab'],['variables']
Modifiability,@tedsharpe How much effort do you think it would be to adapt the current BWA bindings to bwa-mem2?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7014#issuecomment-758176744:55,adapt,adapt,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7014#issuecomment-758176744,1,['adapt'],['adapt']
Modifiability,"@tedsharpe Thanks for your patience! Review complete for now. I have a lot of stylistic comments and a few request for refactoring / some redesign of what appear to me to be redundant classes. Some stylistic comments:. I didn't comment everywhere that I saw it, but we try to avoid any unbracketed control structures. Even 1-liners should have brackets unless they're so short that the statement fits on the same line as the predicate. We typically have starting brackets at the end of the line though instead of on their own line, which saves some pain associated with single line brackets. I.e. gatk typically uses. ```; if ( something ) {; doThing; } else {; otherThing; }; ```. rather than. ```; if ( something ) ; {; doThing; }; else ; {; otherThing; }; ```. 2) You use a lot of raw iterators, which is fine and is necessary in many cases. In other cases those operations can be written much more succinctly with either a for-each loop, or a stream. i.e. . ```; List<Integer> values;; Iterator itr = iterable.iterator();; while(itr.hasNext()){; Element elem = itr.next();; int value = someFunction(elem); if ( value > SOME_CONSTANT) {; values.append(value); }; }; return values;; ```. can be . ```; return StreamSupport.stream(iterable.spliterator, false); .map( elem -> someFunction(elem)); .filter( value -> value > SOME_CONSTANT ); .collect( Collectors.toList()); ```. We should probably add a utilty function to convert an iterator to a stream directly so we can stream iterators easily even if there is no associated iteratable. . 3) The tools need tests. This is important. 4) It would be good to think about how the tools can be composited into a spark pipline and run without writing intermediate files. . 5) Bitwise operations are a rarity in GATK and many of our users will not be very comfortable with them. Please avoid bit twiddling tricks when possible. When it's not possible (i.e. when you are performing tricks to treat a long as a set of byte pairs) please add detailed explanat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394:119,refactor,refactoring,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1435#issuecomment-172985394,1,['refactor'],['refactoring']
Modifiability,"@tedsharpe This looks good to me. In general partition sizes can be much larger than 100kb without problems, so I suspect it's is something funny to do with the task serialization of ctx.paralellize(). . If this is a performance critical tool it would probably be better to rewrite it in a way that it can load the reference in parallel. Since I assume this is something you run essentially once per reference it may not be worth it. . If you're worried about small machines running out of memory, I would expose the parameter that lets you configure how much memory each partition gets. I would expect in any spark configuration each core will have no less than ~1gb to work with and likely 2 -4 on any machines used for biology work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150:274,rewrite,rewrite,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1505#issuecomment-187387150,3,"['config', 'rewrite']","['configuration', 'configure', 'rewrite']"
Modifiability,@tedsharpe and I discussed in person. This approach will cause problems for tests that rely on custom registrators. Back to you Ted until it grows additional layers of indirection.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1775#issuecomment-215145245:158,layers,layers,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1775#issuecomment-215145245,1,['layers'],['layers']
Modifiability,"@tedsharpe not sure if this is what you meant.; Also, I think the ""ultimate"" solution might be marking `SVInterval` not final, but allows it to be extended, and child classes should name themselves with convention baked in.; What do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418886943:147,extend,extended,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418886943,1,['extend'],['extended']
Modifiability,"@tomwhite @droazen This fixes the problem I was seeing, but it's insanely slow for some reason. A 30gb file on a heavy duty cluster was taking between 30-50 minutes depending on how I sharded it. Most of that time is spent on the merging step where the master concatenates the chunks. . I think the refactoring of the writing that I've done isn't a bad thing to have. I added some extra log statements that it much easier to understand what's going on when it's slow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2333:299,refactor,refactoring,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333,1,['refactor'],['refactoring']
Modifiability,"@tomwhite After spending some time searching for this feature for my testing purposes, it would be helpful to expose the NIO adapter toggle directly from the command line in this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235:125,adapt,adapter,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-418494235,1,['adapt'],['adapter']
Modifiability,"@tomwhite I think the general goal of unifying the Spark/non-Spark tool hierarchies is worthwhile, but I don't like the idea of all tools having `if (sparkArgs.useSpark) {} else {}` boilerplate. If we do this, we should have separate abstract methods that get called automatically in the Spark/non-Spark cases. I also think we should wait to perform this refactoring until later in the quarter, after the Spark evaluation, and after we've standardized more on `Path` for inputs/outputs, as it will break a lot of downstream code in gatk-protected -- and we don't want to break all the tools more than once if we can help it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946:355,refactor,refactoring,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946,1,['refactor'],['refactoring']
Modifiability,"@tomwhite Like we discussed this morning, we can and should get rid of the broadcast code but we should ideally first get some sort of plot we can point to in order to justify the change. This will also be useful for future presentations of our performance improvements. The plot would ideally be a comparison between the new distribution strategy and broadcasting compared across a variable number of cores, so the performance improvement can be better understood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226:383,variab,variable,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226,1,['variab'],['variable']
Modifiability,@tomwhite Looks great. Does `GATKTestPipeline` need updating to accept the environment variable as well?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/596#issuecomment-114991281:87,variab,variable,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/596#issuecomment-114991281,1,['variab'],['variable']
Modifiability,@tomwhite Sorry! I missed your message. This reproduces reliably on our cluster if you run the command I posted above on our current master branch. ```; ./gatk-launch MarkDuplicatesSpark -I file:///home/unix/louisb/flag_stat.bam -O file:///home/unix/louisb/testoutput.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. You'll have to adapt it to your own file system or log into ours. Folders under /home/unix/ are visible to every node in our cluster.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1451#issuecomment-188483706:342,adapt,adapt,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451#issuecomment-188483706,1,['adapt'],['adapt']
Modifiability,@tomwhite What do you think about this change? Do you need a scala 10 build of gatk? We could parameterize the build so that it can build both if necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2264#issuecomment-260671311:94,parameteriz,parameterize,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2264#issuecomment-260671311,1,['parameteriz'],['parameterize']
Modifiability,@tomwhite suggested looking into the possibility of using java.nio.file.Path + plugins for Hadoop and Google buckets to satisfy this ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/958#issuecomment-169082726:79,plugin,plugins,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/958#issuecomment-169082726,1,['plugin'],['plugins']
Modifiability,"@tovanadler I believe the refactoring in question will be minimal (only what is necessary to connect `MarkDuplicatesDataflow` to the `ReadsPreprocessingPipeline`), so the conflict with your branch shouldn't be too bad. If you need a git consultant to assist with the rebase, we're glad to help :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125624982:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125624982,1,['refactor'],['refactoring']
Modifiability,"@tovanadler Review complete. Looks good, just a few comments. I have a few comments about the organization of duplicate marking. I think you've inherited some very old style code that could maybe use some refactoring. I think we do need to also include the histogram and the metrics headers. Those could be done in a separate ticket though. I'm a bit worried that the test is indeterministic. Unless I overlooked something which is likely, it seems like it might depend on the ordering of a PCollection which is undefined. This isn't problematic for the actual metric file, but might be for the tests. What do you think about reorganizing to output an annotation on only 1 of the ""best"" reads with the count of all optical duplicates in it's group. That would simplify the code, and since we only care about the global count it wouldn't change the information content.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958:144,inherit,inherited,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-126762958,2,"['inherit', 'refactor']","['inherited', 'refactoring']"
Modifiability,"@tushu1232 Sorry for the delay. I'm looking into this. I think you've hit a serious bug in the gatk-launch script. Do you have $GATK_LOCAL_JAR set in your environment? . It looks like the case where a local jar is explicitly specified results in the environment variables not being properly defined, which means you end up hitting #2026. ; If they were properly defined you should be seeing the line `Snappy is disabled via system property` included in your standard out. . I've opened #2316 to deal with the issue. Until that's fixed though, the workaround is to invoke the jars directly and add `-Dsnappy.disable=true`. i.e. `java -Dsnappy.disable=true -jar $GATK_LOCAL_JAR SortSam --input BAM_BWA/SRR2.sorted.bam --output hpcinfra/hadoop/test.bam --SORT_ORDER queryname`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119418:262,variab,variables,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-267119418,1,['variab'],['variables']
Modifiability,@vdauwera can you clarify? is this a bug or an enhancement?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-93479914:47,enhance,enhancement,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-93479914,1,['enhance'],['enhancement']
Modifiability,"@vdauwera commented on [Fri Dec 18 2015](https://github.com/broadinstitute/gatk-protected/issues/259). Back in July 2015, @vruano made a laundry list of issues suggesting possible improvements to HaplotypeCaller and related internals. They won't be done in GATK3 so I tagged them as ""HC-refactor"" when I closed them. Whoever ports HC should review those suggestions so that @vruano's wisdom is not wasted. . https://github.com/broadinstitute/gsa-unstable/issues?q=label%3AHC-refactor+is%3Aclosed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2829:287,refactor,refactor,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2829,2,['refactor'],['refactor']
Modifiability,@vdauwera is this a bug or enhancement?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-114259940:27,enhance,enhancement,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-114259940,1,['enhance'],['enhancement']
Modifiability,@vdauwera not in my radar either.... the rewrite of the assembly may fix some of these cases where there is actually some variation that we fail to detect (false negative) that would explain those soft clips. However I don't think that would fix all the cases.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279013205:41,rewrite,rewrite,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279013205,1,['rewrite'],['rewrite']
Modifiability,"@vdauwera note that this modifies the path of the CNV methods doc (which is mostly out of date, but is still linked to in some Comms materials) from docs/CNVs/CNV-methods.pdf to docs/CNV/archived/archived-CNV-methods.pdf. The extended abstract on gCNV is a very technical description of the probabilistic model, but it might be worth referencing it for interested users until a preprint or publication is available.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-467976599:226,extend,extended,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-467976599,1,['extend'],['extended']
Modifiability,"@vdauwera when running spark tools through the gatk-launch-soon-to-be-just-gatk script, you use a `--` to separate arguments to the specific tool you're running from arguments that describe what sort of Spark setup you're trying to submit the job to. The most important of these is probably `sparkRunner`, which says what type of spark cluster you're using: `LOCAL` indicates that you want to simulate a spark cluster on your local machine using multithreading; `SPARK` indicates that you want to submit to a configured, dedicated spark cluster, in which case you have to pass the url to the master node, and `GCS` indicates that you want to use a cluster managed by Google Cloud Dataproc, in which case you need the name of the cluster. See https://github.com/broadinstitute/gatk#running-gatk4-spark-tools-on-a-spark-cluster for more info. Day-to-day on the SV team we primarily use dataproc, so the example command I was going to use for one tool looks like:. ```; ./gatk ParallelCopyGCSDirectoryIntoHDFSSpark \; --input-gcs-path gs://my-bucket/my-data-directory/ \; --output-hdfs-directory hdfs://my-dataproc-spark-cluster-m:8020/my-data \; -- \; --sparkRunner GCS \; --cluster my-dataproc-spark-cluster; ```. I wasn't sure if it was cool to use the GCS/dataproc version or if we'd rather not tie ourselves to GCP in the examples. In this particular case I feel like it might be appropriate since it's a GCS-specific tool, but my question was more about our general strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349358724:509,config,configured,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349358724,1,['config'],['configured']
Modifiability,"@vilay-nference Thank you for doing this work. It's nitpicky annoying stuff to figure out.; ; I have one additional request. Instead of addding additional direct implementation dependencies, could we specify the transtive version requirements in a [gradle constraints block](https://docs.gradle.org/current/userguide/dependency_constraints.html)? . That will: ; 1. make it clear that we don't rely on these directly; 2. prevent us from keeping them around if we do something like remove hadoop in the future; 3. lets us rewrite those force blocks to instead define minimum versions so if the libraries move forward in the future we're not accidentally holding on a to an old version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2297074810:520,rewrite,rewrite,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2297074810,1,['rewrite'],['rewrite']
Modifiability,"@vilay-nference Were you able to get this configuration to pass tests on your end? I've attempted to incorporate your changes into https://github.com/broadinstitute/gatk/pull/8998, but I'm running into issues with hadoop and protobuf incompatibilities. I see the same problem with your branch when I try to run tests on it. (I also can't run tests without disabling -Werror on your branch since there are still some unresolved deprecation and other minor issues). Errors look like this:. ```; Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.proto.SecurityProtos [in thread ""IPC Server handler 1 on default port 64812""]; 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.<clinit>(ClientNamenodeProtocolProtos.java); ```. and you can easily trigger one by running `ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680:42,config,configuration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8950#issuecomment-2412827680,1,['config'],['configuration']
Modifiability,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5164:137,inherit,inheritor,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164,1,['inherit'],['inheritor']
Modifiability,@vruano Back to you. This refactoring is starting to look really nice thanks to your review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1918#issuecomment-232558623:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1918#issuecomment-232558623,1,['refactor'],['refactoring']
Modifiability,"@vruano What happens if you make your tool extend `GATKSparkTool` directly, rather than `VariantWalkerSpark`, then do the following in your `runTool()` method?. ```; final VariantsSparkSource variantsSource = new VariantsSparkSource(ctx);; final List<SimpleInterval> intervals = hasIntervals() ? getIntervals() : IntervalUtils.getAllIntervalsForReference(getBestAvailableSequenceDictionary());; final JavaRDD<VariantContext> variants = variantsSource.getParallelVariantContexts(vcf, intervals);; ```. And then do a `variants.mapPartitions()` call on the resulting `variants` RDD to process each variant?. Also, at-mentioning @tomwhite here to comment on the `VariantWalkerSpark` issue, since he wrote that class.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290236139:43,extend,extend,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290236139,1,['extend'],['extend']
Modifiability,@vruano You could extend `CommandLineProgram` rather than `GATKTool`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-576783251:18,extend,extend,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-576783251,1,['extend'],['extend']
Modifiability,@vruano commented on [Wed Jun 17 2015](https://github.com/broadinstitute/gatk-protected/issues/39). There some issues in the documentation text in package-info.java that was not updated properly after a last minute refactoring. This task is neither nor not urgent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2823:215,refactor,refactoring,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2823,1,['refactor'],['refactoring']
Modifiability,"@vruano has pointed out that our method of finding the best haplotype paths in an assembly graph is equivalent to Dijkstra's algorithm for finding the shortest path in a directed graph, and that the latter is a much simpler implementation. [ Do I understand this correctly?]. We could simplify a bunch of code without changing the output of any tools by switching the implementation to Dijkstra's algorithm, which is implemented in jgrapht (our graphs extend this package's graph class) and apache commons. @vruano has also pointed out that our definition of the best paths may not be optimal, which is a separate issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3561:452,extend,extend,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3561,1,['extend'],['extend']
Modifiability,@vruano in addition to refactoring MAthUtils I also engaged in a few boy scout rule activities.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2235#issuecomment-256973513:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2235#issuecomment-256973513,1,['refactor'],['refactoring']
Modifiability,@vruano refactoring is done -- back to you,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7122#issuecomment-841279671:8,refactor,refactoring,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7122#issuecomment-841279671,1,['refactor'],['refactoring']
Modifiability,"@wangdy12 .I get the same issue: when use HaplotypeCallerSpark on a cluster, It lose a lot of variable sites and the result jitter to the same input bam. and running on local mode, the result is good.; But I fix the code according to your way. It does not work and get the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371410958:94,variab,variable,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371410958,1,['variab'],['variable']
Modifiability,"@wujh2017 Did you set up the appropriate conda environment as described in the README?. > Python 3.6.2, along with a set of additional Python packages, are required to run some tools and workflows. GATK uses the Conda package manager to establish and manage the environment and dependencies required by these tools. The GATK Docker image comes with this environment pre-configured. In order to establish an environment suitable to run these tools outside of the Docker image, the conda gatkcondaenv.yml file is provided. To establish the conda environment locally, Conda must first be installed. Then, create the gatk environment by running the command conda env create -n gatk -f gatkcondaenv.yml (developers should run ./gradlew createPythonPackageArchive, followed by conda env create -n gatk -f scripts/gatkcondaenv.yml from within the root of the repository clone). To activate the environment once it has been created, run the command source activate gatk. See the Conda documentation for additional information about using and managing Conda environments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-364748538:370,config,configured,370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-364748538,1,['config'],['configured']
Modifiability,"@xaviloinaz here's that feature you requested for configurable ordering of funotations via `VariantClassifications`. It's basically what we talked about - you supply a TSV file with `VARIANT_CLASSIFICATION SEVERITY`, and Funcotator will respect that new order. . Let me know if this won't work for what you wanted to do.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7673#issuecomment-1040805481:50,config,configurable,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7673#issuecomment-1040805481,1,['config'],['configurable']
Modifiability,"@xysj1989 We primarily run this workflow using the WDL on Terra. In this case, each GermlineCNVCaller shard is run on a separate VM using the GATK Docker. Hopefully, we can always at least guarantee that this default mode of running the workflow is functional and covered by tests. However, if you'd like to instead run multiple instances of GermlineCNVCaller locally, you may need to make sure certain environment variables are set appropriate. For example, I think you can address (2) above (the location of the temporary theano directory) by either setting environment variables or modifying your Theano configuration (see http://deeplearning.net/software/theano/library/config.html) appropriately. You may also want to check the GermlineCNVCaller task in the WDL to see how other variables are set there. Let me look into whether you can also address (1) in this way, or if this will require a GATK code change, and get back to you. (Of course, if you figure it out before me, please follow up!) Thanks again for bringing this to our attention.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548007200:415,variab,variables,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548007200,5,"['config', 'variab']","['config', 'configuration', 'variables']"
Modifiability,"@yfarjoun I'm not understanding... If we're on the reverse strand, then we reach the adaptor at the 5' end of the forward strand i.e. at one `getMateStart() + 1`, which is what the code does now. If we're on the forward strand the equivalent logic would be `getMateEnd() + 1`, but no such method exists, so we use `read.getStart() + abs(read.getFragmentLength())`. Why is this not equivalent?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358740758:85,adapt,adaptor,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358740758,1,['adapt'],['adaptor']
Modifiability,A collection of changes to enhance reliability and ease-of-use. Users no longer have to make a table containing the sample names to extract in GvsPrepareCallset (which was painful) and they don't have to re-supply that same table when rendering the VCF in GvsExtractCallset (which was error prone). GvsPrepareCallet now takes a file of sample names as a parameter as well as an export table _prefix_. The main `sample_info` table is then subset to the sample names in the supplied file and stored in the table `{export_prefix}__SAMPLES`. The export table is created and now named `{export_prefix}__DATA`. GvsExtractCallset now only needs to take this export prefix and is able to get the sample list and data it needs from these tables. @ericsong -- does this fit the AoU use case well?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7272:27,enhance,enhance,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7272,1,['enhance'],['enhance']
Modifiability,"A comment from @ldgauthier:. The threshold for PLs to be considered uninformative is the same between GATK3 and 4, but the stack when it gets called is a little bit different. It might be changes in subsetting again because the methods that evaluate the ""informativeness"" in GATK3 are called updateGenotypeAfterSubsetting and createGenotypesWithSubsettedLikelihoods, which appear to have been refactored into AlleleSubsettingUtils in GATK4. I could see how if we calculate the sum before subsetting in GATK4 then it's smaller and uninformative, but that's speculation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2712#issuecomment-305601294:393,refactor,refactored,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2712#issuecomment-305601294,1,['refactor'],['refactored']
Modifiability,A couple of things we can try:. Make sure that the temp directories created by RunSGAViaProcessBuilderOnSpark are actually being created on the /tmp filesystem. Maybe there's some Spark configuration that is overriding that?. Try removing the assembly part and just running writeToLocal() on the text files dataset. Then we can see if something's getting stuck in the assembly process or if it's a file-handling problem.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-223582523:186,config,configuration,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877#issuecomment-223582523,1,['config'],['configuration']
Modifiability,"A few minor issues:. - [x] Change `--resource <blah>` to `--resource:<blah>` in tool-level documentation. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [x] The VCF writer in VariantRecalibrator has a few conditionals to allow for VCF headers without contig lines, we could do the same for the writer in LabeledVariantAnnotationsWalker. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [ ] Double check whether we should worry about any differences in extraction on test data (provided via email) from https://gatk.broadinstitute.org/hc/en-us/community/posts/7974912707099-VariantRecalibrator-IndexOutOfBoundsException. Probably nothing to worry about, and at least the error messaging in the new tools is more informative.; - [x] We could change the strategy for checking for resource overlaps to require allele-level matching (rather than only matching on start position, as was inherited from VQSR). A quick test on malaria shows that this can reduce the number of overlaps by O(10%), but performance doesn't really change too much. Branch is already open at https://github.com/broadinstitute/gatk/tree/sl_lite_overlap; - [ ] Expand the exact-match tests to cover some of these strategies, which were added separately in #8049 and merged to make a release deadline.; - [x] Catch the exception in https://github.com/broadinstitute/gatk/blob/fd782504d18b56dbc266c2b3bb4eb32f21916776/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/LabeledVariantAnnotationsWalker.java#L389 and throw the same message that is thrown in AS mode. Added in #8074.; - [x] Add message to the score tool that the scores HDF5 file will not be out when the input VCF is empty (such a message is already emitted about the annotations HDF5 file). Added in #8074.; - [ ] Megan suggested in the review of #8074 that dynamic disk sizing could be added to the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946:909,inherit,inherited,909,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946,1,['inherit'],['inherited']
Modifiability,A few tests that are not marked as either `cloud` or `bucket` are now failing in Travis due to undefined environment variables -- should fix this before merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/545#issuecomment-109324558:117,variab,variables,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/545#issuecomment-109324558,1,['variab'],['variables']
Modifiability,"A fix I found for this issue is to modify `start_session_get_args_and_model` function in models.py from this:. ```; def start_session_get_args_and_model(intra_ops, inter_ops, semantics_json, weights_hd5=None, tensor_type=None):; K.clear_session(); K.get_session().close(); cfg = K.tf.ConfigProto(intra_op_parallelism_threads=intra_ops, inter_op_parallelism_threads=inter_ops); cfg.gpu_options.allow_growth = True; K.set_session(K.tf.Session(config=cfg)); return args_and_model_from_semantics(semantics_json, weights_hd5, tensor_type). ```. To this:. ```; import tensorflow as tf. def start_session_get_args_and_model(intra_ops, inter_ops, semantics_json, weights_hd5=None, tensor_type=None):; tf.keras.backend.clear_session(); tf.keras.backend.get_session().close(); cfg = tf.ConfigProto(intra_op_parallelism_threads=intra_ops, inter_op_parallelism_threads=inter_ops); cfg.gpu_options.allow_growth = True; tf.keras.backend.set_session(tf.Session(config=cfg)); return args_and_model_from_semantics(semantics_json, weights_hd5, tensor_type). ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250#issuecomment-839720987:284,Config,ConfigProto,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250#issuecomment-839720987,4,"['Config', 'config']","['ConfigProto', 'config']"
Modifiability,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1453:9,plugin,plugin,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453,4,"['config', 'plugin']","['configuration', 'configure', 'plugin']"
Modifiability,A holdover for this is currently in place where we detect if no funcotations were produced at all. In that case we warn the user that they may have configured the reference version and data sources incorrectly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497:148,config,configured,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497,1,['config'],['configured']
Modifiability,"A logger with configurable verbosity would be great, but low priority is fine. This is a very low priority issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300304840:14,config,configurable,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300304840,1,['config'],['configurable']
Modifiability,"A missing environment variable reports as `DATAFLOW_TEST_PROJECT`, the actual environment variable is `HELLBENDER_TEST_PROJECT`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/571:22,variab,variable,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/571,2,['variab'],['variable']
Modifiability,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2621:306,refactor,refactored,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621,1,['refactor'],['refactored']
Modifiability,"A user building GATK on a POWER system tried to set `GATK_SKIP_NATIVE_BUILD=true`, but it didn't prevent the build from failing. ```; FAILURE: Build failed with an exception. * What went wrong:; A problem occurred configuring root project 'gatk'.; > Exception thrown while executing model rule: NativeComponentModelPlugin.Rules#createBinaries; > Invalid NativePlatform: linux_ppc64. BUILD FAILED; ```. the temporary workaround was the following change:. ```; $ diff build.gradle.org build.gradle; 406c406,408; < VectorLoglessPairHMM(NativeLibrarySpec) {. ---; > if(System.properties[""os.arch""] != ""ppc64""); > {; > VectorLoglessPairHMM(NativeLibrarySpec) {; 458a461; > }; ```. This is a bug and is likely to cause failures on other systems as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1711:214,config,configuring,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711,1,['config'],['configuring']
Modifiability,"A user reported this same issue and the error message does not give any location information in GATK 4.1.9.0 with VariantRecalibrator. ; [Link to the forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360074618292-New-version-of-GATK-leads-to-VariantRecalibrator-error-?page=1#community_comment_360013419291).; Here is the message:; ```; Using GATK jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms24g -jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - --------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:837,polymorphi,polymorphic,837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['polymorphi'],['polymorphic']
Modifiability,"About ""this version also moves the files around so they match the local tree"" what I meant is that it expects the files to be somewhere else. I did the actual shuffling via gsutil.; The change is at BaseRecalibratorDataflowIntegrationTest:21, changing the semantics of the; HELLBENDER_TEST_INPUTS environment variable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107677548:309,variab,variable,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107677548,1,['variab'],['variable']
Modifiability,AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /root/gatk.jar FilterMutectCalls -V gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeVCFs/Abrams_cell-unfiltered.vcf -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -O Abrams_cell-filtered.vcf --contamination-table /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/contamination.table --tumor-segmentation /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/segments.table --ob-priors /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:8677,variab,variable,8677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['variab'],['variable']
Modifiability,"According to [Build times out because no output was received](https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received), we should carefully use travis_wait, as it may make the build unstable and extend the build time. =====================; If there are any inappropriate modifications in this PR, please give me a reply and I will change them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7418:241,extend,extend,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7418,1,['extend'],['extend']
Modifiability,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:487,polymorphi,polymorphisms,487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,1,['polymorphi'],['polymorphisms']
Modifiability,"Actually what you said is correct: it is very painful to extend the `Main` class and use the current implementation. The simplest `Main` class that I'm using it's not extending the GATK one just because of the static methods (see the code [here](https://github.com/magicDGS/thaplv/blob/master/src/main/java/org/magicdgs/thaplv/Main.java)), that's why I would like to include this change. In few minutes I will commit the changes that you propose, including the method and the change from static. Thanks for the feedback, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430:57,extend,extend,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204#issuecomment-255841430,2,['extend'],"['extend', 'extending']"
Modifiability,"Actually, I think this is it: https://github.com/broadinstitute/gatk/compare/sl_het_pulldown_spark. I did take a crack at Spark-ifying the het pulldown in this (ancient) branch, but I can't claim that I knew what I was doing---if I remember correctly, I just tried to adapt some code @akiezun had in https://github.com/broadinstitute/gatk/tree/ak_coverage_spark. I don't think I ever implemented proper CIGAR-string parsing either. Might be better to start from scratch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1558#issuecomment-223475794:268,adapt,adapt,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1558#issuecomment-223475794,1,['adapt'],['adapt']
Modifiability,"Actually, just ran a WGS sample with 250bp bins that took ~4 hours to plot...pretty ridiculous! The R code is neither efficient nor well written, so I'm inclined to completely rewrite plotting in python (for ACNV, as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3554#issuecomment-329195610:176,rewrite,rewrite,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3554#issuecomment-329195610,1,['rewrite'],['rewrite']
Modifiability,"Adapt PGEN extract to work with Cromwell's ""Retry with more memory"" feature to address issues with a small percentage of ""problem"" shards OOMing. Successful run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dd29b0d9-73e5-4e1b-af83-e4fba53c0c65).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8754:0,Adapt,Adapt,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8754,1,['Adapt'],['Adapt']
Modifiability,Adapt for Avro 1.11 behavior of throwing on get()s of non-existent fields [VS-860],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8266:0,Adapt,Adapt,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266,1,['Adapt'],['Adapt']
Modifiability,Adaptive assembly graph pruning,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4867:0,Adapt,Adaptive,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867,1,['Adapt'],['Adaptive']
Modifiability,Adaptive pruning option for local assembly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473:0,Adapt,Adaptive,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473,1,['Adapt'],['Adaptive']
Modifiability,Add PEP8 python style with type hints and use model directories instead of separate arguments for config and weights.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5548:98,config,config,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5548,1,['config'],['config']
Modifiability,Add a file-based configuration mechanism to GATK (with ability to override),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368:17,config,configuration,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368,1,['config'],['configuration']
Modifiability,Add engine level argument to selectively ignore soft-clipped bases due to adapter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346:74,adapt,adapter,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346,1,['adapt'],['adapter']
Modifiability,Add getReadFiltersToUse() to read filter plugin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2364:41,plugin,plugin,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2364,1,['plugin'],['plugin']
Modifiability,Add new configuration entry for plugins,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4611:8,config,configuration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611,2,"['config', 'plugin']","['configuration', 'plugins']"
Modifiability,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370:613,variab,variables,613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370,1,['variab'],['variables']
Modifiability,Add the ability (either as command-line options or config file options) for a user to specify default values for certain annotations (i.e. `Center`).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3783:51,config,config,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3783,1,['config'],['config']
Modifiability,"Addded abstract class MachineLearningUtils to provide an interface and; handle common tasks. These include loading data, splitting data into; training and test sets, cross-validation, and optimizing classifier; hyperparameters. Also added XGBoostUtils which provides a concrete implemention of; MachineLearningUtils (by wrapping xgboost4j) and serves as an example; of how to provide access to a 3rd-party machine learning library. Finally, added an example tool: ExampleTrainXGBoostClassifier. This; demonstrates a typical training use case of loading data, training a; classifier, assessing accuracy, and saving the classifier. It also; demonstrates a typical filtering use case of loading a saved classifer,; and using it to calculate probabilities or class labels. This is working towards issue 4922 by providing the tools necessary to; train classifiers in general, but does not provide tools to train a; BreakpointEvidence filter, so does not resolve it. Additionally, this; framework should eventually be extended to provide a bayesian; hyperparameter optimizer. One outstanding problem with these changes is that xgboost4j threading; does not appear to work on OSX, resulting in slower training. However,; it does work on linux.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5146:1012,extend,extended,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146,1,['extend'],['extended']
Modifiability,Added PossibleDenovo to the VariantAnnotatorEngine/PluginSystem,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663:51,Plugin,PluginSystem,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663,1,['Plugin'],['PluginSystem']
Modifiability,Added a check for whether files can be created and executed within the configured tmp-dir,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8951:71,config,configured,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8951,1,['config'],['configured']
Modifiability,"Added a few comments of my own -- requested that you refactor to check the index modification time in the `FeatureDataSource` constructors, rather than in the sequence dictionary validation routines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324:53,refactor,refactor,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3063#issuecomment-320948324,1,['refactor'],['refactor']
Modifiability,Added ability to override THEANO_FLAGS environment variable in gCNV tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6244:51,variab,variable,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6244,1,['variab'],['variable']
Modifiability,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4472:1597,refactor,refactor,1597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472,1,['refactor'],['refactor']
Modifiability,"Added gCNV PROBPROG 2018 extended abstract, archived notes on CNV methods, and deleted some legacy documentation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732:25,extend,extended,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732,1,['extend'],['extended']
Modifiability,Added in Owner style configuration file with some basic hooks.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447,1,['config'],['configuration']
Modifiability,"Added in argument for MAF out.; Added more ""required"" MAF fields.; Added Funcotation::getDataSourceName; Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4472:706,Refactor,Refactored,706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472,1,['Refactor'],['Refactored']
Modifiability,Added in code to pull config elements from the Owner configuration.; Hooked the configuration into the classes where it is necessary.; Added in a config file with defaults.; Added in utilities and consolidated hooks for configuration code.; Added in help text for configuration file options in gatk-launch. Basic configuration options have been implemented and hooked; into files where appropriate. Configuration values in properties files cannot currently have; trailing spaces - this results in a parsing error. There is a; workaround that has not yet been implemented. Fixes #3126,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447:22,config,config,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447,8,"['Config', 'config']","['Configuration', 'config', 'configuration']"
Modifiability,"Added the ability to specify IntervalMergingRule.NONE so so that no merging is performed. Also added the ability to request from IntervalArgumentCollection the unmerged user intervals. I have not solved the underlying issue that the GenomeLocSet requires non-overlapping intervals, though I acknowledge that replacing or refactoring that class is the long term solution to this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887:321,refactor,refactoring,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887,1,['refactor'],['refactoring']
Modifiability,Added the following methods to `GATKTool`:. - `getReferenceDataSource()`; - `getReadsDataSource()`; - `getFeatureManager()`. `Walker` inherits directly from `GATKTool` and overrides these methods to throw an exception if they are called. No walker should need to directly access the data.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4964:134,inherit,inherits,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964,1,['inherit'],['inherits']
Modifiability,"Added two new optional flags to `SplitIntervals`, and their corresponding tests. 1. `--prefix` for adding a prefix to the created interval files; 2. `--digits` for configuring the number of digits used to enumerate the interval files. This modifications were requested by a community user (#7157)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7488:164,config,configuring,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7488,1,['config'],['configuring']
Modifiability,"Adding a new method `getVariantCacheLookAheadBases` to `VariantWalkerBase` which allows subclasses to set how far to look ahead when caching variants. This may help reduce memory use in GenotypeGVCFs. This also changes the side inputs to use FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES which is `1000`, this is the value used by the other tools. I'm not sure if that's the right thing to do, but it makes variant walkers more consistent with other tools. Alternatively we could add a separate configuration method that lets tools change the side input value. We could also expose an optional parameter in the feature input that lets you set that on a per input basis if we need it. . This doesn't seem to have any negative effect on performance for genotypegvcfs, but it's hard to tell from short runs. It's also hard to tell if it's improving memory usage. It doesn't seem to make an appreciable difference at random places in the genome, but I'm hoping it will make a difference in very bad locations that have a lot of variation. Ideally our caches would be based on size rather than number of variants, but that's a more complicated change. fixes #3471",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:497,config,configuration,497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,1,['config'],['configuration']
Modifiability,Adding in a config file for Funcotator.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960:12,config,config,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960,1,['config'],['config']
Modifiability,Adding in a configurable delay after writing a batch of data to BigQuery,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8188:12,config,configurable,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8188,1,['config'],['configurable']
Modifiability,"Additional feedback from the user for the mutect2 workflow. > ""Of note, it is really difficult and not really 'user-friendly' to have to predict disc space and runtime for Funcotator, which seem to depend (based on calculations you copied above from other Functotator workflows) on outputs of Mutect2 (eg vcf sizes), when here Mutect and Funcotator and bundled together. So I cannot see output of Mutect to predict values for Funcotator - especially not when I get to run this over hundreds of samples. It is also pricey to have jobs failing because of this. It would be much better to have these variables encoded, so that the algorithm uses Mutect outputs to predict memory etc. that it will need to run Funcotator downstream. If this is really how things work (and this is my current understanding), I really do not know how to estimate this for many samples without 'trial and error' that is both costly and it will take extremely long time....""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230:597,variab,variables,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230,1,['variab'],['variables']
Modifiability,"Addresses #6242. Current behavior: when all the reads in a read group are filtered in the base recalibration step, the read group is not logged in the recal table. Then ApplyBQSR encounters these reads, can't find the read group in the recal table, and throws an error. New behavior: if `--allow-read-group` flag is set to true, then ApplyBQSR outputs the original quantities (after quantizing). . I avoided the alternative approach of collapsing (marginalizing) across the read groups, mostly because it would require a complete overhaul of the code. I also think that using recal data from other read groups might not be a good idea. In any case, using OQ should be good enough; I assume that these ""missing"" read groups are low enough quality to be filtered out and are likely to be thrown out by downstream tools. I also refactored the BQSR code, mostly to update the variable and class names to be more accurate and descriptive. For instance:. ReadCovariates.java -> PerReadCovariateMatrix.java; EstimatedQReported -> ReportedQuality",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9020:825,refactor,refactored,825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9020,2,"['refactor', 'variab']","['refactored', 'variable']"
Modifiability,"Addresses https://github.com/broadinstitute/dsp-spec-ops/issues/307. - Increase headroom on VM above Java; - Increase disk space (incidental, not related to OOM); - parameterized Gnarly usage, default to false; - --emit-pls set to false no longer pulls down PLs. Compared results against baseline and saw no changes in GIAB results using ACMG cohort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7245:165,parameteriz,parameterized,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7245,1,['parameteriz'],['parameterized']
Modifiability,"Adds PathSeqPipelineSpark master tool, which required some minor changes:; -Refactored PSScoreUtils class to a PSScorer, which now includes the main Score tool code; -Moved code for paring down the pathogen header into a new function removeUnmappedHeaderSequences(). Spark-related optimizations:; -Removed cache() calls when possible, and replaced with persist(), spilling to disk with serialization, if necessary; -Removed try-with-resources in Filter and Bwa tools, which seemed to be causing the BWA/kmer references to be unloaded prematurely. Other changes:; -Changed ambiguous base filter from using a fraction of bases to number of bases; -Added function for closing all kmer filter instances; -Optimized PSBwaAligner SA tag construction; -Renamed repartitionPairedReads() to repartitionReadsByName(); -Resolved some conflicting tool argument names; -Created PathSeq tool program group; -Filled out tool summary strings",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3271:76,Refactor,Refactored,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271,1,['Refactor'],['Refactored']
Modifiability,"Adds a couple warnings in case the user can write to the configured tmp-dir but can't set the files as executable or execute them, with the assumption in the latter case that the cause is likely that the directory is mounted using the ""noexec"" mount option. I'm not sure if a test makes sense for this, because it would probably require mounting a directory for testing, and the user running the tests may not have permissions to do that. I think only root can mount with options in Ubuntu? Do we require root for other tests?. Fixes #8453",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8951:57,config,configured,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8951,1,['config'],['configured']
Modifiability,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8588:987,refactor,refactoring,987,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588,1,['refactor'],['refactoring']
Modifiability,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354:65,adapt,adapter,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354,1,['adapt'],['adapter']
Modifiability,"Adds the `FuncotateSegments` tool. . *`FuncotateSegments` does not support VCF input!*. This tool will create two output files from a GATK seg file:; - A simple TSV which has each segment of the input file funcotated with all the genes it overlaps and which gene/exon covers each breakpoint. The output format is meant to (closely) match Oncotator. ; - A gene list which has every gene, covered by a segment, listed with the segment that covers it. A gene can appear more than once if a segment breakpoint overlaps the gene (i.e. more than one segment overlaps the gene). The output format is meant to (closely) match Oncotator.; - Output formats may change.; - Input format is only seg files such as those generated from `ModelSegments`. . Dev and reviewer notes:; - Includes refactoring to drive much of the GencodeFuncotation data solely from the transcript. As opposed to a mix of the transcript and gene. This does cause some changes to sorting of the GencodeFuncotations (easily seen in the other transcripts field). It turns out that the transcript type field has different values for each transcript. This causes many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:777,refactor,refactoring,777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['refactor'],['refactoring']
Modifiability,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:706,variab,variable,706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['variab'],['variable']
Modifiability,"After merging #2085, a pluging for `ReadTransformer` should be added using the pluging framework. The arguments should include if it is going to be applied before or after filtering. This was decided after discussion in #2084.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2160:23,plugin,pluging,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2160,2,['plugin'],['pluging']
Modifiability,Agree that we should do the equivalent of what we did for read filters for annotations using the new plugin framework. Thanks for offering to help out on this one @magicDGS -- any contributions you're able to make here are very much appreciated!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288771998:101,plugin,plugin,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288771998,1,['plugin'],['plugin']
Modifiability,"Agree with that, but also it will be nice to be able to configure it if used outside the AnnotationEngine - the same for other usages of OneShotLogger...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3828#issuecomment-345246188:56,config,configure,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3828#issuecomment-345246188,1,['config'],['configure']
Modifiability,"Agree with those above arguing that VCF isn't appropriate for this purpose, and would be a very bad fit. I certainly support the goal of adopting a single unified, standard format for tabular data throughout the GATK, however, and would ideally favor a solution at the HTSJDK/tribble level to get all the benefits that that provides, such as support for NIO and indexing (even if it means that engine team has to extend tribble to support records that are not `Locatable`). . We're happy to help with any efforts in the direction of unification, and would be eager to participate in any methods-wide discussions on this topic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-387100399:413,extend,extend,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-387100399,1,['extend'],['extend']
Modifiability,Agreed! And also with default value stored in the configuration file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3471#issuecomment-324070716:50,config,configuration,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3471#issuecomment-324070716,1,['config'],['configuration']
Modifiability,Ah - rewrite export cohort,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6571:5,rewrite,rewrite,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6571,1,['rewrite'],['rewrite']
Modifiability,"Ah -- maybe I'm mistaken. Or can it be a difference in how they're; applied/invoked?. Maybe there's some inconsistency in behavior. Would be nice to iron this; all out. On Mon, Feb 23, 2015 at 5:17 PM, Louis Bergelson notifications@github.com; wrote:. > I'm pretty sure they all do... or at least all can depending on how you; > configure your MalformedReadFilter.; > ; > example:; > ; > ```; > private static boolean checkHasReadGroup(final SAMRecord read) {; > if ( read.getReadGroup() == null ) {; > // there are 2 possibilities: either the RG tag is missing or it is not defined in the header; > final String rgID = (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; > if ( rgID == null ); > throw new UserException.ReadMissingReadGroup(read);; > throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; > }; > return true;; > }; > ```; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75649333; > . ## . Geraldine A. Van der Auwera, Ph.D.; Bioinformatics Scientist II; GATK Support & Outreach; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686467:329,config,configure,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686467,1,['config'],['configure']
Modifiability,"Ah the ""Tool Docs Index"" link thing is because I left out the extension on purpose; our php server is configured to grab whatever file is present with that basename, with a rule of precedence in case there are several with different extensions (which is useful because of reasons). But locally the browser doesn't know to do that. I'm putting in some logic to handle this, thanks for pointing it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3165#issuecomment-311098545:102,config,configured,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165#issuecomment-311098545,1,['config'],['configured']
Modifiability,"Ah, ok, so you would extend `Main` anyway even if we had a way to control which packages it looks in? That's good to know. Can you not use base test because of that GenomeLocParser? We should fix that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242816026:21,extend,extend,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242816026,1,['extend'],['extend']
Modifiability,"Ah, well, if bash is opaque to you then that's not the right file for you! This script has to be adapted before it can be useful, generally, unless I guess you're looking for the exact same bug I was looking for at the time. I'll try to add more comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/913#issuecomment-143079634:97,adapt,adapted,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/913#issuecomment-143079634,1,['adapt'],['adapted']
Modifiability,"All implementations of `GATKRead` should ideally agree on String representation -- the adapter classes should all implement `toString()`, and do so consistently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/624:87,adapt,adapter,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/624,1,['adapt'],['adapter']
Modifiability,"All of the Locatable collections associated with a sample and backed by a TSV file (e.g., read-count files, copy-ratio files, segment files, allelic-count files) extend SampleLocatableCollection in the new CNV pipeline. SampleRecordCollection is even more generic, in that the records are not required to be Locatables; this will be used to output posterior summaries for modeling results, for example. Eventually we will want to expand the sample metadata to include a sequence dictionary when appropriate. This will be used to define the ordering in SampleLocatableCollection. For now, we keep lexicographical ordering to be consistent with the old read-count collection, but we should switch this over as soon as possible. @asmirnov239 we will fit your new read-count code into this framework when it's in. @droazen would appreciate any thoughts you might have on whether it's worth using the Tribble framework rather than TableReader/TableWriter for this sort of thing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3716:162,extend,extend,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3716,1,['extend'],['extend']
Modifiability,Allow common dataflow options to be specified via a config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508:52,config,config,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508,1,['config'],['config']
Modifiability,"Allow sane combinations of enable/disable plugin arguments, disallow insane combinations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377:42,plugin,plugin,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377,1,['plugin'],['plugin']
Modifiability,Almost all of the CNV tools extend `CommandLineProgram` unless they are walkers or otherwise need to use `-L`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-357534210:28,extend,extend,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-357534210,1,['extend'],['extend']
Modifiability,"Also as part of the mock-up, we should actually package the mock config files inside of our jar, load them off the classpath, and test file-based override ability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353:65,config,config,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353,1,['config'],['config']
Modifiability,"Also extracted some argument collections and genotyping code (see https://github.com/broadinstitute/gatk/issues/3915), fixed up some documentation, and did some refactoring to the Segmenter classes. This is just a first implementation for evaluation and feedback. There is some redundant (but cheap) computation performed in the genotyping step and both the genotyping and segmentation steps are not optimized for memory use. However, since requirements are not onerous (probably around ~10GB memory and <10 minutes for ~10 typical WGS samples), it might not be worth fixing up at the expense of extra code. Likewise, this implementation requires all inputs be available. We could relax this to allow optional dimensions of input (i.e., copy ratios or allele counts) and/or case-only mode (as in ModelSegments), at the expense of extra control-flow code. One could also perform segmentation with an external tool and pass it to ModelSegments, as long as it is properly formatted. Closes #2924.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499:161,refactor,refactoring,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499,1,['refactor'],['refactoring']
Modifiability,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5556:47,variab,variable,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556,1,['variab'],['variable']
Modifiability,Also refactored the `VcfOutputRenderer` sanitization code. Fixes #5671,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5817:5,refactor,refactored,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5817,1,['refactor'],['refactored']
Modifiability,Also some tests for collection classes. Refactoring may avoid code duplication here.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910:40,Refactor,Refactoring,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910,1,['Refactor'],['Refactoring']
Modifiability,"Also, a necessary functionality is fragment-based counts. Perhaps a good starting point is bringing back Valentin's GATKReadPair which was removed as a part of XHMM-related code cleanup. Eventually, it is also useful to have the option of collecting various summary statistics and empirical distribution for features such as insert length, mapping quality, the position with respect to bins. Other things to consider are summary of orphan reads, reads with mates on other contigs, and reads with multiple alternate alignments. This requires a bit of good software engineering but it's a necessary one-time investment. This is a relatively high-priority refactoring given the growing interest in running gCNV on large datasets (gnomAD, gen psy cohort, TCGA).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3153#issuecomment-310507118:653,refactor,refactoring,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3153#issuecomment-310507118,1,['refactor'],['refactoring']
Modifiability,"Also, could you provide at what level is the logging configured @jjfarrell ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4531#issuecomment-373807025:53,config,configured,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531#issuecomment-373807025,1,['config'],['configured']
Modifiability,"Also, good point on the arg name collision. The command line parser handles this (for any args, even across plugins or across anywhere) but I didn't see any tests, so I added tests to both CommandLineParserUnitTest and CommandLineParserPluginUnitTest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-244960357:108,plugin,plugins,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-244960357,1,['plugin'],['plugins']
Modifiability,"Also, in GATK3, PluginManager was used to lookup plugin w/ Reflection, such as:. new PluginManager<RequiredStratification>(RequiredStratification.class).getPlugins()). Is there a similar concept in GATK4?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343577587:16,Plugin,PluginManager,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-343577587,3,"['Plugin', 'plugin']","['PluginManager', 'plugin']"
Modifiability,"Also, the cloud tests should be skipped by default (ie., in the case of no environment variables set).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279083759:87,variab,variables,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404#issuecomment-279083759,1,['variab'],['variables']
Modifiability,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5477:559,config,configuration,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477,1,['config'],['configuration']
Modifiability,"Also, we might want to write some tests for gatk-launch, since it's starting to have lots of different invocation configurations and I find that any line of python code I don't actually run tends to have some horrible error that a compiler would have complained about.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2090#issuecomment-239881662:114,config,configurations,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2090#issuecomment-239881662,1,['config'],['configurations']
Modifiability,"Although I was again wrong by static-blocks and inheritance (see https://github.com/broadinstitute/gatk/issues/3483), I think that in the case of the tests is better to make overridable this config - thus, I keep it open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030:48,inherit,inheritance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030,2,"['config', 'inherit']","['config', 'inheritance']"
Modifiability,Although womtool-65.jar indicates syntax is correct for https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl however there are two external sources that say the syntax is incorrect for [line 1036](https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl#L1036). Both the VS Code WDL plugin by Broad indicates this is incorrect syntax (see image). Also DNAnexus's `dxWDL.jar` also gives a `wdlTools.syntax.SyntaxException: invalid place holder at 1036:40-1036:105 in /home/dnanexus/mutect2.wdl`. gatk/scripts/mutect2_wdl/mutect2.wdl; ![mutect2 master wdl — analysis-workflows 2021-07-16 12-25-27](https://user-images.githubusercontent.com/78239029/125986167-4e8b04a5-c594-42a6-ba1c-ff283949d04a.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7353:339,plugin,plugin,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7353,1,['plugin'],['plugin']
Modifiability,"Am going to address part of this ticket in my next branch by giving tools a way to indicate that particular data sources are required or optional, even when the inherited argument itself is marked as optional. Once this is done, all we probably need to do is change the code that displays the tool arguments to the user so that they take each tools requirements into account when displaying args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/149#issuecomment-76432333:161,inherit,inherited,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/149#issuecomment-76432333,1,['inherit'],['inherited']
Modifiability,"And as an addition to the above case, I have found this problematic representation of alleles also in variable sites in newly generated vcf files (gatk version 4.3.0.0):; NC_041772.1 40006060 . GAC G,AAC; NC_041772.1 40006061 . A T,G,C,*; NC_041772.1 40006062 . C T,*",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8030#issuecomment-1420899124:102,variab,variable,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8030#issuecomment-1420899124,1,['variab'],['variable']
Modifiability,"And in `scripts/sv/manage_sv_pipeline.sh`. As suggested by @cwhelan when reviewing PR #4328 . > One suggested feature enhancement if you feel like it while you're working on these scripts: most of the time I don't want to write out and copy all of the fastq files for the assemblies (mostly because it takes a long time to copy them out of the cluster), but occasionally I do. It would be nice if the scripts by default didn't move the fastqs (this could be accomplished either by not writing them or by excluding them from the copy -- @TedBrookings didn't you have code that did the latter before?), but had an optional flag to enable them. @TedBrookings assigning to you for now, but it may not be a priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4332:118,enhance,enhancement,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4332,1,['enhance'],['enhancement']
Modifiability,"And probably other kinds of files too. The stack below results from handing it a .ADAM file in the MeanQualityByCycleSparkIntegrationTest.test_ADAM test. The ReadSparkSource code is currently delegating to Hadoop-BAM, which is in turn delegating to promiscuous htsjdk code that says anything that doesn't look like known file type [must be a SAM file](https://github.com/samtools/htsjdk/blob/bd92747fd3672635de96473fea6d4b38e8635c8e/src/java/htsjdk/samtools/SAMFileReader.java#L754). It then happily creates a bogus SAMFileHeader from the .ADAM stream. All 3 layers should probably be more discriminating. This currently doesn't break any tests. I discovered it when running the HB tests against a local version of htsjdk with a strict setHeader implementation that attempts to resolve all reference names on every setHeader call. That code caused this test to fail because its using the bogus header. ""main@1"" prio=5 tid=0x1 nid=NA runnable; java.lang.Thread.State: RUNNABLE; at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:113); at htsjdk.samtools.SAMTextReader.readHeader(SAMTextReader.java:200); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:63); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:73); at htsjdk.samtools.SAMFileReader.init(SAMFileReader.java:684); at htsjdk.samtools.SAMFileReader.<init>(SAMFileReader.java:148); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:66); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:47); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:195); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1280:559,layers,layers,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280,1,['layers'],['layers']
Modifiability,AnnotationBaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9hbGxlbGVzcGVjaWZpYy9SZWR1Y2libGVBbm5vdGF0aW9uQmFzZVRlc3QuamF2YQ==) | `2.439% <0%> (-90.244%)` | `1 <0> (-8)` | |; | [...ferenceConfidenceVariantContextMergerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1JlZmVyZW5jZUNvbmZpZGVuY2VWYXJpYW50Q29udGV4dE1lcmdlclVuaXRUZXN0LmphdmE=) | `2.881% <0%> (-94.239%)` | `1 <0> (-25)` | |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `70.115% <0%> (ø)` | `18 <1> (ø)` | :arrow_down: |; | [...er/tools/walkers/GenotypeGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `3.704% <0%> (-80.408%)` | `2 <0> (-37)` | |; | [...haplotypecaller/HaplotypeCallerEngineUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmVVbml0VGVzdC5qYXZh) | `3.704% <0%> (-92.593%)` | `1 <0> (-5)` | |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `7.219% <0%> (-81.016%)` | `4 <0> (-54)` | |; | ... and [1286 more](https://codecov.io/gh/broadinstitute/gatk/pull/5833/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5833#issuecomment-476235495:3841,Plugin,Plugin,3841,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5833#issuecomment-476235495,1,['Plugin'],['Plugin']
Modifiability,"Another con of hacky solution is that it may make calling replicates in T/N configuration difficult. I have not confirmed this, though, so it may not be an issue at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3265#issuecomment-315085428:76,config,configuration,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3265#issuecomment-315085428,1,['config'],['configuration']
Modifiability,"Another option is to invert the dependency by changing htsjdk.samtools.cram.ref.ReferenceSource to be an interface (which I've lobbyied the htsjdk CRAM developer to do anyway for other reasons) ,and change Hadoop-BAM to instantiate an implementation of that interface on an hdfs file. I made the changes to all three layers locally to do this for non-HDFS references and it works fine. The interface itself is only one method at the moment:; public byte[] getReferenceBases(final SAMSequenceRecord record, final boolean tryNameVariants); . This has the advantage of not requiring a dependency on the jsr203 release, but its a point solution for CRAM references only, and it would require a non BWC change in htsjdk since the ReferenceSource interface is public. However as I mentioned I think the htsjdk change is a good one anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165247142:317,layers,layers,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1326#issuecomment-165247142,1,['layers'],['layers']
Modifiability,"Another permutation on this: i dont know if you'll like hooking into the codec, but one could wrap the codec and create a FeatureInputAwareVariantContext. This would go into FeatureDataSource, and would potentially allow Features to be 'aware' of their owning FeatureInput. The code below is not final and needs work, but this or the patch might give an idea:. [FeatureInputAwareVariantContext.patch.txt](https://github.com/broadinstitute/gatk/files/6346205/FeatureInputAwareVariantContext.patch.txt). ```; public static class CodecWrapper<FEATURE_TYPE extends Feature, SOURCE> implements FeatureCodec<FEATURE_TYPE, SOURCE>; {; private final FeatureCodec<FEATURE_TYPE, SOURCE> childCodec;; private final FeatureInput<FEATURE_TYPE> featureInput;. public CodecWrapper(FeatureCodec<FEATURE_TYPE, SOURCE> childCodec, FeatureInput<FEATURE_TYPE> featureInput); {; this.childCodec = childCodec;; this.featureInput = featureInput;; }. @Override; public Feature decodeLoc(SOURCE source) throws IOException {; return childCodec.decodeLoc(source);; }. @Override; public FEATURE_TYPE decode(SOURCE source) throws IOException {; FEATURE_TYPE feature = childCodec.decode(source);. //Either look for marker class or otherwise poke in FeatureInput here:; if (feature instanceof VariantContext); {; feature = new FeatureInputAwareVariantContext(feature, featureInput);; }. return feature;; }. @Override; public FeatureCodecHeader readHeader(SOURCE source) throws IOException {; return childCodec.readHeader(source);; }. @Override; public Class<FEATURE_TYPE> getFeatureType() {; return childCodec.getFeatureType();; }. @Override; public SOURCE makeSourceFromStream(InputStream bufferedInputStream) {; return childCodec.makeSourceFromStream(bufferedInputStream);; }. @Override; public LocationAware makeIndexableSourceFromStream(InputStream inputStream) {; return childCodec.makeIndexableSourceFromStream(inputStream);; }. @Override; public boolean isDone(SOURCE source) {; return childCodec.isDone(source);; }. @Overrid",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766:553,extend,extends,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766,1,['extend'],['extends']
Modifiability,"Another thing that just come to my mind is to rely on [SLF4J](https://www.slf4j.org/) for logging - downstream projects can configure which logger they want to use, and they can have their own ways of setting logging verbosity. If the logging system from HTSJDK wants to be maintain, it can also add a simple implementation of SLF4J with the verbosity levels that are in the current implementation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288:124,config,configure,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-371401288,1,['config'],['configure']
Modifiability,"Any objections to exposing SW parameters to the command line? This looks like something we will want to explore for malaria. I'm also not convinced that our current parameters have been justified and/or optimized in any documented way. A few questions:. 1) There are 3 sets of parameters used in various ways, a) haplotype-to-reference alignment, b) read-to-haplotype alignment, and c) dangling ends. Any chance we can evaluate the effect of consolidating at least c), if not all sets? @emeryj I was told that you might be the one to ask about c) in particular; @davidbenjamin speculated that these might effectively yield STR-specific parameters. In general, if there are any quick and readily available evaluations (which ideally include variant normalization), I'd appreciate pointers to them. 2) Any suggestions on what the resulting command line should look like? I don't want to add 12 parameters, in the worst case. I also think that using integer arrays might be clunky. Perhaps I can suggest the use of args files in the doc string---although I don't think that those are expanded in the `##GATKCommandLine`, right?. 3) Should I touch `SWOverhangStrategy` at all? See e.g. https://github.com/broadinstitute/gatk/issues/6576. It looks like we thread both this and the `SWParameters` through many methods and classes, so the code could stand quite a bit of refactoring, but for now I will stick to the minimal changes required to expose. @droazen @ldgauthier any thoughts?. In some simple experiments of changing the a) parameters (from the somewhat questionable `NEW_SW_PARAMETERS = new SWParameters(200, -150, -260, -11)` back to `STANDARD_NGS = new SWParameters(25, -50, -110, -6)`), I've seen that there are non-negligible differences in the calls (beyond representation) at the few percent level, as well as changes in annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863:1364,refactor,refactoring,1364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863,1,['refactor'],['refactoring']
Modifiability,"Apart of the amount of work in both Barclay and GATK, I think that this shouldn't be implemented for 2 reasons:. * After #3486, some tools are hidden from the command line (and they will be most likely undocumented too). If the bash-completion works with undocumented tools that are hidden from the command line, there will appear anyway after pressing tab-tab. If that tools are treated in a different way, then it requires even more work - Barclay does not use the omitFromCommandLine at all, and that means that GATK should extend the bash-completion to take it into account.; * If a tool can bash-complete but it does not show in the online help pages (the main source for help, taking into account that in the CLI is a bit messy when the parameter space grows), then it will be really difficult to really understand how the tool work. Even if it shows the parameters with tab-tab, the only way of checking what the meaning of each of them is look at the CLI help. Because the bash-completion is a sub-type of help-doclet, it should require the `@DocumentedFeature` annotation: that is the marker interface in Barclay for mark classes as parsed/added to the ""help"" generated by doclets....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-331112758:527,extend,extend,527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3596#issuecomment-331112758,1,['extend'],['extend']
Modifiability,ArgumentsBuilder needs to be refactored.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4550:29,refactor,refactored,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4550,1,['refactor'],['refactored']
Modifiability,As I discovered when making the fix in PR #2021 bam files will fail validation if overhang clipping is used when running SplitNCigarRead because the mate reference start position might be changed. The tool can be refactored to perform a second walker pass over the reads in order to identify locations where this will be a problem by checking for sites where the primary read gets clipped by OverhangClippingManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2075:213,refactor,refactored,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2075,1,['refactor'],['refactored']
Modifiability,"As an aside, I downloaded the gnomAD VCFs locally using gsutil, modified the `gnomAD_exome.config` to refer to them and it works:. ```; $ cat gnomAD_exome.config; name = gnomAD_exome; version = 2.1; src_file = gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz; # src_file = gs://broad-public-datasets/funcotator/gnomAD_2.1_VCF_INFO_AF_Only/hg38/gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz; # [...]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-723357333:91,config,config,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-723357333,2,['config'],['config']
Modifiability,"As an update here, we're currently planning an upgrade to the library that we use to read bams into spark. As part of that upgrade we're going to try to fix the issue that requires 2 separate filesystem plugins for some things to work. That should enable people with hdfs file system plugins to work with gatk without a matching NIO plugin. There's no definite timeline, but hopefully within the next quarter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-375371782:203,plugin,plugins,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-375371782,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"As described in #2034, samtools mpileup and the internal pileup generated by `LocusIteratorByState` (LIBS) is not providing the same result because overlapping read-pairs are not taken into account. In this PR I addressed this issue using the same approach as samtools to combine qualities, including new functionality to LIBS and `LocusWalker`:; - Refactoring constructors for LIBS, solving #1879.; - Including an option for ignore overlapping read-pairs in LIBS; - Including command line option in `LocusWalker`to ignore overlapping read-pairs and to downsample with a maximum coverage by sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041:349,Refactor,Refactoring,349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041,1,['Refactor'],['Refactoring']
Modifiability,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:341,extend,extend,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,1,['extend'],['extend']
Modifiability,"As discovered in one of the recent `Funcotator` PRs, `ArgumentsBuilder` needs to be refactored.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4550:84,refactor,refactored,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4550,1,['refactor'],['refactored']
Modifiability,"As discussed elsewhere, all IGV does is ignore the column headers and take the last column, see https://software.broadinstitute.org/software/igv/SEG. Let's have ModelSegments output two LegacySegmentCollection files with column headers `[SAMPLE, CONTIG, START, END, COPY_RATIO_POSTERIOR_50/MINOR_ALLELE_FRACTION_POSTERIOR_50]`, one for CR_50 (.cr.igv.seg) and the other for MAF_50 (.af.igv.seg). Methods can be added to ModeledSegmentCollection to create these LegacySegmentCollections. LegacySegmentCollection can inherit from AbstractSampleCollection and the write method can be overridden to suppress the SAM-style header, although documentation should be added to explain this idiosyncrasy. Should be no more than a day's work. @LeeTL1220 feel free to take this one if you have time, otherwise I'm happy to take care of it after some more high priority issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407150546:515,inherit,inherit,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407150546,1,['inherit'],['inherit']
Modifiability,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740:644,config,configuration,644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740,1,['config'],['configuration']
Modifiability,"As discussed in https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358040921, we need to refactor `GATKTool` so that all non-Spark tools can comfortably extend it rather than extending `CommandLineProgram` directly, as some tools currently do. In particular, we need to:. * Provide a mechanism for subclasses to selectively disable engine-wide arguments such as `-I` completely (and also the ability to override with their own version of an argument). * Access necessary datasources outside of the engine package. * Add the ability to register input metadata such as sequence dictionaries, so that standard validation rules can be enforced across the toolkit. * Add the ability for each tool to change the defaults for engine arguments such as `--interval-merging-rule`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341:102,refactor,refactor,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341,3,"['extend', 'refactor']","['extend', 'extending', 'refactor']"
Modifiability,"As discussed with @lbergelson, I tested this patch manually on my local machine as well as on a clean Google Cloud VM and found it to work perfectly in all cases. I believe that the test failures here are artifacts of some configuration issue with the Travis CI VM environment, rather than indicative of a real problem. I'll see if I can recruit an external user who ran into the requester pays issue to test this branch for additional confirmation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7730#issuecomment-1079415266:223,config,configuration,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7730#issuecomment-1079415266,1,['config'],['configuration']
Modifiability,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176:242,plugin,plugin,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176,2,"['config', 'plugin']","['configuration', 'plugin']"
Modifiability,"As for your other question: the inheritance was historical, this class didn't add any feature to the SAMRecord. Yes, AVRO needs some extra work - but you can also send Java-serialized objects via Dataflow if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100270176:32,inherit,inheritance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100270176,1,['inherit'],['inheritance']
Modifiability,"As long as we are rolling with a single common environment across all tools, perhaps we could have a documentation annotation for tools that require it?. As I said in #4125, I vote against allowing users to configure their own environment. I really see no benefit to anyone---it's much easier on the users to just use the yml, and it's definitely much easier on us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4127#issuecomment-357037432:207,config,configure,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4127#issuecomment-357037432,1,['config'],['configure']
Modifiability,"As mentioned in the discussion for https://github.com/broadinstitute/gatk/pull/987, we want to compare the manual sharding approach taken to optimizing BQSR in that branch against an alternative approach of broadcasting the reference and variants. The latter approach would be simpler and more flexible/idiomatic (allow spark to handle sharding and data localization rather than doing it manually), but might be slower. Let's find out what the performance is like for both approaches before making a decision.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995:294,flexible,flexible,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995,1,['flexible'],['flexible']
Modifiability,"As noted by @mbabadi, it would be useful to allow TableReader extending classes to process comments present in the input.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2439:62,extend,extending,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2439,1,['extend'],['extending']
Modifiability,"As part of my work in the Pipeline Dev team, I created 2 GATK images to address issue discussed [here](https://github.com/broadinstitute/gatk/issues/8684) (ie. having too many docker layers, we hit ACR limits very quickly). The images are in terrapublic, a premium-tier ACR and is publicly accessible. I made two images, one is squashed to just 1 layer, the other is reduced to just 12 layers (from the original 45). With these changes and the fact that terrapublic is on [premium](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling) tier, the maximum docker pulls per minute becomes 833 (ie. 10k readOps / 12 layers) for the reduced-layers image and 10,000 for the squashed one. We have yet to test these in our pipelines but I anticipate the squashed version to be slower since it won’t be able to take advantage of any parallel pulls or caching, hence the two versions to allow pipeline devs to decide which one is better for their use-case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:183,layers,layers,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,4,['layers'],['layers']
Modifiability,"As requested in #1198, I would like to have a walker to iterate over data sources with a set of overlapping windows based on a reference genome (that is, a `SAMSequenceDictionary`). This first implementation is a very naive one, because it only construct the overlapping windows from the reference (keeping only the ones that overlaps with the provided `intervalsForTraversal`) and then just query in the different sources each of the windows generated. The next plan is to extends this class and `VariantWalker` to create a `SlidingWindowVariantWalker`, which traverse variants using this sliding-window approach.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528:474,extend,extends,474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528,1,['extend'],['extends']
Modifiability,"As the scripts evolve to be more and more complicated, ; it is time to plan a transition from scripts to a Java tool in GATK. Following the structure that is set up by the scripts in PR #4406 ,; the 1st stage development could be:. 1. parse and check the call sets emitted by callers; basically this is to make sure the tool won't be ""surprised"" by the call sets' ""features"" (bash scripts do this); 2. some basic accounting and metrics, e.g. SINE, LINE peaks (bash scripts do accounting and plots); 3. simple overlap-based TP/FP/FN analysis (bash scripts rely on bedtools for such purpose); 4. Basic reporting on FN/FP rates (bash scripts print a slew of information to screen). Variant files from different callers have their own quirks, (the BND records don't help) having a general purpose tool that covers all major callers is going to take a hefty investment, so we could start from PacBio and GATK-SV call sets.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4684:15,evolve,evolve,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4684,1,['evolve'],['evolve']
Modifiability,At some point we deleted the lines in question because the variables are `false` by default.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259#issuecomment-379872013:59,variab,variables,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259#issuecomment-379872013,1,['variab'],['variables']
Modifiability,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337:356,config,configs,356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337,2,['config'],"['config', 'configs']"
Modifiability,Audit use of Utils random generators and refactor if necessary.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6112:41,refactor,refactor,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6112,1,['refactor'],['refactor']
Modifiability,"Authorization settings for the connector are described here: https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml#L50. @jamesemery have you been able to get the connector working?. @droazen what configuration improvements did you have in mind?. Also, I'm not sure what the difference between `google.cloud.auth.service.account.json.keyfile` and `fs.gs.auth.service.account.json.keyfile` is (if any).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373:239,config,configuration,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500755373,1,['config'],['configuration']
Modifiability,"BImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5218,Config,ConfigFactory,5218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"BQSR: avoid throwing an error when read group is missing in the recal table, and some refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9020:86,refactor,refactoring,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9020,1,['refactor'],['refactoring']
Modifiability,"BTW, I wouldn't bother looking at the diff. It's a complete rewrite.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438805556:60,rewrite,rewrite,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438805556,1,['rewrite'],['rewrite']
Modifiability,BUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.448 DEBUG Mutect2 - Processing assembly region at chrM:10185-10484 isActive: false numReads: 0; 12:05:51.460 INFO ProgressMeter - chrM:10185 30.2 40 1.3; 12:05:51.465 DEBUG Mutect2 - Processing assembly region at chrM:10485-10784 isActive: false numReads: 0; 12:05:51.476 DEBUG Mutect2 - Processing assembly region at chrM:10785-11084 isActive: false numReads: 0; 12:05:51.489 DEBUG Mutect2 - Processing assembly region at chrM:11085-11384 isActive: false numReads: 0; 12:05:51.501 DEBUG Mutect2 - Processing asse,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:17460,Extend,Extended,17460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,Back to @akiezun for a refactoring before I review,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/661#issuecomment-124222253:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/661#issuecomment-124222253,1,['refactor'],['refactoring']
Modifiability,Back to @lbergelson for a refactoring,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/682#issuecomment-123811263:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/682#issuecomment-123811263,1,['refactor'],['refactoring']
Modifiability,Back to you @lbergelson. I think that we can wait for the htsjdk release for refactoring `CachingIndexedFastaSequenceFile`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-271819769:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-271819769,1,['refactor'],['refactoring']
Modifiability,"Back to you, @akiezun. I think that I did almost everything that you mentioned, and I refactored names in the tool class to fit in the new framework. I believe that for some API users it would be nice to have shortcuts like the `getNumberOfDeletions()`, so I didn't remove it. But I can do it in a later commit if you really think that is unnecessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-220175654:86,refactor,refactored,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836#issuecomment-220175654,1,['refactor'],['refactored']
Modifiability,"Barclay doesn't have any way to recognize those args as being the same. However, the Pedigree annotation classes already have this problem - its the reason that `pedigreeFile` and `founderIDs` were lifted into `GATKAnnotationPluginDescriptor`, and are not included in the individual annotations - `GATKAnnotationPluginDescriptor` distributes them to the annotation classes as necessary. Its not ideal, but it works. Can you extend that pattern ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823482350:424,extend,extend,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823482350,1,['extend'],['extend']
Modifiability,"Base qualities of two (`#`) are handled specially by BWA and our tools and are typically used to indicate adapter sequence. See reply to jhess in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/35120#Comment_35120>:. > That's correct, Q2 bases are considered to be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic wor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:106,adapt,adapter,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,3,['adapt'],['adapter']
Modifiability,"Based on gs://broad-gotc-test-results/staging/joint_genotyping/exome/scientific/2021-09-03-11-25-15/gather_vcfs_low_memory/small_callset_high_threshold.vcf.gz (from the console output) there are slightly fewer variants filtered with ExcessHet now, which is expected since you said it was an across-the-board shift. Expected (old) has 4335 and actual (new) has 4133 -- no new things, just some now pass. If you can calculate a new equivalent threshold I'd rather use that, but otherwise I'm not overly concerned about the changes. I'm not concerned about the Jenkins call caching unless it's for the GenotypeGVCFs task where ExcessHet actually gets calculated. For the ReducibleAnnotation comments, if you just revert your changes (statics, visibility, etc.) and open an issue I'm fine with that. Admittedly this could be another target for refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-921160043:840,refactor,refactoring,840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-921160043,1,['refactor'],['refactoring']
Modifiability,"Bayesian GMM:. This is essentially an exact port of the sklearn implementation, but only allowing for full covariance matrices. I think it might be good for those in the Bishop reading group to take a look during review. I decided to split this off into its own branch (just updated the existing branch https://github.com/broadinstitute/gatk/tree/sl_sklearn_bgmm_port) and only include stubs for the BGMM backend in the above tools. This is so we can prioritize merging the IsolationForest implementation for @meganshand. We can easily add this module back when it's been reviewed separately. TODOs:. - [x] Class-level docs.; - [x] Method-level docs. I think pointers back to the original sklearn code will suffice for most methods, but I've also included some parameter descriptions. Also note that I've retained original sklearn comments throughout the implementation and have also commented on mathematical expressions where it might be helpful.; - [x] Unit tests. There's already test data (generated using Pyro) checked in and the results match the sklearn implementation to high precision, I just need to write numerical checks. There are also already unit tests for static utility methods. Future work:; - [ ] Expanding unit tests to cover more of the interface. These initial unit tests will almost certainly not completely cover the possibilities allowed by the interface, e.g. warm starts. Could be a good exercise for other developers. EDIT: At least one test of warm starts has been added.; - [ ] As mentioned in the prototyping discussion, expanding this implementation to properly include marginalization might be of future interest. However, I think a very strong case would have to made before proceeding, as I think closely matching the sklearn implementation has obvious benefits for maintainability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948712:1802,maintainab,maintainability,1802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948712,1,['maintainab'],['maintainability']
Modifiability,"Bcftools setGT plugin is probably the best way of fixing this issue currently. ; ```; bcftools plugin setGT -h ; About: Sets genotypes. The target genotypes can be specified as:; ./. .. completely missing (""."" or ""./."", depending on ploidy); ./x .. partially missing (e.g., ""./0"" or "".|1"" but not ""./.""); . .. partially or completely missing; a .. all genotypes; b .. heterozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:15,plugin,plugin,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,4,"['Plugin', 'plugin']","['Plugin', 'plugin']"
Modifiability,"Because in `Main`the call to `Utils.forceJVMLocaleToUSEnglish()` is done in an static block, this is not applied to downstream projects. I wonder if this call can be moved to `Main.mainEntry`, to assess extending classes apply the same locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483:203,extend,extending,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483,1,['extend'],['extending']
Modifiability,"Because it does not make sense to disable a filter that is not enabled. The only withdraw is that if no default filter is provided, this argument does not have possible values. If #2355 is accepted, at least the part for the argument collection, this could be solved on the construction for the plugin by providing a plugin argument collection without the `--disableReadFilter` argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2358:295,plugin,plugin,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2358,2,['plugin'],['plugin']
Modifiability,"Because the `reference` variable is package private. If I want to call apply() from onTraversalSuccess, it needs to be protected—will push a commit to shortly to show you what I mean.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618045813:24,variab,variable,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6512#issuecomment-618045813,1,['variab'],['variable']
Modifiability,"Because you implemented the original plugin descriptor, can you have a look @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274528407:37,plugin,plugin,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274528407,1,['plugin'],['plugin']
Modifiability,"BedToIntevervalList is printed that way because it doesn't specify a long name, so it defaults to using the variable name. I think this is actually a bug, since it isn't using our standard argument names. ```; @Argument(shortName = StandardArgumentDefinitions.INPUT_SHORT_NAME, doc = ""The input BED file""); public File INPUT;; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1149#issuecomment-157864245:108,variab,variable,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1149#issuecomment-157864245,1,['variab'],['variable']
Modifiability,Being handled in a refactoring branch. I can take it out and make a small PR.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3224#issuecomment-313876142:19,refactor,refactoring,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3224#issuecomment-313876142,1,['refactor'],['refactoring']
Modifiability,"Below is probably too specific but I am not sure if this is desired (unlikely IMO) or actually another edge-case bug. Summary: `InsertSizeMetics.WIDTH_OF_99_PERCENT` potentially get larger than `InsertSizeMetrics.MAX_INSERT_SIZE - InserSizeMetrics.MIN_INSERT_SIZE`. Details:; Using the non-empty bam file, fragment length values of valid reads (13 reads) are; {36,36,36,38,38,40,41,41,41,41,44,44,45}, with median value 41.; However, the `CollectInsertSizeMetricsTest.java` unit test is yielding 11 as the value for `InsertSizeMetics.WIDTH_OF_99_PERCENT`, which is greater than 10 (that is, max-min+1).; Debugging step-by-step shows that the problem lies in function . `public void InsertSizeMetricsCollector::addMetricsToFile(final MetricsFile<InsertSizeMetrics,Integer>)` . In short, the logic in the implementation for finding the bin widths is to have two variables, `low` and `high`, that starts from `InsertSizeMetics.MEDIAN_INSERT_SIZE` and incremented and decremented by 1 gradually. The actual widths are calculated by `distance = (max - min)+1`.; In this particular edge case, during the last iteration, `max` was increased from the previous iteration to an out of bound value--46 (that is greater than 45, the actual max in the data). The increment and decrement of `low` and `high` are done at the bottom of the `while` block spanning form line 138 to line 162 in the function. This causes the culprit. In hypothetical scenario, when the distribution under investigation is extremely biased (max-median >> median-min, or the other way around), the `WIDTH_OF_99_PERCENT` could end up much higher than max-min. @akiezun @cwhelan please check. If this is not the intended result, fix should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1519:860,variab,variables,860,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1519,1,['variab'],['variables']
Modifiability,Best way to handle plugins that share arguments?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213:19,plugin,plugins,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213,1,['plugin'],['plugins']
Modifiability,"Blech, I can't figure out how to get it to work properly so you don't have to manually specify the output coder. My implementation:. ```; public static <A,B> PCollection<B> apply(PCollection<? extends A> input, SerializableFunction<A, B> f){; return input.apply(DataflowUtils.lift(f));; }. public static <A,B> PTransform<PCollection<? extends A>, PCollection<B>> lift(SerializableFunction<A, B> f){; return ParDo.of(new DoFn<A, B>() {; @Override; public void processElement(ProcessContext c) throws Exception {; c.output(f.apply(c.element()));; }; });; }; ```. example usage:. ```; @Test; public void testApplyToString(){; Pipeline p = GATKTestPipeline.create();; PCollection<Integer> pints = p.apply(Create.of(Arrays.asList(1, 2, 3)));. PCollection<String> presults = DataflowUtils.apply( pints, Object::toString).setCoder(StringUtf8Coder.of());. DataflowAssert.that(presults).containsInAnyOrder(""1"",""2"",""3"");; p.run();; }; ```. note the `setCoder` call, if you don't include that you get. ```; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-jdk14/1.7.7/25d160723ea37a6cb84e87cd70773ff02997e857/slf4j-jdk14-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.10/b3eeae7d1765f988a1f45ea81517191315c69c9e/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/Users/louisb/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]; java.lang.IllegalStateException: Unable to infer a default Coder for AnonymousParDo.out [PCollection]; either correct the root cause below ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:193,extend,extends,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,2,['extend'],['extends']
Modifiability,"BucketUtils was a solution before we had Filesystem providers. It's stuck around as a parallel set of code because we couldn't trust the providers at first. In the long run it should be removed and replaced entirely by `Files` operations. We need to test that all the functionality exists / works as expected though, and it hasn't been a high priority to do so. Particularly, I'm not sure we have a lot of faith in the HDFS NIO plugin, so we may need to keep around special cases for that. It could definitely at least be simplified a lot though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020:428,plugin,plugin,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3569#issuecomment-328993020,1,['plugin'],['plugin']
Modifiability,But that smoke test doesn't actually test any new functionality. The variable was there in the WDL before and was already being used - it just had no default value.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-483668246:69,variab,variable,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-483668246,1,['variab'],['variable']
Modifiability,"By doing the following, I was able to get a JointGenotyping result for my 343 samples:; - increased the amount of memory allocated to the Java heap in ImportGvcfs to 50000m; - modified the runtime attributes for all the joint genotyping tasks to match the format that Cromwell accepts for HPC environments (https://cromwell.readthedocs.io/en/stable/tutorials/HPCIntro/#specifying-the-runtime-attributes-for-your-hpc-tasks); - increasing the runtime memory attribute for ImportGvcfs and GenotypeGvcfs from 26000 MiB to 60 G; - executing the workflow with the following sbatch parameters:; nodes=4; ntasks=32; mem=248g; tmp=429G; - manually tar'ing up all the genomicsdb directories from the execution directories of all 10 shards of ImportGvcfs after they successfully completed GenomicsDBImport and failed with the error message: ; pure virtual method called ; terminate called without active exception; - running an abbreviated version of JointGenotyping which started at GenotypeGvcfs and executed the remainder of the JointGenotyping workflow unchanged.; ; I think this pretty clearly demonstrates that, whatever is going on, it occurs between GenomicsDBImport's successful creation of genomicsdb and the tar -cf of same. The failure is 100% reproducible with a number of different runtime configurations. The error messages are from C++ and seem to be occurring at the point where native C++ code is handing execution back to Java.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1314069533:1293,config,configurations,1293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1314069533,1,['config'],['configurations']
Modifiability,"By the way, I can take this one. I'm planning to do a big PR with a workaround with some of the problems of the plugin that I found to discuss them there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2397#issuecomment-278248114:112,plugin,plugin,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2397#issuecomment-278248114,1,['plugin'],['plugin']
Modifiability,"By the way, I thought @vdauwera was opposed to using optional inputs in this way at some point (see #3657). Was that question ever decided? (I'm still of the opinion that they *should* be used in this way, but this is one of the reasons I didn't for this iteration of the WDL.). To be clear, the pair WDL right now does not allow all of the workflow paths (tumor-only, no PoN, etc.) that the new tools make possible. It only allows the one that we will most likely run in production (matched-normal + PoN). We should probably make the WDL a little more flexible to cover the most common use cases, but I'm fine if it doesn't completely expose all of the possible workflow paths---this would probably just make the WDL harder to maintain. Users can write their own WDLs in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132:553,flexible,flexible,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362696132,1,['flexible'],['flexible']
Modifiability,"C fields, and add the prior pseudocounts. Lines 111-144 grab the genotype likelihoods from the input. In line 149 we pass the allele counts and likelihoods to `calculatePosteriorGLs`. This method uses the allele counts (prior + resources + input AC) to define a Dirichlet distribution which then serves as the prior on genotypes. Finally, on line 203 we multiply (add in log space) this prior by the genotype likelihoods to get the posterior probabilities on genotypes. It appears to me that we have double-counted the input data, once to get its AC field and once to get its GLs. I believe the correct thing to do is use only the resources to define a prior which is then combined with the GLs to get the posterior. ---. @ldgauthier commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219778100). I will take the blame (both figuratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---. @davidbenjamin commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219833193). The better results using the double-counting might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2918:3724,refactor,refactored,3724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918,1,['refactor'],['refactored']
Modifiability,"CE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	clou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3313,Config,ConfigFactory,3313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"CKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:35:11.509 INFO CountReadsSpark - Start Date/Time: January 9, 2019 1:35:09 PM EST; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.509 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:2095,variab,variables,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"CTORY :; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3315,Config,ConfigFactory,3315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"Caller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3458,Config,ConfigFactory,3458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"Caller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDef",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4385,Config,ConfigFactory,4385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,Can A Walker Subclass Hide an Inherited Argument or Give It A Default?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7287:30,Inherit,Inherited,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7287,1,['Inherit'],['Inherited']
Modifiability,Can we have environmental variable + symlink in Docker please?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3899:26,variab,variable,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3899,1,['variab'],['variable']
Modifiability,"Can you give a bit more information here? If I'm understanding correctly, it's not clear that the same issue is at play here. The original issue was that duplicate/incomplete fragments were causing queries to the workspace to fail. . In this latest instance, it seems you are appending additional samples to the existing workspace. Is that right? If so,; - are you seeing the same/similar error? That is, it's a core dump? Can you share the error messages, any logs, core dump files etc?; - did you clean up the workspace before importing? That is, remove the incomplete fragment @nalinigans identified and the duplicated ones?. My first instinct is that even if the incomplete/duplicated fragments weren't cleaned up, the incremental import shouldn't have an issue -- at least not till it gets to the consolidate phase, which only happens after all batches are imported. Sounds like you were seeing an issue at batch 3 of 4, so might have something to do with the samples in that batch...or some other import issue. You mentioned that previous imports to this particular contig failed -- were those just transient failures that worked when rerun, or was there some configuration that you changed to get that to work?. For completeness, the way I identified duplicate fragments was to do an md5sum check on some of the internal files. If any pair of fragments have the same md5sum they are likely duplicates. So, from the workspace directory, something like:. ```; find . -name ""ALT.tdb"" -exec md5sum {} \;|sort; ```; That will highlight the fragments that are potentially duplicate. To confirm that the fragments are indeed duplicates, you'll then want to take that list of potentially duplicate fragments and check that all corresponding files within each pair of potentially duplicate fragments actually have the same md5sum. I have a crude bash script that I can share if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707:1166,config,configuration,1166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722541707,1,['config'],['configuration']
Modifiability,"Can you make sure that all read filter args have long-ish/unique short and long names? Single-letter names should be reserved only for important universal GATK args, not plugin args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-231199718:170,plugin,plugin,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-231199718,1,['plugin'],['plugin']
Modifiability,Can you please restore `AlignmentContext`? Removing it would cause issues for my incoming HC branch. We can discuss whether to refactor it away later.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1513#issuecomment-187251531:127,refactor,refactor,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513#issuecomment-187251531,1,['refactor'],['refactor']
Modifiability,Cannot access pull request for Owner configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945:37,config,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945,1,['config'],['configuration']
Modifiability,"Cannot this use directly `TableReader`? Maybe it should include a `Path` constructor and being refactored a bit... Another option could be the picard `BasicInputParser`, `TabbedInputParser` or `TabbedTextFileWithHeaderParser`...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3757#issuecomment-340715685:95,refactor,refactored,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3757#issuecomment-340715685,1,['refactor'],['refactored']
Modifiability,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813:1344,variab,variable,1344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813,1,['variab'],['variable']
Modifiability,"Changes to the testing framework to remove references to the test resources, keeping them into the src/test package. This changes include:. * Factor out a `GATKBaseTest` for separate test resources from test utilities in `BaseTest`; * Remove duplicated `CleanSamIntegrationTest`; * Repackage `CommandLineProgramTest` to be in the test sources, and use it's interface in testers; * Move some testers to the src/test package because they are tool-specific (added TODO to other ones that aren't that clear); * Refactor `TargetsToolsTestUtils` to use a provided reference. Closes #3029; Closes #2125",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475:507,Refactor,Refactor,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475,1,['Refactor'],['Refactor']
Modifiability,Changing defaults for mitochondria mode now that we have adaptive pruning,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544:57,adapt,adaptive,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544,1,['adapt'],['adaptive']
Modifiability,Changing variable name in MT WDL to match gatk-workflows,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5694:9,variab,variable,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5694,1,['variab'],['variable']
Modifiability,Check UUID in read adapter equals() and hashCode() methods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/653:19,adapt,adapter,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/653,1,['adapt'],['adapter']
Modifiability,Choose library to use for GATK configuration storage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078:31,config,configuration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078,1,['config'],['configuration']
Modifiability,CircleCI isn't configured yet. We're experimenting with both it and codeship.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101738979:15,config,configured,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101738979,1,['config'],['configured']
Modifiability,ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelecto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3523,config,config,3523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3543,Plugin,PluginManager,3543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginManager']
Modifiability,"ClipReads is already ported. Can we resolve whether this ticket is a bug report or an enhancement request or neither?. @vdauwera nope, but there's the code!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-130136318:86,enhance,enhancement,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-130136318,1,['enhance'],['enhancement']
Modifiability,"Closes #3561. @vruano could you review this? Note that I ended up implementing Dijkstra's algorithm instead of using a library, but it's only a few lines of code. This PR does not affect the outputs of HC or M2 at all. Also, @vruano, I recall your misgivings about the current haplotype enumeration (which this preserves):. >However, the current algorithm and the k-dijkstra still would show the same problems in terms of doing a suboptimal selection of haplotypes in terms of their coverage of plausible variation. I had implemented an alternative that fixed that issue . . . simulate haplotypes based on those same furcation likelihoods and wstop when we have not discovered anything new for a while... the problem of such an approach is to make it deterministic. Although this PR doesn't do that, it could easily be extended to do so just by running Dijkstra's algorithm until you have the amount of variation you want. That is, instead of terminating when the Dijkstra priority queue is empty or when we have discovered the maximum number of haplotypes, we could terminate based on some `Predicate<List<KBestHaplotype>>` on the list of haplotypes found so far. And it's deterministic since Dijkstra's algorithm is greedy. So basically, it's a nice refactoring for now but it also sets up some worthwhile extensions if we want.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5462:819,extend,extended,819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462,2,"['extend', 'refactor']","['extended', 'refactoring']"
Modifiability,Closes #3842 ; Closes #4808 ; Closes #4811 ; Closes #4810 ; Closes #4839. - A lot of refactoring to create `FuncotationMap` and use that to send to OutputRenderers.; - VCF and MAF will honor the canonical vs. all vs best effect.; - MAF entries will now be rendered as `AnnotatedInterval` via tribble,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4838:85,refactor,refactoring,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4838,1,['refactor'],['refactoring']
Modifiability,"Closes #4782 ; Closes #5959. We should discuss a few issues, and modify/cleanup if necessary, before this goes in (if it does at all). In particular, I'd like to understand the original intent behind using the /root directory (e.g., in all of our WDLs) and make sure we don't break typical downstream uses by instead using /gatk; we could even consider ""deprecating"" this over a few releases. I think it's also worth discussing whether we want to continue to release a rootful image, but perhaps parameterize the Docker build script to easily allow the building of an image with a non-root user. I must admit that I don't have good visibility on the various use cases of our Dockers (outside of our typical use with Terra/Cromwell), so if any users would like to chime in, that would certainly be appreciated. In particular, users may still have to do some work on their end to remap user namespaces. In any case, this is at least a proof-of-principle that mounting resources is possible within our test framework with the option for a non-root user. So unless we have other good reasons for requiring a root user, it seems worthwhile to at least allow this option, even if we don't make that the new default. (EDIT: Just to clarify, at some point I was told by another developer that the need to mount testing resources within Travis was at least one reason why we needed a root user---turns out this isn't the case.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6525:496,parameteriz,parameterize,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6525,1,['parameteriz'],['parameterize']
Modifiability,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5688:187,Refactor,Refactor,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688,2,"['Refactor', 'Rewrite']","['Refactor', 'Rewrite']"
Modifiability,"Closes #5085. This improves validations a bit immediately but more importantly will enable more intelligent filtering on the level of haplotypes, which I think is critical to some of the messy data we have seen. This will also benefit HaplotypeCaller some day. @LeeTL1220 Could you review as far as changes to Mutect2 are concerned?. @droazen Could you review the refactoring of `ReadLikelihoods` / extracting `MoleculeLikelihoods`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831:364,refactor,refactoring,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831,1,['refactor'],['refactoring']
Modifiability,Closes #5352. This removes unused command line arguments from Mutect2. @LeeTL1220 This PR looks big but it's really just a lot of moving around `static` methods and giving `HaplotypeCallerArgumentCollection` a `StandardCallerCollection` as a member instead of having`AssemblyBasedCallerCollection` inherit from it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5758:298,inherit,inherit,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5758,1,['inherit'],['inherit']
Modifiability,"Closes #6030. @takutosato This a complete rewrite of the realignment filter, so I would review `FilterAlignmentArtifacts` and its engine from scratch, not from the diff. There's still a bit of tuning to be done, but it's already far superior to the old version and I want to be using the release jar as much as possible for MC3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6143:42,rewrite,rewrite,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6143,1,['rewrite'],['rewrite']
Modifiability,"Closes #6235 . This PR allows users to more easily clobber individual theano flags. This can be used to change the location of the theano compilation directory (see the above issue and #4782 for context). These flags are set upon import of the theano module; see http://deeplearning.net/software/theano/library/config.html for details. This solution is a little hacky, hence the code duplication. Ideally, we'd be able to specify this directory (and potentially other flags) as a parameter to the tools. As discussed with @cmnbroad, probably the cleanest solution would be to modify the PythonScriptExecutor to allow environment variables to be specified, e.g. via ProcessSettings. This solution would also cover the initial import of the `gcnvkernel` package for validation purposes, rather than only the imports in the resource scripts modified in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6244:311,config,config,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6244,2,"['config', 'variab']","['config', 'variables']"
Modifiability,Closing -- we've refactored the README to make it more readable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-319415065:17,refactor,refactored,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-319415065,1,['refactor'],['refactored']
Modifiability,Closing. I will open a PR between today and tomorrow with all the workaround for this plugin (before the change in Barclay). We can discuss in the new one the details in a better way.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2359#issuecomment-278244313:86,plugin,plugin,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2359#issuecomment-278244313,2,['plugin'],['plugin']
Modifiability,"Co-assigning to @jonn-smith, since it's likely to be implemented using the new OWNER configuration facility.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3610#issuecomment-331978713:85,config,configuration,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3610#issuecomment-331978713,1,['config'],['configuration']
Modifiability,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7946:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946,2,"['Refactor', 'refactor']","['Refactoring', 'refactoring']"
Modifiability,CollectAllelicCounts should extend LocusWalker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2968:28,extend,extend,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2968,1,['extend'],['extend']
Modifiability,Command line parser plugins and ReadFilter command line arguments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973:20,plugin,plugins,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973,1,['plugin'],['plugins']
Modifiability,"Comprises the commits after 7992f64. The only commit with real substance is `Updated metadata and abstract collection classes.`. The rest of the commits simply update calling code, related tests, and test files. These updates were slightly less trivial for the plotting classes, so these are also split off into separate commits. Again, probably could be engineered better (there are two parallel class hierarchies for metadata and collection classes, which is kind of gross), but we can refactor later if needed. @asmirnov239 please review. Again, lower priority than gCNV VCF, but the sooner this is in master the easier it will be to get things into FireCloud. Let's try for early next week. I'll start doc updates concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3914:488,refactor,refactor,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3914,1,['refactor'],['refactor']
Modifiability,"Conceptually -- two things happen in this PR. . 1. Removed ""arrays"" code; 2. Refactored package names into. ...gvs; |- ingest; |- filtering; |- extract; \- common. All tests pass (can be run with `./gradlew test --tests ""org.broadinstitute.hellbender.tools.gvs*""`). I will run a full workflow in Terra (import, train, extract) and verify. BEFORE we merge, we'll the existing ""ah_var_store"" to indicate this is where array code lives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7260:77,Refactor,Refactored,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7260,1,['Refactor'],['Refactored']
Modifiability,Concordance testing against GATK3 annotations for HaplotypeCaller (with a configurable tolerance before failure),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1732:74,config,configurable,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732,1,['config'],['configurable']
Modifiability,Config file no longer overrides system properties,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4445:0,Config,Config,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4445,1,['Config'],['Config']
Modifiability,"ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4742,Config,ConfigFactory,4742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"Configuration file overrides system properties, is missing from bundled GATK binary distribution",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4436:0,Config,Configuration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436,1,['Config'],['Configuration']
Modifiability,Configure travis to install/init git lfs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/840:0,Config,Configure,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/840,1,['Config'],['Configure']
Modifiability,"Considering that this PR has lasted through my absence and the holiday season, I want to take the chance of summarizing the concerns you have issued here:. --------. Resolved as requested (or at least made efforts to):. * documenting the logic and methods; documented. * emit VCF instead of custom file format; emit both VCF custom file format now. * bug in determining if alignment signature satisfies `allMiddleAlignmentsDisjointFromAlphaOmega`; bug fixed in commit b4f7568b03b91eb77d256bcfe8117001bce040ec. --------. Unresolved yet:; the fact that gap split happens after the alignment configuration scoring step is considered backwards. I agree in principle but due to AS and MQ were used in the scoring step, and split-copy leads to technically wrong AS & MQ, I originally decided to score first, then split. Splitting the gapped alignments was introduced originally to have a centralized logic in inferring type and location of the events. . The tension is that AS is used in the scoring but becomes practically useless after that. >> Correct, but I am having thoughts about this now (not to pick only one—that; would be wrong—but to ditch them altogether probably under some condition; and redo the alignment step), exactly because of this behavior I observe.; Think about the case where one originating gapped (say insertion); alignment, after splitting, has one of the two children contained in; another alignment (not its sibling, that's impossible) in terms of their; read span. Now the originating gapped alignment probably should be filtered; out, or not, because if we keep it, an insertion would be called but; apparently there are alternative explanations due to the other alignment.; I'm not sure how to deal with this case, and if this scenario is common; enough. It probably is the case that such alignments happen mostly in STR; regions, so getting the exact alignments correct there is no easy task.; ; > Is that enough of a concern to worry about. In such a case I feel like we; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980:589,config,configuration,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-354976980,1,['config'],['configuration']
Modifiability,Correct irrelevant inheritance in Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5758:19,inherit,inheritance,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5758,1,['inherit'],['inheritance']
Modifiability,Could it be possible to move the locale setup and picard-parser config to the first method in `mainEntry` (or another place that can be overrided and/or picked by the tests/downstream toolkits)?. https://github.com/broadinstitute/gatk/blob/51273676b20d25cacf238d1c0429ebc79b321a85/src/main/java/org/broadinstitute/hellbender/Main.java#L38-L50,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5014:64,config,config,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014,1,['config'],['config']
Modifiability,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5013:53,config,configurations,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013,2,"['config', 'extend']","['configurations', 'extending']"
Modifiability,"Could this be related to having sliced objects in the gsutils buckets but not using a code path that goes through a native CRC implementation? I ask because I noticed that when I try to download the file. ```; gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; ```. with gsutil, I get this error:. ```; CommandException: ; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see:. $ gsutil help crcmod. To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file.; ```. Could the GATK command path be computing all of the CRC hashes in Java code, slowing it down?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882:766,config,config,766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-223982882,1,['config'],['config']
Modifiability,Create a central config file containing Java system properties used by both gradle and gatk-launch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2317:17,config,config,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2317,1,['config'],['config']
Modifiability,Create a plugin for IGV that allows it to display the reads as haplotype caller has assembled them. This would be extremely useful for analysts who are doing manual review and would mostly obviate the need for bamOut. Jim Robinson seems interested in this so we may be able to get help from IGV to do the integration https://github.com/igvteam/igv/issues/428.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286:9,plugin,plugin,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286,1,['plugin'],['plugin']
Modifiability,Create haplotype caller reassembly plugin for IGV,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286:35,plugin,plugin,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286,1,['plugin'],['plugin']
Modifiability,"Creates a new ""build-base"" Docker image for the expensive and less frequently changed layers of the build image allowing for much improved variantstore image build times. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/ff13e48c-a9dc-48d7-8056-63d4f2028dc0). Other improvements:. * Bumps version of Google Cloud SDK base Docker image to latest `408.0.1-alpine`; * Bumps Arrow library version from 8.0.0 to 10.0.0; * Simplifies Arrow build to use `ninja`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8085:86,layers,layers,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085,1,['layers'],['layers']
Modifiability,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7721:649,config,configs,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721,1,['config'],['configs']
Modifiability,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4581:58,config,configuration,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581,3,['config'],"['configuration', 'configure']"
Modifiability,"Currently `GATKRead.copy()` is unable to guarantee a deep copy, since we only have a deep copy method for Google `Read`s (`GenericData.clone()`, which it inherits), not `SAMRecord`s. We should write a deep copy method for `SAMRecord`, hook it up to the `GATKRead.copy()` implementation in `SAMRecordToGATKReadAdapter`, and change the method contract to guarantee that a deep copy will be performed. This is not a huge priority, since `GATKRead` already guarantees that defensive copies will be made of all mutable reference types returned from accessor methods (which means that shallow copies should be safe to use freely), but would be nice for consistency and peace of mind.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/623:154,inherit,inherits,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/623,1,['inherit'],['inherits']
Modifiability,Currently filtered variants are not output in MAF files. This is not configurable. This should be changed to be an option.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4358:69,config,configurable,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4358,1,['config'],['configurable']
Modifiability,"Currently the code for the config file parsing will override values passed in on the command-line, so that explains one reason this is happening. The fix should be pretty straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367715460:27,config,config,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-367715460,1,['config'],['config']
Modifiability,Currently the command-line options for `Funcotator` need to be refactored to take the datasources directory as input. This directory will then be parsed to get sub directories and pull in any datasources that are there.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3999:63,refactor,refactored,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3999,1,['refactor'],['refactored']
Modifiability,"Currently the script fails when configured to run tests, since it doesn't have access to the large files in the docker image. These need to be downloaded and mounted for the tests to pass, as we do in the docker tests on travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3191:32,config,configured,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3191,1,['config'],['configured']
Modifiability,"Currently there is no way to supply the pedigree file, but that is dependent on #3287 but this is a first step to implementing the pedigree annotations. . The current plan is still to have the pedigree file be an argument on the tool but to reconcile them using the pluginDescriptor since there is currently one tool which takes a ped file that doesn't have annotations. Fixes #3939 ; Fixes #2542",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3941:266,plugin,pluginDescriptor,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3941,1,['plugin'],['pluginDescriptor']
Modifiability,"Currently tools using the annotation engine are manually defining arguments to control which annotations should be enabled/disabled. See `HaplotypeCallerArgumentCollection` `annotationsToUse`/`annotationGroupsToUse` for an example. We should bundle these into a reusable argument collection, like we did for read filters, as part of the task of making annotations a barclay plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3292:374,plugin,plugin,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3292,1,['plugin'],['plugin']
Modifiability,"Currently we have a dependency on having gcloud and gsutil installed and configured in a certain way, but we don't have any documentation about it. . We're getting authentication partially from gcloud auth login, which is being propagated in a way I don't fully understand through the dataflow pipeline options. . We need to understand exactly what's happening and then write an explanation of what a user needs to do to have it work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1051:73,config,configured,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1051,1,['config'],['configured']
Modifiability,"Currently we set NIO retry settings on a per-Path basis in `BucketUtils.getPathOnGcs()`. This is a bit brittle, since there are places in other libraries like htsjdk that create their own `Path` objects on-the-fly, and these new Path objects don't respect our retry settings (see https://github.com/broadinstitute/gatk/issues/2749). Ideally we'd be able to set these retry settings globally on startup, and all `Path` objects created anywhere would inherit these settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3120:449,inherit,inherit,449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3120,1,['inherit'],['inherit']
Modifiability,"Currently, FilterByOrientation requires replacing a required input's contents with a sed command: . ```; sed -r ""s/picard\.analysis\.artifacts\.SequencingArtifactMetrics\\\$PreAdapterDetailMetrics/org\.broadinstitute\.; hellbender\.tools\.picard\.analysis\.artifacts\.SequencingArtifactMetrics\$PreAdapterDetailMetrics/g"" \; ""metrics.pre_adapter_detail_metrics"" > ""gatk.pre_adapter_detail_metrics""; ```. The required input is the detailed pre-adapter summary metrics from Picard's CollectSequencingArtifactMetrics. The sed command replaces; ```; ## METRICS CLASS	picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```; with ; ```; ## METRICS CLASS	org.broadinstitute.hellbender.tools.picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```. If this class is not replaced, then the tool errors with: ; ```; htsjdk.samtools.SAMException: Could not locate class with name gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadCla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:443,adapt,adapter,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,1,['adapt'],['adapter']
Modifiability,"Currently, `AnnotatedIntervalCodec` has an attribute that is an instance of `XsvLocatableTableCodec`. There should be an abstract class that both classes inherit common code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580:154,inherit,inherit,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580,1,['inherit'],['inherit']
Modifiability,"Currently, `StandardCallerArgumentCollection`, which contains the `-contamination-file` argument, requires that the contamination file be loaded externally, and its contents then passed back in to the argument collection via `setSampleContamination()`. This is rather poor design, and an invitation for bugs. We should refactor so that `StandardCallerArgumentCollection` handles the loading of the contamination file internally, and remove the `setSampleContamination()` method.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4483:319,refactor,refactor,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4483,1,['refactor'],['refactor']
Modifiability,"Currently, only Posix filesystem paths can be passed as workspaces and arrays to GenomicsDB via GenomicsDBImport and SelectVariants. This PR will allow for hdfs and gcs (and emrfs/s3) URIs to be supported as well. ; Examples; ```; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path hdfs://master:9000/gdb_ws -L 1:500-10000; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path gs://my_bucket/gdb_ws -L 1:500-10000; ```; ```; ./gatk SelectVariants -V gendb.hdfs://master:9000/gdb_ws -R hs37d5.fa -O out.vcf; ./gatk SelectVariants -V gendb.gs://my_bucket/gdb_ws -R hs37d5.fa -O out.vcf; ```; GenomicsDB supports GCS via the [Cloud Storage Connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage). Set environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the GCS Service Account json file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5017:785,variab,variable,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017,1,['variab'],['variable']
Modifiability,"Currently, the only way to specify locatable columns for a locatable xsv file is through a config file (and only a sibling config file if tribble compatibility is required). . In the future, we should have structured comments that specify the names of the locatable columns (i.e. contig, start, end). For example:; ```; # This is my annotated segment file; #_ContigHeader=CONTIG;; #_StartHeader=START;; #_EndHeader=END;; CONTIG START END Annotation1 Annotation2 ; 1 100 200 foo bar; ```. These comments should be parsed and the specified columns used. If the structured comments are not there, then fallback on a specified config file, then fallback a default config file in the jar itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4489:91,config,config,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4489,4,['config'],['config']
Modifiability,"Currently, we need to copy files on S3 to local storage before using; them. This patch enables gatk local and spark modes to access s3a://; files directly to reduce copy overhead and local disk usages. s3a file accesses require additional configuration of core-site.xml; located in CLASSPATH as well as other hadoop applications. Spark; already has hadoop dependencies but local modes need to add hadoop; jars in the classpath. Example core-site.xml:. ```; <configuration>; <property>; <name>fs.s3a.access.key</name>; <value>{Your AWS_ACCESS_KEY_ID}</value>; </property>; <property>; <name>fs.s3a.secret.key</name>; <value>{Your AWS_SECRET_ACCESS_KEY}</value>; </property>; </configuration>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698:239,config,configuration,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698,3,['config'],['configuration']
Modifiability,"Currently, we're instantiating our CommandLineProgram before we've parsed its arguments and injected them into the appropriate member variables. This means that the constructors for our tools (eg., the ReadWalker constructor) cannot use argument values during initialization, which is a big problem. We need to delay instantiation until after arguments are parsed and injected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/107:134,variab,variables,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/107,1,['variab'],['variables']
Modifiability,Custom Spark configuration in tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337:13,config,configuration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337,1,['config'],['configuration']
Modifiability,"DK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5535,Config,ConfigFactory,5535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"DK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6174,Config,ConfigFactory,6174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"DK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4218,Config,ConfigFactory,4218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"DK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG Sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3742,Config,ConfigFactory,3742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"DO NOT REVIEW UNTIL PR 3908 is in!. Created an XSV Locatable Codec for Tribble that will allow arbitrarily delimited locatable files to be read in and indexed. It's a tad clunky, as a config file is required for each input file, but it works well. Fixes #3898",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3943:184,config,config,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3943,1,['config'],['config']
Modifiability,"DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6007,Config,ConfigFactory,6007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6646,Config,ConfigFactory,6646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,DRAFT: Changes for htsjdk VCFHeader refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8306:36,refactor,refactoring,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8306,1,['refactor'],['refactoring']
Modifiability,DRAFT: Use temp snapshot of htsjdk with rans refactoring.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8730:45,refactor,refactoring,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8730,1,['refactor'],['refactoring']
Modifiability,"D_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:02:08.892 INFO PrintReadsSpark - Deflater: IntelDeflater; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:6258,config,configuration,6258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"Dear all,. I am a bit confused why GATK uses `[0,1,2]` for the `GT` files, even though VCF specifications clearly state that the `GT` field is `encoded as allele values separated by either of / or |`. They even say that for `diploid calls examples could be 0/1, 1 | 0, or 1/2, etc`. As it is right now, if I read a VCF from GATK CNV germline pipeline through `bcftools`, the `GT` field is changed to `-65`:; ```; 1 17345376 CNV_1_17345376_161326630 N <DUP> 101.19 . END=161326630 GT:CN:NP:QA:QS:QSE:QSS -65:3:13:11:101:3:18. 1 161332119 CNV_1_161332119_161332223 N <DEL> 3.19 . END=161332223 GT:CN:NP:QA:QS:QSE:QSS -65:1:1:3:3:3:3. 1 193091331 CNV_1_193091331_241683022 N <DUP> 268.21 . END=241683022 GT:CN:NP:QA:QS:QSE:QSS -65:3:27:34:268:36:3. 2 96919546 CNV_2_96919546_96931119 N . 62.93 . END=96931119 GT:CN:NP:QA:QS:QSE:QSS -65:2:3:38:63:38:63. 3 10183532 CNV_3_10183532_69928534 N . 469.93 . END=69928534 GT:CN:NP:QA:QS:QSE:QSS -65:2:22:31:470:19:75. 3 69986973 CNV_3_69986973_70014399 N <DUP> 10.12 . END=70014399 GT:CN:NP:QA:QS:QSE:QSS -65:3:8:4:10:4:10; ```. Any reason to not use the standaed `GT` format?. I have also noticed that GATK outputs some non-variable SVs to the VCF without any ALT allele. Why not remove them if they are actually not SVs, if `GT=0` and `CN=2`?. thanks,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-621738904:1164,variab,variable,1164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-621738904,1,['variab'],['variable']
Modifiability,"Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3537,Config,ConfigFactory,3537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,Delete redundant methods in SVCigarUtils; rewrite and move the rest to CigarUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6481:42,rewrite,rewrite,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6481,1,['rewrite'],['rewrite']
Modifiability,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7193:407,extend,extended,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193,1,['extend'],['extended']
Modifiability,Design GATK configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:12,config,configuration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['config'],['configuration']
Modifiability,"Despite that spark supports equals signs in their configuration values the current code in this class does not as it splits the name=value string in all ""="" present resulting in an bad-argument value exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3687:50,config,configuration,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3687,1,['config'],['configuration']
Modifiability,Determine exactly which settings should live in the new Owner config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3080:62,config,config,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3080,1,['config'],['config']
Modifiability,"Discussed in person with @cmnbroad. We decided to go with a different approach that avoids the need for a central registry + connecting code, would be 100% pluggable, and would be easily extensible to variant annotations if we want to add `@Argument` capability there as well:. -replace lambda read filters with explicit class definitions; -patch the argument system to add the generic ability to discover arguments within plugin classes, and pass on populated instances of those plugin classes to the tool (likely involves only a very small amount of additional code, based on our reading of `CommandLineParser`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-228180490:423,plugin,plugin,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-228180490,2,['plugin'],['plugin']
Modifiability,Disk space variables inconsistently exposed in gCNV WDLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6995:11,variab,variables,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6995,1,['variab'],['variables']
Modifiability,Do a refactoring pass on existing htsjdk CRAM code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5201:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5201,1,['refactor'],['refactoring']
Modifiability,"Do you think that this is really necessary for a normal user? I mean, usually `UserException` should have a well-defined message String for point to the user what happened. If the stack traces are necessary, are for debugging and I think that the final user won't benefit for having an extra argument. In addition, it is in the repository README, which made it discoverable for developers and it is what it is really meant for (I guess). Other option may be to print the stack trace for `UserException`only if the verbosity is set to DEBUG, and that will get rid of the environmental variable....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285282288:584,variab,variable,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443#issuecomment-285282288,1,['variab'],['variable']
Modifiability,Docker image (broadinstitute/gatk) has too many unnecessary layers potentially causing throttling issues,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684:60,layers,layers,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684,1,['layers'],['layers']
Modifiability,Doing some additional refactoring and rebasing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434#issuecomment-557828837:22,refactor,refactoring,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434#issuecomment-557828837,1,['refactor'],['refactoring']
Modifiability,"Done with my review. Thanks for doing this much-needed refactor! The BaseRecal stuff looks sound, but I have some other feedback that we should discuss/address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080:55,refactor,refactor,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142750080,1,['refactor'],['refactor']
Modifiability,"EATE_MD5 : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5066,Config,ConfigFactory,5066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"EATE_MD5 : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5705,Config,ConfigFactory,5705,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,EBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5306,Config,ConfigFactory,5306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"EBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (198295559 bp); 16:16:37.373 DEBUG GenomeLocParser - chr4 (190214555 bp); 16:16:37.373 DEBUG GenomeLocParser - chr5 (181538259 bp); 16:16:37.373 DEBUG GenomeLocParser - chr6 (17080597",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5767,Config,ConfigFactory,5767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"EBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.826 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4664,Config,ConfigFactory,4664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"EBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (198295559 bp); 16:16:37.373 DEBUG GenomeLocParser - chr4 (190214555 bp); 16:16:37.373 DEBUG GenomeLocParser - chr5 (181538259 bp); 16:16:37.373 DEBUG GenomeLocParser - chr6 (170805979 bp); 16:16:37.373 DEBUG GenomeLocParser - chr7 (159345973 bp); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5833,Config,ConfigFactory,5833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"EBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.826 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:06:12.479 INFO FeatureManager - Using codec IntervalListCodec to r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4730,Config,ConfigFactory,4730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"EF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4287,Config,ConfigFactory,4287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"ENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5059,Config,ConfigFactory,5059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,ENSEMBL GTF files have variable formats which Funcotator does not expect,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6488:23,variab,variable,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6488,1,['variab'],['variable']
Modifiability,"ERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3614,Config,ConfigFactory,3614,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3187,config,configuration,3187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2642,config,configuration,2642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"ER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG Conf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3234,Config,ConfigFactory,3234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,Edited .travis.yml to add upload a report to coveralls. We're using the coveralls gradle plugin from https://github.com/kt3k/coveralls-gradle-plugin.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/79:89,plugin,plugin,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/79,2,['plugin'],['plugin']
Modifiability,Enhance M2 hapmap sensitivity wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3248:0,Enhance,Enhance,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3248,1,['Enhance'],['Enhance']
Modifiability,Evaluate commons-configuration for use as an overridable master GATK configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:17,config,configuration,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,2,['config'],['configuration']
Modifiability,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:648,adapt,adapted,648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,1,['adapt'],['adapted']
Modifiability,"Even if I'll refactore it to be void, the part that I changed is need it: there is no other part which handle the `CommandLineException` and if the `customCommandLineValidation()` throws the exception no error is printed in the terminal. I guess that returning a `String[]` in Picard is done to output several errors in the command line to avoid the user to re-run with another bug not reported. Nevertheless, I prefer you approach. I'm changing now the code in this PR to add what you suggested. Thanks again for make my development smoother, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377:13,refactor,refactore,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377,1,['refactor'],['refactore']
Modifiability,Extend ReadsPipelineSpark to run HaplotypeCallerSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3452:0,Extend,Extend,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3452,1,['Extend'],['Extend']
Modifiability,"Extends BwaMemImageSingleton into a cache, BwaMemImageCache, that can…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3359:0,Extend,Extends,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3359,1,['Extend'],['Extends']
Modifiability,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3122:35,adapt,adapter,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122,1,['adapt'],['adapter']
Modifiability,"FC was fine with previous model of having normal input bams/bais be optional parameters. I don't think FC would let you do a pair missing a sample. Typically, in FC, you would create two method configurations, one for tumor-only and one for pair. Then the former would be run on samples and the latter run on pairs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352624267:194,config,configurations,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352624267,1,['config'],['configurations']
Modifiability,"FO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDefla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5137,Config,ConfigFactory,5137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5077,Config,ConfigFactory,5077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"FYI, if you set the environment variable TILEDB_DISABLE_FILE_LOCKING=1 before running any GenomicsDB tool, it doesn't try to lock files on POSIX filesystems (Lustre, NFS, xfs, ext4 etc)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-437188003:32,variab,variable,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-437188003,1,['variab'],['variable']
Modifiability,Feature Request: Make Mitochondria Pipeline more portable with string inputs for tool paths,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6258:49,portab,portable,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6258,1,['portab'],['portable']
Modifiability,"FeatureManager dynamically discovers all `FeatureInput` arguments in a tool by accepting a (pre-populated) CommandLineProgram object, which it then passes to static methods in Barclay. Those methods perform the same `@Argument`/`@ArgumentCollection` discovery already implemented by the parser, but using separate, out-of-date code paths that currently don't discover `@PositionalArguments` or plugin descriptor arguments. Rather than fixing the redundant code paths and static methods in Barclay, they can be eliminated and replaced with an instance method on the parser. Since FeatureManager already requires that the parser have been run on the tool, the parser already has the state necessary to just collect the results. However, this means that FeatureManager would require a CommandLineParser object instead of the tool itself. (Alternatively, we could extract the results from the parser and pass them in directly to FeatureManager instead of the parser). @droazen do you have any opinion on this before I refactor this part of the parser ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4480:394,plugin,plugin,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480,2,"['plugin', 'refactor']","['plugin', 'refactor']"
Modifiability,Figure out why the travis tests are running slower in the new configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4989:62,config,configuration,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989,1,['config'],['configuration']
Modifiability,"Finally getting around to this @fleharty, apologies! Decided to punt on most of your requests, but hopefully my reasoning is acceptable. Perhaps we can also get it in by release and @takutosato can use this version of ModelSegments in his CRSP evaluations (and maybe even verify there are no changes from the previous version in typical, single-sample copy-ratio + allele-fraction use---since from that perspective all code changes should just be a refactor---if he has time).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-857808000:449,refactor,refactor---if,449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-857808000,1,['refactor'],['refactor---if']
Modifiability,"Finally got the test passing in travis (silly error from refactoring previous stuff). Ready to go, @droazen!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688:57,refactor,refactoring,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688,1,['refactor'],['refactoring']
Modifiability,"Finished refactoring the production code as detailed above, just need to add some tests. Results are exactly the same as before in single-sample mode---except for allele-fraction-only mode. This is because I refactored all existing segmenters (there were separate ones for copy-ratio-only, allele-fraction-only, and copy-ratio + allele-fraction) as special cases of a single multisample segmenter; however, the Gaussian kernel in the old allele-fraction-only segmenter was missing a normalization factor that is now present in the new multisample segmenter. Thus, users who previously ran in allele-fraction-only mode will have to retune parameters to achieve the same level of sensitivity. I expect that this will be a very small number of users (if any---note that allele-fraction-only mode isn't even exposed in the WDL), but we can probably mention it in the release notes. Might need to update a figure, etc. as well in the tutorial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344:9,refactor,refactoring,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-611833344,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,First candidate is Apache Configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307472726:26,Config,Configuration,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307472726,1,['Config'],['Configuration']
Modifiability,"First commit:; -Added CreateReadCountPanelOfNormals tool. This is an update of CreatePanelOfNormals. Related code is written from scratch.; -Added DenoiseReadCounts tool. This is an update of NormalizeSomaticReadCounts. Related code is written from scratch.; -Added AnnotateIntervals tool. This is an update of AnnotateTargets. Related code (e.g., GCBiasCorrector) is mostly ported and does not have to be closely re-reviewed. I naively introduced RecordCollection and LocatableCollection classes that are analogous to SampleRecordCollection and SampleLocatableCollection, respectively, for collections that are not tied to a sample (e.g., GC-content annotations); we can go back and refactor these classes later.; -SVDDenoisingUtils contains many package-private helper methods for filtering and denoising without unit tests. This is intentional. I have verified that this code exactly reproduces the old PoN results down to the 1E-16 level (with the discrepancy coming from the removal of redundant pseudoinverse operations). Rather than writing or porting unit tests for this code, I think it is best if we simply do not reuse this code or make non-trivial changes to it going forward. We can add unit tests later if we have extra time on our hands...; -SparkGenomeReadCounts now outputs TSV and HDF5.; -Added some tests for SimpleCountCollection, HDF5SimpleCountCollection, and some disabled tests for HDF5Utils.; -Miscellaneous cleanup and boy scout activities. Second commit:; -Updated coverage collection in germline and legacy somatic CNV WDLs to use only integer read counts and account for changes to SparkGenomeReadCounts.; -Added tasks for PreprocessIntevals, AnnotateIntervals, and CollectFragmentCounts.; -Renamed and moved some files. Closes #3570.; Closes #3356.; Closes #3349.; Closes #3246.; Closes #3153.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3820:684,refactor,refactor,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3820,1,['refactor'],['refactor']
Modifiability,"First cut at a rewrite seems to be working fine and is much, much leaner. Building a small PoN with 4 simulated normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:15,rewrite,rewrite,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,1,['rewrite'],['rewrite']
Modifiability,First implementation of Annotations as a Barclay plugin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3851:49,plugin,plugin,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3851,1,['plugin'],['plugin']
Modifiability,Fix ReadFilter plugin descriptor blowing up for default arguments when not provided,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2359:15,plugin,plugin,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2359,1,['plugin'],['plugin']
Modifiability,"Fix a particular ""bug"" observed that caused us dozens of variants:. When an assembly contig has a unique region covered by one or more MQ 0 alignments, the current configuration picker classifies such contig as ""ambiguous"". ; This PR fix this problem by implementing a tie breaker saying that for such contigs, we prefer the unique configuration with no MQ 0 alignments; that is, this tie breaker only saves contigs have one unique configuration with all non-MQ-0 mappings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326:164,config,configuration,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326,3,['config'],['configuration']
Modifiability,Fix adapter boundary for positive strand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2270:4,adapt,adapter,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2270,1,['adapt'],['adapter']
Modifiability,Fix adapter bounday for positive strand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2346:4,adapt,adapter,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2346,1,['adapt'],['adapter']
Modifiability,Fix an oversight in recent refactoring of annotation code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2239:27,refactor,refactoring,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239,1,['refactor'],['refactoring']
Modifiability,Fix bug with MateDistantReadFilter not being configurable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701:45,config,configurable,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701,1,['config'],['configurable']
Modifiability,Fix copy/paste errors with Docker image variable names,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8474:40,variab,variable,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8474,1,['variab'],['variable']
Modifiability,Fix erroneous warning about GCS test configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5987:37,config,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5987,1,['config'],['configuration']
Modifiability,Fixed a rare non-determinism in the AdaptiveChainPruner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7851:36,Adapt,AdaptiveChainPruner,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7851,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"Fixed for filtering in #5498. `Mutect2` itself will be trickier because a lot of unused arguments are a result of `SomaticGenotypingEngine`'s inheritance. This will involve significant refactoring, but the genotyping classes will make more sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5352#issuecomment-445517991:142,inherit,inheritance,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5352#issuecomment-445517991,2,"['inherit', 'refactor']","['inheritance', 'refactoring']"
Modifiability,Fixed missing end-of-line backslashes in commands and made a few additional enhancements to the doc text,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2781#issuecomment-309632487:76,enhance,enhancements,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2781#issuecomment-309632487,1,['enhance'],['enhancements']
Modifiability,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4823:64,config,config,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823,1,['config'],['config']
Modifiability,Fixes #4193. `XsvTableFeature` no longer removes an extra column if start and end in; the config file for a `LocatableXsv` data source are the same.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4915:90,config,config,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4915,1,['config'],['config']
Modifiability,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960:29,config,configuration,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960,3,['config'],"['config', 'configuration']"
Modifiability,"Fixes #4739; Refactored UTR VariantClassification handling.; Added warning statement when a transcript in the UTR has no sequence info (now is the same behavior as in protein coding regions).; Added tests to prevent regression on data source date comparison bug.; Now can run on large data.; Fixed DNA Repair Genes getter script.; Fixed an issue in COSMIC to make it robust to bad COSMIC data.; Gencode no longer crashes when given an indel that starts just before an exon.; Fixed the SimpleKeyXsvFuncotationFactory to allow any characters to work as delimiters (including characters used in regular expressions, such as pipes).; Modified several methods to allow for negative start positions in; preparation for allowing indels that start outside exons.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4817:13,Refactor,Refactored,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817,1,['Refactor'],['Refactored']
Modifiability,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5819:481,variab,variable,481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819,1,['variab'],['variable']
Modifiability,"Fixes #7166 #7385 . This doesn't contain any of the necessary work to support Gencode GFF3 files yet #, that will (probably) come in a subsequent PR as it requires a much more substantial refactoring effort of the Gencode datasources code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8351:188,refactor,refactoring,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8351,1,['refactor'],['refactoring']
Modifiability,Fixes for https://github.com/broadinstitute/gatk/issues/4525 and https://github.com/broadinstitute/gatk/issues/4633. Longer term we should do a more significant rewrite/refactoring.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4680:161,rewrite,rewrite,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4680,2,"['refactor', 'rewrite']","['refactoring', 'rewrite']"
Modifiability,"Fixes https://github.com/broadinstitute/gatk/issues/5362 and https://github.com/broadinstitute/gatk/issues/4637. Log messages from JEXL were being dropped because of https://github.com/broadinstitute/gatk/issues/4637, making the behavior described by https://github.com/broadinstitute/gatk/issues/5362 even more subtle to discern. Now, instead of seeing:. > log4j:WARN No appenders could be found for logger (org.apache.commons.jexl2.JexlEngine).; > log4j:WARN Please initialize the log4j system properly.; > log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. the user will see:. > 11:51:06.321 WARN JexlEngine - ![51,60]: 'QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0 || ExcessHet > 54.69;' undefined variable MQRankSum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5422:784,variab,variable,784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422,1,['variab'],['variable']
Modifiability,"Fixing womtool is a bit more complicated, since it affects FireCloud method configuration parameters as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362118499:76,config,configuration,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4319#issuecomment-362118499,1,['config'],['configuration']
Modifiability,"Following our recommended IntelliJ setup instructions in our README leads to an IntelliJ project that does not respect the user's PATH environment variable when, eg., building via gradle. We instead end up with a default PATH that includes only a few directories such as `/usr/bin/`. . We need to find a way to get IntelliJ to respect the user's PATH when building, and update our README accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/337:147,variab,variable,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/337,1,['variab'],['variable']
Modifiability,"For @davidbenjamin. I've been asked to take the user-perspective and report things I'd want to make our WDL scripts more extensible. So here goes. <img width=""824"" alt=""screenshot 2018-01-17 13 22 27"" src=""https://user-images.githubusercontent.com/11543866/35060379-501cb8c8-fb8c-11e7-845e-a146fc2ced94.png"">. ## Major wants; - The is_bamout Boolian appears to be hardcoded to `false` in the script. Users need to be able to understand that this option can be changed without ambiguity. So this should become a proper optional variable. @LeeTL1220 tells me this can be overwritten. However, why leave this as misinterpretable to newbie WDL-scriptors? Especially since `wdltools inputs` doesn't include it as a variable at all in the generated inputs list. Please can we make this a proper optional argument that `wdltools inputs` will generate a variable for.; - [ ""${variants_for_contamination}"" == *.vcf ] does not allow *.vcf.gz files. It should accept either.; - Outputs should allow either .vcf or .vcf.gz compression by user-specification. Alternatively, if we want to keep it simple and hardcode, then the preference is for compressed files. Some of us prefer to save on storage.; - Need to be able to specify optional string args for SplitIntervals. I would like to be able to use the BALANCING_WITHOUT_INTERVAL_SUBDIVISION mode. Furthermore, I'd like for the tool to automatically interpret this mode, when not given an -L intervals list, to not split reference contigs. I.e. a contig is an interval. (Perhaps already the tool behavior?); - The version of Oncotator is not compatible with GRCh38. Please, can we have an option to switch this out with Funcotator? . ## Minor wants; - The JSON template in the repo should show the optional variables.; - Script calls for a Picard jar. I don't mind specifying this because I like controlling for the Picard version I use. However, users may want to call the Picard version within the GATK jar. I cannot fathom a simple way to allow switching thi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188:527,variab,variable,527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188,3,['variab'],['variable']
Modifiability,"For downstream projects, there is a way to bundle single class files into the toolkit by adding them to the `Main.getClassList`; nevertheless, this only allows to include classes extending the `org.broadinstitute.hellbender.cmdline.CommandLineProgram`. For including a tool from picard, the only way is to add a whole package where `picard.cmdline.CommandLineProgram` extensions are located. Either a new method for include single classes from Picard should be added, or https://github.com/broadinstitute/barclay/issues/127 implemented (a proposal has been done in https://github.com/broadinstitute/barclay/pull/96) and move Picard/GATK to use the common interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4660:179,extend,extending,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660,1,['extend'],['extending']
Modifiability,"For example, contig-ploidy disk space is exposed only in case mode. Variable names are somewhat inconsistent, too (`disk_space_*` vs. `disk_space_gb_*`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6995:68,Variab,Variable,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6995,1,['Variab'],['Variable']
Modifiability,"For large `x`, `NB(x | mu, alpha)` can be replaced with a Gaussian approximation which is much faster to compute. In practice, we get ~3x speedup if we naively replace all `NB` with `Normal`. However, we will lose accuracy on deletions and low-coverage loci. It is desirable to be able to adaptively switch between the two based on a criterion (i.e. if the relative difference of exact/approximate lpdf is below a given threshold). The match can be worked out, however, limitations in `theano` makes the implementation tricky: `theano.tensor.switch(condition, a, b)` is not lazy (undesirable) and evaluates both `a` and `b`, however, it works for tensor `condition` (desirable). On the other hand,`theano.ifelse.ifselse(condition, a, b)` is lazy (desirable) but only works if `condition` is a scalar (undesirable).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4059:289,adapt,adaptively,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4059,1,['adapt'],['adaptively']
Modifiability,"For now, this request will be treated together in this issue. We may split it later. @vdauwera - i moved it here because it's about ReadTransformers. https://www.pivotaltracker.com/story/show/70281952. ReadTransformers should be able to be added by the command line like; filters. Currently the only way to include or add a new readTransformer is; by adding a new argument to the GATK engine, and there is no way to run; only specific transformer by a walker.; When that will be done, there are readTransformers that are currently; written as filter (for example: ReassignMappingQuality) that should be; refactor to readTransformers.; Also it will be good to put all the ReadTransformers together (like the; filters).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6#issuecomment-66436109:604,refactor,refactor,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6#issuecomment-66436109,1,['refactor'],['refactor']
Modifiability,"For running the tests on the cloud, it needs to be configured. Since we don't want to hardcode project details in the code (we don't all write to the same cloud project), I'm using environment variables. For the cloud tests to pass we have to set:TEST_PROJECT and TEST_STAGING_GCS_FOLDER.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/529#issuecomment-105577911:51,config,configured,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/529#issuecomment-105577911,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"For some reason, the environment variable is not getting passed to GenomicsDB at all. To help debug the issue, can you do the following and see if any consolidation lock is created at all? ; ```; find /path/to/genomicsdb_workspace -name "".__consolidation_lock""; ```; Also, what type of Posix filesystem is your GenomicsDB workspace? Is it NFS or Lustre? How is file locking configured on the system?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627#issuecomment-637237653:33,variab,variable,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627#issuecomment-637237653,2,"['config', 'variab']","['configured', 'variable']"
Modifiability,"For the second point (file format) I would prefer something different than Java Properties, because for lists will be a bit messy (separated by comma in Apache Configuration). Maybe YML or JSON will be better for this purpose?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307729448:160,Config,Configuration,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307729448,1,['Config'],['Configuration']
Modifiability,For the tests to work we must define:; HELLBENDER_TEST_PROJECT : Google Project to use; HELLBENDER_JSON_SERVICE_ACCOUNT_KEY : path to a JSON file with service; account credentials. (I've updated the README accordingly). (We've used both before so they should already be configured). This fixes issue #3125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159:270,config,configured,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159,1,['config'],['configured']
Modifiability,Forum user question on Spark configurations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3822:29,config,configurations,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3822,1,['config'],['configurations']
Modifiability,"Found a serious bug in this PR: `passesEmitThreshold()` was calling `passesCallThreshold(configuration.genotypeArgs.STANDARD_CONFIDENCE_FOR_CALLING)` instead of `passesCallThreshold(conf)`, which caused `STANDARD_CONFIDENCE_FOR_CALLING` to get compared against itself! Pushing a fix for this now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-286781671:89,config,configuration,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-286781671,1,['config'],['configuration']
Modifiability,"From a high level, users really shouldn't be worrying about this. The Hadoop defaults are often adequate. This is not a perfect analogy, but you generally don't think about block sizes for your usual Linux file system. Also, the default 4 MB size that it's currently set to is very small. The individual Spark tasks finish in just a few seconds, which means you're probably paying more overhead than you need. > 1. How do you handle situations where you need different split sizes for different file types?. You can manually specify the block sizes for individual files in HDFS. It's possible that you could also specify the mapreduce split sizes in the `InputFormat`, but I'd need to ask @tomwhite. > 1. How do you handle setting reasonable defaults so it works right out of the box without people having to set their spark configs explicitly?. Have we already decided that the default Spark configs are bad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158515933:825,config,configs,825,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1166#issuecomment-158515933,2,['config'],['configs']
Modifiability,"From ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ the file `gencode/hg38/gencode.config` makes reference to `gencode.v27.pc_transcripts.fasta` . ```; # Required field for GENCODE files.; # Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path = gencode.v27.pc_transcripts.fasta; ```. which is not present in the directory. . ```; $ ls; gencode.config; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf.idx; gencode.v27.transcripts.dict; gencode.v27.transcripts.fasta; gencode.v27.transcripts.fasta.fai; ```. Downloading the pc_transcript.fasta from gencode does not fix the errors thrown when trying to use the reference. The error message is a fairly uninformative null pointer exception. . ```; 00:27:35.745 INFO Funcotator - Done initializing engine; 00:27:35.758 INFO Funcotator - Shutting down engine; [March 12, 2018 12:27:35 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2258108416; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521:104,config,config,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521,2,['config'],['config']
Modifiability,"From the comments in the code:. ```; // TODO: Test with multiallelics; // TODO: Test with multiallelics and symbolic at the same time; // TODO: Test with symbolic; // TODO: Test with information missing from the VCF and make sure appropriate exception is thrown.; // TODO: Test with more cutoff variables; // TODO: Once above five TODOs are done (at least), AnnotatePairOrientation can be taken out of Experimental status.; ```; We can raise the priority if this tool gains traction. That seems unlikely since this is still a bit of a niche tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3623:295,variab,variables,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3623,1,['variab'],['variables']
Modifiability,Funcotation Engine Refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5134:19,Refactor,Refactoring,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5134,1,['Refactor'],['Refactoring']
Modifiability,FuncotationMap refactoring and VCF/MAF concordance for protein changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4838:15,refactor,refactoring,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4838,1,['refactor'],['refactoring']
Modifiability,Funcotator - Need to create funcotator config file as an alternative to CLI options,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4581:39,config,config,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581,1,['config'],['config']
Modifiability,Funcotator - Refactor Funcotation class to use a HashMap for each field,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3919:13,Refactor,Refactor,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3919,1,['Refactor'],['Refactor']
Modifiability,Funcotator - turn config file names into enum,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5465:18,config,config,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5465,1,['config'],['config']
Modifiability,Funcotator datasource configs should not honor spaces (and some other special characters) in the datasource name nor version.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5937:22,config,configs,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937,1,['config'],['configs']
Modifiability,Funcotator hg38 data source not working; configuration files contains reference to files not present,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521:41,config,configuration,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521,1,['config'],['configuration']
Modifiability,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4771:513,config,configurable,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771,1,['config'],['configurable']
Modifiability,"Funcotator requires all GENCODE datasources (in the config) to be named ""Gencode""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4791:52,config,config,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4791,1,['config'],['config']
Modifiability,Funcotator throwing error more than one config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:40,config,config,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['config'],['config']
Modifiability,Funcotator: Add user input for default annotation values on command-line/config file.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3783:73,config,config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3783,1,['config'],['config']
Modifiability,Funcotator: get the NCBI build version from the datasource config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522:59,config,config,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522,1,['config'],['config']
Modifiability,GATK Reduced Docker Layers for ACR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:20,Layers,Layers,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,1,['Layers'],['Layers']
Modifiability,"G_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5355,Config,ConfigFactory,5355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,Gatk leaving behind config files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6771:20,config,config,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6771,1,['config'],['config']
Modifiability,GencodeFuncotationFactory refactoring for better separation of concerns between CNV vs small mutation annotation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5932:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5932,1,['refactor'],['refactoring']
Modifiability,GenomicsDB: max # of alleles should be configurable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687:39,config,configurable,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687,1,['config'],['configurable']
Modifiability,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389:88,config,configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389,1,['config'],['configuration']
Modifiability,"Given that no one (not even stackoverflow) seems to know how to set the root logging level programmatically in log4j 2.x (rather than through a config file), perhaps we should consider downgrading to log4j 1.x, where setting the root logging level was a one-liner:. ```; Logger.getRootLogger().setLevel(level);; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/243#issuecomment-76847662:144,config,config,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-76847662,1,['config'],['config']
Modifiability,"Given that this helps you out, but doesn't change the behavior of our tools, it's pretty much a refactor from our end and I'm happy to give it a 👍",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-334156894:96,refactor,refactor,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-334156894,1,['refactor'],['refactor']
Modifiability,"Glad you were able to resolve your issue. Not sure if this is specific to the CNV tool or if the exception caused by the Spark configuration is more general. Tagging engine team @droazen, but closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686#issuecomment-467510488:127,config,configuration,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686#issuecomment-467510488,1,['config'],['configuration']
Modifiability,"Good to know -- forgot that we had `--arguments_file`. . The advantage of the config file approach would be that you wouldn't have to specify any extra args -- it would just look in the current directory or a known location for a file with your dataflow settings. Things like the pipeline runner would probably still be manually specified, though (perhaps with nice aliases like `--cloud` and `--local`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100311128:78,config,config,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100311128,1,['config'],['config']
Modifiability,"Google is deprecating and removing their implementation of the old style GA4GH read and reference API's. . > ; > Reads API functionality is now replaced by the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global ge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:364,flexible,flexible,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,1,['flexible'],['flexible']
Modifiability,"Google made an incompatible change to the dataproc api which is causing all builds to fail. We're seeing errors like this:; ```; ERROR: (gcloud.beta.dataproc.clusters.create) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; ```. It's mentioned in gcloud release notes here:; ```; 260.0.0 (2019-08-27); Breaking Changes; (Cloud Dataproc) Modified --region flag to be mandatory.; To use Cloud Dataproc commands, pass the --region flag on every invocation, or set the dataproc/region configuration variable via gcloud config set dataproc/region.; For gcloud beta dataproc commands, this flag/config value is required.; For gcloud dataproc commands, the default will remain global until January 2020.; ```. I'm going to set the environment variable in our travis config right now, and then open a separate PR to specify region in all the commands.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6129:383,config,config,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6129,8,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1578:153,config,configuration,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578,1,['config'],['configuration']
Modifiability,"Gradle is gradually transitioning to a new more flexible software model. We use this new style model for our native code, but not for java. We should investigate switching our java build to the new model. Some details are here https://docs.gradle.org/current/userguide/java_software.html. It includes some interesting new features like a mechanism for explicitly declaring a public api for a project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1579:48,flexible,flexible,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1579,1,['flexible'],['flexible']
Modifiability,"HI @lbergelson - ; I'm working on a bug/warning in the variant calling workflow where it's complaining about not finding a logger:. `21:04:59.525 INFO ProgressMeter - Starting traversal; 21:04:59.526 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (io.grpc.netty.shaded.io.netty.util.internal.logging.InternalLoggerFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:05:10.018 INFO ProgressMeter - chr1:4642050 0.2 205000 1172992.6`. I found that if I had gatk's build.gradle NOT exclude the log4j.properties file I get rid of that warning, so I'm trying to understand the issue [here](https://github.com/broadinstitute/gatk/blob/33bda5e08b6a09b40a729ee525d2e3083e0ecdf8/build.gradle#L441): (where you found log4j.properties clashed with log4j2.xml) . James Emery is on the git blame for that, but he thinks that's because of the refactoring he did. Thanks in advance - I'm not sure if there's something else I should be doing with the xml version of that file to avoid this warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722:1023,refactor,refactoring,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722,1,['refactor'],['refactoring']
Modifiability,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBU",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5129,Config,ConfigFactory,5129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5768,Config,ConfigFactory,5768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3617,Config,ConfigFactory,3617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5612,Config,ConfigFactory,5612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6251,Config,ConfigFactory,6251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5422,Config,ConfigFactory,5422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"Ha, this PR was much tinier than I expected -- feet are barely damp. My ""wishlist"" would include a much bigger refactor because I made a mess in Java7 and didn't have the time to clean up (especially with generics) after we switched in Java8. I'm still tinkering with the rank sum tests though, so it's not worth tackling the refactor until those are good to go. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2657#issuecomment-299206702:111,refactor,refactor,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2657#issuecomment-299206702,2,['refactor'],['refactor']
Modifiability,HaplotypeCallerSpark lose a lot of variable sites and the result jitter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4488:35,variab,variable,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488,1,['variab'],['variable']
Modifiability,"Hello - I'm not sure if you want me using issues for feedback, but i thought I'd pass this along. In another thread we discussed how to possibly do scatter/gather processing of GenomicsDB workspaces. The general idea is that we want to have long-lived workspaces to which we will repeatedly add more samples. Executing this append would be a lot more convenient to do scattered over intervals. Since GenomicsDB already has the data organized into folders by interval, I figured we might be back to manually split one workspace apart by copying each contig's folder out to make a new workspace, execute the merge over that interval, and then copy them back together. So far as I can tell this works. It seems like it will significantly speed the process of creating new workspaces and also adding samples. As I said on the other thread, since your docs recommend making a backup of a workspace before trying to append samples anyway, copying it out into new working folders to execute that append step isnt all that different. I realize we're doing a non-supported thing here. I post simply to mention that this scheme seems like it will be quite useful and I hope you might keep it in mind as GenomicsDB evolves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620:1204,evolve,evolves,1204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620,1,['evolve'],['evolves']
Modifiability,Hello - we're interested in creating a custom extension of Funcotator with different output formats. This PR should be quite low risk - it just converts a handful of privates fields/methods to protected to make it easier to extend this tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8124:224,extend,extend,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124,1,['extend'],['extend']
Modifiability,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7986:228,config,config,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986,1,['config'],['config']
Modifiability,"Hello @SZLux,. This looks suspiciously like #3050. I suspect this isn't a PathSeq issue, but to be sure can you please try to run another GATK Spark tool such as CountReadsSpark? If that does not work, it's likely an issue with your configuration or Spark/Java versions being incompatible. . What kind of environment are you running in? My suspicion is you are running on a cluster and have the correct Spark/Java version on the driver (master node) but perhaps not on the workers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768:233,config,configuration,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768,1,['config'],['configuration']
Modifiability,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:168,extend,extended,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490,1,['extend'],['extended']
Modifiability,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didn’t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:595,variab,variable,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,1,['variab'],['variable']
Modifiability,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796:519,config,configuration,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796,1,['config'],['configuration']
Modifiability,"Hello, . I have the same problem, . path_input_file=/work/gr-fe/archive/sample_repository/all_exome_gvcfs_hg38/FVH/exomes #patient GVCF; path_name_individu=/work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_name.txt; path_output=/work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_data.vcf.gz; tmp_folder=/scratch/sboutry/logs/combine_gvcf_file; nbr_groups=2. #Path to database and programs; REF=/work/gr-fe/saadat/Reference_Genome/GRCH38_no_alt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa.gz; BCFTOOLS=/work/gr-fe/sboutry/tools/bcftools/install/bin/bcftools; export BCFTOOLS_PLUGINS=/work/gr-fe/sboutry/tools/bcftools/bcftools/plugins; TABIX=/work/gr-fe/sboutry/tools/tabix/tabix-0.2.6/tabix; GATK=/work/gr-fe/sboutry/tools/gatk/gatk-4.2.2.0/gatk. cd ${path_input_file}. ${GATK} --java-options ""-Xmx180G -XX:ParallelGCThreads=36"" CombineGVCFs -R ${REF} --variant ${path_name_individu} -O ${path_output}/patient_data.g.vcf.gz. where all my files are like this . JL0015.g.vcf.gz; JL0016.g.vcf.gz; JL0017.g.vcf.gz; JL0018.g.vcf.gz; JL0019.g.vcf.gz; JL0020.g.vcf.gz; JL0182.g.vcf.gz; JL0183.g.vcf.gz; JL0184.g.vcf.gz; JL0185.g.vcf.gz; JL0186.g.vcf.gz; JL0234.g.vcf.gz; JL0278.g.vcf.gz; JL0412.g.vcf.gz; JL0417.g.vcf.gz; JL0515.g.vcf.gz. A USER ERROR has occurred: Cannot read file:///work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_name.txt because no suitable codecs found. Thanks a lot for any help . Best, . Simon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8255#issuecomment-1918989841:652,plugin,plugins,652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8255#issuecomment-1918989841,1,['plugin'],['plugins']
Modifiability,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:553,sandbox,sandbox,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['sandbox'],['sandbox']
Modifiability,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5948:1170,extend,extended,1170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948,1,['extend'],['extended']
Modifiability,"Hello, I would like to ask for the implementation of a SlidingWindowWalker (both for reads and variants), that could be very interesting for other tools. I was thinking that it could be similar to IntervalWalker, but generating the intervals internally using a window size and step size as parameters (provided by the user and without merging overlapping intervals, like suggested on #302). I think that this issue is different from issue #10 and the pull request #890 because it is a more general Walker that could be abstract and extendable by other tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1198:532,extend,extendable,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1198,1,['extend'],['extendable']
Modifiability,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494:344,variab,variable,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494,1,['variab'],['variable']
Modifiability,"Hello,. I am running GermlineCNVCaller and PostprocessGermlineCNVCalls (GATK v4.2.5) for CNV analysis on our targeted capture. . My output segment vcfs have no SVLEN or SVTYPE values although those are described in their headers. . Info from header includes:; ```; ##INFO=<ID=AC_Orig,Number=A,Type=Integer,Description=""Original AC"">; ##INFO=<ID=AF_Orig,Number=A,Type=Float,Description=""Original AF"">; ##INFO=<ID=AN_Orig,Number=1,Type=Integer,Description=""Original AN"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##INFO=<ID=SVLEN,Number=.,Type=Integer,Description=""Difference in length between REF and ALT alleles"">; ##INFO=<ID=SVTYPE,Number=1,Type=String,Description=""Type of structural variant"">; ```; But the actual vcf output only has the END variable. Example output:. ```; 13	32839931	CNV_13_32839931_32945267	N	.	3076.53	.	END=32945267	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:63:169:3077:523:342; 13	32950659	CNV_13_32950659_32954345	N	<DEL>	3076.53	.	END=32954345	GT:CN:NP:QA:QS:QSE:QSS	0/1:1:7:709:3077:709:831; 13	32968699	CNV_13_32968699_73961012	N	.	3076.53	.	END=73961012	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:14:210:3077:295:630; 14	24883828	CNV_14_24883828_94854954	N	.	3076.53	.	END=94854954	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:72:100:3077:287:299; 15	32992921	CNV_15_32992921_91535389	N	.	3076.53	.	END=91535389	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:35:102:3077:198:331; ```. Commands running are below:. ```; docker run -v /home/dnanexus/inputs:/data $GATK_image gatk GermlineCNVCaller \; -L /data/beds/filtered.interval_list -imr OVERLAPPING_ONLY \; --annotated-intervals /data/beds/annotated_intervals.tsv \; --run-mode COHORT \; $batch_input \; --contig-ploidy-calls /data/ploidy-dir/ploidy-calls/ \; --output-prefix CNV \; -O /data/gCNV-dir. parallel --jobs 8 '/usr/bin/time -v docker run -v /home/dnanexus/inputs:/data $GATK_image \; gatk PostprocessGermlineCNVCalls \; --sample-index {} \; --autosomal-ref-copy-number 2 \; --allosomal-contig X \; --allosomal-contig Y \; --",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7964:787,variab,variable,787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7964,1,['variab'],['variable']
Modifiability,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:628,config,configparser,628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,2,['config'],['configparser']
Modifiability,"Hello,. I'd like to see if your team would potentially be willing to accept a PR to add a feature to GenotypeGVCFs. The general problem is this:. 1) When running GenotypeGVCFs, the default is to output variant sites, and this will therefore vary based on the set of samples. While there is an argument to include every site, calling against every position of the genome takes a very long time.; 2) As you know, a VCF file generally only includes variable sites in the current samples. Therefore, this doesnt differentiate between the situation where all samples have no data and when all samples are wild-type.; 3) We want to merge VCFs with data from different cohorts, including WGS and WES. It's just not practical to call 1000s of samples as one unit through GenotypeGVCFs (we're constantly adding new data and would need to keep re-calling). When merging these VCFs, we see a problem that is especially acute at sites with relatively rare variants. If the variant is only present in one or a few input VCFs, the other VCFs frequently lack that site (they are all wild-type). On merge, this is interpreted as no-data, which can be misleading. . The best solution I can devise is to force the input VCFs to output at a whitelist of sites, including if all samples are non-variant. While GenotypeGVCFs can be made to output at every genomic position, outputting everything is a huge leap in computational time. . I would propose to make a PR to augment GenotypeGVCFs to support an ""--always-output-calls-whitelist"" argument. The user can provide a FeatureInput. If provided, GenotypeGVCFs would output all variable sites (existing behavior), and also output any position spanning the intervals of this file, even if all samples at wild-type. . I only just started to look at how to implement this - i can some back with a more specific proposal. However, my initial thought is that we need to hook into drivingVariants or LocusWalker.traverse. Does your team have any thoughts on this, before we spe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239:446,variab,variable,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239,1,['variab'],['variable']
Modifiability,"Hello,; I was trying to hard-filter the vcf files outputed by GATK HaplotypeCaller, and I want to keep variants that meet the following condition: depth (QD) < 2.0 || FisherStrand (FS) > 60.0 || root mean square mapping quality (MQ) < 40.0 || mapping quality rank sum test(MQRankSum) <−12.5 || ReadPosRankSum <−8.0. Here is my code:. `; for i in *.vcf.gz; do samplename=${i##*/}; sample=${samplename%%.*}; $gatk VariantFiltration -V $i --filter-expression ""QD < 2.0"" --filter-name ""QD2"" --filter-expression ""FS > 60.0"" --filter-name ""FS60"" --filter-expression ""MQ < 40.0"" --filter-name ""MQ40"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" --filter-expression ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8"" -O ../../hardFilter/snp/${sample}.hardfil.snp.vcf.gz ; done; `; But I met some warings:; `; 17:37:07.563 WARN JexlEngine - ![0,9]: 'MQRankSum < -12.5;' undefined variable MQRankSum; 17:37:07.564 WARN JexlEngine - ![0,14]: 'ReadPosRankSum < -8.0;' undefined variable ReadPosRankSum; 17:37:07.564 WARN JexlEngine - ![0,9]: 'MQRankSum < -12.5;' undefined variable MQRankSum; 17:37:07.564 WARN JexlEngine - ![0,14]: 'ReadPosRankSum < -8.0;' undefined variable ReadPosRankSum; 17:37:07.564 WARN JexlEngine - ![0,9]: 'MQRankSum < -12.5;' undefined variable MQRankSum; 17:37:07.564 WARN JexlEngine - ![0,14]: 'ReadPosRankSum < -8.0;' undefined variable ReadPosRankSum; `; It seems that the ""MQRankSum"" and ""ReadPosRankSum"" were not defined.; Then I checked the filtered vcf, and the filter is seemed not working, many variations that do not satisfy the expression are also determined as pass:; ![屏幕截图 2024-08-27 181728](https://github.com/user-attachments/assets/8478ec45-ea1f-4f81-8cf2-f49b8ca5369d); I hope to receive your help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8964:903,variab,variable,903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8964,6,['variab'],['variable']
Modifiability,"Hello,; When I use GenomicsDBImport and GenotypeGVCFs , I get the following error: Couldn't create GenomicsDBFeatureReader, I have no problem with running CombineGVCFs with CombineGVCFs. I reference #6616 , but I think we have different errors, And I checked my environment variable, the parameter is displayed as ' declare -x TILEDB_DISABLE_FILE_LOCKING=""1"" '. Hope you can give me some help, thanks in advance. Exact GATK commands used : gatk GenotypeGVCFs -R path/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home//miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GenotypeGVCFs -R /home//workdir/data_single_cell/sperm/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf; > 21:14:29.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > May 25, 2020 9:14:29 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 21:14:29.494 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.7.0; > 21:14:29.495 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:14:29.495 INFO GenotypeGVCFs - Executing as lbjiang@mu01 on Linux v3.10.0-327.el7.x86_64 amd64; > 21:14:29.495 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; > 21:14:29.495 INFO GenotypeGVCFs - Start Date/Time: May 25, 2020 9:14:29 PM CST; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:274,variab,variable,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['variab'],['variable']
Modifiability,"Hello. I am Adam Yongxin Ye, a PhD candidate in Peking University, supervised by Prof Liping Wei. We have developed MosaicHunter, a bioinformatic tool that can identify postzygotic single-nucleotide mosaicisms (with allele fraction deviated from homozygous 0, 1 and heterozygous 0.5) in bulk sequencing data of a single sample without matched control. After I had the recent lectures on GATK4 tutorial in Beijing, I thought it might be great to merge MosaicHunter into GATK framework, especially for the local assembly function in HaplotypeCaller and Mutect2, to increase the sensitivity & specificity and even extend for mosaic indels, as well as to make MosaicHunter easy for more users to use. MosaicHunter utilized GATK preprocessing, distinguished mosaicisms from germline homozygous and heterozygous sites by a Bayesian genotyper, and applied several stringent hard filters. MosaicHunter has been published (https://academic.oup.com/nar/article/45/10/e76/2962179 and http://www.nature.com/articles/cr2014131) and is publicly available (http://mosaichunter.cbi.pku.edu.cn/ and https://github.com/zzhang526/MosaicHunter (with source code in java)). So I wonder if GATK team is interested in this suggestion. Could someone or may we contribute it into GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632:611,extend,extend,611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632,1,['extend'],['extend']
Modifiability,"Hello:. Thanks for info. . I don’t suppose it is my role to militate/plead for the solid fix of the this omission. But I must say that it would be appreciated and in its own way advance science. Thanks for any consideration. Cheers,; Chuck. > On 10/Feb/2017, at 9:45 AM, Valentin Ruano Rubio <notifications@github.com> wrote:; > ; > @vruano not in my radar either.... the rewrite of the assembly may fix some of these cases where there is actually some variation that we fail to detect (false negative) that would explain those soft clips. However I don't think that would fix all the cases.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279021246:372,rewrite,rewrite,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-279021246,1,['rewrite'],['rewrite']
Modifiability,Help messages for missing environment variables are cruel cruel lies,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/571:38,variab,variables,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/571,1,['variab'],['variables']
Modifiability,"Here is an issue ticket for adding the diagnosetarget feature to DepthofCoverage. This request was created from a contribution made by Benoit Dewitte on February 09, 2022 13:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-). \--. Hi! ; ; I'm currentlly refactoring ours pipeline which use depthofcoverage and diagnosetarget from GATK 3.8. Despite  google researchs I can not found the equivalent of diagnosetarget for gatk 4 and the depthofcoverage tool that i found is still in beta. I found this [github post](https://github.com/broadinstitute/gatk/pull/5913) which talk about pushing the diagnosetarget feature in depthofcoverage but I was not able to find any more informations. Should I stay on the gatk 3.8 version or upgrade our pipe to gatk 4?. I appreciate very much if someone could enlighten me.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270769'>Zendesk ticket #270769</a>)<br> gz#270769</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7702:494,refactor,refactoring,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7702,1,['refactor'],['refactoring']
Modifiability,"Here it is. An overview of what's been added:; - metrics package; - a few general metrics classes (e.g. MultiLevelMetrics); - may want to push these down into HTSJDK later; - added some utils; - utils.gene: gene annotation; - utils.illumina: general Illumina-related utils (adapters, etc); - utils.text.parsers: text parsing; - utils.variant: added dbSNP stuff; - MathUtils: added a few basic things (mean, stddev, etc) with unit tests; - tools; - three major packages:; - analysis: metrics + analyses (including necessary Rscripts); - illumina: Illumina parsing + validation; - vcf: VCF manipulation + GenotypeConcordance; - also two smaller packages, fastq and intervals, containing a few tools each; - tests; - all existing tests were ported; still, overall test coverage goes down by ~6%; - all CLP integration tests have been ported to the new argument system; - test data has also been carried over, and is neatly organized; - there are no huge files, and very few above 100KB (just a few VCFs I think); - however, the Illumina test data is pretty big - ~6MB spread over ~1700 files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347:274,adapt,adapters,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347,1,['adapt'],['adapters']
Modifiability,Here's another one with our exact problem (solved largely by putting the config onto HDFS). http://progexc.blogspot.com/2014/12/spark-configuration-mess-solved.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322554798:73,config,config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322554798,2,['config'],"['config', 'configuration-mess-solved']"
Modifiability,"Here's another way. It's not perfect either; it doesn't get rid of the DoFn. Type erasure really limits our options here. ``` java; public class MapFn<A,B> extends DoFn<A,B> {; protected SerializableFunction<A,B> fn;. public MapFn(SerializableFunction<A,B> fn) {; this.fn = fn;; }. @Override; public void processElement(DoFn<A, B>.ProcessContext c) throws Exception {; c.output(fn.apply(c.element()));; }; }; ```. Used like this:. ``` java; PCollection<Integer> presults = pipeline; .apply(Create.of(Arrays.asList(1, 2, 3))); .apply(ParDo.of(new MapFn<Integer, Integer>(x -> x + 1) {}));; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122997546:156,extend,extends,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122997546,1,['extend'],['extends']
Modifiability,"Here's some code you can add to the test to check that jbwa actually works. ```; @Test; public void testBwaMemAlignSingleRead() throws Exception {; final String libraryPath = NativeUtils.runningOnLinux() ? ""/lib/libbwajni.so"" : ""/lib/libbwajni_mac.jnilib"";; Assert.assertTrue(NativeUtils.loadLibraryFromClasspath(libraryPath), ""jbwa library was not loaded. "" +; ""This could be due to a configuration error, or your system might not support it."");. BwaIndex index= new BwaIndex(new File(b37_reference_20_21));; final BwaMem bwaMem = new BwaMem(index);; //real read taken from src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam; final String name=""20FUKAAXX100202:3:46:9213:168594"";; final byte[] seqs= ""GTTTTGTTTACTACAGCTTTGTAGTAAATTTTGAACTCTAAAGTGTTAGTTCTCTAACTTTGTTTGTTTTTCAAGAGTGTTTTGACTCTTCTTACTGCATC"".getBytes(); ;; final byte[] quals= ""DGFDGFDHFFFFGFEFHEGFFFGGHEHFHGHHGEGGGGGFGFHGHGHEHGGGFGAEFGDACAHHDHGCGFGGGFGDHGHFFHDDCGGDGEE"".getBytes();; final ShortRead read = new ShortRead(name, seqs, quals);; final AlnRgn[] align = bwaMem.align(read);; Assert.assertEquals(align.length, 1);; Assert.assertEquals(align[0].getChrom(), ""20"");; Assert.assertEquals(align[0].getCigar(), ""101M"");; Assert.assertEquals(align[0].getMQual(), 60);; Assert.assertEquals(align[0].getPos(), 9999997-1); #note difference from the bam file (9999997 in bam, 9999996 here); Assert.assertEquals(align[0].getNm(), 0);; bwaMem.dispose();; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636:386,config,configuration,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847#issuecomment-220755636,1,['config'],['configuration']
Modifiability,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:86,adapt,adapt,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['adapt'],['adapt']
Modifiability,"Here’s one example executor from a recent run. This the the variants broadcast (4 seconds using 100MB blocks). ```; 16/04/11 20:55:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3; ...; 16/04/11 20:55:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 3913 ms; ...; 16/04/11 20:55:35 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 655.5 MB, free 2.9 GB); ```. This the the reference broadcast (130 seconds using 100MB blocks). ```; 16/04/11 20:55:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4; ...; 16/04/11 20:57:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 130726 ms; ...; 16/04/11 20:57:47 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 739.6 MB, free 1505.0 MB); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208656364:193,variab,variable,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208656364,4,['variab'],['variable']
Modifiability,"Hey @bbimber I will have to think on this. The most simple solution might be to add a feature context side input for the annotation in question but looking at how that code is threaded in the variant callers it would take a little bit of work to add it to those tools and probably introduce some complicated questions, (like for example: what is the correct featurecontext to send to annotate a variant that only covers one base of the site in question where the feature context object exists?). Its possible to do something like that for variant annotator a little bit more easily but i guess the question comes down to this: How generalized do you think this annotation will be? Does it need to be annotatable with variant annotator or could you write a separate tool that does the variant -> variant association and calculates the annotation without using the plugin framework? If it needs to be generalizable I would agree with @droazen that the easiest approach would be to add the side input as an argument and make the annotation object responsible for querying the feature context. This is inelegant but might be preferable to putting the entire walker context into the `annotate()` function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851:863,plugin,plugin,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851,1,['plugin'],['plugin']
Modifiability,"Hey @magicDGS, since I'm on the forum documentation side of things, I'm not familiar with a plugin. Is this something @cmnbroad typically takes care of?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342820936:92,plugin,plugin,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342820936,1,['plugin'],['plugin']
Modifiability,Hey @valleema. We don't think this is a bug given that the argument `--max-mnp-distance` is intended to remove Multi-Nucliotide polymorphism which generally are adjacent SNPs(i.e. sites with the pattern ref: AA alt: GT for example). Your example site here doesn't have an MNP but rather is a multi-allelic site. Consequently the flag `--max-mnp-distance` is not doing anything wrong in this case. I would direct future questions about your use case (namely how to un-collapse multi-allelic sites) to our forums: https://gatk.broadinstitute.org/hc/en-us/community/topics. Thank you;,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7782#issuecomment-1115218078:128,polymorphi,polymorphism,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782#issuecomment-1115218078,1,['polymorphi'],['polymorphism']
Modifiability,"Hey all, @vruano / @davidbenjamin, is this on your radar at all? I heard rumblings about a rewrite of the assembly machinery; would that address this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/269#issuecomment-278955477:91,rewrite,rewrite,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/269#issuecomment-278955477,1,['rewrite'],['rewrite']
Modifiability,"Hey all, I'm still interested in supporting this. We don't really have a ""plugin API"", I am in fact the API, but if you give me something usable I'll plug it in. As this is marked ""QuixoticDream"" I don't think that's likely. I'm closing the corresponding IGV issue, too many open issues, but it doesn't mean I've lost interest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230,1,['plugin'],['plugin']
Modifiability,"Hi - @kaixinxiaonvwa-hub . I have a couple of questions:. - Can you post your full stack trace with the errors?; - Did you attempt to enable `gnomAD` data sources or is it doing this without any changes to the data sources directory? Did you do any other configuration steps after downloading the datasources and before running funcotator?. If you enable `gnomAD`, the datasources are hosted on google cloud. If you don't have an internet connection or google cloud is blocked, Funcotator will not be able to connect to read the gnomAD data and will show the error in your `1` case above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773:255,config,configuration,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773,1,['config'],['configuration']
Modifiability,"Hi @MattMcL4475 - if you mean the images I used to test these, they are: `terrapublic.azurecr.io/gatk:4.5-squashed` and `terrapublic.azurecr.io/gatk:4.5-min-layers`. Note that these are manually built and pushed for this task and didn't go through any automated tests that are in this repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2159093348:157,layers,layers,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2159093348,1,['layers'],['layers']
Modifiability,"Hi @MattMcL4475 - so sorry I didn't see your reply until now (likely due to my email filters). Anyway, I don't think we have a squashed version of this in the official GATK image. From the conversation above, I think we decided to go with just the reduced layers version of the image which is what this PR is for. I do, however, have a sample squashed version here: `terrapublic.azurecr.io/gatk:4.5-squashed` - but then again, it's not in the official Docker hub repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2231781482:256,layers,layers,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2231781482,1,['layers'],['layers']
Modifiability,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:331,enhance,enhancement,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973,2,"['adapt', 'enhance']","['adapt', 'enhancement']"
Modifiability,"Hi @bhanugandham . I wasn't able to reproduce your problem on my laptop. Based on the error message, it looks like a network config Spark issue. What environment are you running this in? If it's MacOS, can you post the contents of `/etc/hosts`? Also the error message looks like it's truncated. You might be able to get the full stack trace by adding `--verbosity DEBUG`, which would be helpful as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380:125,config,config,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380,1,['config'],['config']
Modifiability,"Hi @cwhelan , I've expanded this PR to do more than what it originally was trying to fix, and separated the patches by commits as usual:. * the originally proposed fix, which brings back the annotation that are available to simple variants but go missing due to a careless bug, is now done in commit 50f1b640a31ddb528dc763b83b26a9d98dce8556; this commit also accordingly refactors the giant class `CpxVariantDetector` into three new classes; * in the 2nd commit 734516383fb665a79796de76535560fc03cb754b, I did more refactoring on how we group the descriptions for the annotation keys, and updated the test VCF files accordingly.; * because of the refactoring, the review comments were gone, so I added them back in the 3rd commit b7619c45a949dfba21d65a5ed876bc72e832aa77, which contains the comments and my replies. They come in as TODO's but are going to be removed ultimately; * in the following commits, I added tests for the CPX code path, selecting three representative cases (there's no limit how complex the scenario can go). One particular commit 224c97c7b736e94ed6b4d8b067ec830a9f8f2403 is large but most of it is for adding a flat file that contains the chromosome names in hg38 and their lengths for building a bare bone sequence dictionary used in building test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525:371,refactor,refactors,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525,3,['refactor'],"['refactoring', 'refactors']"
Modifiability,"Hi @georgiiprovisor ,. Sorry this fell though the cracks. Did you find a workaround? I suspect some of our I/O refactoring caused this warning to be triggered incorrectly. Happy to take a look if that's still useful. -Laura",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8564#issuecomment-2435404906:111,refactor,refactoring,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8564#issuecomment-2435404906,1,['refactor'],['refactoring']
Modifiability,"Hi @hh1985 . Memory tuning is pretty tricky and can depend on a lot of things. How is your cluster configured? ; Are you using YARN? Are you running in client or cluster mode? . I'm assuming you're running with YARN. Mesos should also work but I don't have any experience configuring it. . BQSR should run safely with 4g of memory per core. (It should really work with much less I think, but 4 should definitely be sufficient.) There are a few different parameters that can help you adjust the memory ratios.; A good tuning might be something like; ; ```; --num-executors 5 ; --executor-cores 8 ; --executor-memory 32g ; ```. if you're not running with gatk-launch you'll need to set; ```; --conf spark.yarn.executor.memoryOverhead=600; ```; Without setting a higher than default yarn memory overhead like this we see consistent crashes, it's included in the settings gatk-launch applies already. That should run 5 separate executors with 8 cores each and give each one 32g, so 4g / core. . If you're running in cluster mode you'll have to carve out some memory and cores for the driver. You can set the driver settings with ; ```; --driver-cores 2; --driver-memory 4g; ``` ; or something along those lines. The driver doesn't need much memory or computer for BQSR. In general we've had better luck using the entire cluster for one job and running jobs in sequence rather than trying to run two jobs simultaneously using a subset of the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738:99,config,configured,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738,2,['config'],"['configured', 'configuring']"
Modifiability,Hi @icemduru ; Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use. . Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs. . One final reminder. We strongly recommend users to set the temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332:58,config,configured,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332,1,['config'],['configured']
Modifiability,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:568,extend,extends,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247,2,['extend'],['extends']
Modifiability,Hi @jonn-smith I had some success getting outputs with this. However ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ is no longer accessible. The most recent version of I accessed on 3/19/2018 still contained at least one error. I'm trying to correct them myself as I go using a more recent build of GATK but it would be helpful if the data files required by this program were available. The one I found is in `gencode_xrefseq.config` where it references a source that doesn't exist and I fixed that. After that I was able to get outputs with hg38. Thanks for your work on this!. I'd also point out there are a lot of fields in the MAF with `__UNKNOWN__` as the entry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645:447,config,config,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645,1,['config'],['config']
Modifiability,Hi @lvzenglei ; This is not an issue that needs any fix in fact this is the default behavior that Mutect2 and HaplotypeCaller will have. Extending MNP distance does not change any of the Smith Waterman or PairHMM parameters that will eventually decide on the model used to genotype the region. If you are interested in getting larger complex events genotyped you may need to get longer reads (preferably 2x300) and use read backed phasing before you try genotyping any such events. In that case you may need to experiment with Smith Waterman and PairHMM parameters to prefer mismatches over SNPs but this still may not result in what you are actually looking for. One better approach would be to use read backed phasing along with post processing your phased variants to convert individual phased SNPs and INDELs into COMPLEX calls by other tools. . I hope this helps. Regards.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8310#issuecomment-2421513998:137,Extend,Extending,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8310#issuecomment-2421513998,1,['Extend'],['Extending']
Modifiability,Hi @potter-s ; Our docker image is already built with root account only however PATH is set to be usable by all users so if you wish to keep user priviledges after execution you may add ` -u $UID:$GID` parameter to docker command line therefore the container will run using your user permissions. . This has a catch of course. Temporary folders must be set where your user has RWX permissions therefore we want users to pay attention to that. There is a writing that we posted a while ago which you may refer to for setting up your temporary files for GATK workflows. . [How to setup temporary folder for GATK local executtion](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). For some of the tools such as gCNV or CNN you may need to setup additional environment variables to locate python compilation directory to a place where you have read and write permissions. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965:845,variab,variables,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965,1,['variab'],['variables']
Modifiability,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:73,inherit,inherited,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,1,['inherit'],['inherited']
Modifiability,Hi @yurivict I tested this on my machine and it works for me. Do you see a message saying the version was overridden?; ```; $ ./gradlew printVersion -Drelease=true -DversionOverride=myVersion1.2. > Configure project :; Version number overridden as myVersion1.2. > Task :printVersion; myVersion1.2; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-857796023:198,Config,Configure,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-857796023,1,['Config'],['Configure']
Modifiability,"Hi DarioS. FastaAlternateReferenceMaker is a really simple tool. It actually just looks at the alternate alleles at each site and uses the first non-symbolic one to make the fasta. It doesn't even look at the genotypes. So it should work fine with a multisample vcf but it will give you a mush of samples together as a single fasta. I could be extended to be smarter but it's not a high priority for us right now. . We should improve the documentation, I had to go look in the code to see what it was doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729:344,extend,extended,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729,1,['extend'],['extended']
Modifiability,"Hi Stefan,. If there is no obvious error (e.g., is `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsvx` the correct filename, rather than `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsv`? Does the file exist and is it correctly formatted?), then I would guess that this is likely an error with your nd4j configuration. Just to let you know, we have significantly revamped the both somatic and germline CNV pipelines for the release in January. If you would like a preview of the germline tool, you may want to look at this branch: https://github.com/broadinstitute/gatk/tree/sl_gcnv_ploidy_cli However, be aware that it is still under development.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467:335,config,configuration,335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467,1,['config'],['configuration']
Modifiability,"Hi all. Apologies for writing in an old issue. ; has this been fixed?; With java 1.8 and GATK 4.1.3.0 I think I'm getting the same error (in this case with HaplotypeCallerSpark). Any idea on how to extend the size?. The errors are:. `org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 5000, but locatable size is 8638. Try increasing shard size and/or padding. Locatable: Contig1:65711-74348; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:293); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:281); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$5.hasNext(Iterators.java:547); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994:198,extend,extend,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994,1,['extend'],['extend']
Modifiability,"Hi gatk team,. I'm working with generating CNV for somatic with wdl, using this command:. `java -Xmx75G -Dconfig.file=gatk.conf -jar cromwell-46.1.jar run cnv_somatic_panel_workflow.wdl -i parameters.json `. But I got this error in which I don't know the exact reason for it:. ```; [2019-10-01 02:52:52,49] [info] Running with database db.url = jdbc:hsqldb:mem:e98d186c-96db-46ae-92e5-c326e7aa05d9;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,19] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:901,config,configuration,901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configuration']
Modifiability,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8280:506,Config,Configuration,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280,2,"['Config', 'config']","['Configuration', 'config-file']"
Modifiability,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:562,config,configuration,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,1,['config'],['configuration']
Modifiability,"Hi, I also meet this issue. . The Specificity in `HaplotypeCallerSpark` was a bitter less than local mode in my test. Do you known which configuration can improve the Specificity in `HaplotypeCallerSpark` ? @Atahualkpa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5323#issuecomment-433815315:137,config,configuration,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323#issuecomment-433815315,1,['config'],['configuration']
Modifiability,"Hi, I think this can be closed now. . I just found that this issue might due to the inconsistency between gCNV version and PostProcessGermlineCNVCalls version. ; I updated my python configuration from 4.1.8.0 to 4.2.2.0 and the issue is gone. . Sorry for the troubles here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-908550017:182,config,configuration,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-908550017,1,['config'],['configuration']
Modifiability,"Hi, I'm trying to generate a VCF with Mitochondrial mode of Mutect2 based on MT amplicon (PCR) sequencing. But I encountered two problems:; 1. The base site (Pos: MT:16320) has high depth and good quality, but the result of AF is low. In IGV, AF~=0.5; 2. The total depth(DP) is also quite different from the depth in bam; I tried changing various parameters but nothing seems to make a difference? and even tried turn on --disable-tool-default-read-filters. Is there a parameter I'm missing? What are the filtering criteria? How to adapt to PCR data?. version: GATK4.1.4.1; java -Xmx16g -Djava.io.tmpdir=./tmp -jar gatk-package-4.1.4.1-local.jar Mutect2 -I tmp.bam -R hs37d5.fa -L MT.bed -O raw.vcf --min-pruning 5 --mitochondria-mode --max-reads-per-alignment-start 10000. MT 16182 . A AC,ACC . . DP=262;ECNT=7;MBQ=31,26,29;MFRL=0,0,0;MMQ=60,60,60;MPOS=44,44;OCM=0;POPAF=2.40,2.40;TLOD=84.81,46.04 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2:157,72,31:0.261,0.118:260:122,48,21:0,0,0:0,157,0,103; MT 16183 . A C . . DP=262;ECNT=7;MBQ=30,34;MFRL=0,0;MMQ=60,60;MPOS=45;OCM=0;POPAF=2.40;TLOD=531.07 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:58,204:0.780:262:42,190:0,0:0,58,0,204; MT 16188 . CT C . . DP=262;ECNT=7;MBQ=34,29;MFRL=0,0;MMQ=60,60;MPOS=50;OCM=0;POPAF=2.40;TLOD=19.25 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:243,19:0.070:262:207,15:0,0:0,243,0,19; MT 16189 . T C . . DP=262;ECNT=7;MBQ=35,34;MFRL=0,0;MMQ=60,60;MPOS=51;OCM=0;POPAF=2.40;TLOD=931.43 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:2,260:0.989:262:2,237:0,0:0,2,0,260; MT 16266 . C A . . DP=1073;ECNT=4;MBQ=7,32;MFRL=0,0;MMQ=60,60;MPOS=50;OCM=0;POPAF=2.40;TLOD=3575.47 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:14,1039:0.996:1053:4,469:0,446:0,14,481,558; MT 16274 . G A . . DP=1073;ECNT=4;MBQ=33,12;MFRL=0,0;MMQ=60,60;MPOS=53;OCM=0;POPAF=2.40;TLOD=1.89 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:1057,11:5.214e-03:1068:571,1:337,4:467,590,10,1; MT 16320 . C T . . DP=643;ECNT=4;MBQ=27,36;MFRL=0,0;MMQ=60,60;MPOS=21;OCM=0;POPAF=2.40;TLOD=11.29 GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB 0|1:564,60:0.017:624:48,60:358,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-575001931:532,adapt,adapt,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-575001931,1,['adapt'],['adapt']
Modifiability,"Hi, i can't install gatk via conda/mamba. couldyou pls help; pls see steps that i took. ```; $conda config --add channels conda-forge; $conda config --add channels bioconda; $conda config --add channels defaults; $conda config --set channel_priority strict; ```. install command; ```; bash:iscxf001:/data1/greenbab/users/ahunos/apps/gatk-4.5.0.0 1023 $ conda env create -n gatk -f gatkcondaenv.yml; ```. ```; Channels:; - conda-forge; - defaults; - bioconda; Platform: linux-64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::typing_extensions==4.1.1; - conda-forge::theano==1.0.4; - pkgs/main::tensorflow==1.15.0; - conda-forge::scipy==1.0.0; - conda-forge::scikit-learn==0.23.1; - conda-forge::python==3.6.10; - bioconda::pysam==0.15.3; - conda-forge::pymc3==3.1; - conda-forge::pip==21.3.1; - conda-forge::pandas==1.0.3; - conda-forge::numpy==1.17.5; - conda-forge::mkl-service==2.3.0; - conda-forge::mkl==2019.5; - conda-forge::matplotlib==3.2.1; - conda-forge::keras==2.2.4; - conda-forge::joblib==1.1.1; - pkgs/main::intel-openmp==2019.4; - conda-forge::h5py==2.10.0; - conda-forge::dill==0.3.4. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/r/linux-64; - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8838:100,config,config,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8838,4,['config'],['config']
Modifiability,"Hi, recently I was trying expand the annotation data source when using funcotator, however, the document didn't give much information or example. Now I was trying to add CADD to the data source folder. After running the funcotator, I got the error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path file: funcotator_dataSources.v1.6.20190124s/cadd/hg19/cadd.config; at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:353); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:305); at org.broadinstitute.hellbender.engine.FeatureDataSource.&lt;init&gt;(FeatureDataSource.java:256); at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:234); at org.broadinstitute.hellbender.engine.GATKTool.addFeatureInputsAfterInitialization(GATKTool.java:957); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createAndRegisterFeatureInputs(DataSourceUtils.java:328); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:277); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java:774); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1037); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:413,config,config,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"Hi,. Thanks for the response. Running with -u isn’t ideal as we can’t control; how the user runs this (unless they do this on their own hardware or say a; cloud instance). However, I managed to convert the docker image into a singularity one and; that runs ‘out of the box’ in user space. Simon. On 3 Jun 2024, at 18:43, Gökalp Çelik ***@***.***> wrote:. Hi @potter-s <https://github.com/potter-s>; Our docker image is already built with root account only however PATH is; set to be usable by all users so if you wish to keep user priviledges after; execution you may add -u $UID:$GID parameter to docker command line; therefore the container will run using your user permissions. This has a catch of course. Temporary folders must be set where your user; has RWX permissions therefore we want users to pay attention to that. There; is a writing that we posted a while ago which you may refer to for setting; up your temporary files for GATK workflows. How to setup temporary folder for GATK local executtion; <https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution>. For some of the tools such as gCNV or CNN you may need to setup additional; environment variables to locate python compilation directory to a place; where you have read and write permissions. I hope this helps. —; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965>,; or unsubscribe; <https://github.com/notifications/unsubscribe-auth/ABU3SAWISO2HSCUNHK3SGIDZFSTK5AVCNFSM6AAAAABIWRNXGKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNBVG44DAOJWGU>; .; You are receiving this because you were mentioned.Message ID:; ***@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154:1229,variab,variables,1229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154,1,['variab'],['variables']
Modifiability,"Hi,; Glad to know that you have tested GATK4 with Amazon S3 using NIO file system plugin. ; I have been stuck on this process for long...I would really appreciate if you could share the work around procedure detail for this.; Thanks in advance !; Senthil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-354368839:82,plugin,plugin,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-354368839,1,['plugin'],['plugin']
Modifiability,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:367,config,configuration,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"Hi,; I tried your commands (and many adaptions / changements) but I always get the same problem:; If the command line includes `--`, I get the JNI linkage error as if the spark related parameters were not parsed.; I tried many things, as:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass -- --sparkRunner SPARK --sparkMaster yarn --deploy-mode cluster; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --conf spark.driver.extraJavaOptions='-Dmapr.library.flatclass' --conf spark.executor.extraJavaOptions='-Dmapr.library.flatclass'. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --driver-java-options '-Dmapr.library.flatclass'. It's a non-exhaustive list, I tried a lot of configurations similar to these ones.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061:37,adapt,adaptions,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061,2,"['adapt', 'config']","['adaptions', 'configurations']"
Modifiability,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:80,config,configuration,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['config'],['configuration']
Modifiability,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6071:168,config,configured,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071,1,['config'],['configured']
Modifiability,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:73,plugin,plugin,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,2,['plugin'],['plugin']
Modifiability,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1001,Config,ConfigurationContainerInternal,1001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Config'],['ConfigurationContainerInternal']
Modifiability,"Hmm, I am also getting intermittent build errors like the following much more often:. ```; Could not determine the dependencies of task ':sparkJar'.; > Could not resolve all files for configuration ':sparkConfiguration'.; > Could not download gson.jar (com.google.code.gson:gson:2.2.2); > Could not get resource 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'. Received status code 403 from server: Forbidden; > Could not download core.jar (com.github.fommil.netlib:core:1.1); > Could not get resource 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'. Received status code 403 from server: Forbidden; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142:184,config,configuration,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142,1,['config'],['configuration']
Modifiability,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:469,refactor,refactor,469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,1,['refactor'],['refactor']
Modifiability,Hook arguments from SelectVariants/GenotypeGVCFs/GnarlyGenotyper to GenomicsDB Export Configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6456:86,Config,Configuration,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6456,1,['Config'],['Configuration']
Modifiability,Hooked the annotations barclay plugin into the tools that use vcf annotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4674:31,plugin,plugin,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674,1,['plugin'],['plugin']
Modifiability,"How about: first check an environment variable for the location of the GATK jar, and if it's not set check the script's working directory for a jar? Whatever we do should be consistent across the spark and non-spark jars.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207473378:38,variab,variable,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207473378,1,['variab'],['variable']
Modifiability,How did you install the GATK Conda environment? Looks like a problem with the conda environment configuration. One possible reason could be that conda environment is not the version 4.3.0.0 is requesting.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952#issuecomment-2287925550:96,config,configuration,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952#issuecomment-2287925550,1,['config'],['configuration']
Modifiability,"I _believe_ your issue is that you are assigning 600GB to execution of cromwell, but the error is with the call to **VariantRecalibrator** in one of the tasks not having enough memory. A few tasks call **VariantRecalibrator**, do you know which task failed? Can you post the java call from the STDERR file? For me, it was task **SNPsVariantRecalibrator** which was assigned only 3.5GB of memory by default. In [joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl), the memory assigned for each task can be set via ""machine_mem_gb"", but it looks like the current [input.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json) does not have that variable, but instead ""mem_size"" for each task. . A simple solution would be to replace ${java_mem} with a static value in calls to **VariantRecalibrator** (lines 564 & 684). For example, replace:. `${gatk_path} --java-options ""-Xmx${java_mem}g -Xms${java_mem}g""`. with. `${gatk_path} --java-options ""-Xmx100g -Xms100g""`. I'm not certain this will help, but I think it's a step in the right direction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381:785,variab,variable,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381,1,['variab'],['variable']
Modifiability,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7129:491,config,config,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129,1,['config'],['config']
Modifiability,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:238,flexible,flexible,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,4,"['extend', 'flexible', 'plugin']","['extending', 'flexible', 'plugin']"
Modifiability,"I addressed some of your comments, @droazen. If you would like to have a properties file for the configuration, I will need some help on setting it up (although I will try by my own too). Although I still set up some of the environment in `Main`, now the `CommandLineProgram` class have the same instance passed by `Main`. Looking forward for your comments on the updates.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-271845715:97,config,configuration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-271845715,1,['config'],['configuration']
Modifiability,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:226,adapt,adapter,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314,2,['adapt'],"['adapter', 'adaptor-sequence']"
Modifiability,"I also just noticed that tests are failing on the branch because they still reference the old constants in a number of places:. ```; symbol: variable READ_NAME_LONG_NAME; location: class ReadNameReadFilter; /gatk/src/test/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptorTest.java:117: error: cannot find symbol; { PlatformReadFilter.class.getSimpleName(), ""--"" + PlatformReadFilter.PL_FILTER_NAME_LONG_NAME, ""fakePlatform"" }, ; ```. You'll need to update these references in order to get tests passing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542:141,variab,variable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542,1,['variab'],['variable']
Modifiability,"I also removed the obsolete errorProbability variable line of code in the SomaticGenotypingEngine.java and noted this argument is deprecated in the M2ArgumentCollection. Somewhat relatedly, see request in #3123.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3124:45,variab,variable,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3124,1,['variab'],['variable']
Modifiability,"I also tried the following approach which did not generate an error:. 1. imported the 10 not-reblocked gvcfs from chr16 into genomicsdb ; 2. GenotypeGVCFs with the same command line as number 3 above. . So the error appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:455,variab,variable,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['variab'],['variable']
Modifiability,I am looking to call single nucleotide polymorphism (SNP) and INDEL from my genome mapping results generated from HISAT pipeline. Please suggest process and command for that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6592:39,polymorphi,polymorphism,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6592,1,['polymorphi'],['polymorphism']
Modifiability,I ask that because for htsjdk defaults they must be system properties and they're final and set statically on load so mucking about resetting system properties after the JVM started already is going to be a bit of a fiddly ordering nightmare. . [Stack overflow](http://stackoverflow.com/questions/6736235/set-java-system-properties-with-a-configuration-file) doesn't seem to think that it's possible to initialize them from a file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704:339,config,configuration-file,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704,1,['config'],['configuration-file']
Modifiability,"I attended a journal club some months ago where a paper stated most researchers use default settings of tools. The paper benchmarked tool with default settings and with tweaked parameters. I can dig up the paper if anyone is interested. So there is some expectation from the user community that the default parameters reflect some sweet spot parameterization for running the tool. . I highly support @cmnbroad's suggestion for making argument sets callable by one flag. For exomes, please do not label flag as `WES`. We want to refer instead to _targeted exomes_, so `EXOMES` or variation is preferable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104:342,parameteriz,parameterization,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104,1,['parameteriz'],['parameterization']
Modifiability,I believe I've had this issue before but with different tools as well.; If you are on nextflow below is a config for scope docker. ```; docker.fixOwnership; Fix ownership of files created by the docker container.; ```. There is also another scope that could be set if there is only a single user. ```; docker.runOptions; This attribute can be used to provide any extra command line options supported by the docker run command. See the [Docker documentation](https://docs.docker.com/engine/reference/run/) for details.; ```; This one enables passing -u parameter to docker directly. . If none of them are set in the nextflow config then I would first suggest these options. If not we can escalate this with the team.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078156593:106,config,config,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078156593,2,['config'],['config']
Modifiability,"I believe that the problem is that HaplotypeCallerGenotypingEngine doesn't use the version of StandardCallerArgumentCollection.getSampleContamination() with the sampleID, so the sampleContamination variable is never initialized and we get a NPE. The -contamination argument is standard in our production GATK pipeline... and we're about to start telling everyone to use it! This is an important one to fix. Here's a stacktrace:. java.lang.NullPointerException; 	at java.util.Collections$UnmodifiableMap.<init>(Collections.java:1446); 	at java.util.Collections.unmodifiableMap(Collections.java:1433); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.StandardCallerArgumentCollection.getSampleContamination(StandardCallerArgumentCollection.java:89); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:566); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:218); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:295)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4312:198,variab,variable,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4312,1,['variab'],['variable']
Modifiability,"I can contribute to this one too if I have time, because I will need it too. My suggestion is to create a plugin for the annotation engine. There are also some issues that will be nice to address/discuss too before I start contributing:. * __Allow custom packages for Annotations in VariantAnnotatorEngine__ (https://github.com/broadinstitute/gatk/issues/2155): I think that `VariantAnnotatorEngine` should be modify directly to use annotation classes and let the plugin deal with the reflection code to pick the instances for each String. This will help in my work trying to use `VariantAnnotatorEngine` with custom annotation classes, and let the package constraint to the plugin itself (and at some point, to the GATK configuration).; * __GenotypeAnnotations require GT, but some of them are computable without them__(https://github.com/broadinstitute/gatk/issues/1730): some of this annotations are useful for my work, which does not include samples with genotypes. Nevertheless, I cannot use them unless I hack my `Genotype` object beforehand. I think that this could be solved by setting a flag in the plugin and set the genotype annotation with it, to allow the computation for non-called samples.; * It would be nice to have in the plugin an argument collection (similar to #2355), allowing other implementations for the arg parsing part. I will appreciate if these things are included/addressed at the same time, and I would love to contribute with some code to help you and be a beta-tester from the API point of view.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288768668:106,plugin,plugin,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1880#issuecomment-288768668,6,"['config', 'plugin']","['configuration', 'plugin']"
Modifiability,"I can definitely appreciate that in some cases downstream analysis might be made easier if the original representations of GGA mode alleles were preserved. . Internally, HaplotypeCaller has to convert all variants at a position to share the same reference context so that read alignments can be compared to all possible alternate alleles simultaneously, and it would be a complex and error-prone process to re-map the unified alleles back to their original representations, and would also pose problems in terms of computing the correct values for INFO field annotations such as DP if the output VCF had to be split across multiple lines according to the how things were specified in the input files. I'm going to close this for now unless you strongly object or have an additional case that shows an error in GGA mode output. It's possible that in the future we could implement an enhancement in the form of a mode that preserves the GGA input allele representations, possibly under some stricter conditions upon the input, but that would likely be a tricky implementation. Pinging @ldgauthier to make sure she agrees.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950:882,enhance,enhancement,882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950,1,['enhance'],['enhancement']
Modifiability,"I can't use it because of that, and it have lots of variables that I'm not using. I'm doing my own base test class, but I'd love to have a more general base test class in GATK to extend, without that many variables specific for this repository. Thanks a lot for your interest!. Should I do something for this PR? . So should I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242839085:52,variab,variables,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242839085,3,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8237:675,variab,variable,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237,1,['variab'],['variable']
Modifiability,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:660,config,configuring,660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,2,['config'],"['configuration', 'configuring']"
Modifiability,"I concur, what it looks like we have here is code that switched (accidentally?) from the GCS adapter to GCS-NIO instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265014643:93,adapt,adapter,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265014643,1,['adapt'],['adapter']
Modifiability,"I definitely like the idea of moving in this direction, and it will become more compelling as we extend the GATKSparkTool hierarchy, which is currently pretty flat and doesn't mirror the GATKTool hierarchy. I think we should also consider using some of the concepts from the metrics refactoring, which introduces a layer that separates the implementation of the processing logic from the containing tool/driver, and allows a single implementation (i.e. metrics collector, but could be FlagStats, CountReads, SelectVariants, whatever) to be independent of the source and/or destination of the data. It adds more moving parts, but has the advantage of allowing a single implementation to be used from any of Spark tool, standalone tool, Spark pipeline, standalone pipeline, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254216118:97,extend,extend,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254216118,2,"['extend', 'refactor']","['extend', 'refactoring']"
Modifiability,"I did not use the official download of the Funcotator datasources (1.2), but I did find some extraneous files that should be removed. Complete list is below:. ```; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 achilles/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cancer_gene_census/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 clinvar/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_fusion/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_tissue/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dbsnp/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dna_repair_genes/; -rwxrwx--- 1 root vboxsf 6148 Apr 30 15:38 .DS_Store*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 familial/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xhgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xrefseq/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 hgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 .idea/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 oreganno/; -rwxrwx--- 1 root vboxsf 5274 Apr 30 15:38 README.txt*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 simple_uniprot/; -rwxrwx--- 1 root vboxsf 1557 Apr 30 15:38 template.config*. ```; `.DS_Store` and `.idea` should not be in the official download.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4722:1175,config,config,1175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4722,1,['config'],['config']
Modifiability,"I did some additional runs on the Broad cluster on a 14 GB bam, twice as large as the bam used in the plot above. This was with 60 cores, 4 cores per executor, and 16 GB of memory per executor. Results:. Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; Sharded (3 runs): 19m33.310s, 13m36.466s, (died from out-of-memory error). Not sure what's going on here, but something about this configuration is favorable to Broadcast and unfavorable to Sharded. We should try to understand what and why. Below are the commands I used to run each implementation on dataflow01, for reference:. ```; spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 16G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///user/droazen/bqsr/CEUTrio.HiSeq.WGS.b37.NA12878.1m-130m.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:///user/droazen/bqsr/dbsnp_138.b37.1m-130m.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster yarn-client. spark-submit \; --master yarn-client \; --driver-memory 8G \; --num-executors 16 \; --executor-cores 4 \; --executor-memory 16G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --conf spark.akka.frameSize=1024 \; $JAR BaseRecalibratorSparkOptimized \; --input gs://droazen-testing/spark/CEUTrio.HiSeq.WGS.b37.NA12878.1m-130m.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.fasta \; --knownSites /local/dev/droazen/new_test/dbsnp_138.b37.1m-130m.vcf \; -L 1:1-130000000 \; --apiKey $API_KEY \; --sparkMaster",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157825077:391,config,configuration,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157825077,1,['config'],['configuration']
Modifiability,I did this PR because of the GenomeLocParser; what you suggested was the first thing that I tried. I did this instead of refactoring the BaseTest because I didn't want to cause major changes in the code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242854061:121,refactor,refactoring,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242854061,1,['refactor'],['refactoring']
Modifiability,"I didn't do a line by line review since it seemed like Adam did a very thorough job there. The overall structure looks good to me though. It seems like metrics collection would be a good candidate for being made a plugin system, as in the ReadFilter branch. That should definitely be a different pass on this though if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1959#issuecomment-233690780:214,plugin,plugin,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959#issuecomment-233690780,1,['plugin'],['plugin']
Modifiability,"I discovered that one of the 345 input gvcfs failed VCF validation. When I removed that file and reran with no other changes, I did not get the ""terminate called without an active exception"" error. However, ImportGvcfs still fails; the failure seems to occur immediately after GenomicsDBImport logs success in importing all batches, in each shard. From all the Cromwell logs it looks like everything is working, but the top level workflow execution fails. I've been trying various configurations of memory, scatter count, and #nodes, so I don't have those log files around still. I can rerun with -DGATK_STACKTRACE_ON_USER_EXCEPTION=true and see if I get anything useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651:481,config,configurations,481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651,1,['config'],['configurations']
Modifiability,"I do not think you should have two versions. Here is a task example from; another workflow:. ```; output {; File cnv_acs_conversion_skew = ""${output_skew_filename}""; Float cnv_acs_conversion_skew_float =; read_float(output_skew_filename); String cnv_acs_conversion_skew_string =; read_string(output_skew_filename); }; ```. While this adds some clutter, at least the task (and workflow) produces a; file, float, or string. Then you can decide which you actually want to; attach to the data model via the method configuration output. Clutter vs. fork? I say ""clutter"". Also, you may only need one alternate; type. On Tue, Jul 2, 2019 at 9:52 AM ldgauthier <notifications@github.com> wrote:. > *@ldgauthier* requested changes on this pull request.; > ------------------------------; >; > In scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; > <https://github.com/broadinstitute/gatk/pull/6017#discussion_r299492696>:; >; > > @@ -242,6 +250,7 @@ workflow CNVGermlineCaseWorkflow {; > Array[File] gcnv_tracking_tars = GermlineCNVCallerCaseMode.gcnv_tracking_tar; > Array[File] genotyped_intervals_vcf = PostprocessGermlineCNVCalls.genotyped_intervals_vcf; > Array[File] genotyped_segments_vcf = PostprocessGermlineCNVCalls.genotyped_segments_vcf; > + Array[File] qc_status_files = CollectSampleQualityMetrics.qc_status_files; >; > Ideally I'd want to be able to flag failing samples in an obvious way in; > the workspace, like having new fields in the data model called; > ""sample_quality"" and ""model_quality"" with the QC status reported there. Are; > we violently opposed to having a Cromwell version and a Firecloud version; > of this WDL? (@LeeTL1220 <https://github.com/LeeTL1220>); > ------------------------------; >; > In scripts/cnv_wdl/cnv_common_tasks.wdl; > <https://github.com/broadinstitute/gatk/pull/6017#discussion_r299493991>:; >; > > @@ -453,3 +453,98 @@ task PostprocessGermlineCNVCalls {; > File genotyped_segments_vcf = genotyped_segments_vcf_filename; > }; > }; > +; > +task Col",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507695717:510,config,configuration,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507695717,1,['config'],['configuration']
Modifiability,"I don't really like the idea of a `--fix_misencoded_quality_scores` arg at the `GATKTool` level hooked into the data source -- I'd prefer to see your read transformer PR (https://github.com/broadinstitute/gatk/pull/2085) get merged, and then make read transformers a plugin that can be turned on/off on the command line, like we can with read filters. I've asked @cmnbroad to add a comment here outlining what would have to be done to make read transformers a plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245959488:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245959488,2,['plugin'],['plugin']
Modifiability,"I don't think it's much different to have a zip file with 2 jars in it vs a zip file with a directory tree. This will add an additional layer of complexity to testing. I.e. do we test the packaged jars, now our unit tests run with a different configuration than the other tests, etc.. . What we probably really want is some sort of executable archive that wraps whatever we put in it into a single runnable file. I'm not sure how to do that exactly, but if self extracting zip files are a thing there should be a way to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-215791645:243,config,configuration,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-215791645,1,['config'],['configuration']
Modifiability,"I don't think that M2 should be spewing out a concatenation of all alleles,; since that does imply ploidy via the VCF spec. Multiallelics on multiple; lines violates the spec too, right?. On Mon, Sep 18, 2017 at 9:28 PM, David Benjamin <notifications@github.com>; wrote:. > @chandrans <https://github.com/chandrans> @chapmanb; > <https://github.com/chapmanb> @ldgauthier <https://github.com/ldgauthier>; > The -ploidy argument is one of several arguments inherited from; > AssemblyRegionWalker that apply only to HaplotypeCaller and do nothing in; > Mutect2. (We should refactor this once the engine team's workload; > lightens enough to review lower-priority things like this.) The GT field; > emitted by Mutect is just the concatenation of all called alleles -- 0/1,; > 0/1/2, 0/1/2/3 etc -- and doesn't imply anything about the ploidy. Maybe we; > should get rid of it entirely since AF is so much more informative.; >; > I like the idea of splitting multiallelics into multiple lines. It would; > make filtering a lot easier. @LeeTL1220 <https://github.com/leetl1220> do; > you have an opinion?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330401348>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2Bmhl2i8xOPd4zC_WqQ9GtNbRKNks5sjxi1gaJpZM4PTWbd>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330527699:455,inherit,inherited,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330527699,2,"['inherit', 'refactor']","['inherited', 'refactor']"
Modifiability,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:1225,flexible,flexible,1225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,2,"['extend', 'flexible']","['extend', 'flexible']"
Modifiability,"I don't think the `@author` tags will show up in the doc; they generally don't show up in javadoc, and our doc system doesn't include them in the generated doc.`@author` tags are a bit sketchy to begin with, because they're block tags, so the extend all the way to the start of the next tag. But since they're stripped out, putting such a tag at the start of the javadoc can result in everything after it being silently dropped from the output. We found such a case in GATK a while back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349354828:243,extend,extend,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349354828,1,['extend'],['extend']
Modifiability,"I don't understand the filtering approach you're suggesting. ReadSource crashes, adding a filter afterwards wouldn't help. But that's not a problem anymore because those crash-inducing reads weren't supposed to be there in the first place and now we're working with an input that doesn't have them. I looked into DataflowReadsPipeline but can't use it. I would suggest putting together a library of helpful functions rather than a collection of base classes, since the former is more flexible. I can't change to ReadSource because it doesn't return all the reads, breaking ABQSR. So it looks like I have to wait for a fixed ReadSource (incidentally, can we document this limitation in the class docs? In this case it would have saved us all some time).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/556#issuecomment-111206864:484,flexible,flexible,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/556#issuecomment-111206864,1,['flexible'],['flexible']
Modifiability,I fixed these by update the dataproc image from 1.0 -> 1.1. . @davidbernick Is there a way to make that into a variable that's settable in one place and shared between every jenkins spark job?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289#issuecomment-264497363:111,variab,variable,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289#issuecomment-264497363,1,['variab'],['variable']
Modifiability,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4651:582,plugin,plugin,582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651,1,['plugin'],['plugin']
Modifiability,"I found this online., this might be the reason why use include spark is different from system spark in this case. In spark, when i try to cast the variable to the same 'class' (with exact the same class name, in this case, you could try to cast any ""Variant"" to ""Variant"" in BroadcastJoinReadsWithVariants ), it always throw out the class exception error. It is because that ""The equality of two classes in Java depends on the fully qualified name and the class loader that loaded it.""; In system spark, it might split the JVM, which means the broadcast variable may cause problem to identify if it is the same class?. This is what i test in spark:; List<Variant> vs = variants.take(10);; MinimalVariant v = (MinimalVariant) vs.get(0);. this will throw ClassCastException error... http://stackoverflow.com/questions/826319/classcastexception-when-casting-to-the-same-class",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181:147,variab,variable,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386#issuecomment-166160181,2,['variab'],['variable']
Modifiability,"I got it to work by using the runtime switch --disable-sequence-dictionary-validation . . If that is not used it crashes. . . Docker commandline. . /gatk Funcotator --disable-sequence-dictionary-validation \. -R mydata/refs/Homo_sapiens_assembly19.fasta \. -V mydata/P50513_mutect2_filtered.vcf \. -O mydata/P50513_mutect2_funcotator.maf \. --output-file-format MAF \. --data-sources-path mydata/dataSourcesFolder/funcotator_dataSources.v1.6.20190124s/ --ref-version hg19. . . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 10:26 AM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . @rdbremel <https://github.com/rdbremel> This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VB4ZCHMAJUHBKE2SP3QRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI#issuecomment-547962085> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VHRV5JESZYAYX55YHTQRGRQFANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VAS2WE5TDCUC6G5LETQRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382:975,layers,layers,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382,1,['layers'],['layers']
Modifiability,"I grok the refactoring changes and those are 👍, but I think I'm going to need a walkthrough for the core spanning deletion work...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1135250576:11,refactor,refactoring,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1135250576,1,['refactor'],['refactoring']
Modifiability,"I guess it depends on how general we want to keep this input path. I think most purely read-depth based callers won't really be able to discover events smaller than 800bp or so (maybe 500bp at the lower limit) with any accuracy. I also don't know of any tools that we're considering that will describe individual breakpoints. . What about this for a rule: create two intervals for the start and end of the CNV interval + or - 151 bases (allowing a read length of slop). If the two intervals overlap, merge them together into a single evidence interval. . We could also make the slop amount parameterizable per input file, since different tools might have different characteristics, although that would be a feature we could just make a ticket for until we need it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3542#issuecomment-327499474:590,parameteriz,parameterizable,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3542#issuecomment-327499474,1,['parameteriz'],['parameterizable']
Modifiability,I guess that those problems are all GenomeLoc(Parser) inheritance and since we are breaking away from GenomeLoc perhaps is a good time to lift this restriction as well; I don't think being conservative at this point is necessarily a good thing considering the longer road ahead.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/373#issuecomment-97539279:54,inherit,inheritance,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/373#issuecomment-97539279,1,['inherit'],['inheritance']
Modifiability,"I have a PR that fixes this. It's on the branch tws_desparkify_svdiscovery, which is a big refactor on some SV code. Someone could have a look and pull out just that bit of code to make a new, smaller PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662574860:91,refactor,refactor,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662574860,1,['refactor'],['refactor']
Modifiability,"I have a pull request out for this code. Will you refactoring maybe that; obsolete?; On Jul 28, 2015 9:04 AM, ""Louis Bergelson"" notifications@github.com; wrote:. > @davidadamsphd https://github.com/davidadamsphd 1 comment. Either; > address that now or in your next refactoring commits. Otherwise [image:; > :+1:] merge at will.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/759#issuecomment-125620920; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125621800:50,refactor,refactoring,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759#issuecomment-125621800,2,['refactor'],['refactoring']
Modifiability,"I have been running into an issue with Funcotator where some mutations are causing Funcotator to crash because it attempts to query a segment that extends beyond the boundary of the transcript ( see https://github.com/broadinstitute/gatk/issues/6345 ). This pull request addresses the issue by adding a check for transcript length before executing the query. I looked at the code, and Funcotator currently handles problematic sequence queries in `getFivePrimeUtrSequenceFromTranscriptFasta()` by returning an empty string. I modified `getFivePrimeUtrSequenceFromTranscriptFasta()` to also return an empty string when the segment it is trying to retrieve extends beyond the boundary of the transcript. . I have a small VCF that can be used to reproduce the problem using the current code on `master` and the hg38 data source, and I have verified that this pull request allows Funcotator to process the problematic variant without crashing. I did not add the VCF to the tree, but can provide it if that is preferred. Is there any guidance for how to implement integration tests with funcotator? The Funcotator data source I am using is ~12gb, but I would think the problem could be reproduced with 1 transcript and 1 variant. This is my first pull request to GATK, so please let me know if there is anything you would like me to adjust, I'm happy to address any comments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6546:147,extend,extends,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6546,2,['extend'],['extends']
Modifiability,"I have noticed that when running spark tools (e.g. CountReadsSpark or MarkDuplicatesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:684,config,configure,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['config'],['configure']
Modifiability,"I have pull requests in flight for both (1) and (2). They are 1469; <https://github.com/GoogleCloudPlatform/google-cloud-java/pull/1469> and; 1470 <https://github.com/GoogleCloudPlatform/google-cloud-java/pull/1470>. Cheers,; JP. On Tue, Dec 6, 2016 at 3:54 AM, Tom White <notifications@github.com> wrote:. > Yes, Hadoop-BAM uses the NIO API to do file merging, whereas in GATK we; > were using the Hadoop APIs (and therefore the GCS<->HDFS adapter) to do it.; >; > It looks like there are a couple of things needed in GCS-NIO to use the; > NIO API for this.; >; > 1. GoogleCloudPlatform/google-cloud-java#1450; > <https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1450>; > so that we don't have to special-case gs URIs to remove everything; > except the scheme and host when looking up the filesystem (see; > https://github.com/HadoopGenomics/Hadoop-BAM/; > blob/master/src/main/java/org/seqdoop/hadoop_bam/util/; > NIOFileUtil.java#L40; > <https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L40>; > ); > 2. GoogleCloudPlatform/google-cloud-java#813; > <https://github.com/GoogleCloudPlatform/google-cloud-java/issues/813>; > to support path matching (https://github.com/HadoopGenomics/Hadoop-BAM/; > blob/master/src/main/java/org/seqdoop/hadoop_bam/util/; > NIOFileUtil.java#L90; > <https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L90>; > ); >; > There may be more, as I stopped there. The best way forward is probably to; > go back to the old code in GATK while the deficiencies in GCS-NIO are fixed; > and then released.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-266151447:441,adapt,adapter,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-266151447,1,['adapt'],['adapter']
Modifiability,"I have several use cases for this:; 1. I didn't mention it in the first comment, but if a tool does not want to provide custom `ReadFilter`from the user, but allow to disable the ones applied by the tool, this will keep in sync the parameter name.; 2. I would like to have a `ReadFilter` plugin that is applied ""after analysis"", but the parameters here said that they are applied ""before analysis"". This will be misleading for my users. For this case I also opened #2353, to allow custom parameters based on the same read filter plugin descriptor implemented in GATK.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274753122:288,plugin,plugin,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274753122,2,['plugin'],['plugin']
Modifiability,"I have the following instruction in a handson tutorial:. > If you haven't already done so, create a symlink to the gatk-launch script. Navigate back to /gatk and test the symlink by listing the tools available.; ```; cd /usr/local/bin; ln -s /gatk/gatk-launch gatk-launch; cd /gatk; gatk-launch –-list; ```. @vdauwera says:; > wouldn't it be simpler to export to path?. My reply:; > Environmental variables persist ephemerally. I haven't tested persistence when containers are stopped and restarted. @vdauwera requests:; > hmm, could also add to path in the bash profile... we should ask the devs if it's possible to set that up in the docker itself, for next time. Could we have both an environmental variable and a symlink that invokes the launch script in the Docker from any location? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3899:397,variab,variables,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3899,2,['variab'],"['variable', 'variables']"
Modifiability,"I have three main reasons to propose to move the arguments in CLP to an argument collection that is configurable by downstream tools/projects:. 1. Support hiding some arguments for downstream projects. For example, I do not want to support a config file by the user, but rather decide the settings for the framework and expose only some configuration.; 1. Set custom defaults for some downstream tools (including GATK). For example, a concrete tool might want to force the temp directory to be specified to avoid failures due to no space (and specify that in the documentation).; 1. Support old-style arguments (not kebab-case) for downstream projects that rely on the current argument definitions. I am specially affected by this one, because updating GATK to the 4.0.0 release of January will be a breaking change that will cause some nightmares for my users - and I don't want to do a major version bump yet (I have to re-work a bit my own framework before it). Thus, the first commit of this PR holds the proposal for the new argument collection. As I know that the team is also trying to normalize arguments and documentation, I included two more commits to help with the task (they can be removed if you think that it is better after the argument collection):; * Use `java.nio.Path` for temp directories (to support temp directories in HDFS, for example); * Change arguments moved to the collection to kebab-case (to help with #3853)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998:100,config,configurable,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998,3,['config'],"['config', 'configurable', 'configuration']"
Modifiability,"I haven't been able to reproduce @vdauwera error, but there are issues with the https checkout at the moment. ; There's one annoying issue witwhere it will prompt for a password before every individual file download. This will be fixed in https://github.com/github/git-lfs/issues/755. It can be worked around by using `git config credential.helper cache` but the easiest thing to do at the moment is just using the ssh checkout. . I need to investigate what happens with ssh checkout if you don't have a key set up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513:323,config,config,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/952#issuecomment-150376513,1,['config'],['config']
Modifiability,"I havent published to github yet, pending getting these core changes in; however, the purpose is pretty simple: allow VariantEval to inherit from MultiVariantWalker, but not require it to include the required argument -V. this seemed comparable to VariantWalkerBase (no arguments), and VariantWalker (specifies -V). GATK3's VariantEval uses the --eval argument and I generally tried to keep everything in this port in sync with GATK3, within reason. If there is another way to subclasses to negate some @argument defined by a superclass this would work too. If you want to see more I'll push to github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776:133,inherit,inherit,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776,1,['inherit'],['inherit']
Modifiability,I imagine that @skwalker's scripts could be adapted for the task -- I'll try to set up a meeting with her next week to discuss.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947:44,adapt,adapted,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947,1,['adapt'],['adapted']
Modifiability,I inherited broadinstitute/gatk-protected#795 from @davidbenjamin.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-276161535:2,inherit,inherited,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-276161535,1,['inherit'],['inherited']
Modifiability,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579:439,config,configuration,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579,1,['config'],['configuration']
Modifiability,"I like option 2. If one of the other options is specified, use the specified value. For example, the following would use 0.1 for interval-psi-scale:. --set-defaults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:482,parameteriz,parameterized,482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['parameteriz'],['parameterized']
Modifiability,"I looked at the first new commit and skimmed the second. I'm assuming; tests still pass. If you want another set of eyes on the test data; refactor it'll have to wait until Monday. -L. On Fri, Oct 21, 2016 at 12:19 PM, David Benjamin notifications@github.com; wrote:. > @ldgauthier https://github.com/ldgauthier I _think_ you approved; > without needing it to send back to you, but I'm paranoid and not fully; > confident with the new github review system. Also, the changes to the test; > code to get rid of the duplicated likelihood-setting were pretty big.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2185#issuecomment-255445145,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AGRhdEDyfInQUcZiSZ5qBFhrAOnJpNZiks5q2RBVgaJpZM4KFNEm; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-256139184:139,refactor,refactor,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-256139184,1,['refactor'],['refactor']
Modifiability,"I made the change you suggested but just realized I forgot to push before; leaving for my flight. I'll push when I get back. On Tue, Nov 20, 2018, 12:04 PM ldgauthier <notifications@github.com wrote:. > *@ldgauthier* commented on this pull request.; >; > Looks lovely, thanks for the quick fix! I have one refactoring suggestion; > -- take it or leave it.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java; > <https://github.com/broadinstitute/gatk/pull/5435#discussion_r235089237>:; >; > > return Arrays.asList(squareSum,totalDP);; > } catch (final NumberFormatException e) {; > throw new UserException.BadInput(""malformed "" + GATKVCFConstants.RAW_RMS_MAPPING_QUALITY_KEY + "" annotation: "" + rawDataString);; > }; > }; >; > + /**; > + * Private getter function to replace VariantContext::getAttributeAsIntList in instances where there is a chance; > + * that ints will overlow beyond Integer.MAX_VALUE; > + * @return VariantContext attribute indexed by key, as list of long.; > + */; > + static private List<Long> getAttributeAsLongList(final VariantContext vc, final String key, final Long defaultValue) {; >; > I agree with your reticence to put this into htsjdk. The type handling; > there is super awkward, but I don't think it's worth dealing with until it; > gets improved in 3.0. However, I might suggest making this method public; > and moving it to a utility class. GATKProtectedVariantContextUtils has a; > lot of similar methods; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5435#pullrequestreview-176875488>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCBnSuwz30W-52kw9uZTXywWUCua-ks5uxDYbgaJpZM4Yp2Gl>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5435#issuecomment-440649639:306,refactor,refactoring,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5435#issuecomment-440649639,1,['refactor'],['refactoring']
Modifiability,I moved Mark duplicates into it's own package (instead of a single file). There is more refactoring that could be done. I might stop here for now and sync with @jean-philippe-martin on his planned changes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/770:88,refactor,refactoring,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/770,1,['refactor'],['refactoring']
Modifiability,I realized that the `--disable X --disable X` cannot blow up because the argument is a `Set`. Every repeated argument is hidden from the plugin. Either the signature should change to throw the exception or ignore that issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276979349:137,plugin,plugin,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276979349,1,['plugin'],['plugin']
Modifiability,"I recommend putting that in the GitHub wiki.; On Wed, Dec 23, 2015 at 12:20 PM Adam Kiezun notifications@github.com; wrote:. > scenario: someone wants to use gatk4 as a framework and add new tools.; > They need show on the list of tools etc. They package names are the user's,; > ie not org.broadinstitute.hellbender*. The way to do this is to extend Main; > but we have no example and not documentation of this (other that in the; > Main class, which is not the right place - I think it should be in README; > or some such).; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/issues/1397.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1397#issuecomment-166961552:344,extend,extend,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1397#issuecomment-166961552,1,['extend'],['extend']
Modifiability,I refactored ProgressLogger and I'm pushing it up for review. (See https://github.com/samtools/htsjdk/pull/281.) I will feel much happier just implementing a `log` function than copying that code over and having to test and maintain it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/612#issuecomment-122434826:2,refactor,refactored,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/612#issuecomment-122434826,1,['refactor'],['refactored']
Modifiability,I refactored SplitNCigar reads to use the ReadWalker interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1919:2,refactor,refactored,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1919,1,['refactor'],['refactored']
Modifiability,"I refactored some of the stream methods to address @akiezun's (well-founded, as running HaplotypeCaller showed) performance concerns. That, along with @lbergelson's suggestion to lazily evaluate error messages, leaves HaplotypeCaller's performance unaffected. Tests are passing locally but Travis is giving me a mysterious error:. > compileTestJavaerror: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; error in opening zip file; > error: error reading /home/travis/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.1/964cd74171f427720480efdec40a7c7f6e58426a/commons-httpclient-3.1.jar; cannot read zip file. I am fully rebased onto master, including @akiezun's PR from this afternoon. @droazen and @lbergelson do you have insights?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710:2,refactor,refactored,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-232447710,1,['refactor'],['refactored']
Modifiability,I removed an unused type parameter and refactored some functionality into a class MultiPloidyGenotyper which handles dealing with multiple ploidies in a somewhat unified way. It could probably be renamed and expanded slightly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8598:39,refactor,refactored,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8598,1,['refactor'],['refactored']
Modifiability,"I should have been more specific. `--genotype-given-alleles` is HaplotypeCaller functionality, which is very similar if not outputting GVCFs. If providing dbSNP is similar to something you might want to do, then I think this approach will work. My concern was that if you were interested in a particular locus, but had never seen a variant there in any sample (i.e. didn't have an ALT allele to compare to), that implementation could get tricky. We should be able to output 100% reference sites with what I have in mind. @davidbenjamin while you're waiting for the restoration of the TCGA data, would you be interested in extending GGA mode to GenotypeGVCFs? (Obviously without the graph part.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549503239:622,extend,extending,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549503239,1,['extend'],['extending']
Modifiability,"I still think it's bad that sometimes you have tests for the AS annotations in the same class as its non-AS equivalent, and other times you have the AS tests in a separate class (eg., `AS_QualByDepthUnitTest`). It might be better to have the AS annotations always in a separate class, but extend the non-AS test class to share `DataProviders`, etc. Up to you, but should be consistent at least.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1840#issuecomment-225961429:289,extend,extend,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1840#issuecomment-225961429,1,['extend'],['extend']
Modifiability,"I suppose that's true (about always having the dependency) if we want GCS capability in all tools that do I/O (which is all tools, I imagine). I guess I was imagining that there might be a set of Spark-only tools that could have a skinnier dependency tree if we decided to segregate them.; Calling it FileSystemUtils or IOUtils or extending the standard URL resolution framework would ease the cognitive dissonance, though, so thumbs up on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1887#issuecomment-224377784:331,extend,extending,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1887#issuecomment-224377784,1,['extend'],['extending']
Modifiability,I suspect this is related to the jar configuration (3 separate uber- jars) that is unique to the docker CI tests. I'll debug and resolve this after #6351 is merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1251462525:37,config,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1251462525,1,['config'],['configuration']
Modifiability,I tested this successfully on a cluster. The changes make it possible to override Spark configuration - e.g. I was able to override `spark.yarn.executor.memoryOverhead`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1211#issuecomment-162842908:88,config,configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211#issuecomment-162842908,1,['config'],['configuration']
Modifiability,"I think all of our dataflow / spark code is at least almost entirely using `GATKRead`. GATKRead is designed to not provide access to the header because it's not available from a google `Read` backed `GATKRead`. It sounds like there is some information that `Read` includes that is missing from a headerless `SAMRecord`. I think we could audit the `SAMRecordToGATKReadAdaptor` to find any places it touches the header and then cache that information in the adaptor before stripping the header. We don't need to add back in the headers at any point, because we provide library functions to perform any header related operation with a provided header.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659:456,adapt,adaptor,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141107659,1,['adapt'],['adaptor']
Modifiability,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:492,adapt,adapters,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762,2,"['adapt', 'flexible']","['adapters', 'flexible']"
Modifiability,I think now that we have the NIO plugin working we can probably replace everything that used AuthHolder with NIO.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277828180:33,plugin,plugin,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277828180,1,['plugin'],['plugin']
Modifiability,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:151,config,config,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,3,['config'],"['config', 'configure']"
Modifiability,"I think that no `VariantAnnotation` is documented yet, because there is no even a plugin. It will be nice to have them anyway...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342818332:82,plugin,plugin,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342818332,1,['plugin'],['plugin']
Modifiability,I think that the annotation plugin is assigned to @jamesemery here: https://github.com/broadinstitute/gatk/issues/3287. But of course it can be documented before it is a plugin. I just wanted to point out that all of them should be annotated properly...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823849:28,plugin,plugin,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823849,2,['plugin'],['plugin']
Modifiability,"I think that this is a nice feature (at least for me) and not a bug. For example, if in GATK someone runs a tool with `-RF read_filters.args`, then the pipeline cannot be reproduced in a different dataset unless the file is accessible. I can understand that it could be also nice to preserve the `-RF read_filters.args` to be able to modify the file an re-run the tool with different parameters, but for me the purpose of storing the command line in the header or other places is keep track of the exact params that I used: if a file is modified, then it is impossible to trace the params. For input files, this is expected (if the input has changed, it is expected that the result change), but for arguments it shouldn't be the case (independently of the file changing, the tool was running with exactly that parameters). I vote for solve this in Barclay in a configurable way, to allow users to decide which kind of verbosity of the command line they want (I definitely prefer to expand as currently).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092:861,config,configurable,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092,1,['config'],['configurable']
Modifiability,"I think that this is prepared for merging, @cmnbroad. As soon as it gets in, I would submit a PR fixing all the problems found in the plugin descriptor for discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-278391496:134,plugin,plugin,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-278391496,1,['plugin'],['plugin']
Modifiability,"I think that this is related with https://github.com/broadinstitute/gatk/issues/1880, and I've already develop a rough system in a small project to have a plugin for variant annotation. I would love to contribute to this, and I kind of started with https://github.com/broadinstitute/gatk/pull/2534",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2542#issuecomment-290173938:155,plugin,plugin,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2542#issuecomment-290173938,1,['plugin'],['plugin']
Modifiability,I think the only failure left is a Travis thing. Back to @vruano after heavy refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216,1,['refactor'],['refactoring']
Modifiability,"I think the piece we need is just the parser, though -- we can keep the Picard code that injects values into instance variables & parses the annotations, etc., we just need something to go from the raw args array into a parsed set of names + values.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69632895:118,variab,variables,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/72#issuecomment-69632895,1,['variab'],['variables']
Modifiability,"I think the problem is that you need to do https://github.com/broadinstitute/gatk/issues/1130 first, then refactor the tests to use a .2bit reference for the BROADCAST-based tests as part of this ticket (SHUFFLE should still have test coverage, though).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1131#issuecomment-157478894:106,refactor,refactor,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1131#issuecomment-157478894,1,['refactor'],['refactor']
Modifiability,"I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, https://github.com/dbpedia/distributed-extraction-framework/issues/9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169:466,Config,Configuration,466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169,1,['Config'],['Configuration']
Modifiability,I think we can finally unblock this using PAPIv2's ability to request machine types on Google Cloud. We just have to configure our jenkins server to use PAPIv2.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4356#issuecomment-415862887:117,config,configure,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4356#issuecomment-415862887,1,['config'],['configure']
Modifiability,I think we will add a logger for GenomicsDB with configurable verbosity - but this is low priority for us.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300298389:49,config,configurable,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300298389,1,['config'],['configurable']
Modifiability,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:58,variab,variable,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215,1,['variab'],['variable']
Modifiability,"I tried setting that environment variable, but it did not resolve the issue. Please note that I have this filesystem mounted as `CIFS` not `NFS`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453657941:33,variab,variable,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453657941,1,['variab'],['variable']
Modifiability,I want to point out a bug regarding the read plugin that I reported in #357 (and fix with in #2359): disable a default filter with arguments blows up as a `CommandLineException` as if the user provided the arguments for a disabled filter. This is quite important in this regard of insane combinations.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276625536:45,plugin,plugin,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276625536,1,['plugin'],['plugin']
Modifiability,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1752:737,extend,extends,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752,1,['extend'],['extends']
Modifiability,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:156,plugin,plugin,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,5,"['Refactor', 'plugin']","['Refactor', 'plugin']"
Modifiability,"I was thinking after checking your `ReadWindowWalker` implementation and the HaplotypeCaller branch that with this class we can do a general `SlidingWindowWalker`, which could be extended by `ReadWindowWalker` or it could be possible that `HaplotypeCaller` directly implements `SlidingWindowWalker`. @droazen, could you have a look and tell me what do you think about the idea?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210077798:179,extend,extended,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210077798,1,['extend'],['extended']
Modifiability,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:304,variab,variable,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['variab'],['variable']
Modifiability,"I was thinking that if we relied on PyPI for distribution, it would only be for released builds, not a release for every repo merge commit. But, I'm increasingly inclined to think that in the short term we should just include the python archive/zip file right in the gatk distribution zip, and modify the env .yml to install from that. Then every configuration (docker image, git clone user, and end user) could use exactly the same method to establish the environment. That seems like the simplest solution for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343:347,config,configuration,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343,1,['config'],['configuration']
Modifiability,"I will do a big PR with a commit for each of the issues that we found in the plugin, and including tests. Although the plugin interface is going to change in Barclay, I think that before that the PR should setup all the tests for the issues to ensura that the change does not broke anything. Any opinion on this, @cmnbroad, @droazen, @lbergelson?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278248587:77,plugin,plugin,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278248587,2,['plugin'],['plugin']
Modifiability,"I would like to have this feature for a new project that will rely on the built-in walkers of GATK. The main problem that I am facing is while grabbing the info from the METAINF file. For fixing this, I have a proposal:. * Create a new interface with the single method to print the startup message; * Create a base-class with the current code in GATK, which can be extended to override some parts of the startup message, but not all.; * Make a non-final static field in `CommandLineProgram` , which is settable. This could be set in the `Main` for toolkits (similarly to how the config file is set). The default will be the GATK implementation. If you agree with the proposal, let me know to implement it and submit a PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-381504458:365,extend,extended,365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-381504458,2,"['config', 'extend']","['config', 'extended']"
Modifiability,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2260:255,variab,variable,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260,3,"['refactor', 'variab']","['refactored', 'variable']"
Modifiability,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:278,plugin,plugin,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"I would like to start a new project to extend the engine of GATK (mostly walker types, e.g. https://github.com/broadinstitute/gatk/issues/1198 and common utilities), and thus I require to have an idea of how the versioning scheme is related to the public API if at all. This will allow me to say that the extensions works with GATK between 4.0.0.0 and 4.1.0.0, for example, and to know when some work is required to move to the next released version. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4603:39,extend,extend,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603,1,['extend'],['extend']
Modifiability,"I'd be OK with adding some sort of experimental tag, but I'm not sure where to put it. Does that goes in the docs or somewhere in the code to post a warning to the log? I think the new tests should cover that it's working as intended. There is definitely room for future work (e.g. paying closer attention around the boundaries, the refactoring ideas, etc), but for now the docs should describe the expected behavior well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847926227:333,refactor,refactoring,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847926227,1,['refactor'],['refactoring']
Modifiability,"I'd like to get to the point where most/all GATK tools extend `GATKTool` rather than `CommandLineProgram` directly, so I think we have to keep this one open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358025246:55,extend,extend,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358025246,1,['extend'],['extend']
Modifiability,"I'll also mention that as part of https://github.com/broadinstitute/gatk/issues/4341 we plan to give tools more control over the arguments inherited from `GATKTool`, including selectively disabling and redefining engine arguments, so once a mechanism is in place for that `CalculateGenotypePosteriors` could make `--sequence-dictionary` required. Currently this ability only exists for a few `GATKTool` arguments, and the tool has to override methods like `requiresReads()` to make use of it. Until the ability to do this is generalized, recommend the stopgap solution with the check in `onTraversalStart()` + a note in the tool's docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4383#issuecomment-364497042:139,inherit,inherited,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4383#issuecomment-364497042,1,['inherit'],['inherited']
Modifiability,"I'll be adding documented feature tags to the 57 annotation modules. Before I get to these, I need a new ANNOTATORS category to exist in HelpConstants.java. . I've taken the liberty to name the category ANNOTATORS. Here is the relevant info I added to HelpConstants.java:. - group name variable and descriptor: DOC_CAT_ANNOTATORS = ""Annotation Modules""; - group summary variable and descriptor: DOC_CAT_ANNOTATORS_SUMMARY = ""Annotations available to HaplotypeCaller, Mutect2 and VariantAnnotator""; - super category: Utilities (same group as read filters)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3835:286,variab,variable,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3835,2,['variab'],['variable']
Modifiability,I'll take it. The tab completion task is printing out too many warnings for all of the new instances of unresolvable backtrace variables due to Picard. I'll do....something with it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3710#issuecomment-337386911:127,variab,variables,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3710#issuecomment-337386911,1,['variab'],['variables']
Modifiability,"I'm adding some issues and PRs for make the plugin usable in other cases too, @cmnbroad. Maybe you prefer that solution instead of make it extensible. Just let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544:44,plugin,plugin,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544,1,['plugin'],['plugin']
Modifiability,"I'm always confused with static-block initializers, and I just wondered in the config PR if the static block in `Main` it is correctly setting everything in sub-classes. I understood that this won't work in Main-derived classes; if that's not the case, feel free to close this PR...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324627658:79,config,config,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324627658,1,['config'],['config']
Modifiability,"I'm assuming you will have the subset of samples before creating a GenomicsDBFeatureReader object (and before creating the corresponding Protobuf export configuration object). More precisely, you are NOT requesting a line by line filter similar to:; At pos 100, compute INFO fields etc including only the samples whose QUAL > 5; At pos 102, compute INFO fields etc including only the samples whose QUAL > 5; ....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322:153,config,configuration,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322,1,['config'],['configuration']
Modifiability,"I'm exploring the usage of the `AssemblyRegionWalker` engine for custom tools and I realized that there is very little support for customization. One of the things that will be nice is to use a custom `ActivityProfileState.Type` for assigning custom reasons to be an active region (or non active at all). My proposal is the following:. * Abstract `ActivityProfileState.Type` with one method to get the value as a `Number`.; * Make the current enum extend this abstract class.; * Change the method to get the value to delegate into the `Number`. This will allow custom types for the activity profile, and value range for the result value (now it should be positive or null).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2539:448,extend,extend,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2539,1,['extend'],['extend']
Modifiability,"I'm glad it's working now, but a PS since you asked about `-independent-mates`: several months ago we made Mutect2 force paired reads to share the latent random variable indicating which haplotype they are derived from in the somatic genotyping model. This is correct because paired reads come from the same molecule of DNA. `-independent-mates` disables this and tells Mutect2 to forget about pairing. We only created the option because some synthetic validation data is generated by spiking in variation without regard to pairing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833:161,variab,variable,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833,1,['variab'],['variable']
Modifiability,"I'm going to close this issue because it's not a bug. Several things in the code of Mutect2 and FilterMutectCalls adapt as they traverse the genome and it's possible that some learned parameter shifts minutely. For example, the assembly graph pruning algorithm uses knowledge of previously assembled regions to better distinguish between errors and somatic variation. It's also possible that somewhere we forgot to give something a fixed random seed. In full honesty, I _wish_ that I knew exactly what causes the 3142 to become 3143, and I regret that I don't have time for it. Nonetheless, in principle it is not cause for alarm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338:114,adapt,adapt,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338,1,['adapt'],['adapt']
Modifiability,"I'm in favor of doing a ruthless refactor involving changing some of the; default behavior and prohibiting contradictory options. On Fri, Dec 12, 2014 at 11:59 AM, lbergelson notifications@github.com; wrote:. > We definitely need a way to combine vcfs that does the right thing and; > isn't horrible to use.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/16#issuecomment-66800878; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66802166:33,refactor,refactor,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/16#issuecomment-66802166,1,['refactor'],['refactor']
Modifiability,"I'm looking into migrating custom GATK3 variant Info/GenotypeAnnotations to GATK4. The annotate() method in GATK3 was passed a sizable amount of context. This is greatly reduced in GATK4. I understand a desire to simplify, such as not passing the Walker. FeatureContext in particular would be helpful, is there another way to access that from VariantAnnotations?. Stepping back: the one scenario I want to support is to annotate genotype concordance between the input VCF and a reference VCF. In our GATK3 implementation, the user supplied that VCF on the command line when executing VariantAnnotator. This plugin used GATK3's walker.getResourceRodBindings(), which seems analogous to GATK4 FeatureContext, to find that binding. It then queries that VCF to find any VariantContext from the current site. . I realize this is raising a couple issues: a) access FeatureContext from within annotate(), , b) efficiently query VariantContext from another resource, and c) plugin that would ideally provide its own command-line argument. . Are there any existing GATK annotations or other plugins that deal with these issues?. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930:607,plugin,plugin,607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'm more familiar with working with interval data files so I can't speak much to variant call formats except that VCF tends to be a bit wonky with interval data (such as SV calls) because, as @samuelklee mentioned, most of the fields are likely to be irrelevant and thus waste space. I feel strongly that BED-like formats are the way to go for interval data, i.e. #contig start end x1 x2 x3 x4 ...; chr1 2938 3949 3.9 0 + cat ...; ... where x1, x2, x3, x4 ... are columns for variables of different types (which could be specified by additional header lines like in VCF).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457:476,variab,variables,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457,1,['variab'],['variables']
Modifiability,"I'm not sure exactly what's happening but I suspect it has something to do with the way the files are mounted. My guess is that there is some sort of transient interruption happening in the connection between the EC2 instance and the file server, and it's causing an error in gatk. When reading from a local file GATK does not expect any errors since errors in local files are usually fatal problems caused by a broken disk. Its probably some sort of bug in amazon's fuse implementation which isn't properly hiding network problems from the software. . I expect that your output is truncated at the point the error occured, and you probably need to rerun those shards. Instead of mounting them with amazon's fuse, you could try to either copy the files to a local disc, or access them using an NIO filesystem plugins like this plugin https://github.com/awslabs/aws-java-nio-spi-for-s3 or as signed URLs using https://github.com/broadinstitute/http-nio/ (included in gatk 4.6).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942:809,plugin,plugins,809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'm not sure how this worked before, but adding a variable fixes it. @ahaessly, let me know what you think!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6939:50,variab,variable,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6939,1,['variab'],['variable']
Modifiability,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:1612,extend,extend,1612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,1,['extend'],['extend']
Modifiability,"I'm not sure why the `gs` provider doesn't get installed (I think JP was seeing this before), but in any case there's a workaround in GATK for it: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java#L380-L390. However, since the Hadoop-BAM code is doing the path lookups, it can't call that code directly (the dependency is the wrong way round). It would be best if we could fix the underlying problem of course, so that it gets picked up properly - I wonder if this can be done by fixing the service provider so it survives relocation (see http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer). BTW I'm afraid I'm travelling this week, so I won't have time to look at it until next week either :(",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-263997131:635,plugin,plugins,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-263997131,2,['plugin'],"['plugin', 'plugins']"
Modifiability,I'm not totally clear from your response but I think you've resolved the problem? . If you're encountering a bug merging bai files could you open an issue describing that with your stack trace and any relevant information about the configuration you're running?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623:232,config,configuration,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623,1,['config'],['configuration']
Modifiability,"I'm pretty sure they all do... or at least all can depending on how you configure your MalformedReadFilter. example:. ```; private static boolean checkHasReadGroup(final SAMRecord read) {; if ( read.getReadGroup() == null ) {; // there are 2 possibilities: either the RG tag is missing or it is not defined in the header; final String rgID = (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; if ( rgID == null ); throw new UserException.ReadMissingReadGroup(read);; throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; }; return true;; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75649333:72,config,configure,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75649333,1,['config'],['configure']
Modifiability,"I'm still unclear if this is a bug or enhancement. Does the current ClipReads do what it promises to do? If yes, then it's not a bug. If not, what is the test case that shows the bug?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-99303878:38,enhance,enhancement,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-99303878,1,['enhance'],['enhancement']
Modifiability,"I'm thinking about lists such as filters packages (in the filter plugin): I can imagine a list with long package names separated by commas, which might be complicated to read due to the repetition of the same organization name. I think that it might be also more organize if the configuration file is an YML with sections for the different configurations: this will make the file more readable and easier to modify. Something like the codecov configuration will be interesting, separating configurations for spark, plugins, feature codecs, etc. For example, if I want to use a custom codec for BED files while including the HTSJDK codec packages, I would find a problem. Doing it in a granular level may be interesting for having something like:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - org.magicdgs.htsjdk.codecs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307803675:65,plugin,plugin,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307803675,6,"['config', 'plugin']","['configuration', 'configurations', 'plugin', 'plugins']"
Modifiability,"I'm trying to genotype 2388 whole genome samples of a wild species. This species has a large genome and lots of diversity. I've created my genomicsDB for my combined set and genotypeGVCF gets stuck when trying to make the VCF. ; I've been giving it 120G of ram, 32 cores and 30 minutes and it only prints out the first variable 12 sites (corresponding to about 800bp of the genome) to the VCF . I understand that is certainly going to be slow, and I'm prepared to heavily parallelize it, but this is currently unusable to me. Is there any way to speed it up?. Here's my command:. ```; /gatk/gatk-launch --java-options ""-Xmx120G"" GenotypeGVCFs \; -R /home/user/bin/ref/reference.fa \; --intervals $contig \; -V gendb://${chr}_$pos \; -O /scratch/wild_gwas/$genomicsdb/${chr}_$pos.tmp.vcf.gz \; --seconds-between-progress-updates 5 --verbosity DEBUG; ```; Here's standard out:. > 21:13:04.092 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/install/gatk/lib/gkl-0.8.2.jar!/com/intel/gkl/native/libgkl_compression.so; > 21:13:04.108 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/gowens/libgkl_compression3380966567685792416.so; > 21:13:04.218 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.219 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; > 21:13:04.219 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:13:04.219 INFO GenotypeGVCFs - Executing as user@cdr806.int.cedar.computecanada.ca on Linux v3.10.0-693.5.2.el7.x86_64 amd64; > 21:13:04.219 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-2ubuntu1.16.04.3-b11; > 21:13:04.219 INFO GenotypeGVCFs - Start Date/Time: January 15, 2018 9:13:04 PM UTC; > 21:13:04.219 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.220 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:319,variab,variable,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['variab'],['variable']
Modifiability,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2033:257,extend,extending,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033,3,['extend'],['extending']
Modifiability,"I'm using spark 2.1.0. I can confirm it works with the command ; ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; --sparkMaster yarn; ```; so I guess it's really a minor issue. I can see it confusing other spark users though, who might expect spark configuration arguments to go through `spark-submit` rather than the application args, especially since the --sparkMaster app arg is optional. Just my two cents.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301945697:572,config,configuration,572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301945697,1,['config'],['configuration']
Modifiability,"I've added a few commits that clean up some of the code inherited from VQSR regarding the use of labeled resources when using allele-specific annotations. This should be ready for review and/or experimentation with importing into the WARP repo, @meganshand. There are a few unrelated failing tests, which I think others are seeing in their branches as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1323953256:56,inherit,inherited,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1323953256,1,['inherit'],['inherited']
Modifiability,"I've added a new end-to-end test for SelectVariants that writes to GCS. Sadly, the IntegrationTestSpec class uses Files throughout, so it wasn't possible to do this simply without first completely refactoring IntegrationTestSpec (which should probably be its own pull request). . Doing this refactoring would have the advantage that changing existing end-to-end tests from local to GCS would be trivial. For now instead I went with an ad-hoc approach. It works, and the test passes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612:197,refactor,refactoring,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612,2,['refactor'],['refactoring']
Modifiability,"I've had several Travis test failures (on my picard removal branch) that appear to be failures during kryo serialization of a mocked ReferenceMultiSource object (based on the failing class name, (org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource$$EnhancerByMockitoWithCGLIB$$b0dc631f, which looks like the CGLIB names mentioned [here](https://github.com/mockito/mockito/issues/319)). We're on an ancient version of mockito anyway, and newer versions no longer use cglib, so it seemed like a good time to upgrade. To do so I also had to replace usage of the method getArgumentAt, which has been [deprecated](https://github.com/mockito/mockito/pull/373) in favor of getArgument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581:267,Enhance,EnhancerByMockitoWithCGLIB,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581,1,['Enhance'],['EnhancerByMockitoWithCGLIB']
Modifiability,"I've pulled the problem VCF and a couple of successful ones locally and I can confirm that when running with 4 VCFs:. - VCFs that succeeded through JointGermlineCNVSegmentation in our pipeline succeeded for me locally; - The VCF that was flagged in the error message `Exception thrown at chrX:6383391 [VC SAMPLE_ID.segments.vcf.gz ...` completes just fine with other successful partners; - The VCF that was not identified in the error message, but was inferred to be a sex chromosome aneuploidy causes a failure with any combination of other VCFs; - If there are more than 2 VCFs run together, including the failing VCF/aneuploid sample, the error message indicates the problem originates in a non-aneuploid VCF, which misleading and makes this hard to treat. This behaviour was consistent in `4.5.0.0`. Command used in my toy dataset:. ```; gatk --java-options ""-Xms4000M -Xmx6000M"" JointGermlineCNVSegmentation -R /data/Homo_sapiens_assembly38_masked.fasta -O /data/out.vcf.gz -V /data/SAM1.segments.vcf.gz -V /data/SAM2.segments.vcf.gz -V /data/SAM3.segments.vcf.gz -V /data/SAM4.segments.vcf.gz --model-call-intervals /data/preprocessed.interval_list -ped /data/inferred_sex_pedigree.ped; ```. - In this configuration, `SAM4` is aneuploid, and `SAM1` is always the flagged VCF; - If I remove `SAM1` and re-run with 3 VCFs, `SAM3` is mentioned in the error message. It's not derived from alphabetical order, first argument specified with `-V`, or first in the PED file",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736:1208,config,configuration,1208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736,1,['config'],['configuration']
Modifiability,INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2874,Config,ConfigFactory,2874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"ISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4483,Config,ConfigFactory,4483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.L",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4949,variab,variable,4949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,Ideally using a standard mechanism like Apache `Configuration`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368:48,Config,Configuration,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368,1,['Config'],['Configuration']
Modifiability,"If I understand correctly, @bbimber, this sounds a lot like the `--genotype-given-alleles` change David B. made recently -- discover new sites in the provided samples, but _also_ output calls against the variants specified in the resource file. This is a small change from what you propose because the resource file would specify a particular ALT allele, so if you were interested in positions where you haven't seen a variant yet we'd need to extend functionality to call against <NON_REF> and output or you could put some random placeholder allele, although the identity of that allele would affect your likelihoods. On the other hand, having an ALT specified from a previous cohort would improve your likelihoods for all-reference cohorts compared to what you get now for non-variant site output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549456607:444,extend,extend,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549456607,1,['extend'],['extend']
Modifiability,"If a tool requires a reference, for example, it should be able to indicate so via an annotation, instead of manually checking the inherited argument value for null. The engine should perform the check on behalf of the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/120:130,inherit,inherited,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/120,1,['inherit'],['inherited']
Modifiability,"If it helps, this is a dummy walker that illustrates the kind of parallelization I'm exploring whether I can implement. The main sticking point I think exists relates to different threads interacting with FeatureManager and the feature reading code:. I would appreciate any thoughts on how to approach this in GATK. ```. /**; * This is a contrived example. It is designed to explore multithreading; */; public class DemoMultiVariantEval extends MultiVariantWalkerGroupedOnStart {; protected List<VariantEvalEngine> engines = new ArrayList<>();. @ArgumentCollection; protected VariantEvalArgumentCollection variantEvalArgs = new VariantEvalArgumentCollection();. @Override; protected void initializeDrivingVariants() {; getDrivingVariantsFeatureInputs().addAll(VariantEvalEngine.getFeatureInputsForDrivingVariants(variantEvalArgs));. super.initializeDrivingVariants();; }. private ExecutorService pool = null;; private final int threads = 2;. @Override; public void onTraversalStart() {; //Again, contrived. The real case would set up multiple engines with different parameters:; engines.add(new VariantEvalEngine(variantEvalArgs, this, getSequenceDictionaryForDrivingVariants(), getSamplesForVariants(), logger));; engines.add(new VariantEvalEngine(variantEvalArgs, this, getSequenceDictionaryForDrivingVariants(), getSamplesForVariants(), logger));. final ThreadFactory threadFactory = new ThreadFactoryBuilder(); .setNameFormat(""dummywalker-thread-%d""); .setDaemon(true); .build();. pool = Executors.newFixedThreadPool(threads, threadFactory);; }. @Override; public void apply(final List<VariantContext> variantContexts, final ReferenceContext referenceContext, final List<ReadsContext> readsContexts) {; // Parallelization should help; however, these steps interact with FeatureManager and the FeatureInputs.; // Is there an appropriate way to approach this?; Utils.runInParallel(threads, () -> {; this.engines.parallelStream().forEach(engine -> {; engine.apply(variantContexts, referenceContext);;",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7013#issuecomment-750442132:437,extend,extends,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7013#issuecomment-750442132,1,['extend'],['extends']
Modifiability,If it's actually 27**MB** I would just check it into lfs or rewrite to use -L,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1745#issuecomment-212969722:60,rewrite,rewrite,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745#issuecomment-212969722,1,['rewrite'],['rewrite']
Modifiability,"If the assembler has not discovered an allele it could be simply that the samples does not have that allele but it could also be the case that it failed to assemble due to some edge case. . If the former it makes sense to assign depth 0 but for the latter that would be misleading and a ""I don't know"" output (""."") would be more appropriate. For example, what about those HOM-REF sites that end up with PL=0,0,0 because the reference-confidence-model found reads that don't support the reference sequence yet the assembly did not produce a concrete alternative. Fast forward and the same sample is joint-genotyped with in a cohort with other samples for which HC assembled the alternative haplotype/allele (correctly). Then we will assign AD=0 to those alternative alleles in the original (no-quite)-hom-ref sample. . I think the better answer would be AD=""."" in light of the lack of confidence on the hom-ref call. . Would this even extend to cases where we are confident on hom-ref? Unless any single read is exactly the reference at that site there is a potential for that allele to have gone unnoticed. . Would make sense that if someone wants to know the AD for every alt. allele at a sample where some weren't discovered in, he must re-run HC in GGA mode with the full list of alt alleles?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7290:934,extend,extend,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7290,1,['extend'],['extend']
Modifiability,"If the encapsulation of the datasources is causing issues for tools that extend `GATKTool` directly, we can relax it -- it was intended to prevent walker authors from directly manipulating the datasources used for the traversal, but it may be doing more harm than good at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358333054:73,extend,extend,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358333054,1,['extend'],['extend']
Modifiability,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8266:430,adapt,adapt,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266,1,['adapt'],['adapt']
Modifiability,"If this passes, we can probably remove encrypted_29f3b7c4d8c3_key from the repo variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4627:80,variab,variables,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4627,1,['variab'],['variables']
Modifiability,"If you manually specify the fullName attribute for an argument in an Option annotation, it is not respected, and instead the full name of the argument gets set to the name of the annotated member variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/111:196,variab,variable,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/111,1,['variab'],['variable']
Modifiability,"If you override `makeReadFilter` in a tool with this arguments and the user provides them, either the developer should code a warning message saying that read filters will be ignored or the user will not be aware of what is happening in the processing. @cmnbroad, I think that it could be a good idea to add a `requiresReadFilters` to `GATKTool`, and like that tools which requires reads does not need to implement all this even if they are not extending `ReadWalker` or `LocusWalker`, for instance. What do you think? This is related with #1076",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-226123264:445,extend,extending,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900#issuecomment-226123264,1,['extend'],['extending']
Modifiability,"If you set the environment variable `SPARK_LOCAL_IP=""127.0.0.1""` , as suggested by this SO post: https://stackoverflow.com/questions/34601554/mac-spark-shell-error-initializing-sparkcontext, the error goes away, so I think it's related to some change in the localhost network configuration our VPN sets up.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-325239463:27,variab,variable,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-325239463,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,Implement 1D and 2D adaptive quadrature,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3318:20,adapt,adaptive,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3318,1,['adapt'],['adaptive']
Modifiability,"Implement ReadsHtsgetData source, refactor HtsgetReader",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6662:34,refactor,refactor,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6662,1,['refactor'],['refactor']
Modifiability,"Implement a `PythonScriptExecutor` that is similar to the existing `RScriptExecutor` (invokes Python with a given set of arguments). Then ask the CNV team to prototype an example tool or two that use `PythonScriptExecutor` to call into a Python machine-learning library, and do an assessment of maintainability, etc. `PythonScriptExecutor` will come with an attached set of conditions for its use, intended to address the most serious issues raised by the engine and support teams with having Python code in the GATK. We should document these conditions in the docs for `PythonScriptExecutor` when it's implemented:. 1. All tools that use `PythonScriptExecutor` must have a Java-based front-end, with standard GATK (barclay-based) arguments. We put a lot of development effort into our arg parser and into striving for user-interface consistency across tools, and cannot afford to duplicate this effort in Python. Geraldine (CC'd) and the rest of the support team can back me up on this one!. 2. An honest effort should be made to minimize the amount of code written in Python -- as much of each tool's work as possible should be done in Java. In particular, reading/writing final inputs and outputs should happen in Java. This is important for a number of reasons, including the engine team's goal of ensuring universal GCS support, consistent Google authentication handling, etc. Again, we really don't want to have to duplicate that work in Python, or for the tools that call into Python to be inconsistent with the rest of the toolkit. 3. All dependencies (Python and native) of Python libraries used will be clearly documented, and included in the default GATK docker image. I don't think I need to explain why this one is important :) . 4. Before we go any further down this path, we prototype one or two tools using `PythonScriptExecutor`, and do a fair assessment of maintainability and other concerns of the engine/support teams, such as whether it will even be possible to package all depend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:295,maintainab,maintainability,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,1,['maintainab'],['maintainability']
Modifiability,"Implements two new tools and updates some methods for a revamp of the `CombineBatches` cross-batch integration module in [gatk-sv](https://github.com/broadinstitute/gatk-sv). - `SVStratify` - tool for splitting out a VCF by variant class. Users pass in a configuration table (see tool documentation for an example) specifying one or more stratification groups classified by SVTYPE, SVLEN range, and reference context(s). The latter are specified as a set of interval lists using `--context-name` and `--context-intervals` arguments. All variants are matched with their respective group which is annotated in the `STRAT` INFO field. Optionally, the output can be split into multiple VCFs by group, which is a very useful functionality that currently can't be done efficiently with common commands/toolkits.; - `GroupedSVCluster` - a hybrid tool combining functionality from `SVStratify` with `SVCluster` to perform intra-stratum clustering. This tool is critical for fine-tuned clustering of specific variants types within certain reference contexts. For example, small variants in simple repeats tend to have lower breakpoint accuracy and are typically ""reclustered"" during call set refinement with looser clustering criteria.; - `SVStratificationEngine` - new class for performing stratification.; - Updates to breakpoint refinement in `CanonicalSVCollapser` that should improve breakpoint accuracy, particularly in larger call sets. Raw evidence support and variant quality are now considered when choosing a representative breakpoint for a group of clustered SVs.; - Added `FlagFieldLogic` type for customizing how `BOTHSIDE_PASS` and `HIGH_SR_BACKGROUND` INFO flags are collapsed during clustering.; - `RD_CN` is now used as a backup if `CN` is not available when determining carrier status for sample overlap.; - Removed no-sort option in favor of spooled sorting.; - Bug fix: support for empty EVIDENCE info fields; - Bug fix: in one of the JointGermlineCnvDefragmenter tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8990:255,config,configuration,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8990,1,['config'],['configuration']
Modifiability,Improvements and refactoring of Nucleotide.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4846:17,refactor,refactoring,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846,1,['refactor'],['refactoring']
Modifiability,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857:284,Adapt,AdaptiveChainPruner,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"In `PicardCommandLineProgram` there is an `instanceMain` that gets called when a subclass program is called. This ""main"" sets a number of static variables to values parsed from the arguments, then calls `super.instanceMain`. The problem is that the values haven't been parsed before they are referenced in this function, so the default values are _always_ used. The ignored flags are:; - `VALIDATION_STRINGENCY`; - `COMPRESSION_LEVEL`; - `MAX_RECORDS_IN_RAM`; - `CREATE_INDEX`; - `CREATE_MD5_FILE`. These flags are ignored for _all_ Picard tools, of which we have dozens. A rough sketch of the solution is here:; https://github.com/broadinstitute/gatk/compare/da_fix_picard; If this seems like a reasonable approach, I'll retake the bug and fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1155:145,variab,variables,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1155,1,['variab'],['variables']
Modifiability,"In high-depth calling (eg @meganshand's work with mitochondria) it is necessary to tweak the `min-pruning` argument. If it is too low, base errors render the assembly graph nearly dense, causing a loss of sensitivity when the assembly engine essentially chooses random haplotypes. If it is too high, we also lose sensitivity because true variants are pruned. Setting the command line argument differently for each sample is not only cumbersome. It also doesn't solve the problem because depth varies within the same bam. Thus, pruning must adapt to each assembly region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4867:540,adapt,adapt,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867,1,['adapt'],['adapt']
Modifiability,"In my case, as an API user, my main usage of GATK is for traverse `GATKRead` and `VariantContext`, so I would like to have in `GATKTool` a simpler way of access to the `FeatureInput<VariantContext>` instead of getting them from `FeatureManager features`. It will be useful in the `VariantWalker` as a step to issue #692, to get all the variants provided by the user in the same walker. My idea is modify the `GATKTool` to include:; - A `public abstract boolean requiresVariant()`, which will be used to determine if we should detach or not all the variants inputs from the `FeatureManager features`.; - A `private void initializeVariants()`, which will implement a way to extract the `FeatureInput<VariantContext>` from `features` and initialize a `FeatureManager variants` or a extended class which includes only `VariantContext` inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1710:779,extend,extended,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1710,1,['extend'],['extended']
Modifiability,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:235,variab,variable,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,6,"['config', 'variab']","['config', 'variable']"
Modifiability,"In other words, saying that broadcast took X minutes is not meaningful unless it's framed as a fraction of total runtime, and we understand whether this is a fixed or variable cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208920951:167,variab,variable,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208920951,1,['variab'],['variable']
Modifiability,"In particular add output GATKTool.getDefaultToolVCFHeaderLines to the VCF header, and rewrite the integration test for GenerateVCFFromPosteriors so that it validates the equivalence of variant context records, instead of file equivalency",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4267:86,rewrite,rewrite,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4267,1,['rewrite'],['rewrite']
Modifiability,"In terms of the two tools, I don't think it's necessary at this point to make an inheritance structure. `CallVariantsFromAlignedContigsSAMSpark` is more of a one-off for dealing with de novo assembly files and I'm not sure if it will be supported long term. However, I did extract a `callVariantsFromAlignmentRegions` method in `CallVariantsFromAlignedContigsSpark` that `CallVariantsFromAlignedContigsSAMSpark` can use, which reduces code duplication a lot. There's not much left in `CallVariantsFromAlignedContigsSAMSpark` except for the logic to convert GATKReads into AlignmentRegions, which seems appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240514475:81,inherit,inheritance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240514475,1,['inherit'],['inheritance']
Modifiability,"In the `ReadFilter` plugin, the `--disableAllReadFilters` option disable all read filters that are provided by the tool and by the user. From my point of view, it does not make sense to disable all the filters and provide some, but I think that the option may be a shortcut to disable all the provided ones and allow the user to set their own filters (or none, if they prefer it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2361:20,plugin,plugin,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2361,1,['plugin'],['plugin']
Modifiability,"In the latest master, running for example `java -jar build/libs/gatk.jar FixVcfHead` returns:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:366,adapt,adapters,366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['adapt'],['adapters']
Modifiability,"In the news file of a structural variant software I use, I read. >Added FIX_SA and FIX_MISSING_HARD_CLIP; >FIX_SA: rewrites split read SA tags; >corrects GATK indel realignment SA tag data inconsistency; >FIX_MISSING_HARD_CLIP: infers missing hard clipping if split read records have different lengths; >corrects for GATK indel realignment stripping hard clipping when realigning. Could such issues perhaps be resolved in an update to GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6459:115,rewrite,rewrites,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6459,1,['rewrite'],['rewrites']
Modifiability,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:83,rewrite,rewrite,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['rewrite'],['rewrite']
Modifiability,"In this case, the FDR threshold is not honored. The explanation of this is complex, but essentially has to do with the Benjamini-Hochberg procedure not playing well with suppression factor when extended to more than one artifact mode. The definition of ``FilterByOrientationBias`` will have to be changes from ""guaranteeing less than 1% FDR over all mutations"" to ""guaranteeing less than 1% FDR in each specified artifact mode"". This could make the filter more aggressive, so we may have to adjust the FDR threshold. - [x] code fix; - [x] doc updates",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3344:194,extend,extended,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3344,1,['extend'],['extended']
Modifiability,"In working on #8296 we have discovered that in the `MafOutputRendererConstants.java` there are myriad constants that hard code aliases with the pattern ""Gencode_34_hugoSymbol"". This can lead to bad behavior if a non-bundled Gencode version is used in Funcotator, specifically it can cause the MAF file to be missing most of its hard-coded fields as they will be mis-identified by the output-renderer resulting in mostly empty outputs. We should rewrite the logic in the MAF code to be completely agnostic to Gencode versions used to generate the Funcotations to drop this hard-coding all-together.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8482:445,rewrite,rewrite,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8482,1,['rewrite'],['rewrite']
Modifiability,"Includes latest Gencode and an implicit fix for #6564. Had to make some code changes for latest liftover Gencode data(v34 -> hg19). . The associated DS test release correctly annotates data on hg19 and hg38. Left to do:. - [x] Update data sources downloader.; - [x] Update data source version validation code. Code updates:; - Now both hg19 and hg38 have the contig names translated to `chr__`; - Added 'lncRNA' to GeneTranscriptType.; - Added ""TAGENE"" gene tag.; - Added the MANE_SELECT tag to FeatureTag.; - Added the STOP_CODON_READTHROUGH tag to FeatureTag.; - Updated the GTF versions that are parseable.; - Fixed a parsing error with new versions of gencode and the remap; positions (for liftover files).; - Added test for indexing new lifted over gencode GTF.; - Added Gencode_34 entries to MAF output map.; - Minor changes to FuncotatorIntegrationTest.java for code syntax.; - Pointed data source downloader at new data sources URL.; - Minor updates to workflows to point at new data sources. Script updates:; - Updated retrieval scripts for dbSNP and Gencode.; - Added required field to gencode config file generation.; - Now gencode retrieval script enforces double hash comments at; top of gencode GTF files. Bug Fixes:; Removing erroneous trailing tab in MAF file output. - Fixes #6693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660:1104,config,config,1104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660,1,['config'],['config']
Modifiability,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4611:13,Config,Configuration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611,4,"['Config', 'config', 'plugin']","['Configuration', 'configuration', 'plugin']"
Modifiability,Initial support for spanning deletions and refactoring for testability,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857:43,refactor,refactoring,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857,1,['refactor'],['refactoring']
Modifiability,"Initial version should consist of:; - A superinterface called `NativeLibrary` that has `getLibraryPath()` and `isSupported()` methods.; - A `PairHmmBinding` interface (name open to negotiation!) that extends `NativeLibrary` and has signatures for `jniComputeLikelihoods()` and other PairHmm JNI methods. Once created, we need to publish a jar on maven for this repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1801:200,extend,extends,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1801,1,['extend'],['extends']
Modifiability,"Instead of calling setHeader() to temporarily give headerless reads; a header and then calling into htsjdk's SAMRecordCoordinateComparator,; adapt the htsjdk code directly to work with headerless reads. This should; be safer (especially in a multithreaded context), as mutating the objects; being compared within a comparator is a violation of contract.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1276:141,adapt,adapt,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1276,1,['adapt'],['adapt']
Modifiability,Instrument FuncotationEngine to provide SEG file configs as a parameter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5962:49,config,configs,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962,1,['config'],['configs']
Modifiability,"Interesting! Thanks for generating these. I am already convinced by #4519 we should at least switch over to a ‘CollectReadCounts’ strategy for initial evaluations. A few comments:. -I’m guessing that the equal insert size and uniform sampling is enhancing many of these artifacts to a level that we probably don’t see in the real world. Can we take a look at some real-world examples?. -Same goes for the fact that homs will be unlikely. -Not sure about the dropouts. Might be worth running without SNPs as a confounding factor. -How flexible is SVGen? Might be worth putting together a more realistic simulated data set. Any chance @MartonKN might be able to use it to cook up some realistic tumor data?. -I don’t recall having a `CollectBaseCallCoverage` type tool in beta—which tool are you thinking of? On a related note, it seems there is some demand to port `DepthOfCoverage` from GATK3. However, I’d prefer that we roll a CNV-specific version of the tool even if it does get ported. In any case, I think along with findings from the other issue, we should issue a quick PR for `CollectReadCounts` and go ahead to change the `CollectCounts` WDL task to call it—it’s for this very reason that the task is named generically! @sooheelee note that we may have to update the tutorials, etc. at some point, but perhaps the right time will be until all evaluations are more complete. Speaking of which, this PR should not delay getting the first round of automated evaluations up and running. Again, the whole point of those is to have a reproducible baseline metric against which we can easily experiment with and adopt these sorts of changes. Although these sorts of theoretical/simulated/thought experiments are clearly useful to us, unfortunately, they may not be as compelling to some of our users as demonstrable improvement seems on real data!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976:534,flexible,flexible,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976,1,['flexible'],['flexible']
Modifiability,"Interesting, that's somewhat disturbing news, I wonder if we're paying for ssd's without actually being able to use them... It's also possible there's a different setting that's configuring the ssd's to be used for shuffle output. . We should investigate this further and 1) see if setting spark.local.dir makes a performance difference 2) ask the dataproc team about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564:178,config,configuring,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564,1,['config'],['configuring']
Modifiability,"Interesting, this is the first time that I have seen a CentOS-7 install without zlib and uuid - even the minimal installations include it. Your options are:; - Install zlib and uuid (yum -y install zlib libuuid); - Ask your admins whether these packages are installed in some other location. For example, if the zlib library is at /opt/my_install/lib64/libz.so, then you can set your environment variable LD_LIBRARY_PATH; ```; export LD_LIBRARY_PATH=/opt/my_install/lib64:/opt/my_install/lib:$LD_LIBRARY_PATH; ```; - Wait for the next GenomicsDB binary jar to show up",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357067214:396,variab,variable,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357067214,1,['variab'],['variable']
Modifiability,"Intermittent failure at https://travis-ci.com/github/broadinstitute/gatk/jobs/297047618. ```; [TileDB::FileSystem] Error: hdfs: Cannot list contents of dir gs://hellbender-test-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c//chr20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:448,config,configuration,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,4,"['Config', 'config']","['ConfigurationUtil', 'configuration', 'configure']"
Modifiability,"IntervalWalker, VariantWalker enhancements, and GenomeLoc -> SimpleInterval migration in the engine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/297:30,enhance,enhancements,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/297,1,['enhance'],['enhancements']
Modifiability,Is it guaranteed that one of the configurations won't include any of the MQ0 regions? Why is that?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-364663550:33,config,configurations,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-364663550,1,['config'],['configurations']
Modifiability,Is there a way to have java load a config file as system properties on startup?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998:35,config,config,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998,1,['config'],['config']
Modifiability,"Is this the only `CommandLineException` which should be an `UserException`? If not, how is going to work the development of new exceptions in Barclay. For instance, in https://github.com/broadinstitute/barclay/pull/11 there is a new exception that I made for values out of range, which extends `BadArgumentValue`. I agree that this errors should be decoupled from the ones that are not, but in this case I think that this is already implemented:. * `UserException` are handled in the `mainEntry()`, distinguishing errors that comes from the user's side regarding some specifications in the tools/framework.; * `CommandLineException` are handled in `parseArgs()`, distinguishing errors that comes from the command line from the user side while parsing. I expect that any command line error that is not `CommandLineParserInternalException ` or `ShouldNeverReachHereException` comes from the user's side. The contract in Barclay says that are `CommandLineException` are _""Exceptions thrown by CommandLineParser implementations.""_, and I think that if other parts of the code (outside arg parsing) is throwing this exception is a bug that does not come from the user. I guess that this is the problematic part.; * Any other `Exception` is thrown in `Main.handleNonUserException()`, which may be caused by non-user exceptions. I propose that `CommandLineException` is handled as currently to separate ""errors that are the user's fault regarding input and/or assumptions"" (`UserException`), ""errors that are the user's fault while providing parameters to the command line"" (`CommandLineException`) and ""errors that are not the user's fault"" (other `Exception`s). Actually, this is reasonable because the exit status is different for any kind of errors in the current `Main`. The only problem that I see with this approach is the silently failing of a ""bug"" in tools/engine code, which can be rethrow easily in `CommandLineProgram.instanceMain`` as following:. ```java; public Object instanceMain(final Strin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161:286,extend,extends,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161,1,['extend'],['extends']
Modifiability,Issue 2968 collect allelic counts extend lw,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3203:34,extend,extend,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203,1,['extend'],['extend']
Modifiability,Issue spotted in GATK3 and fixed by pull-request https://github.com/broadinstitute/gsa-unstable/pull/1377. Original issue: https://github.com/broadinstitute/gsa-unstable/issues/1340. Needs to be ported to GATK4 as part of a larger fix involving the refactoring of AFCalculators (Issue TBA).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858:249,refactor,refactoring,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858,1,['refactor'],['refactoring']
Modifiability,"Issue: Integer overflow error caused Mutect2 v4.1.4.0 to generate a stats file with a negative number. Solution is to change the int data type to long. User report:. Hello, I've just adapted my pipeline to the new filtering strategies, while looking at the files I noticed that for a WGS run I obtained a stats file with a negative number:; [egrassi@occam biodiversa]>cat mutect/CRC1307LMO.vcf.gz.stats; statistic value; callable -1.538687311E9. Looking around about the meaning of the number I found https://gatkforums.broadinstitute.org/gatk/discussion/24496/regenerating-mutect2-stats-file, so I'm wondering if I should be worried by having a negative number of callable sites :/; What's more puzzling is that FilterMutectCalls after ran without any error. Before running mutect I used the usual best practices pipeline, then:; ; gatk Mutect2 -tumor CRC1307LMO -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa -I align/realigned_CRC1307LMO.bam -O mutect/CRC1307LMO.vcf.gz --germline-resource /archive/home/egrassi/bit/task/annotations/dataset/gnomad/af-only-gnomad.hg38.vcf.gz --f1r2-tar-gz mutect/CRC1307LMO_f1r2.tar.gz --independent-mates 2> mutect/CRC1307LMO.vcf.gz.log; ; gatk CalculateContamination -I mutect/CRC1307LMO.pileup.table -O mutect/CRC1307LMO.contamination.table --tumor-segmentation mutect/CRC1307LMO.tum.seg 2> mutect/CRC1307LMO.contamination.table.log; ; gatk LearnReadOrientationModel -I mutect/CRC1307LMO_f1r2.tar.gz -O mutect/CRC1307LMO_read-orientation-model.tar.gz 2> mutect/CRC1307LMO_read-orientation-model.tar.gz.log; ; gatk FilterMutectCalls -V mutect/CRC1307LMO.vcf.gz -O mutect/CRC1307LMO.filtered.vcf.gz -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa --stats mutect/CRC1307LMO.vcf.gz.stats --contamination-table mutect/CRC1307LMO.contamination.table --tumor-segmentation=mutect/CRC1307LMO.tum.seg --filtering-stats mutect/CRC1307LMO_filtering_stats.tsv --ob-priors mutect/CRC1307LMO_read-orientation-model.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6302:183,adapt,adapted,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6302,1,['adapt'],['adapted']
Modifiability,Issues that need to be addressed as part of this ticket:; - support for arguments defined in filter classes; - PluginManager -- to port or not to port?; - performance issues with searching the classpath; - separating out the readmetrics code from the code that applies filters,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773:111,Plugin,PluginManager,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6#issuecomment-69800773,1,['Plugin'],['PluginManager']
Modifiability,"It *looks like* it doesn't. I ran a job and looked at the ""environment"" tab in the Spark page for the job and didn't see ""spark.local.dir"" mentioned in the list of properties or the command line. Based on [the documentation](http://spark.apache.org/docs/latest/configuration.html), the setting must thus still be at its default value of ""/tmp"". . /tmp is on the HDD, the SSD one would have to be on /mnt/1/.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283210934:261,config,configuration,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283210934,1,['config'],['configuration']
Modifiability,"It can easely be extended to multiple sequences... it just happened that I didn't need it personally. . I took a quick pick to your branch... do you really care about the contig descriptions?.... I would say that this ""aligner"" class should not be responsible of compose the multi sequence fasta file but rather accept one as a constructor argument and the construction of the fasta is delegated back to the invoker code; in this new more general aligner the current single contig could be implemented as a public static method call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587:17,extend,extended,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587,1,['extend'],['extended']
Modifiability,"It could just be natural variability in the user's runtime environment, but it's worth doing some longer-running tests to be sure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-324453449:25,variab,variability,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-324453449,1,['variab'],['variability']
Modifiability,"It depends if the current implemented functionality is useful or not. If yes, then this is an enhancement (to ultimately have both functionalities). If not, then this is a bug (to replace the current functionality by the desired one). . Maybe Yossi could weigh in on use cases?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/263#issuecomment-94342834:94,enhance,enhancement,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/263#issuecomment-94342834,1,['enhance'],['enhancement']
Modifiability,"It depends why it fails the filter. One reason is consecutive indel elements, but consider for example:. ref: ACGTTTA; read: AC TTTTA; cigar: 2M1D1I4M. Especially in long-read technologies with a lot of indel errors it seems draconian to throw out reads where this happens once. And okay, I understand that an aligner could represent this as a G->T subsitution, but what about the same thing but with 2D followed by 2I? That strikes me as a much better cigar than calling it a DNP. In general, a bad cigar should mean either that the aligner is bad (in which case why are we filtering isolated reads and not just rejecting the entire BAM?) or the read is malformed. But consecutive indels in a technology with many indel errors is neither of these!. Anyway, I don't think there's a problem with allowing these reads in the GATK, and if there is the refactoring in this #6403 should let us fix any problem easily enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079:849,refactor,refactoring,849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079,1,['refactor'],['refactoring']
Modifiability,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6072:182,config,config,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072,7,['config'],['config']
Modifiability,"It looks like all the changes in my original commit with the raw GATK3 code (except for one file) got squashed out somehow, so I can't see just the changes from GATK3 anymore. I'll probably have to go back and re-commit those when you're ready to make this tractable to review. I'll wait to comment on #1 until that happens. As for a default plugin descriptor, I'd prefer not to take one unless its fully implemented, with tests. Plus, although we could develop it here, it should really live in the Barclay repo if its truly generic. More importantly, I'm not sure the plugins in this PR should be plugins at all. Historically, plugins have required a lot of test development and iteration because they have command line arguments (the plugin system is for extending the command line parser with discoverable, re-useable components that are shared across multiple tools, and need shared command line arguments). I haven't looked at the new ones closely, but I'm not sure they're a good fit. As for the files, it look like about 400MB (?) Thats pretty big - you should try to squeeze them down or target some existing files if you can.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008:342,plugin,plugin,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008,6,"['extend', 'plugin']","['extending', 'plugin', 'plugins']"
Modifiability,It looks like there's some minor refactoring in your new graph handler. I'm not a real stickler about sneaking that in but just want to check it was intentional.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518:33,refactor,refactoring,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518,1,['refactor'],['refactoring']
Modifiability,"It seems plausible to me, though, that the Google auth library may have been patched to perform checks that it wasn't performing previously. Maybe our project permissions have always been mis-configured :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762:192,config,configured,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762,1,['config'],['configured']
Modifiability,"It seems that there are a lot of soft clips that aren't bacterial reads.; What's your mean insert size? I've seen lots of aberrant soft clips when; the insert size is small and Picard doesn't catch adapter sequences with; multiple mismatches. Does the Picard percent adapter in alignment summary; metrics seem high? I've also seen lots of soft clips when the chimera rate; is high, sometimes because of bad sample extraction. What's the percent; chimeras in your alignment summary metrics? 5% is bad and I've seen up to; 15%, but that was an FFPE tumor sample. On Mon, Mar 25, 2019 at 8:48 PM jjfarrell <notifications@github.com> wrote:. > When the --dontUseSoftCliiped flag is used, the GQ=0 is much lower- N=1355; > for '0/0' calls.; >; > zcat; > A-ADC-AD004288-BL-NCR-15AD82285.hg38.realign.bqsr.dontUseSoftclipped.g.vcf.gz; > |tr '\t' '\n'|grep '0/0'|tr ':' '\t'|cut -f2,3|awk '$2 == ""0"" {print; > $0}'|cut -f1|sort -n|uniq -c; > 1355 0; > 6 0,0,0; > 7 0,0,0,0; > 602 1; > 537 2; > 520 3; > 595 4; > 441 5; > 511 6; > 583 7; > 701 8; > 403 9; > 468 10; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476431178>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdA_gZKYn3vuqNDvvDadvM9tgzQqGks5vaW5IgaJpZM4YxgEF>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476654990:198,adapt,adapter,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476654990,2,['adapt'],['adapter']
Modifiability,"It should be fairly easy to create a read transformer plugin; pretty much follow the pattern of read filter plugins: clone and modify GATKReadFilterPluginDescriptor, and add an instance of the new descriptor to the list of plugins passed in to the command line parser in (appropriate) tool base classes. And of course tests!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245968764:54,plugin,plugin,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245968764,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"It should probably be a `ReadWalker`, yes -- but if it can't be one for some reason, it is possible to extend `GATKTool` directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222225456:103,extend,extend,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1864#issuecomment-222225456,1,['extend'],['extend']
Modifiability,"It turns out I was mistaken that setting the environment variables fixes the problem (stupid error on my part). It's possible the BaseTest message is unrelated. I haven't tested this branch out yet, but building from the commit immediately before the update works. I am going to try the next version to see if it helps. Edit: #3594 does not fix the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419:57,variab,variables,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419,1,['variab'],['variables']
Modifiability,It will be useful if this is added to the configuration system (#2368 and #3081).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2155#issuecomment-316078724:42,config,configuration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2155#issuecomment-316078724,1,['config'],['configuration']
Modifiability,It will be very useful to have an abstract class for the plugin arguments (as I did for the read filters plugin in #2355) to be able for downstream projects to change default values or hide arguments to the final user.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921:57,plugin,plugin,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921,2,['plugin'],['plugin']
Modifiability,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:313,config,configuration,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,5,['config'],"['configuration', 'configure', 'configured']"
Modifiability,It would be good for progress meter to be more flexible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-575773895:47,flexible,flexible,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-575773895,1,['flexible'],['flexible']
Modifiability,It's a little disturbing that we need to use different classpaths for the executor vs. the driver...is this symptomatic of a deeper problem in our configuration? Should we be worried?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190816499:147,config,configuration,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1518#issuecomment-190816499,1,['config'],['configuration']
Modifiability,"It's looking like we might have to fix the issues with NIO here after all @tomwhite @jean-philippe-martin, as @lbergelson has been unable to get this working reasonably with the GCS adapter (it runs, but veeeerrryyy slowly).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271691417:182,adapt,adapter,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271691417,1,['adapt'],['adapter']
Modifiability,"It's not a surprise that a local disk is faster than a remote one, but the; magnitude of the difference is a lot more than I would expect. I remember; at the time using direct GCS access to get the best possible performance in; the bit I was working on, but I don't remember exactly how much of a; difference it made. From my desktop it takes 2m25s to download the whole file, so the ~6min; difference seems really excessive, something is broken. One thing to look; into is whether the sharding is working correctly (are we getting the; correct number of parallel downloads?). Presumably this code is using the HDFS adapter. It'll be interesting to; compare vs the NIO version (and then the optimized NIO version once I write; it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600:616,adapt,adapter,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755#issuecomment-213094600,1,['adapt'],['adapter']
Modifiability,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1303,plugin,plugin,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,1,['plugin'],['plugin']
Modifiability,"It's not perfect, but one thing we can do is this:. ``` java; /**; * SerializableFunction -> PTransform representing a map.; */; public class Map {; public static <A,B> PTransform<PCollection<? extends A>, PCollection<B>> of(SerializableFunction<A, B> f, B sample) {; return ParDo.of(new DoFn<A, B>() {; @Override; public void processElement(ProcessContext c) throws Exception {; c.output(f.apply(c.element()));; }; @Override; protected TypeDescriptor getOutputTypeDescriptor() {; return TypeDescriptor.of(sample.getClass());; }; });; }; }; ```. You use it like this:. ``` java; PCollection<Integer> p1 = pipeline; .apply(Create.of(Arrays.asList(1, 2, 3))); .apply(Map.of(x -> x + 1, 1);; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122440966:194,extend,extends,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122440966,1,['extend'],['extends']
Modifiability,"It's weird, usually java should output an error message if it runs out of memory. The exception would be when java is assigned so much memory that the SYSTEM kills it instead of java killing itself. ; You could try adding `dmesg | tail -100` to your wdl after m2 runs to see if there are any messages from the OOMkiller. . What's your total available memory on the machine and your -Xmx setting? You typically need to leave some memory room for the OS and other native sofware. (although by default our pipelines SHOULD have that configured correctly.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414:530,config,configured,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414,1,['config'],['configured']
Modifiability,Its use in `ValidateBasicSomaticShortMutations` seems limited to the integration test. Can I rewrite the test to do without `AnnotatedInterval` and call it a day?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913:93,rewrite,rewrite,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913,1,['rewrite'],['rewrite']
Modifiability,"I’ve also revisited this work for MalariaGEN, additionally including further cleanup of the canonical part of the WDLs (mostly low hanging fruit like adding structs, which help a lot for cutting down parameter cruft on Terra). For ease of iteration, this work broke things up into 3 pushes of a button: 1) data collection, 2) preclustering (done in a relatively modular way, so you can swap in whatever clustering script you like, as long as it outputs hard/soft responsibilities) +random selection of training cohorts, and 3) cohort mode + scattered case mode on all clusters. But no reason we couldn’t link some of those up. No problem running 16k samples, with 6 clusters and 300 training samples per, but also note I was only running a single genomic shard containing CNVs of interest for this use case. (I did manage to break Terra for a few days when I tried to attach collected counts to the data model in what I would’ve thought would be a relatively trivial way, but that’s another matter.). I’ve shared some version of these WDLs over Slack previously, but happy to also open up a branch here. I think some of this work may be replicated in GATK-SV and I’m also not sure what we want to make canonical. Surely most users will run only a single cluster. But from the perspective of our MalariaGEN collaborators, the more of what I put together for them being made canonical, the better, as this will ease future maintainability. But will leave it up to other current stakeholders.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360:1421,maintainab,maintainability,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360,1,['maintainab'],['maintainability']
Modifiability,"JDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4310,Config,ConfigFactory,4310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,Jexl Support for Extended Attributes Doesn't Work with Lists,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4631:17,Extend,Extended,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4631,1,['Extend'],['Extended']
Modifiability,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:65,refactor,refactored,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['refactor'],['refactored']
Modifiability,"Just an idea: it will be nice to propagate the configuration from `Main` to the tools, and obtain from it the packages/classes to include in the command line tools (this is one of the things that I implemented in #2322).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309680944:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309680944,1,['config'],['configuration']
Modifiability,"Just checked and this is not new behavior. I was afraid this would be an unintended consequence of the DRAGEN branch but that doesn't seem to be the case. Looking a little closer into the code I actually think the only difference this makes is explicitly for the contamination and nothing else. There are a few layers where we filter reads before the annotations are called (filtered by QC before active region determination, MQ/etc-filtered/un-softclipped/overlap-score-adjusted before assembly, filtered based on poor concordance with existing haplotypes, reads are realigned and re-filtered by position, and again filtered due to contamination downsampling). It seems to me that the two likelihoods arrays fed to the `prepareReadAlleleLikelihoodsForAnnotation()` have had all of the above steps applied to them EXCEPT for the contamination downsampling step applied to them looking thorough the code in the HaplotypeCaller. I guess the question is whether there is a strong reason to make the annotations with/without the contamination adjustment... I think the argument `--use-filtered-reads-for-annotations` is misleadingly labeled though the description looks correct to me since it really does only seem to make a difference for the contamination step. . There is another wrinkle to all of this. For DRAGEN-GATK we evaluated calculating the overlaps/annotations exactly how they do it in DRAGEN and decided against it. In DRAGEN they retain the original reads from the bam (i.e. no realignment/no score adjustments/etc...) and use THOSE for annotation and for genotyping. I would have to pick through their debug output to tell just which subsets get used for genotyping (for example they still use non-haplotype-matching reads for FRD and BQD but I don't remember if those are also used for calculating annotations).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324:311,layers,layers,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324,1,['layers'],['layers']
Modifiability,"Just for posterity:. jhess 1:55 PM; just to clarify: what is the logic behind approximating σ_(τ/min/maj) ≈ (post90 - post10)/2? (edited) ; 1:55; what is the scale factor of 1/2 for?; 1:58; one other thing — how come σ_(min/maj) is the sum of the total CR segment’s variance (i.e. σ_τ) and the allelic segment’s variance?; 2:00; this would imply that the allelic segments are actually a sum of the random variables corresponding to the allelic and total segmentation, which I’m not sure is the case?. slee 5:32 PM; Sorry, just now seeing your questions!; 5:33; The scale factor of 1/2 is pretty arbitrary. Just trying to give an estimate of posterior width when the credible interval might be skewed. A better approach would be to refit posteriors with Gaussians/Betas as mentioned previously.; 5:35; However, I'm not actually convinced that these credible intervals are what we want to pass to the sigmas. As I also mentioned above, if sigma.tau is supposed to be a global quantity, probably the posterior median of the parameter that controls the global variance (given in the .cr.params file) might be a better thing to use. However, I never got a straight answer from anybody about whether this was a segment-level or global quantity---any idea?. slee 5:41 PM; As for using the sum of the CR variance and the MAF variance, you're right---we should be propagating error for the product of the two random variables. Not sure what I was thinking...probably just a brain fart. Not sure if it will make a difference for ABSOLUTE, but thanks for catching that!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494:405,variab,variables,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494,2,['variab'],['variables']
Modifiability,Just noting for posterity that removing build strings in the conda YML seems to have improved portability to different OS environments: https://gatk.broadinstitute.org/hc/en-us/community/posts/360061666671-Broken-conda-env-create-n-gatk-f-gatkcondaenv-yml,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774:94,portab,portability,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774,1,['portab'],['portability']
Modifiability,"Just occured to me--Are we saying the application of a _germline workflow_ extends to the creation of a PoN consisting of germline normals for the _somatic workflow_?. If so, we should reorganize the directory structure of the CNV scripts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3163#issuecomment-310873860:75,extend,extends,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163#issuecomment-310873860,1,['extend'],['extends']
Modifiability,Just some minor instances that slipped through during the rewrite.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4119:58,rewrite,rewrite,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4119,1,['rewrite'],['rewrite']
Modifiability,"Just to add some clarity around the the granularity of these log levels: the setLoggingLevel API is setting the level of all of the loggers that inherit from the caller's context configuration, which as I understand it, is essentially global for Hellbender, since it currently has only one configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122898121:145,inherit,inherit,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603#issuecomment-122898121,3,"['config', 'inherit']","['configuration', 'inherit']"
Modifiability,"K Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/goo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3990,Config,ConfigFactory,3990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,Karthik had suggested setting the environment variable TILEDB_DISABLE_FILE_LOCKING=1 for NFS (see https://github.com/broadinstitute/gatk/issues/4753) -- maybe give that a shot?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453600476:46,variab,variable,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453600476,1,['variab'],['variable']
Modifiability,Keep variable names/strings consistent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/989:5,variab,variable,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/989,1,['variab'],['variable']
Modifiability,"L; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3931,Config,ConfigFactory,3931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"LAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3921,Config,ConfigFactory,3921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,LIBS/LocusWalker refactoring and overlapping read-pairs handling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041:17,refactor,refactoring,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041,1,['refactor'],['refactoring']
Modifiability,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8525:185,refactor,refactoring,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525,1,['refactor'],['refactoring']
Modifiability,"Lee, just letting you know I've tagged you in a forum question. ---; The oncotated maf output includes many rejected mutations (using the configuration from the public spaces). This is bad practice. The unfiltered VCF (or preferably a tsv) should have these sites but we should not be annotating them or putting them in final outputs. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11440/m2-gatk4-oncotated-maf-output-includes-rejected-mutations/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4421:138,config,configuration,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4421,1,['config'],['configuration']
Modifiability,"Let us take an example. Suppose, we configure GenomicsDB with 3 column partitions - 0-10, 10-100, 100-300 and want to run GenomicsDBImport tool with an interval [0,100]. In this case, the import tool will only write contigs between 0 and 100 into first two partitions (according to the loader JSON file). Is this what you had in mind? The command line will look like:; $ gatk-launch GenomicsDBImport -L 0-100 --loaderJSONfile loader.json --streamIdJSONFile stream.json. This can definitely be done. However, the client still needs to know the column partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931:36,config,configure,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931,1,['config'],['configure']
Modifiability,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:44,config,configuration,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,3,['config'],"['config', 'configuration']"
Modifiability,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:195,flexible,flexible,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['flexible'],['flexible']
Modifiability,"Let's expand the existing `ReadsPipelineSpark` in both directions, so that it extends from `BWA` to the `HaplotypeCaller`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1646#issuecomment-202432982:78,extend,extends,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1646#issuecomment-202432982,1,['extend'],['extends']
Modifiability,"Let's hear what others say, but I think I would strongly prefer to simply take over VariantEval in another repo if this was something you'd consider. I'd likely do much of what you propose anyway (certainly WRT testing); however, perhaps not the microscope we went through with the core GATK changes earlier. On plugins: I like what seems to be shaping up w/ Barclay. I carried over the Stratifier and Evaluator as plugins because it seems like it would make sense to allow tools to provide extensions (VariantEval, our tool, does). If I took this PR a step further, I would have migrated many arguments currently top-level on VariantEval into the plugins themselves (a good feature in Barclay). As an aside: I dont think VariantAnnotator is migrated yet, but we have many GATK3 plugins related to annotation, and hope that tool retains Annotator plugins when it get migrated. My impressions of barclay are probably a little out of date. I agree the main argument parsing framework is pretty robust. Specifically on plugins, it seems a little less so, or at least there are not many tools I visibly see exercising that part of the code. For example, there really should be a default implmentation or base class between Barclay's plugins and ReadFilter plugins. I'm guessing if more tools in GATK4 were using plugins this would have happened. I created something like this for VariantEval, and without a ton of work that could probably get generalized; however, doing so would throw a lot higher bar on me and as noted above I'm trying to take on less, not more at the moment. If we do take over VariantEval, I'm certainly happy to try to contribute code and experiences to improve the core, through more targeted PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501:312,plugin,plugins,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501,9,['plugin'],['plugins']
Modifiability,"Let's see if we can get Azure Blob support working in GATK using the existing azure-storage-blob-nio NIO plugin:. https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/storage/azure-storage-blob-nio. https://search.maven.org/artifact/com.azure/azure-storage-blob-nio/12.0.0-beta.8/jar. Currently auth info needs to be manually passed in, unlike with the Google NIO plugin, but I've opened a feature request to get the auth info automatically from the environment:. https://github.com/Azure/azure-sdk-for-java/issues/23653",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7425:105,plugin,plugin,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7425,2,['plugin'],['plugin']
Modifiability,Limiting scattering size in ingest to keep beta customers under quota. Successful run on NHGRI AnVIL dataset: https://app.terra.bio/#workspaces/gvs-dev/NHGRI_AnVIL_3K%20hatcher/job_history/a9c2a81b-d81c-4f7a-a433-4096ccc7b579. Quota behavior during successful run:; ![Screenshot 2023-02-08 at 3 37 36 PM](https://user-images.githubusercontent.com/110987709/217656881-793e8a87-3e8c-40fc-90a6-6f640ff1c976.png). Next successful run after minor refactoring: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/8a34477f-af3b-4e19-8184-862ed1c2cba3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8193:442,refactor,refactoring,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8193,1,['refactor'],['refactoring']
Modifiability,"LoadData `maxRetries` parameterized, default increased [VS-383]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7791:22,parameteriz,parameterized,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7791,1,['parameteriz'],['parameterized']
Modifiability,Look into adaptive pruning in GATK 4.2.0.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:10,adapt,adaptive,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['adapt'],['adaptive']
Modifiability,"Looking at MetricFile and with it heavy use of Reflexion looks a bit nasty, if there is a better alternative the better. I guess a refactoring of MetricFile would use annotations to allow one to customize output variable name... force one to have those not-so-good looking CAPITAL_FIELD_NAMES for the sake of it is harsh. Don't understand why One has to commit to ; particular type for all histograms either. . Anyway, only if the use of MetricFile is an overkill I would ask you to do your custom one (i.e if it can be done in a few lines of code).... probably not the case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-336521031:131,refactor,refactoring,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-336521031,2,"['refactor', 'variab']","['refactoring', 'variable']"
Modifiability,"Looking back into this PR... at some point you are using 'N' to pad what seem to be gaps on the read sequence. Although the end result would be the same perhaps is better to be more explicit and just use '-' instead. In that case my suggestion of using `Nucleotide.intersect` wouldn't cover for the '-' character so you need a explicity ""&&"" or ""II"". When you compare the cost of each different alignment the gap-open and gap-ext are ignored (you only look at base call mismatches). I wonder whether it would be more correct to actually take them in consideration... so imagine that there is no gaps in the original alignment what-soever and that adding a 1bp gap decreases the number of mis-matches by just 1 which is typically Q30 increase in the Lk but the gap itself default penalty is Q45 so can one say that that read wouldn't still support the reference over a 1-bp gap alternative? . Example with a 2-bp gap making the trick:; ```; Ref: ....GCATGTGATATATATATATATATATATATACACACAC....; Read: ....GCATGTG--ATATATATATATATATATATAC <end-of-the-read>; ```. That could happen in STRs with impurities... but if the original alignment did not added itself the gap to reduce the number of mismatches is because precisely due to the added cost of the gap-open and necessary extends that we would be ignoring here. This is all hypothetical until some one quantifies how often this might occur ... so I'm happy to keep ignoring the gaps for now until we get a report on a real-live dataset that would benefit of such a change or some enthusiastic dsde-methods member investigates this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5172#issuecomment-420743269:1270,extend,extends,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5172#issuecomment-420743269,1,['extend'],['extends']
Modifiability,"Looks great!. One quick note: I don't get the idea behind `Poisson` -- shouldn't we simply use negative binomials w/ modeled `mu_sj` and `alpha_sj`, evaluated at observed counts (`tt.arange(min_count, max_count + 1)`), and weighted with the number bins for each count (`_hist_sjm`)? i.e. if one observes an empirical distribution `P_obs(x)` rather than `x` draws, then the appropriate max likelihood objective function is `\sum_x P_obs(x) log P_model(x | \theta)`. Perhaps this is exactly what you've done and I don't get it. Another quick note: what I had in mind was _either_ modeling `mu_sj` at quantized ploidy states, _or_ let the ploidy state be unrestricted w/ a penalty via. a Bernoulli process (possibly w/ different per-contig penalties to account for e.g. higher rate of X/Y loss). We have enough samples in the cohort to select the quantized model (and those samples pin down the per-contig biases `b_j`). The samples that do not conform to quantized ploidy states can then choose whatever (variable) ploidy state they wish by paying a (hefty) price. We would also need to mask contigs that have variable ploidy calls from gCNV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536:1003,variab,variable,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536,2,['variab'],['variable']
Modifiability,"Lots of refactoring was done for the Segmenter classes in #6499. At least for segmentation, all use cases (CR-only, AF-only, CR+AF, single-sample, multi-sample) now go through `MultisampleMultidimensionalKernelSegmenter`. `AlleleFractionKernelSegmenter` and `CopyRatioKernelSegmenter` classes still exist, but both simply call the `MultisampleMultidimensionalKernelSegmenter` class; this was done so preexisting tests for those two classes could be reused. I'm fine with calling this done. We can always open a new issue in the unlikely event we refactor the modelling code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908:8,refactor,refactoring,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908,2,['refactor'],"['refactor', 'refactoring']"
Modifiability,Low-hanging gcnvkernel refactoring and code improvement,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4058:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4058,1,['refactor'],['refactoring']
Modifiability,"MTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 14:39:24.083 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 14:39:24.083 INFO DetermineGermlineContigPloidy - Initializing engine; 14:39:26.111 INFO DetermineGermlineContigPloidy - Shutting down engine; [May 26, 2019 2:39:26 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1511522304; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python -c import gcnvkernel. Stdout:; Stderr: Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:2849,config,configdefaults,2849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configdefaults']
Modifiability,"M_READER_FACTORY :; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4014,Config,ConfigFactory,4014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573:75,refactor,refactored,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573,1,['refactor'],['refactored']
Modifiability,Make HaplotypeCallerSpark extend AssemblyRegionWalkerSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5386:26,extend,extend,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5386,1,['extend'],['extend']
Modifiability,Make ReadsSparkSource Sorting configurable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4859:30,config,configurable,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4859,1,['config'],['configurable']
Modifiability,Make RobustBrentSolver more flexible,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2971:28,flexible,flexible,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971,1,['flexible'],['flexible']
Modifiability,Make adaptive pruner smarter in complex graphs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520:5,adapt,adaptive,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520,1,['adapt'],['adaptive']
Modifiability,Make annotations a barclay plugin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287:27,plugin,plugin,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287,1,['plugin'],['plugin']
Modifiability,Make several Funcotator methods and fields protected so it is easiest to extend the tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8166:73,extend,extend,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8166,1,['extend'],['extend']
Modifiability,Make sure to run the following to be able to push to GAR from your machine:; ```; gcloud auth configure-docker us-central1-docker.pkg.dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8783:94,config,configure-docker,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8783,1,['config'],['configure-docker']
Modifiability,"Mappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may be experimented with, but it seems unlikely in the near future (their own quirkiness, lack of API for JNI, etc); it seems more and more likely to me that eventually it's inevitable to have a custom alignment m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1740,config,configuration,1740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,2,['config'],['configuration']
Modifiability,MarkDuplicates performance optimizations. This includes:; - a small refactor of the original MarkDuplicatesDataflow so that most of the core code can be reused directly in the optimized version; - helper classes to keep the optimized code organized and dataflow-like; - reworked input for performance; - the optimized code spends a lot less time moving data across machines; - performance bugfixes:; - UUID generation; - format conversion,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/863:68,refactor,refactor,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/863,1,['refactor'],['refactor']
Modifiability,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:400,plugin,plugin,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,1,['plugin'],['plugin']
Modifiability,MendelianViolation class needs to be refactored,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5034:37,refactor,refactored,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5034,1,['refactor'],['refactored']
Modifiability,Migrate FuncotateSegments to use Owner for its configuration files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5963:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963,1,['config'],['configuration']
Modifiability,Migrate GATK engine to new configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3081:27,config,configuration,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3081,1,['config'],['configuration']
Modifiability,Minor enhancements to match VariantRecalibrator tweaks,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2780#issuecomment-309635824:6,enhance,enhancements,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2780#issuecomment-309635824,1,['enhance'],['enhancements']
Modifiability,Mock up an example configuration setup using Owner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:19,config,configuration,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,1,['config'],['configuration']
Modifiability,Modify the PythonScriptExecutor to allow environment variables.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6256:53,variab,variables,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6256,1,['variab'],['variables']
Modifiability,"Modifying what I wrote earlier, got confused with another issue. I am not familiar with Lustre and Lustre configuration. Did the excessive file locking from Lustre(FUTEX_WAIT_PRIVATE?) go away with `--genomicsdb-shared-posixfs-optimizations`? . Is there anyway to configure Lustre buffer sizes for writing? If not, can you try setting environment variable TILEDB_UPLOAD_BUFFER_SIZE to something like 5242880(5M) and try `GenomicsDBImport`? Does it help with performance? Is the amount of file locking lower than before?. If the performance is still not acceptable... What version of gatk are you using? Can you use the latest gatk and try using the `--bypass-feature-reader` option with `GenomicsDBImport`? Does this help with performance?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554:106,config,configuration,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554,3,"['config', 'variab']","['configuration', 'configure', 'variable']"
Modifiability,"More concrete runtime numbers are forthcoming but the profiler shows the following. Note that these numbers are generated on this branch hanging off of master ca. November and does not include the other optimizations to this part of the code that have been made recently.; Before:; <img width=""1068"" alt=""screen shot 2019-01-25 at 2 59 53 pm"" src=""https://user-images.githubusercontent.com/16102845/51772199-6c541880-20b9-11e9-8823-7249e7f4d874.png"">; ; After: ; <img width=""1092"" alt=""screen shot 2019-01-25 at 3 00 05 pm"" src=""https://user-images.githubusercontent.com/16102845/51772174-57778500-20b9-11e9-9d74-9f93d76358a0.png"">. Beyond the tests that I have written explicitly to illuminate discrepancies, I have run HaplotypeCaller in GVCF mode on the input bams in large and explicitly checked for places where the refactored method mismatches from the previous code and it appears to be matching over a wide range of cases, there probably could be more. . Resolves #5488",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5607:821,refactor,refactored,821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607,1,['refactor'],['refactored']
Modifiability,More flexible integer copy number transition priors in gCNV,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2998:5,flexible,flexible,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2998,1,['flexible'],['flexible']
Modifiability,More flexible matching of dbSNP variants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6626:5,flexible,flexible,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6626,1,['flexible'],['flexible']
Modifiability,More refactoring PDHCE and preparing for joint detection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8467:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8467,2,['refactor'],['refactoring']
Modifiability,More refactoring of Mark duplicates and pipeline hookup,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/770:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/770,1,['refactor'],['refactoring']
Modifiability,"Most ReadWalkers apply the WellformedReadFilter, but their spark equivalents; were not doing so. This creates an inherited method GATKSparkTool.makeReadFilter(); that defaults to the WellformedReadFilter and gets automatically applied to the; RDD of reads returned from GATKSparkTool.getReads(). Only BaseRecalibratorSpark needed to override this method to apply a custom; read filter in order to match the walker filtering settings. Resolves #1158",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159:113,inherit,inherited,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159,1,['inherit'],['inherited']
Modifiability,"Most of these changes are to support automated evaluation of GATK CNV. - Updates `AnnotatedIntervals` (formerly `SimpleAnnotatedGenomicRegion`) to use the tribble framework for reading. Writing is done in a way that should be concordant with a future tribble writing framework, as per discussion with @droazen.; - Changes to `XsvLocatableTableCodec` to support usage of arbitrary config files. This cannot be done when using tribble features in the CLI. Already reviewed with @jonn-smith . Support for SAM File headers and comments is included.; - *Note:* The reading of `AnnotatedIntervals` cannot be done automatically on the command line, unless the config file is a sibling. The tools below do not even attempt this, since the use cases involved will never have a sibling config file.; - Created a default config file in the jar file resources to read tsvs with locatable fields from the CNV collection files. This is much less strict than the framework used by the CNV tools. The reader will accept any columns (or subset of the columns). CLIs (both experimental quality): ; - `TagGermlineEvents` is a simple tool that attempts to identify events in a tumor seg file that correspond to a germline events. ; - This is done purely with concordance on the breakpoints of the events (within some padding). ; - Input germline segments must have calls. ; - If a germline call is broken into multiple segments, this tool will handle that appropriately (ditto if there are multiple tumor segments overlapping the germline call). - `MergeAnnotatedRegions` will merge all overlapping regions and resolve annotation value conflicts. Closes #3995",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276:380,config,config,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276,4,['config'],['config']
Modifiability,"Most spark tools use the one in GATKSparkTool, but some have some special requirements that make it not work for them. They have to specify different sequence dictionaries or something like that in a way that isn't exposed. Maybe something could be refactored there, but they needed manually adjusting to match the new behavior because of their special handling of the writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389:249,refactor,refactored,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389,1,['refactor'],['refactored']
Modifiability,Most things were addressed here. A bunch of follow on tickets created to address more complicated refactorings that we don't have time to hit now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-368648535:98,refactor,refactorings,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-368648535,1,['refactor'],['refactorings']
Modifiability,Move ReadFilter plugin integration up to GATKTool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2218:16,plugin,plugin,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218,1,['plugin'],['plugin']
Modifiability,Move read filter plugin initialization to GATKTool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2175:17,plugin,plugin,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2175,1,['plugin'],['plugin']
Modifiability,"Moved R dependencies to conda environment, cleaned up R/python dependencies, and updated base Docker/Travis configuration.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026:108,config,configuration,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026,1,['config'],['configuration']
Modifiability,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7224:463,config,configuring,463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224,1,['config'],['configuring']
Modifiability,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:1118,config,configuration,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,1,['config'],['configuration']
Modifiability,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the –bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf  . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFO  Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:8,Adapt,Adaptive,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['Adapt'],['Adaptive']
Modifiability,Mutect2 WDL: Add Funcotator Adjustable diskspace and Memory variables,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680:60,variab,variables,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680,1,['variab'],['variables']
Modifiability,Mutect2 WDL: Funcotate task has useless variables - no way to increase memory for Funcotate task only,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:40,variab,variables,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,1,['variab'],['variables']
Modifiability,"Mutect3 dataset enhancements: optional truth VCF for labels, seq error likelihood annotation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7975:16,enhance,enhancements,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7975,1,['enhance'],['enhancements']
Modifiability,"My $0.02:. 1. In general it's ok with me to not provide a template for WDLs in the GATK repo as long as you guys help us (ie @bshifaw) produce appropriate templates to include in the gatk-workflows repo and in FireCloud. . 2. Re: Picard tools, going forward they should be invoked from the GATK jar by default. Among other benefits, that will reduce support entropy wrt possible combination of versions of tools people might be using. 3. I like the idea of focusing on the auto-generated wrappers for improvements like the string variable for adding arbitrary extra args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358488159:530,variab,variable,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358488159,1,['variab'],['variable']
Modifiability,"My 2 cents: actually the argset strategy would be nice also for plugins. For example, in ReadFilters it might allow to specify ""recommended"" filters but not necessary; and in the annotations to convert the groups to a argument set. +1 to the argset for many use-cases and not only this one!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758:64,plugin,plugins,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758,1,['plugin'],['plugins']
Modifiability,My current plan is to rewrite all the bam/sam files to update them to 1.5.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/758#issuecomment-125371895:22,rewrite,rewrite,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/758#issuecomment-125371895,1,['rewrite'],['rewrite']
Modifiability,"My lab has a variety of custom walkers. Many subclass MultiVariantWalkerGroupedOnStart, which is a useful iteration pattern. We tend to scatter/gather on our cluster, where each job is given an interval set. When doing this, handling variant spanning those borders is critical. We just had an issue around this, which stems from MultiVariantWalkerGroupedOnStart and the fact that ignoreIntervalsOutsideStart defaults to false. For our usage, we almost never want this to be true, and it's a really subtle problem if the user doesnt remember to set this. So my question is: is there a best-practice way for subclasses to override / remove or set default on inherited arguments? Granted, individual walkers could simply change the value of ignoreIntervalsOutsideStart during the init phase, but I dont like that solution since it basically leaves an useless/ignored argument. . thanks in advance for any ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7287:656,inherit,inherited,656,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7287,1,['inherit'],['inherited']
Modifiability,"My recollection is that this is a use case we never put any priority on so there's no test in the GATK suite for access to private files. There should be, of course. The feature is there and (at least locally) it worked when I tried it. NIO does not use the API_KEY, it uses default credentials. Those are environment variables that are set by the `gcloud` command or pre-set for you in the case of virtual machines on Google. There are two cases: local execution and Spark. . I just tested local execution and it worked fine for me:. ```; $ ./gatk-launch PrintReads -L Broad.human.exome.b37.small.interval_list -I gs://jpmartin-private/bench/WGS-G94982-NA12878.bam -O t_gcs.bam; ```. this command worked even though (unless I'm mistaken) neither the bucket nor the file are public. One challenge however is that the way to set default credentials has changed recently. Calling `gcloud auth login` used to be enough but now we have to call (IIRC) `gcloud auth application-default login`. For Spark, the default credentials are set as whoever owns the dataproc environment that's used to run the show. So it should be set so it has access to the buckets necessary. NIO has mechanisms for accessing buckets that belong to someone other than who is running the Spark job, but they are not hooked into GATK yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658:318,variab,variables,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658,1,['variab'],['variables']
Modifiability,"My worry about camel case is that it trips up people a lot, especially those whose native language doesn't have a concept of case (like Chinese). . Maybe long arguments with lots of dashes need to be refactored to have fewer... can you give some examples?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323747625:200,refactor,refactored,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323747625,1,['refactor'],['refactored']
Modifiability,"NAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3380,Config,ConfigFactory,3380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,NFO GenomicsDBImport - HTSJDK Defaults.CREATE_MD5 : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4300,Config,ConfigFactory,4300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3748,Config,ConfigFactory,3748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2810,Config,ConfigFactory,2810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"NVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3832,Config,ConfigFactory,3832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,Namespaced arguments in barclay are something we've talked about before that could help with this. So multiple argument collections / plugins objects could declare the same argument and it would be de-ambiguated by the full name of the plugin/collection. Something like `--ReadNameFilter.invert --MappingQualityFilter.invert`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502173995:134,plugin,plugins,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502173995,2,['plugin'],"['plugin', 'plugins']"
Modifiability,Need to create a tool that allows a user to import / create a simple delimited data source (i.e. from a given CSV / TSV file). See how Oncotator structured its config files for insights on how to do this. It may be possible to simply reuse that config file format.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3785:160,config,config,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3785,2,['config'],['config']
Modifiability,"Needs to:. -support overriding config settings via a simple mechanism (like providing an override config file); -use a simple, easy-to-edit file format like Java Properties (name = value); -be widely used in the Java community & well-maintained.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078:31,config,config,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078,2,['config'],['config']
Modifiability,"New implementation of `SlidingWindowWalker` with some ideas from the discussion in #1528. The thinks that are requested in #1198 still holds, but now it is more general: padding option is added and construction of windows are done by interval. The code contain a lot of TODO because it relies on changes implemented in #1567, and because it is suppose to be a walker over `ReadWindow` instead of `SimpleInterval`+`ReadsContext` if reads are available. I think that with these changes it could be general to be extended by `ReadWindowWalker` and by users that needs a different way of ""slide"" over intervals.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708:510,extend,extended,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708,1,['extend'],['extended']
Modifiability,"New tool aiming to call all types of precise variants detectable by long read alignments (not fully functioning yet in the sense that not all types of variants are detected yet&mdash;to be handled by later PRs in this series).; This new tool splits the input long reads by scanning their alignment characteristics (number of alignments, if strand switch is involved, if mapped to the same chromosome, if have equally good alignment configurations based on the scoring tool, etc), and send them down different code path/logic units for variant type inference and VCF output.; This PR would only deal with simple INSDEL, for long reads having exactly 2 alignments (no other equally good alignment configuration) mapped to the same chromosome without strand switch or order switch (translocation or large tandem duplications), because we already have this type of variant covered in master. __UPDATE__; See updated roadmap in #2703. NEEDS TO WAIT UNTIL PART 1 IS IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3456:432,config,configurations,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3456,2,['config'],"['configuration', 'configurations']"
Modifiability,"No problems. The walkers have no built in parallelism so there's no problem with using state. It makes it harder to adapt to spark, but that's probably not a big deal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4447#issuecomment-368091726:116,adapt,adapt,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4447#issuecomment-368091726,1,['adapt'],['adapt']
Modifiability,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:287,adapt,adapt,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981,1,['adapt'],['adapt']
Modifiability,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:881,variab,variable,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,1,['variab'],['variable']
Modifiability,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7649:1385,refactor,refactors,1385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649,1,['refactor'],['refactors']
Modifiability,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:278,refactor,refactor,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146,2,"['refactor', 'variab']","['refactor', 'variable']"
Modifiability,"Not sure if this is related, but @chandrans and I had some trouble with the dataproc launcher yesterday (didn't recognize some yarn argument). Changing the ""image"" setting in the cluster config solved it, afaiu.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2230#issuecomment-278729124:187,config,config,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2230#issuecomment-278729124,1,['config'],['config']
Modifiability,"Note separate method configuration, but uses the same WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811803:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811803,1,['config'],['configuration']
Modifiability,"Note that before this is merged, we'll need to do a datasource release in which the following property is added to the gencode config files:. ```; # Required field for GENCODE files.; # NCBI build version:; ncbi_build_version = X; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522#issuecomment-447141409:127,config,config,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522#issuecomment-447141409,1,['config'],['config']
Modifiability,"Note that there is an AnnotateIntervals tool in the CNV pipeline (awaiting review in sl_denoising) that will output a TSV with column headers CONTIG, START, END, and GC_CONTENT. It takes -L, which can do the padding for you. If this doesn't exactly fit the bill for you, then it's probably best if you roll your own implementation rather than modify or refactor that code---should be easy enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3859#issuecomment-345807469:353,refactor,refactor,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3859#issuecomment-345807469,1,['refactor'],['refactor']
Modifiability,"Note that this is all new code, not directly ported from the old GATK, so should be given appropriate scrutiny. Also note that the initial, primitive walker interface here will certainly change/evolve as we gain a better understanding of what facilities the new GATK engine needs to provide. I will open a separate ticket to port existing hellbender tools to the new ReadWalker interface once this is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/112#issuecomment-70037077:194,evolve,evolve,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/112#issuecomment-70037077,1,['evolve'],['evolve']
Modifiability,"Note to self: the gcloud API changes a bit with the new release, apply the changes in [jp_gcloud_17_snapshot](https://github.com/broadinstitute/gatk/tree/jp_gcloud_17_snapshot) to adapt.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2822#issuecomment-306241927:180,adapt,adapt,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822#issuecomment-306241927,1,['adapt'],['adapt']
Modifiability,"Note: these are not hooked up to the code anywhere, I have another branch where I am doing the work to plug these in. This also does not currently resolve the equivalent issue to https://github.com/broadinstitute/gatk/issues/3848 but it does add tests to both plugins enforcing what the correct behavior should be. . fixes #3624",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3851:260,plugin,plugins,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3851,1,['plugin'],['plugins']
Modifiability,"Noticed this minor typo while doing some refactoring for a new feature branch. The issue is that the MultidimensionalKernelSegmenter incorrectly uses `maxNumChangepointsPerChromosome = maxNumSegmentsPerChromosome`, when it should be using `maxNumChangepointsPerChromosome = maxNumSegmentsPerChromosome - 1` like the CopyRatioKernelSegmenter and AlleleFractionKernelSegmenter do. @fleharty mind reviewing?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6497:41,refactor,refactoring,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6497,1,['refactor'],['refactoring']
Modifiability,"Now that there's been some refactoring of this code, it might be relatively straightforward to rewire the GVCFBlockCombiner to take in likelihood data from the pileup without creating a VariantContext, then pass the combined data to a VC and then to the writer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5618#issuecomment-590953474:27,refactor,refactoring,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618#issuecomment-590953474,1,['refactor'],['refactoring']
Modifiability,"Now that we have important `VariantWalker` tools that use reads as a side input (such as @lucidtronix 's `CNNScoreVariants`), we need to add caching to `ReadsContext` for good performance on nearby reads queries during a traversal. The caching should be modeled after the existing caching in `FeatureContext` as implemented in the `FeatureCache` class, but it should include the ability to cache ""around"" the current locus, rather than just ahead of it as in `FeatureCache`. Ideally, the tool itself should be able to configure the default caching behavior via arguments to control bases to cache before and after current locus.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4781:518,config,configure,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4781,1,['config'],['configure']
Modifiability,"Now that we're using git lfs to manage our large test resources, we need to configure travis to install/init git lfs before running the test suite.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/840:76,config,configure,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/840,1,['config'],['configure']
Modifiability,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:560,variab,variable,560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,1,['variab'],['variable']
Modifiability,"OK so just following along; the problem appears related to the Google Cloud Storage Connector and its configuration. When running on Cloud we need to ask for the `https://www.googleapis.com/auth/devstorage.read_write` scope, as described in [the install docs](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md). But you're right that `https://www.googleapis.com/auth/cloud-platform` should imply that so it should work... The command line argument is `--scopes` (plural) and not `--scope` but that's probably not the issue, the tool would have complained if you actually typed `scope` in there. . Perhaps the code is trying to do the non-cloud setup and that's what's making it not work on cloud?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331047616:102,config,configuration,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331047616,1,['config'],['configuration']
Modifiability,"OK, got lfs working. I understand the reasoning for limiting to 20-21 and will do this when i rewrite the GATK3 tests in the port. However, for the purpose of verifying consistent behavior of this ported tool would be great to use the same resources as GATK3. Since it seems like this is shareable, would it still be possible to get the full genome versions of:. /humgen/1kg/reference/human_g1k_v37.fasta (and FAI, dict); /humgen/gsa-hpprojects/GATK/data/dbsnp_132_b37.vcf + index?. While i can probably figure out the right version off public databases, these are outdated and it would be preferable to get exactly what GATK3 uses just in case there's a difference. . Beyond that, glad to hear it seems like /VariantEval/* is OK to share too. I'm happy to provide a place these could be uploaded, or download from you guys. . Thanks again for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358432986:94,rewrite,rewrite,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358432986,1,['rewrite'],['rewrite']
Modifiability,"OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by. `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`. This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:. ````; [global]; base_compiledir = PATH/TO/COMPILEDIR_#; ````. Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?. This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort. However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?. @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809:272,variab,variable,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"OK, thanks. I tried to keep edits here limited and protected. I'm happy to describe more about what I'm trying to do in VariantQC if that's helpful. Also - i have not forgotten about trying to refactor VariantQC to better handle arguments (i.e. dont pass the walker to the VariantEvaluator, and to separate a VariantEvalEngine class, somewhat like VariantAnnotationEngine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502259266:193,refactor,refactor,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502259266,1,['refactor'],['refactor']
Modifiability,"OK. Another thing: since in GATK4 it inherits from LocusWalkerByInterval, -L is now required. the usage examples still say -L is optional. . Tangentially is there a shortcut way to pass ""all intervals in the genome"" to GATK in the -L argument? Is there some trick using the DICT file or something like this? Certainly it's not that hard to convert a .dict file to an interval list, but not automatic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634784068:37,inherit,inherits,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634784068,1,['inherit'],['inherits']
Modifiability,"OK. As a reference, how does GATK deal with max-alternate-alleles for normal human variant calling? Presumably really high alternate alleles would primarily happen in repetitive/index prone-regions? FWIW, When we execute GenotypeGVCFs, we run as ~1000 jobs where each takes an even chunk of the genome, by base pairs. . Yes, I did see the bypass-feature-reader option, but we have jobs in-flight and I'm reluctant to change too many things as once. We will try this when possible though. As far as number of batches imported: I would need to check, but I believe it's only ~5 batches with perhaps 50-100 samples/ea. So I guess it's not that many new batches in the scheme of things, but anecdotally we have noticed that with the last couple rounds of import we needed to reduce batch size to make it work (i.e. not get hung). It is conceivable there is some other factor that is causing that variable performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581:892,variab,variable,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581,1,['variab'],['variable']
Modifiability,"OK. I found a potential solution. For this solution, we do not need to add or remove any dependencies. The only change is to the `log4j.properties` file (which configures log4j 1.x) to match the config specified in `log4j2.xml` (which configures log4j2). Now, GKL will use log4j 1.x to log, but the format will match the rest of GATK, which uses log4j2. This means that we have only one GKL for both GATK 3 and 4, at the expense of having to keep to config files, `log4j.properties` and `log4j2.xml`, in sync (which they probably should have been anyway, thought they weren't).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320779413:160,config,configures,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320779413,4,['config'],"['config', 'configures']"
Modifiability,"OR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4981,Config,ConfigFactory,4981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"O_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5466,Config,ConfigFactory,5466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"O_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6105,Config,ConfigFactory,6105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"O_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFacto",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4167,Config,ConfigFactory,4167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"O_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5643,Config,ConfigFactory,5643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,Obviated by ModelSegments rewrite.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3181#issuecomment-356697320:26,rewrite,rewrite,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181#issuecomment-356697320,1,['rewrite'],['rewrite']
Modifiability,"OfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; > + }; > +; > + // get the sequence dictionary; > + final SAMSequenceDictionary sequenceDictionary = getBestAvailableSequenceDictionary();; > + final List<SimpleInterval> intervals = hasIntervals() ? intervalArgumentCollection.getIntervals(sequenceDictionary); > + : IntervalUtils.getAllIntervalsForReference(sequenceDictionary);; > +; > + // create an IntervalList by copying all elements of 'intervals' into it; > + IntervalList intervalList = new IntervalList(sequenceDictionary);; > + intervals.stream().map(si -> new Inte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:4993,inherit,inherited,4993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['inherit'],['inherited']
Modifiability,"Oh, I hadn't noticed that there was a compilation warning causing the test to fail. ```; /gatk/src/test/java/org/broadinstitute/hellbender/MainTest.java:55: warning: [serial] serializable class ExitNotAllowedExcepion has no definition of serialVersionUID; private static final class ExitNotAllowedExcepion extends SecurityException {; ^; error: warnings found and -Werror specified; ```. Please fix that also :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772:306,extend,extends,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772,1,['extend'],['extends']
Modifiability,"Oh, also note that there might be some variable-name references in the Javadocs for the VQSR-lite tools that are not rendered properly in online docs; see https://github.com/broadinstitute/gatk/issues/8146 for more context. However, if you're just looking at the Javadocs via IntelliJ, everything should look fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414063049:39,variab,variable-name,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414063049,1,['variab'],['variable-name']
Modifiability,"Oh, interesting. That's a real problem. We inherited that code from picard and I don't think anyone ever paid attention to it. I'm assuming it's in there because someone encountered a tmp dir they couldn't read/write to but could set permissions on at some point, which seems weird. . At most we should be setting it for owner only I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4513#issuecomment-371635375:43,inherit,inherited,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513#issuecomment-371635375,1,['inherit'],['inherited']
Modifiability,Ohh. This looks like what we have really wanted when we refactor the test suite to test spark and other tools using the same methods. This should bring restful nights to us all. Unfortunately it looks like most of the docker tests have failed with errors along the lines of this: ; ```; org.gradle.api.internal.tasks.testing.TestSuiteExecutionException: Could not complete execution for Gradle Test Executor 1.; 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(Exec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:56,refactor,refactor,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['refactor'],['refactor']
Modifiability,"On the first question, we definitely appreciate how much work this will take. Often, porting the code is the easy part; developing new tests and test data can be a huge effort. I can try to find out if it would be possible for you to take the tool over - I know this kind of thing has come up before for other tools, but I'd have to ask around to find that out. @vdauwera do you have input on this ?. As for the plugins, currently in your branch `VariantStratification` and `VariantEvaluator` are modeled as Barclay command line plugin descriptors, and I was questioning whether thats necessary. Being a plugin is not necessarily required - `ReadFilter` and `Annotation` are both plugins, but they didn't have to be, and it takes quite a bit of work (again, mostly test development) to get a plugin right. Also, I'd consider the Barclay plugin framework to be pretty developed at this point, so I'd be curious to learn more about what issues you see. And yes, definitely don't check any of the large GATK3 test files into the repo, even temporarily. Take a look at [General guidelines for GATK4 developers](https://github.com/broadinstitute/gatk#dev_guidelines) if you haven't already. As you pointed out, new GATK4 tests that use smaller files would have to be developed. We'd want those to be included, and passing tests on the CI server, before we started reviewing the branch, so we know we're reviewing code that works and is covered by tests as much as possible. The second commit in my list above would have only your GATK3 java test files, etc (but not the big files, which you appear to have locally). The third commit would have your ported tool code, as well as the new test code, with the new tests enabled, as well as the smaller input files and expected results files. At the end we'd remove commit #2.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633:412,plugin,plugins,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633,6,['plugin'],"['plugin', 'plugins']"
Modifiability,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:43,config,configuration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['config'],['configuration']
Modifiability,"One concern I have is the maintainability of the test (having been burned by this in other places myself). When we add a new output field, etc we need a very easy way to update/generate these results. At the very least some instructions would be helpful (and imagine someone to follow those as part of a PR)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7192#issuecomment-821234533:26,maintainab,maintainability,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7192#issuecomment-821234533,1,['maintainab'],['maintainability']
Modifiability,"One major goal is to replace all the system properties in `gatk-launch`. Config options would be a combination of:; -Java system properties (like in gatk-launch); -Other engine-wide settings (like codec package names in `FeatureManager`, or NIO retries)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467920:73,Config,Config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467920,1,['Config'],['Config']
Modifiability,"One more thing: I'm also wondering if it would be possible to get a quick, preliminary evaluation of such a process without actually doing the work of adding it into the training tool. It's probably possible to do a slightly more ""manual"" validation split (say, using one or a few chromosomes), run the score tool on that validation set, use some external code to calculate the desired threshold from the resulting scores, and then use that threshold going forward. Actually, now that I've written it out, that sounds a lot cleaner and more flexible! Let me try to hack together the corresponding workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611:541,flexible,flexible,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611,1,['flexible'],['flexible']
Modifiability,"One observation that illustrates the need for care when optimizing metrics: for a few of the F1 optimizations, the haplotype-to-reference match-value parameter gets driven to its minimal value (1). Not 100% sure, but I'm guessing this might effectively boost precision by somehow cutting down on the complexity of proposed haplotypes---it depends on what the exact behavior of our SW algorithm is for negative scores. @davidbenjamin any thoughts on this behavior?. Something I don't quite understand yet is if we can impose some effective constraints on the parameters or otherwise reduce the number of independent dimensions. For example, it seems reasonable to me to fix the gap-extend penalties to -1 and let all other parameters be defined w.r.t. them. But perhaps we can also fix the match values similarly?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193:681,extend,extend,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193,1,['extend'],['extend']
Modifiability,One of the non-cloud tests in BQSRSparkIntegrationTest tries to use the API key when its not required; as a result the test fails when the key isn't set even though it should pass. Introduced in the test refactoring that was part of https://github.com/broadinstitute/gatk/pull/1533.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1590:204,refactor,refactoring,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1590,1,['refactor'],['refactoring']
Modifiability,"One part of this ticket is done: https://github.com/broadinstitute/gatk/pull/4964 added accessors that allow direct descendants of `GATKTool` to directly access engine datasources, while still forbidding direct access for tools that extend a Walker base class (except for Walker types living in the engine package, which still have access).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878:233,extend,extend,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878,1,['extend'],['extend']
Modifiability,"One proposal for moving forward would be to have a default properties file with a known name/location that is included in the gatk jar (say, ""gatk.default.properties""), which is always loaded and populates the initial configuration, and then use the classloader getResources method to also load all resources with some other known name (say, ""gatk.properties""). That way any properties files on the classpath with the known name would be automatically discovered and loaded. The apache commons API allows looks like it has good support for handling this using a [composite](http://commons.apache.org/proper/commons-configuration/userguide/howto_compositeconfiguration.html#Composite_Configuration_Details) configuration. We would have to define some rules around override semantics, but it looks like the api provides a lot of control over that as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954:218,config,configuration,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954,3,['config'],['configuration']
Modifiability,"One thing to try is to configure cromwell to retain the log directory via a workflow option when we run the tests. Then at the end of the build we can copy them somewhere, either always, or via the travis after_failure entry in the build matrix. Then we'd be able to see exactly what failed in the travis environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4130#issuecomment-357059677:23,config,configure,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4130#issuecomment-357059677,1,['config'],['configure']
Modifiability,"One variable that we need to control for is OpenJDK vs. Oracle JDK. Apparently these errors happened with OpenJDK, which is known to be flakier in the networking department than Oracle JDK. We should test with Oracle's JDK and see if the errors persist.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874:4,variab,variable,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874,1,['variab'],['variable']
Modifiability,One way to mitigate this problem is to set the GRADLE_USER_HOME environment variable to move the gradle cache onto a shorter path. This solved the problem for me for BaseRecalibratorDataflow. ; In my case I saw about over 100 jars listed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/580#issuecomment-114291341:76,variab,variable,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/580#issuecomment-114291341,1,['variab'],['variable']
Modifiability,"Oof, that's a nasty problem. We can definitely do something about it. It feels more like a Microsoft bug than a GATK one though. It seems crazy that each layer pull has to be a separate web request and there's no batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:400,layers,layers,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,2,['layers'],['layers']
Modifiability,"Ops reported several instances in which the allele-specific filtering failed. In the case I examined, the MQ distribution is much tighter around the mode at 60, which causes lin alg failures because that variable is effectively constant. Added more jitter, which has served well in the past.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6262:204,variab,variable,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6262,1,['variab'],['variable']
Modifiability,"Options include:. -Do nothing, and enforce via coding conventions (and hope the dataflow team comes to its senses). -Making a copy of the input before every `apply()` / `processElement()` / etc. (only affects dataflow code, but is inefficient (since it copies even when there's no mutation) and brittle since it only affects tools that go through our interface). -Make our types immutable, and add builders for mutation that perform copies (affects/penalizes non-dataflow code as well, since it will no longer be possible to modify in place). -Have both mutable and immutable views of each type, plus a builder (hard to manage given dataflow's poor support for polymorphism)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702:661,polymorphi,polymorphism,661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702,1,['polymorphi'],['polymorphism']
Modifiability,Options only used when running locally. Internally options are passed either by appending to java invocation directly with; command-line options or using the JAVA_OPTS environment variable. Also added environment variable printout with --dryRun; Other minor formatting / whitespace changes. resolves #2694,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2783:180,variab,variable,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2783,2,['variab'],['variable']
Modifiability,"Original report by @samuelklee (see https://github.com/broadinstitute/barclay/issues/189):; > I noticed that Javadoc @value tags are not being rendered correctly in e.g. https://gatk.broadinstitute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so I’ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8146:298,variab,variables,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146,1,['variab'],['variables']
Modifiability,"Originally from @vruano . Depending of what ploidy we use AR may return different active region boundaries. This differences cause the haploid assembly to fail with the larger region hightlight the lack of robustness of the current approach. More concretely the problem seem to be the presence of cycle in the larger region. Files are located in . ```; /humgen/gsa-hpprojects/dev/valentin/bug-reports/non-rubsassembly-with-ploidy4. cd $THAT_DIR; sh ./run.sh; ```. in CEUTrio*ploidy4.vcf the variant 20:22064431 is missing (as some other in the same region) which is a TP in knowledge base. . If you look into the debug output ploidy2.err and ploidy4.err, the latter attempts to assemble a larger region failing due to a cycle. . AR traversal comes out with different active region boundaries because the engine used takes as a parameter the ploidy. That is not by itself a bug and a bad think is just that the assembly fails for the extended region. . The task here is to improve the assembly algorithm to cope with this situations better (perhaps handle cycles appropriately).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/267:933,extend,extended,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/267,1,['extend'],['extended']
Modifiability,"Originally, we just had the normal be optional. You also had automated tests in the WDL Travis. . In FC, for tumor only, you would probably want a separate method configuration that ran on sample entity type. I'm open to other suggestions, but I can't think of another way. This could in theory be used for germline calling, too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811723:163,config,configuration,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811723,1,['config'],['configuration']
Modifiability,"Our data source classes are an inconsistent mess -- let's refactor so that we have ONE centralized reads source used by all tools (walkers and spark), one reference source, etc. This will have the side benefit of making it easier for new features like CRAM support to propagate transparently to all tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/959:58,refactor,refactor,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/959,1,['refactor'],['refactor']
Modifiability,OutputStream.java:1529) ~[?:?]; at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438) ~[?:?]; at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181) ~[?:?]; at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350) ~[?:?]; at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; 11:00:54.078 INFO AbstractConnector - Stopped,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:22987,adapt,adapted,22987,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"Overall the refactoring looks good and makes sense… but I'm not seeing how this fixes the problem of eating exceptions we saw during a recent run. Can you explain what was happening before, and how the new code addresses it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-927995357:12,refactor,refactoring,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-927995357,1,['refactor'],['refactoring']
Modifiability,Override mechanisms (in order of priority). -Individual config options specified on the command line manually / Or explicit config file ; -Override config file packaged into a downstream project; -Default GATK config file,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520:56,config,config,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520,4,['config'],['config']
Modifiability,Overriding inherited options,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/149:11,inherit,inherited,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/149,1,['inherit'],['inherited']
Modifiability,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7498:294,parameteriz,parameterized,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498,3,"['enhance', 'parameteriz']","['enhanced', 'parameterize', 'parameterized']"
Modifiability,"PPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5249,Config,ConfigFactory,5249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"PPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5888,Config,ConfigFactory,5888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"PPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3933,Config,ConfigFactory,3933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:97,config,configuration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,3,['config'],['configuration']
Modifiability,Package example GATK config file in the startup dir of the GATK docker image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4485:21,config,config,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4485,1,['config'],['config']
Modifiability,Parameterize the logging frequency for ProgressLogger.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8662:0,Parameteriz,Parameterize,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662,1,['Parameteriz'],['Parameterize']
Modifiability,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4436:17,config,config,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436,3,['config'],['config']
Modifiability,"Passing in the properties file did work (with the --gatk-config-file option). However, of the four tools I tested (MarkDuplicates, BaseRecalibrator, ApplyBQSR, and HaplotypeCaller) all of the tools accepted the --gatk-config-file option except for MarkDuplicates, which complains that it is not a recognized option. Perhaps this should be turned into a separate issue?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-368036324:57,config,config-file,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-368036324,2,['config'],['config-file']
Modifiability,PathSeq Illumina adapter trimming and simple repeat masking,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354:17,adapt,adapter,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354,1,['adapt'],['adapter']
Modifiability,"PathSeqFilterSpark and PathSeqPipelineSpark clear all the sequences from the input header file, as the Bwa step only accepts unaligned reads. However, the header sequences were being cleared before the reads were loaded, causing WellformedReadFilter to remove any mapped reads (by failing to find the corresponding sequence name in the header). This PR fixes this bug by creating a deep copy of the header. It also refactors this code, which is used in both the Filter and Pipeline tools, into a utility function `checkAndClearHeaderSequences()` in PSUtils. Tests have also been added/updated accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3453:415,refactor,refactors,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3453,1,['refactor'],['refactors']
Modifiability,"Per discussion with @droazen, we'll do the Spark tool equivalent of https://github.com/broadinstitute/gatk/issues/365 (once the various branches with ReadsSparkSink refactorings are merged in).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1480:165,refactor,refactorings,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1480,1,['refactor'],['refactorings']
Modifiability,"Per discussion with @kgururaj, this will probably take the form of a flag that suppresses materializing the genotype data. Also see the protobuf-based enhancements described [here](https://github.com/broadinstitute/gatk/issues/3689).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-336965003:151,enhance,enhancements,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-336965003,1,['enhance'],['enhancements']
Modifiability,"Picard had its Optical duplicate finding code refactored recently, additionally it has been noticed as part of #4656 that we are currently not properly accounting for the read groups when we stratify reads in MarkDuplicatesSpark which will likely cause problems for bams with more than one read group. Additionally better test coverage for multiple read groups should be added to ensure we are handling them sanely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4700:46,refactor,refactored,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4700,1,['refactor'],['refactored']
Modifiability,"PipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloud",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4903,Config,ConfigFactory,4903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"Please feel free to close this out. Just noticing small typos as I read through the docs for #1027; Not sure whether y'all prefer to avoid making trivial changes until a larger refactoring occurs, or would want them fixed on their own when they are noticed..",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1048:177,refactor,refactoring,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1048,1,['refactor'],['refactoring']
Modifiability,"Please need help : . I ran the script for VQSR . gatk-4.2.0.0/gatk VariantRecalibrator\; -V variants_sitesonly.vcf.gz\; 	-trust-all-polymorphic\; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0\; -an FS -an ReadPosRankSum -an MQRankSum -an QD -an SOR -an DP\ ; -mode INDEL\; -max-gaussians 4\; -resource:mills,known=false,training=true,truth=true,prior=12:Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\; -resource:axiomPoly,known=false,training=true,truth=false,prior=Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz\; -resource:dbsnp,known=true,training=false,truth=false,prior=2:Homo_sapiens_assembly38.dbsnp138.vcf\; -O cohort_indels.recal\; --tranches-file cohort_indels.tranches. ERROR - >. A USER ERROR has occurred: Argument resource was missing: Argument 'resource' is required. Any help would be really great !. thank you; Smeeta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593:132,polymorphi,polymorphic,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593,1,['polymorphi'],['polymorphic']
Modifiability,Please use the template in the WDL GATK repo doc that was shared. Or we can modify that template. I'd like the document to match what is generated automatically. The template in that document includes optimizations and is quite portable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295:228,portab,portable,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295,1,['portab'],['portable']
Modifiability,Plugin descriptors (filters and annotations) should use configurable package lists,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4036:0,Plugin,Plugin,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4036,2,"['Plugin', 'config']","['Plugin', 'configurable']"
Modifiability,Plugin for read transformers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2160:0,Plugin,Plugin,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2160,1,['Plugin'],['Plugin']
Modifiability,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ​; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. ​. .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; ​@Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); ​public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213:0,Plugin,Plugins,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213,9,"['Plugin', 'extend', 'plugin']","['Plugins', 'extends', 'plugin', 'plugins']"
Modifiability,Possible enhancements to MCMC.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2824:9,enhance,enhancements,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824,1,['enhance'],['enhancements']
Modifiability,Possibly inaccurate warning about mismatching parameter configs in PostprocessGermlineCNVCalls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6994:56,config,configs,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6994,1,['config'],['configs']
Modifiability,"PostProcessGermlineCNVCalls is currently single-sample, using input calls and model for the whole cohort. Specifying a sample index is not particularly user friendly. Given that we already output calls as a directory of files, including a sample map could enable the user to specify a sample name rather than an index. This would involve changes to GermlineCNVCaller as well. Alternatively, extending PostProcessGermlineCNVCalls to process all the samples at the same time would eliminate this problem and allow us to avoid some irritating transposes by parallelizing by shard instead of by sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659:391,extend,extending,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659,1,['extend'],['extending']
Modifiability,"PostprocessGermlineCNVCalls performs a check of the denoising/calling hyperparameter configs used to generate the model in GermlineCNVCaller cohort mode against those used to generate the case-mode result passed to PostprocessGermlineCNVCalls. However, although some of these hyperparameters are not exposed in case mode (since they have no effect on the sample-level parameters inferred in case mode, e.g., `psi_t_scale`), their python default values are nevertheless written to the case-mode config. I think that this results in a spurious mismatch between the cohort/case mode configs, which causes PostprocessGermlineCNVCalls to emit the following warnings in case mode when non-default values are used:. ````; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different denoising configuration between model and calls -- proceeding at your own risk!; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different calling configuration between model and calls -- proceeding at your own risk!; ````. I'm pretty sure that inference is actually performed correctly, but we may want to double check and clean up these warnings. We should probably just copy the non-exposed values from the model config on the python side when running GermlineCNVCaller in case mode. Not sure if there's any way to emit sensible warnings on the Java side. These hyperparameters are still exposed to the Java command line in case mode, they just aren't passed on to the python command line. So the user can change their values from their engine defaults without having any effect at all, but this is probably what we want. Perhaps we can document, though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6994:85,config,configs,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6994,6,['config'],"['config', 'configs', 'configuration']"
Modifiability,"Preparation for some refactoring related to TileDB integration. Extracted classes are package-protected for now, since they are not intended for direct use outside of the engine package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1929:21,refactor,refactoring,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1929,1,['refactor'],['refactoring']
Modifiability,"Preparation for upcoming SAMRecord -> Read refactoring. Wanted to; separate out this trivial package rename, as it was cluttering; the diff on my main branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/352:43,refactor,refactoring,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/352,1,['refactor'],['refactoring']
Modifiability,Prevents a bug that occurs when a file path contains characters that are illegal in URIs. Specific example was when using `--tmp-dir file:///tmp/workflow#main` GATK would initially correctly interpret this as `/tmp/workflow#main` but then when setting the Java temp directory in `CommandLineProgram.java` line 164 it would send `/tmp/workflow#main` to `IOUtils.getAbsolutePathWithoutFileProtocol` which would then mangle it by turning it into a URI and then removing `file://` resulting in `/tmp/workflow%23main` which later causes issues when things like the Codecs attempt to write configuration files to the temp directory that doesn't exist. CWLTool often creates path names that contain `#` so workflows made by CWLTool and containing GATK can fail because of this bug.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6769:584,config,configuration,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6769,1,['config'],['configuration']
Modifiability,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3217:465,variab,variable,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217,1,['variab'],['variable']
Modifiability,"Previously, a temporary table is created as part of extract of the VQSR features, and it goes into a separate `temp_tables` dataset in the current project -- that is no longer true, and it now goes into the default dataset as a short living temp table with the task name and a hash. This pr should:. - default to the current dataset (with the VET etc tables) rather than a different dataset. - give a prefix to the temp tables so we know which one came from which step. - temp table TTL---not a changeable option, but default to 24 hours across the board. Still to discuss:; Parameterization of the location (dataset) to create the temp table in (default to the default dataset); manual clean up/TTL is a changeable option and TTL is parametrizable (currently the TTL is a parameter for the prepare step -- but then we set a default as 24 hrs in the WDL) . ![Screen Shot 2022-06-10 at 1 27 58 PM](https://user-images.githubusercontent.com/6863459/173121781-4486c1d1-ef7a-4ab8-aa62-fdc5018fd3b9.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7742:575,Parameteriz,Parameterization,575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7742,1,['Parameteriz'],['Parameterization']
Modifiability,"Prior to assembly (in `AssemblyBasedCallerUtils.assembleReads`, we transform reads in several ways that are meant to be permanent (that is, we want to use them in both assembly and genotyping) within `finalizeRegion`. (Additionally, we error reads within `ReadThreadingAssembler.runLocalAssembly`, but this is done on temporary copies of reads that are used for kmers and discarded). These transformations include hard clipping low-quality ends, adaptor sequences, and, optionally, soft-clipped bases, as well as correcting the base qualities of overlapping mates. According to the git history, these transformations have been accidentally temporary for quite a while. Let's look at the relevant code. First, in `Mutect2Engine.callRegion` we have (comments added and code simplified for clarity). ```; final AssemblyRegion assemblyActiveRegion = AssemblyBasedCallerUtils.assemblyRegionWithWellMappedReads(originalAssemblyRegion . . .);. // assembleReads finalizes region, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:446,adapt,adaptor,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['adapt'],['adaptor']
Modifiability,Probably CircleCI might not be at Gradle 2.1 or higher for the [Gradle Download Task](https://github.com/michel-kraemer/gradle-download-task) plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101733759:142,plugin,plugin,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/519#issuecomment-101733759,1,['plugin'],['plugin']
Modifiability,Probably git. See `core.autocrlf` at https://git-scm.com/book/id/v2/Customizing-Git-Git-Configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431474812:88,Config,Configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431474812,1,['Config'],['Configuration']
Modifiability,"Problem was ocurring in the presence of insertions and deletions. fixes #6139. 1. Changed ReadClipper unit tests:; - The tests in many cases assumed that the unclipped alignment locations do not change when the read is clipped. This is not true: for example if start for cigar; 1M1I3M is 100, the unclipped start for 2H3M is 99. All assertUnclipped calls were; removed; - Alignment now check that the read length remains to be consistent with the CIGAR, that the aligned bases span are consistent with the CIGAR and that the number of clipped bases from the read is consistent with the requested clipping; 2. Hard clipping in ClippingOps was buggy, thus we introduced new tests for it.; 3. Text was refactored for readability; 4. Clipping in ClippingOps did not treat insertions and deletions in the clipped parts of the CIGAR correctly. This was fixed; 5. Alignment re-calculation after clipping did not work correctly if the initial CIGAR contained insertions and deletions; 6. Hard clipping applied to the hard clipped read did not behave correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6280:699,refactor,refactored,699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6280,1,['refactor'],['refactored']
Modifiability,"ProgressMeter: allow labels to be configurable per-traversal/tool, and hook up to GenomicsDBImport",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2690:34,config,configurable,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690,1,['config'],['configurable']
Modifiability,Promote gradle build change that helps with certain spark config errors during build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:58,config,config,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,1,['config'],['config']
Modifiability,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2740:420,refactor,refactoring,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740,1,['refactor'],['refactoring']
Modifiability,"Prototype a PythonScriptExecutor, and assess maintainability of an example tool that calls into a Python machine-learning library",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:45,maintainab,maintainability,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,1,['maintainab'],['maintainability']
Modifiability,Qual = 0 sites don't count as polymorphic for GVCF mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4967:30,polymorphi,polymorphic,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4967,1,['polymorphi'],['polymorphic']
Modifiability,"READER_FACTORY :; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4418,Config,ConfigFactory,4418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,REATE_MD5 : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4785,Config,ConfigFactory,4785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4245,Config,ConfigFactory,4245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"RENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3844,Config,ConfigFactory,3844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"RROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.mis",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5850,variab,variable,5850,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,"RY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3867,Config,ConfigFactory,3867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"R_FACTORY :; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3457,Config,ConfigFactory,3457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"R_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3974,Config,ConfigFactory,3974,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"RankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz --use-allele-specific-annotations`. #### Error Message; ```; Using GATK jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms24g -jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCred",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:2185,polymorphi,polymorphic,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['polymorphi'],['polymorphic']
Modifiability,"Rather than delaying argument validation to a second pass, I changed the sequence so the tool instantiates the descriptor and gives it the tool defaults right from the start, (and then passes the descriptor instance(s) to the arg parser) so it has all the state it needs to validate at arg parsing time. Also, I reverted the removal of the package limitation for plugins, at least temporarily, since searching through all packages looked like it slowed down the integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-232046070:363,plugin,plugins,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1973#issuecomment-232046070,1,['plugin'],['plugins']
Modifiability,"Read counts at different stages of the PathSeq pipeline are now logged using `MetricsFile`. The filter metrics contains the number of reads remaining and number of reads filtered at each step (after filtering pre-aligned reads, low quality/complexity reads, host reads, and duplicates). The score metrics give number of pathogen-mapped and unmapped reads. These metrics are now validated in the PathSeq integration tests, which have also been refactored to use DataProviders instead of separate functions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611:443,refactor,refactored,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611,1,['refactor'],['refactored']
Modifiability,ReadFilter plugin tests does not live in the same package than the plugin itself,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2532:11,plugin,plugin,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532,2,['plugin'],['plugin']
Modifiability,"Really we need some tests for gs:// files in ReadsSparkSinkUnitTest - e.g. a GCS version of testWritingToFileURL. This needs knowledge of how to configure the Hadoop GCS connector (outside dataproc), which I lack. Perhaps someone else knows how to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-270615942:145,config,configure,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-270615942,1,['config'],['configure']
Modifiability,RecalibrationArgumentColleciton arguments need to be refactored to new standard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3974:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3974,1,['refactor'],['refactored']
Modifiability,"RecalibratorEngine.java:43); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:625); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I believe this is derived from an error earlier in the log, since the `stderr` gives the same Java heap space error: ; ```; [2019-09-16 19:05:59,50] [error] WorkflowManagerActor Workflow 9f7a01a4-0632-4817-8622-aa51e520abf1 failed (during ExecutingWorkflowState): Job JointGenotyping.SNPsVariantRecalibratorClassic:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /path/to/stderr.; ```. I have read past issues (https://gatkforums.broadinstitute.org/gatk/discussion/23880/java-heap-space) regarding this that may suggest it is a bug. It has pointed me to increasing the available heap memory through the primary command of -Xmx. Is this the way to do it? ; ```; java -Xmx600G -Dconfig.file=' + re.sub('input.json', 'overrides.conf', input_json) + ' -jar ' + args.cromwell_path + ' run ' + re.sub('input.json', 'joint-discovery-gatk4.wdl', input_json) + ' -i ' + input_json; ```; where I substitute in the corresponding config, json, and wdl files. . Is 600G enough? Each vcf is around 6G large and since I have 150, does that mean I should be allocating more than 900G (6G x 150)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165:2809,config,config,2809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165,1,['config'],['config']
Modifiability,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8489:7,refactor,refactoring,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489,1,['refactor'],['refactoring']
Modifiability,Refactor *Context classes to return empty Collections/iterators when there is no backing data source,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/249:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/249,1,['Refactor'],['Refactor']
Modifiability,Refactor AlleleListUtilsUnitTest to have no skips and be robustly deterministic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,1,['Refactor'],['Refactor']
Modifiability,Refactor ArtificialReadUtils to make it easier to select backing implementation (SAMRecord vs. Google Read),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/641:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/641,1,['Refactor'],['Refactor']
Modifiability,Refactor CNV collection classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3976:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3976,1,['Refactor'],['Refactor']
Modifiability,"Refactor Dataflow transforms by ""top level"" transform",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/651:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/651,1,['Refactor'],['Refactor']
Modifiability,"Refactor Dataflow transforms by ""top leveltransform",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/649:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/649,1,['Refactor'],['Refactor']
Modifiability,"Refactor Funcotator scripts to take arguments, not internal configurations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5346:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346,2,"['Refactor', 'config']","['Refactor', 'configurations']"
Modifiability,Refactor GATKTool so that more tools can comfortably extend it directly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341,2,"['Refactor', 'extend']","['Refactor', 'extend']"
Modifiability,Refactor GATKVariantContextUtils' variant context merging methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/132:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/132,1,['Refactor'],['Refactor']
Modifiability,Refactor GVCFWriter to allow push/pull iteration.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5311:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5311,1,['Refactor'],['Refactor']
Modifiability,Refactor GenomeLocParser and GenomeLoc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100,1,['Refactor'],['Refactor']
Modifiability,Refactor MT wdl to make validations easier,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5708:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708,1,['Refactor'],['Refactor']
Modifiability,Refactor ReadsSparkSource to accommodate non-sorting bam output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4818:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4818,1,['Refactor'],['Refactor']
Modifiability,Refactor VariantEvalUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5441:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5441,1,['Refactor'],['Refactor']
Modifiability,Refactor VcfFuncotationFactoryCache to go into any funcotation factory that does not need Gencode/Transcript Funcotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4974:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974,1,['Refactor'],['Refactor']
Modifiability,Refactor WGS and WES coverage collection to be more analogous.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2964:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2964,1,['Refactor'],['Refactor']
Modifiability,Refactor WGS coverage collection and GC annotation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3153:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3153,1,['Refactor'],['Refactor']
Modifiability,Refactor `CallVariantsFromAlignedContigsSpark` to prep for calling SV.INS & SV.DEL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2258:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258,1,['Refactor'],['Refactor']
Modifiability,"Refactor backend LocusWalker iterator (currently LocusIteratorByState) to support emitting uncovered/""empty"" loci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2678:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2678,1,['Refactor'],['Refactor']
Modifiability,Refactor com.intel.genomicsdb package references to org.genomicsdb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,1,['Refactor'],['Refactor']
Modifiability,Refactor control flow in ModelSegments and gCNV CLIs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3951:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3951,1,['Refactor'],['Refactor']
Modifiability,Refactor dockerfiles to reduce docker layer count,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8686:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8686,1,['Refactor'],['Refactor']
Modifiability,"Refactor exceptions: add UserException, remove ReviewedHellbenderException, rename HellbenderException to GATKException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/85:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/85,1,['Refactor'],['Refactor']
Modifiability,Refactor gatk-launch to use argparse,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1330:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1330,1,['Refactor'],['Refactor']
Modifiability,Refactor het genotyping code in ModelSegments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915,1,['Refactor'],['Refactor']
Modifiability,"Refactor python code from extract dir into a scripts directory. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f85602d0-6dc5-49d6-82d1-eb58e9966021); Passing VAT Creation work [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ddc7fcf9-5fb7-44e2-8117-721389d4f858), [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/0d705f21-3362-4890-b925-5bed2646fe4d), and [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/15cfe125-e700-44c8-b9d0-c3e98d7db4c0)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9017:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9017,1,['Refactor'],['Refactor']
Modifiability,Refactor segment classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2836:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836,1,['Refactor'],['Refactor']
Modifiability,Refactor the docker image to correspond to the minimum necessary for user execution,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3930:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3930,1,['Refactor'],['Refactor']
Modifiability,Refactor the test suite on Github Actions to run faster,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7798:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798,1,['Refactor'],['Refactor']
Modifiability,Refactor to put 'Lite' functionality into ExtractCohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8295:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8295,1,['Refactor'],['Refactor']
Modifiability,Refactor two VariantEval methods to allow subclasses to override,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998,1,['Refactor'],['Refactor']
Modifiability,Refactor/improve allele-specific annotation reduce interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3293:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3293,1,['Refactor'],['Refactor']
Modifiability,Refactored CalcNIndelInformativeReads() use dynamic programming and cached results,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5607:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607,1,['Refactor'],['Refactored']
Modifiability,Refactored JointVcfFiltering WDL and expanded tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074,1,['Refactor'],['Refactored']
Modifiability,Refactored and enhanced ArgumentsBuilder,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6474:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6474,2,"['Refactor', 'enhance']","['Refactored', 'enhanced']"
Modifiability,Refactored the docker build script to only only include the gatk bundle in order to shrink the docker image size,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955,1,['Refactor'],['Refactored']
Modifiability,Refactoring / housekeeping Mutect2IntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6184:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6184,1,['Refactor'],['Refactoring']
Modifiability,Refactoring a confusing method in GATKVariantContextUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8690:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8690,1,['Refactor'],['Refactoring']
Modifiability,Refactoring gCNV WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5176:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5176,1,['Refactor'],['Refactoring']
Modifiability,Refactoring of ModelSegments Segmenter and Modeller backend classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2258:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258,1,['Refactor'],['Refactoring']
Modifiability,Refactoring of variant context and genotype comparison code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6417:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6417,1,['Refactor'],['Refactoring']
Modifiability,Refactoring read orientation model to run within Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5840:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840,1,['Refactor'],['Refactoring']
Modifiability,Refactoring windows and padding for assembly and genotyping,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6358:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6358,1,['Refactor'],['Refactoring']
Modifiability,Refactoring won't happen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6877#issuecomment-1377803387:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6877#issuecomment-1377803387,1,['Refactor'],['Refactoring']
Modifiability,ReferenceDependentFeatureCodecs are broken and need to be refactored,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/234:58,refactor,refactored,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/234,1,['refactor'],['refactored']
Modifiability,Remove hardcoded system properties from gatk frontend script that collide with those in the GATK config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4484:97,config,config,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4484,1,['config'],['config']
Modifiability,"Removed the inheritance. Moved the file, though I can move it again if someone feels strongly about where it should be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/487#issuecomment-99599956:12,inherit,inheritance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/487#issuecomment-99599956,1,['inherit'],['inheritance']
Modifiability,Repackage ReadFilter plugin tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3525:21,plugin,plugin,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3525,1,['plugin'],['plugin']
Modifiability,Replace literal arguments with variables in several integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4416:31,variab,variables,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4416,1,['variab'],['variables']
Modifiability,Request: fine-grained configuration for codec packages for downstream projects,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:22,config,configuration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['config'],['configuration']
Modifiability,Request: read filter plugin method for get a list with default filters and CMD instances,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362:21,plugin,plugin,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362,1,['plugin'],['plugin']
Modifiability,Resolved the issue of adding gs:// to the beginning and / to the end of the environment variable GATK_GCS_STAGING. Tested it locally.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5452:88,variab,variable,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5452,1,['variab'],['variable']
Modifiability,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:257,config,configuration,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890,1,['config'],['configuration']
Modifiability,"Reviving this. This will essentially be a major refactor/rewrite of CreatePanelOfNormals to make it scalable enough to handle WGS. - [x] CombineReadCounts is too cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:48,refactor,refactor,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,2,"['refactor', 'rewrite']","['refactor', 'rewrite']"
Modifiability,"Reworks classes used by `JointGermlineCNVSegmentationIntegration` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally. A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentationIntegration`. This is the first of a series of PRs for an experimental Java-based implementation of some modules in `gatk-sv` pipeline, including SV vcf merging, clustering, evidence aggregation, and genotyping.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243:388,enhance,enhanced,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243,1,['enhance'],['enhanced']
Modifiability,Rewrite complex SV functional annotation in SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8516:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite detection of interval file types,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/167:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/167,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite haplotype construction methods in PDHapComputationEngine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8367:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8367,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite of the FilterVariantTranches tool without python dependencies. Uses @takutosato's shiny new TwoPassVariantWalker. @takutosato or @cmnbroad care to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4800:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite strand artifact docs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4477:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4477,1,['Rewrite'],['Rewrite']
Modifiability,"Right now our docker image is much larger than it needs to be. This is at least in part because it contains our entire git clone as well as the packaged jars. This is not necessary and could potentially come with it shrinking our docker image substantially. . This change would involve a major refactoring of how we execute our tests through gralde inside the docker image, as removing the test dependencies will mean we probably have to externally mount the git clone from the docker image in order to pull in the proper dependencies.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3930:294,refactor,refactoring,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3930,1,['refactor'],['refactoring']
Modifiability,"Right now the docker image is too large. It appears that the if the build_docker.sh script its set to run the tests, then it is uploading the test resources into the docker and removing them, resulting in multiple layers which uses unneeded space. We need to remove the resources files from the docker.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3414:214,layers,layers,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3414,1,['layers'],['layers']
Modifiability,"Right now, we pair reads with overlapping variants. We can generalize this for any two PCollections of classes that extend Locatable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/663:116,extend,extend,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/663,1,['extend'],['extend']
Modifiability,"Right now, when we create a bucket-based test, we upload files into `gs://hellbender/test/resources/`, which Travis uses for `HELLBENDER_TEST_INPUTS` (the environment variable used by dataflow tests). These files are currently unversioned, which is bad -- we need to come up with a better way of managing our dataflow test inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/739:167,variab,variable,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/739,1,['variab'],['variable']
Modifiability,"Right, the `try` block needs to catch the `java.lang.UnsatisfiedLinkError` exception. We'll fix that in the next GKL release. As a workaround, you can try defining this environment variable: `export GKL_USE_LIB_PATH=1`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265859639:181,variab,variable,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265859639,1,['variab'],['variable']
Modifiability,"Right. I want to subset by sample name, effectively taking a slice of the; position by sample genotype matrix and computing Info annotations based; only in the kept samples. On Mon, Mar 4, 2019, 8:52 PM Karthik Gururaj <notifications@github.com>; wrote:. > I'm assuming you will have the subset of samples before creating a; > GenomicsDBFeatureReader object (and before creating the corresponding; > Protobuf export configuration object).; >; > More precisely, you are NOT requesting a line by line filter similar to:; > At pos 100, compute INFO fields etc including only the samples whose QUAL; > > 5; > At pos 102, compute INFO fields etc including only the samples whose QUAL; > > 5; > ....; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMZjIbDJ2eDZcB69XHiUycnumzHrks5vTc3PgaJpZM4Z7pF2>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469680991:416,config,configuration,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469680991,1,['config'],['configuration']
Modifiability,Run the GATK tests using the refactored rans code in htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8730:29,refactor,refactored,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8730,1,['refactor'],['refactored']
Modifiability,"Running a particular bam sort takes ~20minutes with hdd and 16 minutes with ssd. So it's definitely being used somehow. It looks like spark.local.dir is over ridden by the environment variable LOCAL_DIRS, and I don't see that set, but it's possible it's being set but not recorded correctly in the UI or something like that. Someone will need to poke at a bit more to be more clear about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370:184,variab,variable,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370,1,['variab'],['variable']
Modifiability,"Running:; spark-submit --master yarn-client --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar PrintReadsSpark -I /gatk4/output.bam -O /gatk4/output_2.bam --sparkMaster yarn-client; 14:19:09.870 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:19:10.155 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 11, 2017 2:19:10 PM CST] PrintReadsSpark --output /gatk4/output_2.bam --input /gatk4/output.bam --sparkMaster yarn-client --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 11, 2017 2:19:10 PM CST] Executing as hdfs@mg on Linux 3.10.0-514.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:1261,variab,variables,1261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4527,Config,ConfigFactory,4527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,SSION_LEVEL : 2; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3286,Config,ConfigFactory,3286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,Saw it again [here](https://travis-ci.com/broadinstitute/gatk/jobs/180435353) (now restarted). If I'm reading the serialization stack in the right order:. ```; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); ```. it looks like we're trying to serialize a ClassLoader. The FieldSerializer does appear to use a ClassLoader to load classes during serialization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740:262,Config,Configuration,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740,1,['Config'],['Configuration']
Modifiability,"ScoreVariantAnnotations:. Scores variant calls in a VCF file based on site-level annotations using a previously trained model. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use trained models that contain both SNP and INDEL scorers as input) ; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Double check or add behavior for handling previously filtered input, clearing present filters, etc. Future work:. - [ ] The `score_samples` method of the sklearn IsolationForest is single-threaded. See (possibly stalled) PR at https://github.com/scikit-learn/scikit-learn/pull/14001 and some workarounds using e.g. `multiprocessing` ibid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563:199,config,configurations,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563,1,['config'],['configurations']
Modifiability,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:160,refactor,refactoring,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,2,['refactor'],['refactoring']
Modifiability,"See https://github.com/broadinstitute/gatk/issues/4888, which is an older report for the same issue. As mentioned there, I think we should patch our fork of `google-cloud-java` to do a channel reopen on `UnknownHostException` for now as a quick fix. @jean-philippe-martin is eventually going to add an official configuration mechanism for clients of `google-cloud-java` to customize which errors should trigger a retry/reopen, which should provide a better way to deal with these errors as they crop up without having to modify the NIO library itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420:311,config,configuration,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420,1,['config'],['configuration']
Modifiability,See individual commit messages; - fix median bug; - parameterized sample_list and enable subsetting; - support pre-query bytes processed estimate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6374:52,parameteriz,parameterized,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6374,1,['parameteriz'],['parameterized']
Modifiability,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:144,rewrite,rewrite,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,1,['rewrite'],['rewrite']
Modifiability,SelectVariants JEXL filter fixes and refactor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092:37,refactor,refactor,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092,1,['refactor'],['refactor']
Modifiability,Set gcloud config directory in travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7525:11,config,config,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7525,1,['config'],['config']
Modifiability,"Several backwards-incompatible changes in VCF 4.3 (eg., escape sequences) have made it difficult to update without first doing a major refactoring in HTSJDK to better version/isolate our parsers. @cmnbroad can provide further details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-471719969:135,refactor,refactoring,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-471719969,1,['refactor'],['refactoring']
Modifiability,Several improvements to SV contig alignment configuration picker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326:44,config,configuration,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326,1,['config'],['configuration']
Modifiability,"Should this be closed so that your other code can be refactored to use `GenomicsConverter` for now?. Let's not move either `GenomicsConverter` or `ReadConverter` to hellbender just yet. It's unclear whether they'll be needed at all once the read common interfaces branch is in, and also they have some issues (eg., `ReadConverter` ""encodes"" attribute values by calling `toString()` on them, which is bad for certain attribute types such as byte arrays).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100273393:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/493#issuecomment-100273393,1,['refactor'],['refactored']
Modifiability,Should we refactor datasources to have two separate class hierarchies for segments vs. small mutations?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5961:10,refactor,refactor,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5961,1,['refactor'],['refactor']
Modifiability,SimpleAnnotatedGenomicRegion refactoring to use Tribble,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3738:29,refactor,refactoring,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3738,1,['refactor'],['refactoring']
Modifiability,"Since @davidadamsphd is out today, would either @tomwhite or @laserson be available to review JUST the spark part of this branch (specifically, just the classes `BaseRecalibratorSpark`, `BaseRecalibratorSparkFn`, `ApplyBQSRSpark`, `ApplyBQSRSparkFn`, `ReadsPipelineSpark`, `BaseRecalibratorSparkIntegrationTest`, and `ApplyBQSRSparkIntegrationTest`)? . The rest of the branch is just a BQSR engine refactoring that will be reviewed by @jean-philippe-martin as soon as we do another rebase on top of master (we'll ping him as soon as that part of the branch is ready for review).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142645705:398,refactor,refactoring,398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142645705,1,['refactor'],['refactoring']
Modifiability,"Since Geraldine is away till the end of the week, and we are under the Nov 23 deadline for review, I will proceed with changes. I think it useful for me to go through the motions and see what other discussion items turn up. Notes on factors I think are of interest to users re annotators:; - cohort vs sample level annotation; - InfoFieldAnnotation; - GenotypeAnnotation; - minimum number of samples, e.g. 10 for inbreedingcoefficient; - standard annotations for each tool (HC, M2 and VariantAnnotator), standard allele-specific annotations.; - StandardMutectAnnotation; - PerAlleleAnnotation; - StandardAnnotation (extends Annotation); - StandardHCAnnotation; - VariantAnnotation; - noticing `public class` vs `public final class`. Not annotating `abstract` class nor `public interface`.; - What is a reducible annotation?; - I would really find helpful the acronym for the annotation, e.g. MBQ, be listed with the annotation summary, e.g. Median base quality of bases supporting each allele.; - Annotations that are specific to a tool. E.g. DepthPerSampleHC can only be used by HaplotypeCaller and not VariantAnnotator. Doc doesn't say anything about Mutect2. ; - Not sure what VariantOverlapAnnotator does ~~but went ahead and summarized as ""Annotate ID field and attribute overlap FLAG"".~~ `did not tag`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344427246:616,extend,extends,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344427246,1,['extend'],['extends']
Modifiability,"Since not all Picard tools operate on Sam files, and some GATK tools do (but won't extend this new class), I'd advocate for a more generalized name.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/83#issuecomment-69979216:83,extend,extend,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/83#issuecomment-69979216,1,['extend'],['extend']
Modifiability,"Since we are going to change many of those argument names (camel-back to kebab-case) I think we should take this opportunity to use constants to specify argument names in the code and use them in our test code so further changes in argument names don't break tests. . Take as an example [CombineReadCounts](https://github.com/broadinstitute/gatk/blob/3ec7399a54ccf89d2b323b2be71b8b7e4931174c/src/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java). Extract enclosed below. It might be also beneficial to add public constant for the default values. ```java; public final class CombineReadCounts extends CommandLineProgram {. public static final String READ_COUNT_FILES_SHORT_NAME = StandardArgumentDefinitions.INPUT_SHORT_NAME;; public static final String READ_COUNT_FILES_FULL_NAME = StandardArgumentDefinitions.INPUT_LONG_NAME;; public static final String READ_COUNT_FILE_LIST_SHORT_NAME = ""inputList"";; public static final String READ_COUNT_FILE_LIST_FULL_NAME = READ_COUNT_FILE_LIST_SHORT_NAME;; public static final String MAX_GROUP_SIZE_SHORT_NAME = ""MOF"";; public static final String MAX_GROUP_SIZE_FULL_NAME = ""maxOpenFiles"";; public static final int DEFAULT_MAX_GROUP_SIZE = 100;. @Argument(; doc = ""Coverage files to combine, they must contain all the targets in the input file ("" +; TargetArgumentCollection.TARGET_FILE_LONG_NAME + "") and in the same order"",; shortName = READ_COUNT_FILE_LIST_SHORT_NAME,; fullName = READ_COUNT_FILE_LIST_FULL_NAME,; optional = true; ); protected File coverageFileList;. @Argument(; doc = READ_COUNT_FILES_DOCUMENTATION,; shortName = READ_COUNT_FILES_SHORT_NAME,; fullName = READ_COUNT_FILES_FULL_NAME,; optional = true; ); protected List<File> coverageFiles = new ArrayList<>();. @Argument(; doc = ""Maximum number of files to combine simultaneously."",; shortName = MAX_GROUP_SIZE_SHORT_NAME,; fullName = MAX_GROUP_SIZE_FULL_NAME,; optional = false; ); protected int maxMergeSize = DEFAULT_MAX_GROUP_SIZE;. @ArgumentCollection; protect",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346175904:618,extend,extends,618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346175904,1,['extend'],['extends']
Modifiability,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312:148,refactor,refactoring,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312,1,['refactor'],['refactoring']
Modifiability,"Size=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2 --jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar -- CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn; Job [acdae2af-e0ce-4822-87f5-dcd165d85cf4] submitted.; Waiting for job output...; 20:39:42.869 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:39:43.053 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/acdae2af-e0ce-4822-87f5-dcd165d85cf4/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [November 27, 2017 8:39:43 PM UTC] CountReadsSpark --input gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [November 27, 2017 8:39:43 PM UTC] Executing as root@droazen-test-cluster-m on Linux 3.16.0-4-amd64",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:2637,variab,variables,2637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"So I just updated one of the newer tests, and now all of the tests for HaplotypeCaller seem to be passing locally. The previous commits updating the copy code were preserved when Louis reverted, so there were basically no changes I had to make to get this ""working."" That does leave us with one question now:. When looking into this a little with James and Louis earlier, we realized that the code for setting up the ActiveRegionGenotyper uses a weird partial copy of the standard CLI args method that has existed in the code for whoever knows how long. Conceptually this seems like a bad idea, but changing it now would possibly cause some older tests to fail, if they were based on this faulty method reasoning. Should we try to merge the PR as it is now, with all tests passing, and hopefully consistency with previous behavior, or try to update the logic around this genotyper as well at the same time? It's possible we can try to address the latter point as well at some point in the future when we try to get Louis's refactor code actually working. Maybe there could be some quarter goal around a HaplotypeCaller code revamp sometime inspired by some of these ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216:1023,refactor,refactor,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216,1,['refactor'],['refactor']
Modifiability,"So I've tracked this down to being a problem with line 193 in `ReadSparkSink`. . ```; finalOut.saveAsNewAPIHadoopFile(outputFile, SAMRecord.class, SAMRecordWritable.class, SparkHeaderlessBAMOutputFormat.class);; ```. saveAsNewAPIHadoopFile is not writing any part files when `outputFile` (a `String`) is a relative path. i.e. `test.bam`. . If you change it to a fully specified path i.e. `/Users/louisb/Workspace/gatk/testoutput.bam` then it works as expected. I discovered this by running the driver in the debugger which is possible using . ```; --driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005; ```. as an option to spark-submit (or gatk-launch). The quick fix is to have input paths be converted into absolute paths always. Also to add a check in `mergeBam()` to make sure that there's at least 1 part file specified. The longer fix is to ; 1. understand why this is happening only in certain spark configurations; 2. figure out how to unit test for this",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1448#issuecomment-175192707:943,config,configurations,943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1448#issuecomment-175192707,1,['config'],['configurations']
Modifiability,So we typically override the config files on the command line. We'll have to make sure we wire the log4j 1.x logger to respect our command line overrides if it doesn't already. You can check that by testing if you can control the log output with the --verbosity command. If not we'll have to update `LoggingUtils.setLoggingLevel()`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320787794:29,config,config,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320787794,1,['config'],['config']
Modifiability,"Solves to some extend #2362, by adding a new getter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2364:15,extend,extend,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2364,1,['extend'],['extend']
Modifiability,"Some always throw, some always filter, some depend on the arguments you; pass to the filter constructor.; On Feb 23, 2015 9:19 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > Ah -- maybe I'm mistaken. Or can it be a difference in how they're; > applied/invoked?; > ; > Maybe there's some inconsistency in behavior. Would be nice to iron this; > all out.; > ; > On Mon, Feb 23, 2015 at 5:17 PM, Louis Bergelson <notifications@github.com; > ; > > wrote:; > > ; > > I'm pretty sure they all do... or at least all can depending on how you; > > configure your MalformedReadFilter.; > > ; > > example:; > > ; > > private static boolean checkHasReadGroup(final SAMRecord read) {; > > if ( read.getReadGroup() == null ) {; > > // there are 2 possibilities: either the RG tag is missing or it is not; > > defined in the header; > > final String rgID =; > > (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; > > if ( rgID == null ); > > throw new UserException.ReadMissingReadGroup(read);; > > throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; > > }; > > return true;; > > }; > > ; > > —; > > Reply to this email directly or view it on GitHub; > > <; > > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75649333; > > ; > > .; > ; > ## ; > ; > Geraldine A. Van der Auwera, Ph.D.; > Bioinformatics Scientist II; > GATK Support & Outreach; > Broad Institute; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75686467; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686777:562,config,configure,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-75686777,1,['config'],['configure']
Modifiability,"Some changes after the comments from @droazen (in commit #2360):; * Change the argument name from `--disableAllReadFilters` to `--disableToolDefaultReadFilters`; * Make `--disableToolDefaultReadFilters` mutex w.r.t. `--disableReadFilter`; * As pointed out in a different PR (#2355), make all the plugin arguments common.; * Make `isDisabled()` and `getAllInstances()` honor the `--disableToolDefaultReadFilters` argument (solves #2363). I think that this makes more sense than disable absolutely all the read filters, including the ones provided by the user. The cases where this is more useful are:; - Process the data without filters: provide just `--disableToolDefaultReadFilters`; - Process the data without default filters and add any other: provide `--disableToolDefaultReadFilters` and `--readFilter` with the rest of filters; - Process the data with the default filters and include more: as in the previous behaviour, provide `--readFilter` with the rest of filters; - Process the data with some default filters but change the order: provide `--disableToolDefaultReadFilters` and the list of filters in the new order. . Can you have a look to this one, @cmnbroad and/or @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2365#issuecomment-275633852:296,plugin,plugin,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2365#issuecomment-275633852,1,['plugin'],['plugin']
Modifiability,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:73,rewrite,rewrite,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,1,['rewrite'],['rewrite']
Modifiability,Some info on Spark configurations:. https://stackoverflow.com/questions/29441316/specifying-an-external-configuration-file-for-apache-spark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322552565:19,config,configurations,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322552565,2,['config'],"['configuration-file-for-apache-spark', 'configurations']"
Modifiability,"Some offline discussions have led us to the conclusion that this is best handled by tools upstream. Adapters should not be simply soft-clipped, so it shouldn't be the responsibility of M2 or HC to include logic to remove adapters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816:100,Adapt,Adapters,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816,2,"['Adapt', 'adapt']","['Adapters', 'adapters']"
Modifiability,"Some questions before this is code reviewed in detail:. 1) A number of query methods in GoogleGenomicsReadAdapter adapter; throw if the corresponding field is not present in the underlying read. For some; of these there are guard methods you can call to avoid this (see for example; the changes in ReadUtils.java), but for some of the others I'm not sure how to; usefully query the state without already knowing the answer, ie.:. -isSupplementaryAlignment; -isSecondaryAlignment,; -failsVendorQualityCheck; -isDuplicate; -mateIsReverseStrand. To have fidelity with SAMRecord.getSAMString , we need to be able to query these; (as does ReadUtils.getFlags, which has a similar problem, but I changed that to; use guard methods to prevent throwing). In a couple of cases I had to change; the Read adapter to not throw. We need to figure out if this kind; of change is ok. or what the alternative is. 2) This is incidental to this PR, but there are a few inconsistencies between how; GenomicsConverter.makeSAMRecord and ReadUtils compute derived state values, ie. flags.; I can work around these in the getSAMString tests (I'm using Read->SAMRecord; conversions to validate the tests), but the underlying format conversions; are inconsistent. Should we align them ?. For example, GenomicsConverter sets the firstInPair flag on the SAMRecord if readNumber==0,; even if numberOfReads==1, whereas the ReadUtils/GoogleReadAdapter requires readNumber==0; and numberOfReads==2. Likewise the unmapped flag is determined differently: Genomics converter: (http://google-genomics.readthedocs.org/en/latest/migrating_tips.html):; final boolean unmapped = (read.getAlignment() == null || ; read.getAlignment().getPosition() == null || ; read.getAlignment().getPosition().getPosition() == null);; ReadUtils:; private boolean positionIsUnmapped( final Position position ) {; return position == null ||; position.getReferenceName() == null || position.getReferenceName().equals(SAMRecord.NO_ALIGNMENT_REFERENCE_NAME) ||; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/871:114,adapt,adapter,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/871,2,['adapt'],['adapter']
Modifiability,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7739:153,adapt,adapter,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739,1,['adapt'],['adapter']
Modifiability,Some refactoring of where the main WDLs live. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/009b92ea-9b51-4ebe-8ddd-924c53f28a55).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8970:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8970,1,['refactor'],['refactoring']
Modifiability,"Some travis jobs are still failing even with the reduced set of CRAN mirrors. This is a possible alternative solution that restores the previous set of fallback repos, but relaxes the treatment of remote warnings. Might be overkill but it seems to work. See `R_REMOTES_NO_ERRORS_FROM_WARNINGS ` under https://github.com/r-lib/remotes#environment-variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5602:346,variab,variables,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602,1,['variab'],['variables']
Modifiability,Someday we could even figure out how to configure it at will!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/295#issuecomment-78489115:40,config,configure,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/295#issuecomment-78489115,1,['config'],['configure']
Modifiability,"Sorry @droazen @LeeTL1220, can you give me a bit more context? @LeeTL1220 is no longer using any of the CNV-specific collections classes that I had hoped might be Tribble-ized in the future, so I'm OK with any decisions you guys make that are specific to his classes (does @jonn-smith have an opinion?) I think that moving towards storing the config in the header is a good thing, in general. If we need to make corresponding changes to the CNV-specific collections classes, then we should talk more. Not all of those collections describe locatables, so I'm not sure how we could fit them in the Tribble framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330:343,config,config,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330,1,['config'],['config']
Modifiability,"Sorry for generating a big one:. I've tried to put the relevant commits together. . 10cdeba is the biggest, but mostly refactoring the breakpoint and complication logic into 2 classes: `BreakpointsInference` and `BreakpointComplications`, which now both have class hierarchies. After that cleanup, in the following commits I've put all the efforts for inferring the alt haplotype sequence into the class `BreakpointsInference`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4305:119,refactor,refactoring,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4305,1,['refactor'],['refactoring']
Modifiability,"Sorry, again confounded by static-blocks and inheritance (dup of my own issue https://github.com/broadinstitute/gatk/issues/3483)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946:45,inherit,inheritance,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946,1,['inherit'],['inheritance']
Modifiability,"Sorry, but this bug still isn't fixed as of v4.2.6.1. Reproduce as follows:. ```; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; ```. Instead of a run-time exception (as in v4.2.5.0), HaplotypeCaller simply produces no variant calls at all. Expected behavior would be to exclude paired-end mappings whose TLEN exceeds the parameterized value. Perhaps there is an implementation bug, unrelated to the original problem, that contains faulty logic for doing this. Thanks...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692:344,parameteriz,parameterized,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692,1,['parameteriz'],['parameterized']
Modifiability,"Sorry, it's difficult for me to spot git notifications in my email. . > Maybe @bshifaw can chime in? Are the featured workspaces covered by tests elsewhere? What is the current SOP for taking workflows from this repo, turning them into featured workspaces, and populating their configurations?. Example JSONs with input test data are usually introduced in the gatk-workflows git repos and carried over to the featured workspaces. That isn't to say they are not welcomed from the gatk repo. > @bshifaw related to what Sam was saying - we also have a few standard resources needed to run the workflows that we would like to share with users. What is the standard procedure for doing so? Ideally they would be bundled with featured workspaces, but also accessible from outside of Terra. Workflow resources files that are not already in [broad-references](https://console.cloud.google.com/storage/browser/broad-references) would be saved in the [gatk-best-practices](https://console.cloud.google.com/storage/browser/gatk-best-practices) bucket. In the past i've separated the resources files per workflow directory (e.g. pathseq, cnn-hg38) but you can organize them a different way if the resources files would be shared by other workflows (e.g. somatic-hg38, somatic-b37).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507703719:278,config,configurations,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507703719,1,['config'],['configurations']
Modifiability,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:548,Config,Configured,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193,1,['Config'],['Configured']
Modifiability,"Sounds like a good idea, thanks! I will use a copy of the current plugin in my project til all these changes of the plugin descriptor are in (both Barclay and my proposals here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-275707975:66,plugin,plugin,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-275707975,2,['plugin'],['plugin']
Modifiability,Spark metrics collector refactoring checkpoint.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:24,refactor,refactoring,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['refactor'],['refactoring']
Modifiability,Spark multilevel metrics collection refactoring.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1959:36,refactor,refactoring,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959,1,['refactor'],['refactoring']
Modifiability,"SparkCommandLineArgumentCollection does not support ""="" in the values of spark configuration variables",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3687:79,config,configuration,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3687,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"Spin up a public jenkins server for long-running validation tests (and other tests that can't or shouldn't run in travis), or switch from travis to a more flexible CI provider",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1400:155,flexible,flexible,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400,1,['flexible'],['flexible']
Modifiability,SplitNCigar reads walker refactor 1864,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1919:25,refactor,refactor,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1919,1,['refactor'],['refactor']
Modifiability,Stack trace from failed job:. ```; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Null value when trying to read system resource. Cannot find: org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval/annotated_region_default.config; 	at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsFile(Resource.java:90); 	at org.broadinstitute.hellbender.utils.codecs.AnnotatedIntervalCodec.<init>(AnnotatedIntervalCodec.java:55); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.lang.Class.newInstance(Class.java:442); 	at org.broadinstitute.hellbender.engine.FeatureManager.getCandidateCodecsForFile(FeatureManager.java:511); 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:464); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:324); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:304); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.Abstra,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:259,config,config,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,1,['config'],['config']
Modifiability,Standardize on system properties vs. environment variables,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1666:49,variab,variables,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1666,1,['variab'],['variables']
Modifiability,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2238:224,adapt,adaptor,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238,3,['adapt'],"['adapter', 'adaptor']"
Modifiability,Store the NCBI build version in the Gencode datasource config files; in order to resolve an issue where this value was not always available; when annotating IGR variants. Resolves #4404,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522:55,config,config,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522,1,['config'],['config']
Modifiability,"Subsampling seems to be the way to go, see #2858. For the record, I did try to implement caching, but this results in excessive cache checking. In general, I think a better solution is to structure code so that expensive global quantities are not unnecessarily recomputed locally. At some point, this sort of undesirable recomputation snuck in during a refactoring of the allele-fraction likelihood code, probably when we tried to make the method for computing site likelihoods pull double duty based on the presence or absence of an allelic PoN. With an allelic PoN, we need to compute a log gamma at each site based on the site-specific bias hyperparameters; without a PoN, we only need to do this once for all sites, since the bias hyperparameters are now global, but the code naively recomputes it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709:353,refactor,refactoring,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709,1,['refactor'],['refactoring']
Modifiability,"Substituting `STANDARD_CONFIDENCE_FOR_CALLING/3` for `STANDARD_CONFIDENCE_FOR_EMITTING` seems wrong, given that `STANDARD_CONFIDENCE_FOR_CALLING` is a user-configurable value. We talked to @vdauwera just now and she agrees -- we're going to close this PR here and open a ticket against GATK3 to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2194#issuecomment-259794842:156,config,configurable,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2194#issuecomment-259794842,1,['config'],['configurable']
Modifiability,"SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1199); at org.testng.TestNG.runSuitesLocally(TestNG.java:1124); at org.testng.TestNG.run(TestNG.java:1032); at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:111); at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:204); at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:175); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:125); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140); Caused by: com.google.cloud.dataflow.sdk.coders.CannotProvideCoderException: Cannot provide a Coder for type variable B because the actual type is unknown due to erasure.; at com.google.cloud.dataflow.sdk.coders.CoderRegistry.getDefaultCoder(CoderRegistry.java:545); at com.google.cloud.dataflow.sdk.coders.CoderRegistry.getDefaultCoder(CoderRegistry.java:516); at com.google.cloud.dataflow.sdk.coders.CoderRegistry.getDefaultCoder(CoderRegistry.java:165); at com.google.cloud.dataflow.sdk.transforms.ParDo$Bound.getDefaultOutputCoder(ParDo.java:741); at com.google.cloud.dataflow.sdk.transforms.ParDo$Bound.getDefaultOutputCoder(ParDo.java:660); at com.google.cloud.dataflow.sdk.transforms.PTransform.getDefaultOutputCoder(PTransform.java:334); at com.google.cloud.dataflow.sdk.values.TypedPValue.inferCoderOrFail(TypedPValue.java:152); at com.google.cloud.dataflow.sdk.values.TypedPValue.getCoder(TypedPValue.java:46); ... 46 more. ===============================================; Custom suite; Total tests run: 1, Failures: 1, Skips: 0; ===============================================; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248:5698,variab,variable,5698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/658#issuecomment-122314248,1,['variab'],['variable']
Modifiability,"Sure thing. I can find out which changes I needed to make in gatk to get certain tools to work, like `PrintReads` and `MarkDuplicates`, though they were certainly not exhaustive. We will also see about open sourcing our s3 nio library which is basically a rewrite of https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 with changes for handling s3 endpoints.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319206378:256,rewrite,rewrite,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319206378,1,['rewrite'],['rewrite']
Modifiability,"T : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - creat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4561,Config,ConfigFactory,4561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"TA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5280,Config,ConfigFactory,5280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"THe only issue with running PRs on Jenkins is that I would prefer if an ""admin"" (via the comment section -- can be any Broad employee) actually activate the test. Why? Because essentially this enables someone to run arbitrary code on OUR environment. While it's fairly locked down, we want some sort of verification that, at first glance, the code doesn't look like it's going to hack us. Jenkins itself is full of holes that can probably be exploited from a well-crafted job. I'd prefer to mitigate that. Will you or any Broad person be able to type, ""Test Please"" in the comments of a PR to kick off a PR? Otherwise I'll be doing this on Travis which has better sandboxing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287538625:664,sandbox,sandboxing,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287538625,1,['sandbox'],['sandboxing']
Modifiability,TODO: refactor duplicated VC generation code from PosteriorProbabilitiesUtilsUnitTest in ReblockGVCFUnitTest by extracting to VariantContextTestUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932:6,refactor,refactor,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932,1,['refactor'],['refactor']
Modifiability,"TOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3473,Config,ConfigFactory,3473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"TSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3757,Config,ConfigFactory,3757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,Test GATK4 with the existing S3 NIO plugin and get basic S3 read support working,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708:36,plugin,plugin,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708,1,['plugin'],['plugin']
Modifiability,"Testing the tool behavior when given an incomplete PED file. PED; <img width=""847"" alt=""screenshot 2019-01-22 16 42 07"" src=""https://user-images.githubusercontent.com/11543866/51567234-b42e3200-1e64-11e9-942c-2934980dc04a.png"">. Command; ```; gatk CalculateGenotypePosteriors \; -V precomputed/trioGGVCF.vcf.gz \; -ped duo.ped \; --skip-population-priors \; -O sandbox/duoCGP.vcf.gz; ```. Results; <img width=""842"" alt=""screenshot 2019-01-22 16 44 01"" src=""https://user-images.githubusercontent.com/11543866/51567337-ef306580-1e64-11e9-8ca6-051ccb3fa18d.png"">. The line of interest reads:; ```; 16:27:00.401 INFO CalculateGenotypePosteriors - No PED file passed or no *non-skipped* trios found in PED file. Skipping family priors.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-456575220:361,sandbox,sandbox,361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-456575220,1,['sandbox'],['sandbox']
Modifiability,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5847:414,polymorphi,polymorphic,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847,1,['polymorphi'],['polymorphic']
Modifiability,"Thank you @kshakir. What I see there is that the code sets the default NIO option, and as part of this is creates a google cloud `StorageOptions` object. Sadly for us, when this object is created it determines which Google credentials to use, and if nothing was specified by the user it will send some network messages to try to figure out whether it's running on a Google Compute Engine machine. When we wrote the default-setting code we didn't realize that setting the number of retries was going to cause a network message to be sent, with the associated potential retries and delays. We can't change the way Google Compute Engine works, or how the Google authentication works either. Ideally we'd want some way to only search for credentials when we know NIO is going to be used. The point of these defaults is that they're used for anything that uses NIO, including third-party library code. We can't fully replicate this behavior in a different way from the outside. So I think the ""correct"" fix would be to go deep inside the Google NIO library and change it so that instead of providing a default configuration (that the user would have to put together, causing the problem you've seen), we can provide a *callback* that sets the configuration when the Google Cloud NIO provider is loaded. This is harder for future developers to wrap their heads around, but at least it would prevent this delay if NIO is not used. I'd like to think about this some more before doing something quite this drastic, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504:1105,config,configuration,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504,2,['config'],['configuration']
Modifiability,"Thank you @mwalker174 for the suggestions. I ended up writing for loops to test which configurations work. Driver memory: 2-50g; executor memory: 2-50g; executor cores: 1-20; bamPartitionSize: 1-64m. Some combinations failed in minutes, some failed in hours, and some finished without errors. Bellow are three of which work for a ~33X WGS data:; ```; ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 20 ; --executor-memory 10g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 5 ; --executor-memory 50g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 64000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.core",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314:86,config,configurations,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314,1,['config'],['configurations']
Modifiability,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:1292,variab,variables,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,1,['variab'],['variables']
Modifiability,Thank you for all the work on GATK4 and for including a wrapper script to help in setting up Java options. I've included GATK4 in bioconda (https://anaconda.org/bioconda/gatk4) with the `gatk-launch` wrapper and wanted a way to be able to pass java options to the local run. This PR uses the `GATK_JVM_OPTS` environmental variable to pass Java options like memory specification to the gatk-launch script. Thanks for considering this change,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2778:322,variab,variable,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2778,1,['variab'],['variable']
Modifiability,"Thank you. From: Adam Kiezun ; Sent: Monday, May 11, 2015 5:07 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] #253 - Added ""final"" keyword to classes that are not inherited, added… (#512). Merged #512. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/512#issuecomment-101100346:193,inherit,inherited,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/512#issuecomment-101100346,1,['inherit'],['inherited']
Modifiability,"Thanks @davidbenjamin for essentially refactoring this code three times now!. Looking forward to reviewing your latest changes, but it may have to wait until early next week. Apologies for the delay. In the meantime, I see that there is a minor rebase conflict—up to you if you want to address it now, no biggie if you want to wait until after review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1070960072:38,refactor,refactoring,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1070960072,1,['refactor'],['refactoring']
Modifiability,"Thanks @droazen. Because you assigned it to me, I would like to know a couple of details on how this should be implemented in GATK4:. * GATK3 use to have a the `MisencodedBaseQualityReadTransformer` always on, with a switch for checking/fixing the qualities. If we follow this approach in GATK, the only change for this is to include the checking step every n reads and then #2160 will do the rest. Nevertheles, I think that it's quite dangerous to allow an user to disable it with the plugin (because the name suggest that it is only fixing the qualities), so I suggest to integrate in the read data source an iterator for checking every x reads if the qualities are misencoded, independently on the transformer.; * GATK3 throws an UserException for ""putatively misencoded"" qualities, using 60 as maximum base quality for throwing. I think that in the case of GATK4 could be more useful to use a warning if it is over 60 (I do not know what is the reasoning behind this value), and use `SAMUtils.MAX_PHRED_SCORE` for throwing. I'd be happy to implement this if there is a consensus about what to do here, so I'll wait for your ideas...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2082#issuecomment-288760814:486,plugin,plugin,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2082#issuecomment-288760814,1,['plugin'],['plugin']
Modifiability,"Thanks @lbergelson for looking into this. Users can definitely squash the image after pulling, and then push it to their private registries - that's the best workaround here, so this is likely a low-priority issue. Docker images can only be pulled by layers currently; there's no way to pull an image that has multiple layers with one HTTP request. In the [TES runner](https://github.com/microsoft/ga4gh-tes), we are also increasing the docker pull retry count to help. I'll try to update the `dockerfile` and send a PR, thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1935007776:251,layers,layers,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1935007776,2,['layers'],['layers']
Modifiability,"Thanks @lbergelson! I agree that it might be good to break into more layers—could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do that—could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:69,layers,layers,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,1,['layers'],['layers']
Modifiability,"Thanks @lbergelson. There are some issues with the current implementation of the filter plugin and barclay that is preventing me to use the latest master, but once they are solved I will try it out and if I find any problem I will report it. For me, as a solo developer, this is going to be amazing for keeping everything up-to-date without breaking compatibility before releases of my own software.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1995#issuecomment-279985534:88,plugin,plugin,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1995#issuecomment-279985534,1,['plugin'],['plugin']
Modifiability,"Thanks @mwalker174! I think I responded to or addressed everything. The code paths for reading TSVs all go through the abstract CNV collection classes. Those require a bit of boilerplate, but were IMO a huge improvement over the horrowshow of utility methods from the old code... Happy to discuss possible further refactoring and improvement (and there are already catch-all issues open), if needed. If we decide to stream other locatable collections, we can start to extract more of these streaming/subsetting methods to `AbstractLocatableCollection`, which would give us something like the `LocatableTableReader` you're envisioning in your edit. We've discussed using @jonn-smith's `XSVLocatableTable` machinery as well. I think the only downsides are the conventional reliance on extensions/config files for decoding, as well as the need to accommodate CNV headers. Encoding is also not handled. We also still need to represent non-Locatable TSVs, ideally with a minimal number of code paths, although that probably won't present any major refactoring issues. Also recall that we discussed moving from Files -> Paths in previous PRs, so we should instead go from Files -> FeatureDataSources where it makes sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770:314,refactor,refactoring,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770,3,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,"Thanks @ruqianl, you may want to read through the comments at https://github.com/broadinstitute/gatk/issues/6235 and the corresponding PR https://github.com/broadinstitute/gatk/pull/6244, which both address this issue. See also the following bit of documentation added in that PR:. > Advanced users may wish to set the THEANO_FLAGS environment variable to override the GATK theano configuration. For example, by running THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR"" gatk GermlineCNVCaller ..., users can specify the theano compilation directory (which is set to $HOME/.theano by default). See theano documentation at https://theano-pymc.readthedocs.io/en/latest/library/config.html. So you can specify a unique compilation directory for each of your jobs to avoid the compilelock, e.g., `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/0"" gatk GermlineCNVCaller ...`, `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/1"" gatk GermlineCNVCaller ...`, etc. Alternatively, you can increase `config.compile.timeout` as discussed in those comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899:344,variab,variable,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899,4,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:226,config,config,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432,1,['config'],['config']
Modifiability,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:183,parameteriz,parameterized,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['parameteriz'],['parameterized']
Modifiability,"Thanks for indulging me on this. To me it seems like `UnfilledReadsLikelihoods` diverges too much from `ReadsLikelihoods` to extend it. In effect it's letting `ReadsLikelihoods` sometimes be a wrapper for something that is not a `ReadsLikelihoods`. I haven't worked this out but I would hope that it's possible to construct a `ReadsLikelihoods` from a pileup. I mean, the idea of pileup calling is that you use just a single base for the likelihoods and not the whole read (via Pair-HMM), so we should be able to fill the likelihoods from the base qualities.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856:125,extend,extend,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856,1,['extend'],['extend']
Modifiability,"Thanks for looking into this @davidbenjamin. I followed the best practices using bwa mem, mark duplicates etc., to create these input bams for HaplotypeCaller. This is Novaseq 2 x 150 data, I ran Fastqc on the reads and everything looks really good, the only thing I can find that might explain the soft-clipping is that there's some Nextera adapter read through on a small percentage of the reads. I haven't been using -Y with bwa (I see it's used in GATK 4 wdls), so it seems like there should be less soft-clipping than normal. I'll admit these are definitely messy regions we're dealing with, but we really need to make the F5 calls for our clinical pipeline. I just tried --dont-use-soft-clipped-bases and I wasn't able to pick the SNP up in the 55-55003_F5_region.bam, but using forceActive/dontTrimActiveRegions does work on this call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402690747:342,adapt,adapter,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402690747,1,['adapt'],['adapter']
Modifiability,"Thanks for reporting this, @Stikus! That change you highlighted indeed fixes the issue. There was an oblique mention of issues with the previously specified version of pip in the comments of that PR. Note that we now use conda 23.10.0 with the libmamba solver in the GATK Docker image. Please feel free to reopen if you have issues with that specific configuration!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618#issuecomment-1851818671:351,config,configuration,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618#issuecomment-1851818671,1,['config'],['configuration']
Modifiability,"Thanks for that info and for sharing the files, @asmirnov239. I suspect that there are essentially two types of bins: ""nice"" and ""not so nice"". The sampling noise in the former is determined by Poisson observation noise, whereas that in the latter is determined by uncertainty in the bias posteriors. This is a bit hard to see in the plots above, and even in this version where I tried to adjust the point size and alpha:. ![image](https://user-images.githubusercontent.com/11076296/137733810-16a79ea9-ea7b-47cc-a42f-40130a949015.png). However, plotting a measure of the difference in the dCRs (from 20 and 200 posterior samples) vs. the dCR is more suggestive:. ![image](https://user-images.githubusercontent.com/11076296/137734587-1b9f6551-74b2-4097-a02c-f51d7341251c.png). As are the dCR histograms:. ![image](https://user-images.githubusercontent.com/11076296/137733867-ce0f5573-a5cc-412c-9060-56fbb09d1ef0.png). I would guess that the nice spike around CR ~ 2 and the fatter base extending up to dCR ~ 100 are distinct populations of bins. So the punchline would be that differences at high dCR are probably just noise within the noise. For ""nice"" bins at dCR ~ few, the sampling noise looks to be <1%. Not really sure what's going on at very high dCR, but I think it's safe to say that these are ""not so nice"" bins!. I've seen this pattern in other WES cohorts when plotting the posterior means vs. std devs for the biases; tried to dig up the plots on Slack, but I can't find them at the moment. Perhaps something along those lines might be worth visualizing in your model-criticism notebooks, if you don't already?. Again, hard to say this is indeed the case from the dCRs alone, but if so, it might be worth baking this sort of mixture into future versions of the model or coming up with other strategies to deal with such bins.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946:985,extend,extending,985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946,1,['extend'],['extending']
Modifiability,"Thanks for the analysis @laserson! Not sure I fully understand it yet, but will re-read it until I do :) By tomorrow I should have plots for https://github.com/broadinstitute/gatk/issues/995 comparing JP's existing code against the broadcast approach, and will link them here -- let's decide whether or not to devote more effort to refactoring JP's code after seeing those numbers. In the meantime, recommend focusing on https://github.com/broadinstitute/gatk/issues/1015, which is probably the most urgent blocking issue for spark at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151881484:332,refactor,refactoring,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1007#issuecomment-151881484,1,['refactor'],['refactoring']
Modifiability,"Thanks for the feedback, @cmnbroad.  @droazen, should I open a ticket for implement the plugin and close this issue? What's about the checking of the quals?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652:88,plugin,plugin,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652,1,['plugin'],['plugin']
Modifiability,"Thanks for the review.; Regarding the code complexity, I definitely agree. I was trying to not do too much code surgery. For your suggestion regarding just outputting the total, perhaps if I name it something like total optical duplicates? The issue is that you could have two optical duplicate clusters, but I guess we don't really care, right? We just need the total?; Also, I can rewrite the test to load the information in as a map and then just make sure the sizes are the same and query the map. Would that be better?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/749#issuecomment-127370106:383,rewrite,rewrite,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/749#issuecomment-127370106,1,['rewrite'],['rewrite']
Modifiability,"Thanks for the suggestion, @droazen! I did this PR before the read filter plugin was included, and actually I was thinking about remove this PR because it is very clunky. Should I close this and open a discussion about the plugin?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245960477:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245960477,2,['plugin'],['plugin']
Modifiability,"Thanks guys!. On Sat, Sep 23, 2017 at 11:38 PM, David Benjamin <notifications@github.com>; wrote:. > *@davidbenjamin* requested changes on this pull request.; >; > Done with my review. Mainly the usual stuff about writing more idiomatic; > Java that all C++ coders go through!; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646010>:; >; > > +import org.broadinstitute.hellbender.utils.IntervalUtils;; > +import org.broadinstitute.hellbender.utils.SimpleInterval;; > +; > +import java.io.File;; > +import java.util.List;; > +; > +; > +; > +@CommandLineProgramProperties(; > + summary = ""Split intervals into sub-interval files."",; > + oneLineSummary = ""Split intervals into sub-interval files."",; > + programGroup = VariantProgramGroup.class; > +); > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; >; > @samuelklee <https://github.com/samuelklee> is the boss of the copy; > number code, but personally I don't see the need to be extremely concise; > with short names and would prefer width.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646054>:; >; > > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; > + public static final String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + pri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:967,extend,extends,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['extend'],['extends']
Modifiability,"Thanks. This shows that bcast can go faster when given more RAM: going from; the earlier experimental envelope of 192GB of RAM to 256GB instead, the; bcast version can go from 19 min to 10min (sharded takes 7min with the; small amount of memory). This is consistent with my earlier experiments; that also showed that increasing the RAM-per-core sped up the bcast; computation. I wouldn't worry about having found a configuration where sharded performs; poorly. My experiments so far have shown that there are lots of; configuration points where one or the other performs poorly; the onus is on; finding the good configuration values that result in better performance, or; identifying which algorithms may be performant for a larger range of; parameter choices. To maximize understanding I would suggest that we not change too many; parameters at a time. For example it seems unnecessary to have multiple; variants of the 128Mbp input. I found there can be significant (>50%) variation between identical runs.; One way to reduce this is to set spark.task.maxFailures=1 which I strongly; recommend for all experiments going forward. One concern I have is that our cluster memory is already far larger than; the input size. In fact here each machine can fit the whole input; comfortably in RAM. This is not going to be the case for the full input.; Since it would take too long to iterate using the full input, it seems wise; instead to reduce both the input size and to keep a close eye on the amount; of memory we're using to make sure we're not going down a path that would; not be able to cope with the full input. On Wed, Nov 18, 2015 at 11:08 AM, droazen notifications@github.com wrote:. > I did some additional runs on the Broad cluster on a 14 GB bam, twice as; > large as the bam used in the plot above. This was with 60 cores, 4 cores; > per executor, and 16 GB of memory per executor. Results:; > ; > Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; > Sharded (3 runs): 19m33.310s, 13m3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734:415,config,configuration,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734,3,['config'],['configuration']
Modifiability,That code checks whether DP is 0 or the maximum PL value is 0. If any of the conditions is satisfied then plugin will assign nocall ./. to that site.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-2117184307:106,plugin,plugin,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-2117184307,1,['plugin'],['plugin']
Modifiability,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:975,config,config,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816,2,['config'],['config']
Modifiability,"That sounds like a good thing to look at. If someone has already written it, it would be great to not have to rewrite it..",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4086#issuecomment-356366388:110,rewrite,rewrite,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4086#issuecomment-356366388,1,['rewrite'],['rewrite']
Modifiability,"That's a good sign. However the code above only checks the providers at the client. It'd be interesting to also check what happens at the workers. I wrote ExampleNioCheckFS for this purpose earlier, you can use it. It's only in a test branch of mine (since it's just test code) but it's pretty short. Looks like this:. ````java; /**; * Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector.; */; @CommandLineProgramProperties(; summary = ""Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector"",; oneLineSummary = ""Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector"",; programGroup = ReadProgramGroup.class; ); public class ExampleNioCheckFS extends SparkCommandLineProgram {; private static final long serialVersionUID = 1L;. @Argument(fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = ""Output file (if not provided, defaults to STDOUT)"", common = false, optional = true); private File OUTPUT_FILE = null;. @Argument(fullName = ""inputPath"", shortName = ""P"", doc = ""Input path (eg. gs://foo/bar.bam)"", optional = false); private String path = null;. // Typically set to number of executors times number of cores per executor.; @Argument(fullName = ""parts"", doc = ""number of partitions"", optional = false); private int parts = 3;. private void countReads(JavaSparkContext ctx) {; PrintStream outputStream;. try {; outputStream = OUTPUT_FILE != null ? new PrintStream(OUTPUT_FILE) : System.out;; }; catch ( FileNotFoundException e ) {; throw new UserException.CouldNotReadInputFile(OUTPUT_FILE, e);; }. NioBam input = new NioBam(path, path + "".bai"");; List<String> ret = input.getReads(ctx, parts).mapPartitions(ExampleNioCheckFS::getFS).collect();; outputStream.println(""**** Results **** : "" + String.join("", "", ret));; }. private static Iterator<String> getFS(Iterator<SAMRecord> rs) {",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267424466:785,extend,extends,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267424466,1,['extend'],['extends']
Modifiability,"That's what we do for the spark jar, easy to extend it to work for a non-spark jar too. What I mean though, is when someone downloads a packaged gatk, what do they get/ how do they install it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207474072:45,extend,extend,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1693#issuecomment-207474072,1,['extend'],['extend']
Modifiability,"That's why I am not using in ReadTools and other developmental toolkit the base class from GATK, due to the polluted command line with unused arguments. I think that for give flexibility, some of that arguments should be configurable by extending classes. For example, some tools that does not require reads at all should be able to turn off the read arguments. That will be very useful, although I am not sure how to do it in a proper way without adding more and more interfaces for argument collections. In context case of this PR, I think that adding it does not have any real effect on the GATK codebase, and a lot is gained by downstream projects. For example, if the wrapper script adds another argument that should be parsed in `Main` and documented, the GATK team just add it to its class. If a toolkit has a similar wrapper script, it can also add its own only-doc argument by simply overriding the method...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090:221,config,configurable,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090,2,"['config', 'extend']","['configurable', 'extending']"
Modifiability,"The AlleleFrequencyQC tool subclasses VariantEval, but doesn't provide it's own tool annotations, so it inherits VariantEval's `@BetaFeature` status and command line description. It also appears to directly clobber several of the command line argument values provided by the user, including the name of the output file. It should have its own `@CommandLineProgramProperties` and `@BetaFeature/@Experimental` annotations, and preferably better argument handling. Longer term, when https://github.com/broadinstitute/gatk/issues/5439 is done, it should be refactored so it uses the `VariantEval` engine class that will be part of that work, instead of subclassing the VariantEval tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6997:104,inherit,inherits,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6997,2,"['inherit', 'refactor']","['inherits', 'refactored']"
Modifiability,The Engine Team discussed this internally and we're going to pull out a subset of all the configuration options into the config file. These options should be those that will change only infrequently (like the data sources directory).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685:90,config,configuration,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685,2,['config'],"['config', 'configuration']"
Modifiability,"The HaplotypeCaller has slightly different behavior between VCF and GVCF output in some cases, which means that the variants at the edges of the active region may not be called in both. This is due to the following line:; https://github.com/broadinstitute/gatk/blob/89ea9e01225db5c9bbe262c888a0abb74509f94c/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyRegionTrimmer.java#L318. The behavior for VCF mode should be made to conform to GVCF mode by defining `callableRegion = originalRegion.trim(callableSpan, extendedSpan);`. Super easy fix, but will break a bunch of tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5676:543,extend,extendedSpan,543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5676,1,['extend'],['extendedSpan']
Modifiability,"The MendelianViolation class, as ported from GATK3, is used both to get mendelian violation state for a single variant/set of samples, as well as an accumulator for counting violations for multiple variants. It contains two isViolation methods, one of which clobbers the cumulative state without warning. The class should be refactored to make the two usage patterns distinct and less prone to accidental misuse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5034:325,refactor,refactored,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5034,1,['refactor'],['refactored']
Modifiability,"The VCFHeader and header line hierarchy classes in htsjdk need refactoring to fix a number of bugs, and to support clean handling of new versions of VCF and BCF. This work is mostly done, PR is [here](https://github.com/samtools/htsjdk/pull/835).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2601:63,refactor,refactoring,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2601,1,['refactor'],['refactoring']
Modifiability,The actual code for the funcotation factories is all set up for this. The required update is that `GencodeFuncotationFactory` needs to be refactored to take in the name of the data source. Right now it's assumed that it can only be `Gencode`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3956#issuecomment-378314286:138,refactor,refactored,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3956#issuecomment-378314286,1,['refactor'],['refactored']
Modifiability,"The behavior of the GATK3 CombineVariants was very inconsistent and the arguments weren't entirely clear. I also suspect that some operations weren't possible with the arguments given. Rather than port that old broken version, I would advocate for an overhaul or rewrite. @bhanugandham it's going to be a big project to collect requirements and expected behavior for this tool. For example, what should the MQ be for the combined VCF for two different input VCFs with different MQ values? Much of the confusion stemmed from the old ability to merge VCFs containing the same sample. In the case where we take one genotype for each sample name (e.g. the old ` -genotypeMergeOptions PRIORITIZE`) then I believe the old behavior was wrong in some cases, taking the filter status from an input VCF at random. We also need to clarify `FilteredRecordMergeType` options, e.g. https://github.com/broadinstitute/gsa-unstable/issues/935",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167:263,rewrite,rewrite,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167,1,['rewrite'],['rewrite']
Modifiability,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3068:529,variab,variable,529,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068,1,['variab'],['variable']
Modifiability,"The build.gradle code below builds the native shared library for AVX PairHMM using gcc and copies the .so file to the desired location. The jar task will archive the .so file in the GATK jar file. ``` gradle; apply plugin: 'cpp'; model {; components {; VectorLoglessPairHMM(NativeLibrarySpec) {; binaries.withType(SharedLibraryBinarySpec) { binary ->; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include""; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include/linux""; cppCompiler.args ""-mavx""; linker.args ""-static-libgcc"". task copySharedLib(type: Copy) {; from binary.tasks; into ""build/classes/main/org/broadinstitute/hellbender/utils/pairhmm""; }; jar.dependsOn copySharedLib; }; // skip static library build; binaries.withType(StaticLibraryBinarySpec) { binary ->; buildable = false; }; }; }; }; ```. The gradle gcc plugin expects to find the C++ source code in the default location shown below. We can use a different directory structure, if desired. ```; src/; |-- main; |-- test; `-- VectorLoglessPairHMM; |-- cpp; `-- headers; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1492:215,plugin,plugin,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1492,2,['plugin'],['plugin']
Modifiability,The client secret file or the api should be able to be set as an environment variable or in a properties file somewhere so they don't have to be entered every single time someone runs a program.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/701:77,variab,variable,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/701,1,['variab'],['variable']
Modifiability,"The code difference between OSX and Linux is pretty small. . Here's the change in the native code. ; - Linux will define the `threads` variable and use it in the OpenMP pragma.; - OSX will not see the `threads` declaration and the OpenMP pragma will be a comment. ``` C; #ifdef linux; int threads = min((int)maxNumThreadsToUse, omp_get_max_threads());; #endif; #pragma omp parallel for schedule(dynamic, 1) num_threads(threads); ```. We also need a minor change in `build.gradle`. I'll submit the changes to this PR and you can decide from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218857915:135,variab,variable,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813#issuecomment-218857915,1,['variab'],['variable']
Modifiability,"The code is hard to read and I don't know how to get that info from IntelliJ, but I think we need all of the subclasses in walkers/na12878kb/ including all in assess/ and probably most in core/:. gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/:; ExportReviews.java; ExtractConsensusSites.java; ImportCallset.java; ImportReviews.java; NA12878DBWalker.java; NA12878KnowledgeBaseServer.java; SummarizeConsensus.java; UpdateConsensus.java. gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/assess:; AllSitesWriter.java; AssessNA12878.java; Assessment.java; AssessmentType.java; Assessor.java; BadSitesWriter.java; ROCCurveNA12878.java (a different walker, but something else we use often); SitesWriter.java. gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/core:; CallSet.java; ConsensusMaker.java; ConsensusSummarizer.java; MongoDBManager.java; MongoGenotype.java; MongoVariantContext.java; NA12878DBArgumentCollection.java; NA12878KBMain.java; NA12878KnowledgeBase.java; *NewlyAddedSites.java; *OneChunkIterator.java; PolymorphicStatus.java; *RawSiteIterator.java; SiteIterator.java; SiteManager.java; SiteSelector.java; TruthStatus.java; - I haven't come across these classes before, so I'm not sure if they're essential. (I haven't explored this code, but it's probably necessary); gsa-unstable/private/gatk-tools-private/src/main/java/org/broadinstitute/gatk/tools/walkers/na12878kb/core/errors:; InvalidRecordHandler.java; InvalidRecordsLogError.java; InvalidRecordsRemove.java; InvalidRecordsThrowError.java; MongoVariantContextException.java",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/55#issuecomment-94767432:1176,Polymorphi,PolymorphicStatus,1176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/55#issuecomment-94767432,1,['Polymorphi'],['PolymorphicStatus']
Modifiability,The concise message is:. ```; cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle; > Configure project :; Executing: git lfs pull --include src/main/resources/large. > Task :condaStandardEnvironmentDefinition; Created standard Conda environment yml file: gatkcondaenv.yml. > Task :pythonPackageArchive; Created GATK Python package archive in /home/cb2/gatk/build/gatkPythonPackageArchive.zip. > Task :gatkDoc FAILED; Unable to find the 'javadoc' executable. Tried the java home: /usr/lib/jvm/java-11-openjdk-amd64 and the PATH. We will assume the executable can be ran in the current working folder. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Get more help at https://help.gradle.org; ```. And stacktrace flag output looks like:. ```; `cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle --stacktrace; > Task :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(Execu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:77,Config,Configure,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['Config'],['Configure']
Modifiability,"The current FindBreakpointEvidence code is classifying reads pairs that overlap such that the start position of the reverse read is before the start position of the forward read as ""OutiesPair"" discordant read pair evidence. However, these are likely due to sequencing of very short inserts that causes some of the adapter to be sequenced and potentially aligned. This change requires a read pair to not be overlapping to be counted as an 'OutiesPair'. On the CHM dataset this causes the number of intervals discovered to drop from 23152 to 21633, and the number of called variants to drop from 3467 to 3366. . @tedsharpe could you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2515:315,adapt,adapter,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515,1,['adapt'],['adapter']
Modifiability,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213:181,config,configuring,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213,2,['config'],"['configuration', 'configuring']"
Modifiability,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4493:405,config,configuration,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493,1,['config'],['configuration']
Modifiability,"The debug variable of AsseemblyResultSet wasn't set anywhere, and therefore the command line argument debug didn't propagate to the function buildEventMapsForHaplotypes in EventMap.java. I added the function setDebug to AssemblyResultSet, and set its value in HaplotypeCallerEngine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5455:10,variab,variable,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5455,1,['variab'],['variable']
Modifiability,"The docker image only uses OpenJDK. However, the way travis is configured, the docker tests will be run once for OpenJDK and once for OracleJDK, but that distinction has no meaning, since the native JVM on the travis VM is irrelevant to what is happening in the docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2748:63,config,configured,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2748,1,['config'],['configured']
Modifiability,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2404:424,variab,variable,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404,1,['variab'],['variable']
Modifiability,"The fact that it's circular means that there are reads and bases right up against the edge of the contig and often have soft clips that extend beyond the contig. So it looks a bit like this:. <img width=""945"" alt=""screen shot 2018-07-19 at 11 37 45 am"" src=""https://user-images.githubusercontent.com/13020550/42953344-3ba4f544-8b48-11e8-9250-db75e56c2069.png"">. So yes, circular means it's probably throwing in a weird edge case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036#issuecomment-406322137:136,extend,extend,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036#issuecomment-406322137,1,['extend'],['extend']
Modifiability,The fact that you can print stack traces on UserException is not very discoverable. We should probably include instructions to do so in the UserException message itself. We might want to write the stack trace to a file so that people don't have to rerun the program to get it as well. . It's also weird that it's set through an environment variable instead of as an argument. (Although it may be difficult to implement as an argument since it has to be set correctly even if argument parsing fails. @cmnbroad Any thoughts on that? ),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443:340,variab,variable,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443,1,['variab'],['variable']
Modifiability,"The file size only went from 1.9MB to 3MB -- there's no perceptible; difference in test runtime that I can see. On Wed, Apr 5, 2017 at 2:04 PM, droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In src/test/java/org/broadinstitute/hellbender/tools/spark/; > ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest.java; > <https://github.com/broadinstitute/gatk/pull/2540#discussion_r109987028>:; >; > > +; > +; > +public class ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest extends CommandLineProgramTest {; > +; > + @Override; > + public String getTestedToolName() {; > + return ParallelCopyGCSDirectoryIntoHDFSSpark.class.getSimpleName();; > + }; > +; > + @Test(groups = {""spark"", ""bucket""}); > + public void testCopyFile() throws Exception {; > + MiniDFSCluster cluster = null;; > + try {; > + final Configuration conf = new Configuration();; > + // set the minicluster to have a very low block size so that we can test transfering a file in chunks without actually needing to move a big file; > + conf.set(""dfs.blocksize"", ""1048576"");; >; > Instead of switching to a larger file, is it possible to just decrease the; > block size further? (thinking about test runtimes here); >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2540#discussion_r109987028>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZYV_vD1XwS5IPvZiNZKOe6QzDJDVks5rs9e2gaJpZM4MtGXX>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-291948506:555,extend,extends,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-291948506,3,"['Config', 'extend']","['Configuration', 'extends']"
Modifiability,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:236,variab,variable,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,2,['variab'],['variable']
Modifiability,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2357:237,extend,extends,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357,5,"['extend', 'plugin']","['extends', 'plugin']"
Modifiability,The generated online doc currently doesn't include the names of the default annotations or annotation groups used by various tools. The values are already propagated from the annotation plugin to the freemarker map by Barclay; it should be easy to update the freemarker template to display these similar to the way we display default read filters.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5577:186,plugin,plugin,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5577,1,['plugin'],['plugin']
Modifiability,"The help message was wrong when an environment variable was missing. I've changed it so the same string is used to lookup the variable and report it missing so that can't ever be broken again. This does change it from loading the variables once at startup to loading them every time they are queried. I assumed that isn't an issue, but I can change it to cache them if someone can see a problem with that.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/572:47,variab,variable,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/572,3,['variab'],"['variable', 'variables']"
Modifiability,"The htsjdk and Hadoop-BAM parts of this are done (some refactoring will be needed for ReadSparkSink). The gating factor is getting Hadoop-BAM to move to Java 8 (this will be a topic for the call with Hadoop-BAM scheduled for 1/21) and upgrading to a newer htsjdk with ref-factored CRAM support. It would possible to implement this without that upgrade, but the it would require each partition to write a complete CRAM file, which would make the merging at the end much less efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-171421991:55,refactor,refactoring,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-171421991,1,['refactor'],['refactoring']
Modifiability,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:116,refactor,refactored,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,1,['refactor'],['refactored']
Modifiability,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:78,config,configurable,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183,1,['config'],['configurable']
Modifiability,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4556:306,plugin,plugin,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556,1,['plugin'],['plugin']
Modifiability,The list of packages used by GATKReadFilterPluginDescriptor and GATKAnnotationPluginDescriptor to find plugin instances should be a configurable setting.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4036:103,plugin,plugin,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4036,2,"['config', 'plugin']","['configurable', 'plugin']"
Modifiability,The main issue with this task was that the query results were being limited to 100 by default. So we use the -n param now in the query. Another issue was that we were running bq show on a table variable $TABLE which is never defined.; I also changed this because the approach (returning all the samples names of the samples that have been loaded) didn't seem scalable. I wanted to only return at most the number of samples we are trying to ingest.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230:194,variab,variable,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230,1,['variab'],['variable']
Modifiability,"The master branch failed on BaseRealibratorSpark when running WGS. Try to test this branch, but got hit by a strange error message. The jar file looks right to me. @tomwhite did you have some environment variables? . ````Using GATK jar /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar; Running:; /home/genomics/Projects/spark/bin/spark-submit --master spark://n001:7077 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --executor-memory 25G --driver-memory 5G /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar BaseRecalibratorSpark -I hdfs://n001:54310/GATK4TEST/LargeBroadData/WGS-G94982-NA12878.bam -knownSites hdfs://n001:54310/GATK4TEST/DBSNP/dbsnp_138.hg19.vcf.gz -R hdfs://n001:54310/GATK4TEST/OldData/human_g1k_v37.2bit -O hdfs://n001:54310/GATK4TEST/LargeOutput/WGS_BQSR --sparkMaster spark://n001:7077; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; java.lang.ClassNotFoundException: org.broadinstitute.hellbender.Main; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.ja",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877:204,variab,variables,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877,1,['variab'],['variables']
Modifiability,"The model currently gives each read an independent latent variable indicating which haplotype it was derived from. This latent variable should be a property of fragments, not reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5085:58,variab,variable,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5085,2,['variab'],['variable']
Modifiability,"The naming of the different classes (`AlignmentRegion`, `AssembledBreakpoint`, `BreakpointAllele`) was very confusing especially with the id variable `breakpointId` in several classes. I've renamed `AssembledBreakpoint` to `BreakpointAlignment`, since I think that the main thing it's trying to capture is a split alignment that's indicating there might be a breakpoint at a given location. Then, the actual breakpoint is represented by `BreakpointAllele` which the information about the breakpoint junction, but not the alignment-related fields. Does that make more sense?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240475247:141,variab,variable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240475247,1,['variab'],['variable']
Modifiability,"The output printed at startup (`CommandLineProgram.printStartupMessage()`) should be easy for toolkits that build on top of GATK to customize. Currently it can be customized via overriding methods, but this is awkward if you want to extend built-in walker classes rather than your own subclass of `CommandLineProgram`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101:233,extend,extend,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101,1,['extend'],['extend']
Modifiability,"The packages for codecs is a key feature for downstream tools implementing new codecs for other formats or to include overrides of codecs already included. Nevertheless, the current implementation (at version 4.0.0.0) the only way of configuring this is at the package level using the `codec_packages` configuration. I request support for the following fine-grained configuration:. * Add/Remove concrete codec classes; * Exclude single classes from a concrete `codec_package` specified (this can be done by the previous requirement if it uses fully qualified codec names); * Exclude sub-packages from a concrete `codec_package` specified. Representing this in an YML format, I would like to have the ability to configure the codecs as following:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude_class: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - exclude_package: gencode; - org.magicdgs.htsjdk.codecs; - classes:; - org.external.htsjdk_codecs.CustomBedCodec; ```. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:234,config,configuring,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,4,['config'],"['configuration', 'configure', 'configuring']"
Modifiability,"The pedigree-checking warning for the PossibleDeNovo annotation is always output because it happens in the constructor, and in GATK4 all the InfoFieldAnnotations get instantiated. It's a little weird to have this warning even when the annotation is not requested. But if this is the cost we pay for getting rid of the PluginManager I will gladly deal with it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3714:318,Plugin,PluginManager,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3714,1,['Plugin'],['PluginManager']
Modifiability,"The problem is that htsjdk supports reading multiple versions of vcf, but only knows how to write the current version. Traditionally this has worked because older vcf versions could be trivially written out as a newer version. But v4.3 restricts some values to a narrower range than previous versions, so its not always possible to write out a pre-v4.3 version as a v4.3 compliant file in a non-destructive way. Hence the need to refactor to better support full versioning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-472037659:430,refactor,refactor,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-472037659,1,['refactor'],['refactor']
Modifiability,The problem looks like it's due to the Hadoop version. Hadoop 2.7.0 or later is required for https://issues.apache.org/jira/browse/HDFS-3689. We rely on this change to concatenate the VCF parts together (which are variable lengths).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218#issuecomment-546884621:214,variab,variable,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218#issuecomment-546884621,1,['variab'],['variable']
Modifiability,The problem seems to be fixed in picard with @cmnbroad's change to the cloud configuration. Thew pom for 2.18.1+ looks like it won't include nio.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4556#issuecomment-375432298:77,config,configuration,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556#issuecomment-375432298,1,['config'],['configuration']
Modifiability,"The problem seems to be that some of the ""deep"" filters inherited from ReadWalker require to look into the read-base/qualities (just to compare their lenghts) which seems to be quite costly to decode and some of the read-attributes when this particular tool does not need to look into those fields. . It seems to me that we could override these check with a more lightweight alternative. . <img width=""1009"" alt=""screen shot 2018-09-27 at 3 03 36 pm"" src=""https://user-images.githubusercontent.com/791104/46168553-d32a0800-c266-11e8-96fd-d60d7c0380d5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425207954:56,inherit,inherited,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425207954,1,['inherit'],['inherited']
Modifiability,"The properties needed are listed here (for testing in this case): https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java#L83-L86. They are; * `fs.gs.impl`; * `fs.AbstractFileSystem.gs.impl`; * `fs.gs.project.id`; * `google.cloud.auth.service.account.json.keyfile`. Note that to set them as Spark configuration values, they need to be prefixed with `spark.hadoop`. So from the `spark-submit` command line you would write. ```; --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500745498:374,config,configuration,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500745498,1,['config'],['configuration']
Modifiability,The rebasing is non-trivial due to the [Allele Subsetting Refactoring](https://github.com/broadinstitute/gatk/commit/df9ad9b56c9eeaabb8b39ad63731edcf7aaf3a70). I will add unit tests for the new SAC methods once this is complete.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-238874246:58,Refactor,Refactoring,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-238874246,1,['Refactor'],['Refactoring']
Modifiability,"The regression tests added as part of #4344 fulfill this requirement. However, they need to be refactored to take advantage of the newly-included full `hg19` and `hg38` reference sequences.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5295#issuecomment-430312265:95,refactor,refactored,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5295#issuecomment-430312265,1,['refactor'],['refactored']
Modifiability,"The script is dangerous in its current state because it uses rm -Rf ${dir} arguments which can result in unwanted deletion if something goes wrong with the input arguments. We should either fix those arguments or remove the necessity for the script to be run with root permissions altogether to avoid any future problems that might arise and make the script safer for users. Additionally, the whole script could be refactored to be cleaner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3798:415,refactor,refactored,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3798,1,['refactor'],['refactored']
Modifiability,"The snapshot builds get published to an artifact repository, but I don't think those are accessible from outside of Broad. The build from this morning with your branch is [here](https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot-local/org/broadinstitute/gatk/4.0.11.0-30-g9c4a27b-SNAPSHOT/) if you can access it. Otherwise, for local development, you can do the following:. - pull gatk master from today so it includes your commit; - run `git fetch --tags` (this is optional but it will give your local build a more reasonable version tag); - run `./gradlew install printVersion` to install the locally built gatk into your local machine's maven repository; - change your VariantQC gradle project to include the `maven` gradle plugin if its not already there; - add `mavenLocal()` to your projects' `repositories `closure; - change your gatk dependency to the version number printed out by 'printVersion'; - rebuild VariantQC. Having said all that, what code are you dependent on ? I expect the command line interface to VariantEval, and the VariantUtils and StratificationManager and friends classes all to undergo some refactoring and evolve a bit before the tool has the beta tag removed and the interfaces are stabilized. See https://github.com/broadinstitute/gatk/issues/5439 and https://github.com/broadinstitute/gatk/issues/5440.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440782148:737,plugin,plugin,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440782148,3,"['evolve', 'plugin', 'refactor']","['evolve', 'plugin', 'refactoring']"
Modifiability,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:140,refactor,refactoring,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525,1,['refactor'],['refactoring']
Modifiability,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1465:255,inherit,inherited,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465,1,['inherit'],['inherited']
Modifiability,"The underlying issue here is is that the GATK conda env environment isn't established since bioconda doesn't appear to configure it. The NPE needs is fixed by #7816. In this particular case it appears that some of the requirements are satisfied, since the code gets past the initial check to see if the GATK python code is available. But then the actual CNN code can't be loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269:119,config,configure,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269,1,['config'],['configure']
Modifiability,"The use of Targets to refer to genomic intervals is unnecessary and confusing. It obfuscates the fact that most of the tools and code can be applied to not only counts from WES targets, but also counts from WES baits, WGS bins, etc. Requiring that Targets be named also adds unnecessary storage and memory burden. We should just use SimpleIntervals everywhere. We should also get rid of the target file format. In terms of external visibility, we can just rename tools and edit javadoc. Internally, there will be many classes that need to be both renamed and refactored. I instead suggest that we rebuild new versions of the classes and tools as necessary in the tools/copynumber package. - [ ] Rename tools: AnnotateTargets -> AnnotateIntervals, TargetCoverageSexGenotyper -> ReadCountSexGenotyper. ; - [ ] Deprecate tools: CalculateTargetCoverage, ConvertBedToTargetFile, and PadTargets will be replaced by @asmirnov239's new CollectReadCounts tool and on-the-fly padding specified by --interval_padding parameters.; - [ ] Deprecate target file format and change all other affected file formats.; - [ ] Refactor/rename/rebuild classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3246:559,refactor,refactored,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246,2,"['Refactor', 'refactor']","['Refactor', 'refactored']"
Modifiability,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5440:197,variab,variables,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440,1,['variab'],['variables']
Modifiability,The various *Context objects should be refactored to return empty lists upon lack of input instead of being Optional,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/244:39,refactor,refactored,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/244,1,['refactor'],['refactored']
Modifiability,"The workflow in cnv_somatic_panel_workflow.wdl defines:; "" Int? mem_gb_for_create_read_count_pon"". But when it's used in the call to CreateReadCountPanelOfNormals, the variable is referred to as ""mem_for_create_read_count_pon"" (i.e. no ""_gb""). This causes the workflow to fail...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4281:168,variab,variable,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4281,1,['variab'],['variable']
Modifiability,"Then we need to refactor the test to write the file itself, since HTJDK won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104392000:16,refactor,refactor,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/526#issuecomment-104392000,1,['refactor'],['refactor']
Modifiability,"Theory from @cmnbroad is below:. ```; I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, dbpedia/distributed-extraction-framework#9.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579:504,Config,Configuration,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579,1,['Config'],['Configuration']
Modifiability,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5764:334,Config,Configure,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764,1,['Config'],['Configure']
Modifiability,"There are a number of Python-based unit tests for `NVScoreVariants` in `src/main/python/org/broadinstitute/hellbender/scorevariants/tests`, which use the standard Python `unittest` framework (https://docs.python.org/3/library/unittest.html). We should ideally hook these up to the GATK test suite, and run them either via a gradle plugin for Python tests, or via a `PythonScriptExecutor` from a Java-based TestNG test. We'll need to figure out how to parse the test report for these tests and get the results to display nicely in github alongside the Java-based test results. We'll also need to look into whether the tests are cleaning up temp files properly, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9011:331,plugin,plugin,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9011,1,['plugin'],['plugin']
Modifiability,"There are a number of skipped tests on a successful run of `AlleleListUtilsUnitTest` These are deliberately skipped because the tests share a single data provider, but each test can only use a subset of the data. These should be refactored to avoid skipping tests. These tests also make use of random number generators. It looks like these may not be properly isolated and may introduce coupling between what should be independent tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:229,refactor,refactored,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,2,"['coupling', 'refactor']","['coupling', 'refactored']"
Modifiability,"There are currently a few known issues that reviewers should bear in mind (these will become tickets shortly):. -All codecs marked as implementing `ReferenceDependentFeatureCodec` are currently non-functional. I need to either refactor them to not require a GenomeLocParser or delete them entirely. -The `IndexFeatureFile` tool is not currently working on block-compressed files -- will fix this soon. -`IndexFeatureFile` needs integration tests (will work on this during code review). -`BQSR` needs integration tests for the case of multiple simultaneous known sites files (now that it supports them!). -All codecs should move to tribble, and must implement `canDecode()` correctly (this is a new requirement, since it's no longer possible to manually request a particular codec). Most `canDecode()` implementations can be file-extension-based; only things like VCF format detection need to examine file contents to determine file type. -`FeatureDataSource` supports querying by interval, as well as full traversals, but not full traversal by a set of intervals (yet). This latter feature will be needed for the `VariantWalker` traversal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75658867:227,refactor,refactor,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/224#issuecomment-75658867,1,['refactor'],['refactor']
Modifiability,"There are no Java code changes in this PR. Tests were done manually. As a reminder, the modified files are still considered experimental. Changes:; - combine_tracks.wdl: Fixes bug where string was compared to a float. Closes #5284 ; - combine_tracks.wdl: Converts the processed seg file into a format for GISTIC2. This is a trivial conversion. Closes #5283 ; - Other changes in `aggregate_combine_tracks.wdl` to support the above, including aggregation of individual GISTIC2 seg files into a single GISTIC2 seg file.; - Added gs urls for necessary auxiliary files in the documentation.; - Added multiple output types for the ABSOLUTE skew parameter to support heterogeneous execution configurations. File, Float, and String. All are the same value.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5287:684,config,configurations,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5287,1,['config'],['configurations']
Modifiability,"There are several parts to this that all exist, but need to be committed in this order:. -Minor refactoring of CRAM container writing code in htsjdk to allow separation of the writing of CRAM containers from the writing of the CRAM file header, SAM file header, and EOF container for part-merging.; -Hadoop-BAM needs to go to Java8 and upgrade to newer htsjdk.; -Add CRAMOutputFormat, CRAMRecordWriter, etc. to Hadoop-BAM (these depend on the htsjdk code mentioned above).; -Modify ReadSparkSink to be CRAM aware.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-169339543:96,refactor,refactoring,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1270#issuecomment-169339543,1,['refactor'],['refactoring']
Modifiability,"There are still too many variables here. Do you know that the input bams are the same? Are you using BWA-MEM? My theory is that it's choosing different secondary alignments in MQ0 cases, preferring bases that are capital. If you can show that the same reads going in produce different variants with your two different references then this will be a lot easier to debug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6825#issuecomment-707755358:25,variab,variables,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6825#issuecomment-707755358,1,['variab'],['variables']
Modifiability,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:78,inherit,inherit,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189,1,['inherit'],['inherit']
Modifiability,"There have been requests for some additional clarity on ""how; much test coverage is enough"" for hellbender tools. Rather than; mandate a particular coverage target, I proposed a more flexible; set of guidelines which I've added to the README in this commit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/382:183,flexible,flexible,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382,1,['flexible'],['flexible']
Modifiability,"There is a first attempt to have some configurable settings in #2322. If in that PR this could be included, feel free to let me know how do you want to do this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272485359:38,config,configurable,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272485359,1,['config'],['configurable']
Modifiability,"There is an existing NIO filesystem provider for Amazon S3 that has been used successfully with GATK4 by at least one user (with some minor tweaks to the engine). We should add the S3 plugin as a dependency, add basic tests for read support, and make whatever changes are needed to get it working.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708:184,plugin,plugin,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708,1,['plugin'],['plugin']
Modifiability,"There is no conceivable worst case for this PR -- the only reason for having a `CountSet` was to be be able to have quick `min` and `max` operations (in log(n) time), *but*. * these operations are not used anywhere outside of unit tests, so to make an illuminating worst case you would have to rewrite the assembly engine.; * Even if we did use these operations they would be done once per assembly region, and therefore we could make this class 1000x slower and we would add about a second to the run time of a WGS bam.; * a plain old `TreeSet`, which is what this PR replaces the `CountSet` with, also has these operations in log time.; * the number of kmer sizes used is usually 2, and will be up to 6 in very rare cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5467#issuecomment-443463884:294,rewrite,rewrite,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5467#issuecomment-443463884,1,['rewrite'],['rewrite']
Modifiability,"There is some duplication of code to handle CR-only, AF-only, and CR+AF that could be eliminated with some refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625:107,refactor,refactoring,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625,1,['refactor'],['refactoring']
Modifiability,"There seems to be no obvious way to read thru the unmapped read pairs in a bam file in Spark. Looking at the code in ```ReadSparkSource#getParallelReads(String, String, List, long)``` it seems that ; perhaps it is possible by setting the appropriate property in Configuration returned by ```ctx.hadoopConfiguration()``` however there is no documentation as to what property that could be. . @droazen I assign it to you initially so that you route it to whoever might be most suited to address this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2572:262,Config,Configuration,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2572,1,['Config'],['Configuration']
Modifiability,"There were a few issues with this case. First, the data source was not constructed 100% correctly. The config file is correct. . The index file is for the tar.gz version of the source data and not for the uncompressed version that they're using. The index should correspond to the source data in the file referenced by the config file itself (not a zipped or otherwise transformed version). Secondly, the source `tsv` data file has the header line for the table commented out. The Xsv codec is aware of leading hash marks as comments and will ignore any such lines. Because of this, the leading hash in the table header is ignored and the file cannot be properly parsed. The fix is simple - just remove the leading hash from the table header (the preceding line with the two hash marks is correctly interpreted as a file header because of the leading hashes acting as comments). Lastly, even if the user fixed the file they would still need to index it with`IndexFeatureFile`. At some point the code underlying this in `HTSJDK` was broken such that no Xsv files can currently be indexed. I have submitted a pull request in `HTSJDK` (https://github.com/samtools/htsjdk/pull/1429) for this and have another ready to go in GATK (#6224) that includes a test for this case so this reversion cannot happen again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183:103,config,config,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183,2,['config'],['config']
Modifiability,"There were some good basic examples in the original ticket:. - get all the contiguously aligned reads (e.g, xxM); - get reads with soft clipping (e.g., xSxxM, could be reads with partial adapter sequence still left after trimming); - get reads with insertions (e.g., xxMxxIxxM, could be spliced reads, e.g., reads spanning exon-exon, or intron-intron junction); - get reads with deletions (g.g., xxMxxDxxM, could point at SV). Those would be the basic must-haves. Then the next step of nice-to-haves would be to be able to find specific patterns like ""D followed by I"" or specific numbers of operators like ""exactly five D in a row"" or ""five D in total, not necessarily in consecutive order"". Do you need me to be more specific than that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/588#issuecomment-307890525:187,adapt,adapter,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-307890525,1,['adapt'],['adapter']
Modifiability,"There's a lot of room for improvement in the ported code, but as you said we can deal with refactoring later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89331480:91,refactor,refactoring,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347#issuecomment-89331480,1,['refactor'],['refactoring']
Modifiability,"There's such feature for GATKTool, but not yet for GATKSparkTool. Much of the code is copied from the GATKTool version; Engine team, please comment if a refactor is needed and how. Thanks!; (Tagging @droazen @lbergelson and @cmnbroad )",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4981:153,refactor,refactor,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4981,1,['refactor'],['refactor']
Modifiability,"These codecs require a `GenomeLocParser` (and therefore a sequence dictionary), and so are currently broken in hellbender, which does not assume the presence of a sequence dictionary for Feature-containing files. We need to either refactor these codecs to not require a `GenomeLocParser` (and remove the `ReferenceDependentFeatureCodec` interface), or delete them if they are no longer needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/234:231,refactor,refactor,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/234,1,['refactor'],['refactor']
Modifiability,"These downstream protected tests only need to run on each merge into master, not when a PR is opened, so that should address the security issue. (I misspoke earlier, sorry -- only the tests in https://github.com/broadinstitute/gatk/issues/2298 need to run for each PR). The goal is just to have a badge on github that lets us know whether protected is working with the current build of public/master (and it often won't be in the course of development). Workflow would be:; 1. A merge goes into GATK public/master; 2. Job grabs the HEAD of public/master, and builds and installs a snapshot into the local maven repository using `./gradlew install printVersion`; 3. Job needs to save the last line of output from `./gradlew install printVersion` in a `gatkPublicVersion` variable -- this is the version of the snapshot of public that was installed to maven local.; 4. Job checks out latest GATK protected/master and executes the command to build it and run the full test suite, but as part of that command it overrides the GATK public version to point to the snapshot version installed locally and saved in the `gatkPublicVersion` variable above. It is trivial to patch protected's `build.gradle` to add the ability to override the GATK public version when running gradle commands (we can help with this part).; 5. New github badge for ""Downstream GATK protected tests"" on the front page of the GATK public github repo gets updated with the results of step 4, and can be clicked on to view the full test output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287543585:770,variab,variable,770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1758#issuecomment-287543585,2,['variab'],['variable']
Modifiability,"These kind of errors are typically seen when the field description in the VCF header is incorrect. For example, describing the field length to be a fixed integer when the field is really variable length. I would recommend closely scanning the VCF header for inconsistencies first.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035:187,variab,variable,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035,1,['variab'],['variable']
Modifiability,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391:250,refactor,refactor,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391,1,['refactor'],['refactor']
Modifiability,"Things left for later:; * `GenotypeIndexCalculator` sometimes interacts with primitive arrays, sometimes with `GenotypeAlleleCounts`; * `GenotypeLikelihoodCalculator` has extraneous responsibilities and doesn't interact with `GenotypeAlleleCounts` as well as it should.; * `alleleCountsToIndex(final GenotypeAlleleCounts newGAC, final int[] newToOldAlleleMap)` in `GenotypeIndexCalculator` needs refactoring.; * `GenotypeLikelihoodCalculators` is really just a cache of `GenotypeAlleleCounts`.; * `GenotypeAlleleCounts` has some unused and barely-used methods, and it precomputes a lot of quantities that are not often needed and could be computed on-the-fly without difficulty or expense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217:396,refactor,refactoring,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217,1,['refactor'],['refactoring']
Modifiability,This PR adds sputnik CI https://sputnik.ci as a code reviewer on pull Reqs. I have configured it to use only FindBugs to limit messages to potentially useful ones. @droazen @lbergelson wdyt? we could give it a try and see if it helps us or annoys us.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1747:83,config,configured,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1747,1,['config'],['configured']
Modifiability,"This PR attempts to eliminate long-running, useless assemblies that significantly extend runtime on some samples:. - Conducts a scan over the genome to find intervals of excessive depth, defined as an interval where coverage is greater than a lower factor times the average coverage of the sample and containing a coverage peak greater than an upper factor times the average coverage.; - Nearby high-coverage regions within one read-length of each other are merged together.; - Excludes reads that map exclusively inside high coverage regions from evidence gathering.; - Excludes reads that map exclusively inside high coverage regions from QName finding for seeding assemblies. In addition, after observing that many long-running assemblies occur on non-primary reference contigs, we also exclude reads that map to non-primary contigs (as defined by the ""cross-contig to ignore set"") from evidence gathering. Runtime on the CHM mix sample with this change is approximately 38 minutes, and our NA19238 snapshot now takes only 22 minutes, a significant drop in runtime. There are a few changes in the resulting call set but they appear to be minimal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4438:82,extend,extend,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4438,1,['extend'],['extend']
Modifiability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to ; * the same chromosome with reference order switch but without strand switch, or; * different chromosomes. This brings us (unfiltered) ~6000 mated BND records, half of which are on canonical chromosomes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3571:91,config,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3571,1,['config'],['configuration']
Modifiability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, but NOT significantly overlapping each other. We used to call inversions from such alignments, but it is more appropriate to emit BND records because a lot of times such signal is actually generated from inverted segmental duplications, or simply inverted mobile element insertions. To confidently interpret and distinguish between such events, we need other types of evidence, and is better to be dealt with downstream logic units. Inverted duplications are NOT dealt with in this PR and is going to be in the next. NEEDS TO WAIT UNTIL PART 1 & 2 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3457:91,config,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3457,1,['config'],['configuration']
Modifiability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, significantly overlapping each other on their reference spans. We used to call inversions from such alignments when feasible, but it is more appropriate to emit inverted duplication records. NEEDS TO WAIT UNTIL PARTS 1, 2 AND 3 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3464:91,config,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3464,1,['config'],['configuration']
Modifiability,"This PR dynamically sets the logging level for command line tools at runtime using the current version of log4j (we were headed down a path of downgrading to a previous version of log4j in order to implement this). However, it uses an API that is normally used in code for extending log4j rather than acting as a client to it, and requires an explicit cast of the value returned from LogManager.getContext. The Apache project site illustrates the use of this api in the first line of code in an example here: https://logging.apache.org/log4j/2.x/manual/customconfig.html#AddingToCurrent. We need to decide if we want to take this and stay on the current version or continue with the downgrade…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603:273,extend,extending,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603,1,['extend'],['extending']
Modifiability,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8113:681,rewrite,rewrite,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113,2,"['refactor', 'rewrite']","['refactoring', 'rewrite']"
Modifiability,"This PR fixes two problems I noticed relative to how we are treating cross-contig evidence that comes from the alt contigs:. - The cross-contig exclusion rule in `ReadClassifier` was incorrect. Due to the structure of the `if`-`else if` logic in `checkDiscordantPair()`, `SameStrandPair` or `OutiesPair` evidence was still being created for cross-contig pairs based on the strand configuration of the two reads (even though they were aligned to different contigs).; - The `runWholePipeline` script we've been using was not actually setting the cross-contig kill list parameter, meaning no cross-contig evidence was being filtered out. The change makes the number of intervals for CMHMIX WGS1 drop from 31962 to 27645.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3262:380,config,configuration,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3262,1,['config'],['configuration']
Modifiability,"This PR is a finalized version of https://github.com/broadinstitute/gatk/pull/5017. I've copied the branch into our repo so that travis will run cloud tests on it. Currently, only Posix filesystem paths can be passed as workspaces and arrays to GenomicsDB via GenomicsDBImport and SelectVariants. This PR will allow for hdfs and gcs (and emrfs/s3) URIs to be supported as well. ; Examples; ```; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path hdfs://master:9000/gdb_ws -L 1:500-10000; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path gs://my_bucket/gdb_ws -L 1:500-10000; ```; ```; ./gatk SelectVariants -V gendb.hdfs://master:9000/gdb_ws -R hs37d5.fa -O out.vcf; ./gatk SelectVariants -V gendb.gs://my_bucket/gdb_ws -R hs37d5.fa -O out.vcf; ```; GenomicsDB supports GCS via the [Cloud Storage Connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage). Set environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the GCS Service Account json file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197:949,variab,variable,949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197,1,['variab'],['variable']
Modifiability,"This PR is intended to introduce several new tools related to the CleanVcf workflow in GATK-SV, which the use of these tools being documented in https://github.com/broadinstitute/gatk-sv/pull/733. These tools are intended to introduce several enhancements over the existing implementation, including but not limited to:; - Introduce various unit and integration tests into the workflow.; - Create more robust and generalizable tools that can be used independent of _CleanVcf_.; - Improve runtime and execution speed by leveraging Java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8996:243,enhance,enhancements,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8996,1,['enhance'],['enhancements']
Modifiability,This PR is the culmination of work from myself and @lbergelson to improve the runtime for MarkDuplicatesSpark on a single machine. This involved a rewrite of the tool as well as a number of improvements which should bring it into closer agreement with MarkDuplicates from picard. . Note: this is merely a checkpoint and there is still work that must be done to bring the work into agreement with recent MarkDuplicates development in picard. . Resolves #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4656:147,rewrite,rewrite,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656,1,['rewrite'],['rewrite']
Modifiability,"This PR is the initial stage of implementing the calling of IMPRECISE variants in the SV pipeline. It introduces the concept of an evidence-target link, which joins an evidence interval to its distal target. This is an extension of the 'coherent' evidence concept previously used in determining evidence thresholds for assembly. The code in this PR contains the following changes:. - Evidence intervals and distal targets now are treated as stranded, and evidence-target link clustering depends on overlaps between both intervals and strands.; - Evidence target interval and distal target interval calculations have been modified to make sure that evidence supporting the same event clusters together (has overlapping intervals). This includes several changes such as extending the 'rest-of-fragment-size' calculation to try to capture almost all non-outlier fragment sizes in the library; increasing the split read location uncertainty a little; and being more precise about the boundaries of distal target intervals by taking advantage of information in the MD and MC tags if available.; - Evidence target links are gathered for every piece of evidence supporting a high-quality distal target. ; - Evidence target links are clustered together and store the amount of split-read and read-pair evidence that went into each cluster.; - All evidence target link clusters that are composed of at least 1 split read or at least 2 read pairs are collected in the driver and emitted in a BEDPE formatted file specified in the command line parameters.; - A `PairedStrandedIntervalTree` data structure is introduced to allow `SVIntervalTree`-style lookups for paired intervals. To finish this work, future PRs will 1) use the collected evidence target links to annotate our assembly called-variants with the number of split reads and read pairs observed in the original mappings and 2) create IMPRECISE VCF records for events that have enough evidence-target-link support, first for deletions and then possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469:768,extend,extending,768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469,1,['extend'],['extending']
Modifiability,This PR merges the BQ Write API with the ref ranges table and refactors the Import wdl to be single sample.; I pulled out the create tables into it's own wdl. It doesn't make sense to try this for each individual sample. For now it need to be run separately. We are also not setting the is_loaded field in the sample_info table - this needs to be resolved when we decide how we want to track that info. See https://docs.google.com/document/d/1_ox38x7YjSeQx1I-6K_6kB4TTlonaEah2LRGevN9GmM/edit# for details,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7530:62,refactor,refactors,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7530,1,['refactor'],['refactors']
Modifiability,"This PR only has a subset of the tools, but I wanted put something out there quickly to get comments and make sure I'm on the right track.; - Created tools.picard subpackage.; - Extend CommandLineProgram with PicardCommandLineProgram.; - Ported the following CLPs, with tests and small test files from Picard:; - AddCommentsToBam; - CleanSam; - CreateSequenceDictionary; - FastqToSam; - MergeBamAlignment; - RevertSam; - SamFormatConverter; - SamToFastq; - ValidateSamFile. Some notes:; - doWork() returns null for most CLPs. The exception is ValidateSam; in Picard, it returns a meaningful exit code (0 if input SAM is valid, 1 if not). Various unit tests were relying on this behavior. For now, I preserved it by returning a boolean.; - MergeBamAlignment actually involves a fair amount of logic, a la MarkDuplicates. It combines an aligned BAM with an unmapped BAM. Its helper classes have been placed in utils.sam.mergealignment. More information can be found there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/124:178,Extend,Extend,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/124,1,['Extend'],['Extend']
Modifiability,This PR tries to solve several issues:; - Refactoring constructors for LIBS (#1879); - Adding maximum depth per sample argument to `LocusWalker` to avoid memory overload; - Fixing `Pileup`/`CheckPileup` tools for command line read filters; - Method for fix overlaps in `ReadPileup` in the same way as samtools (#2034),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154:42,Refactor,Refactoring,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154,1,['Refactor'],['Refactoring']
Modifiability,"This addresses https://github.com/broadinstitute/gatk/issues/1015 and https://github.com/broadinstitute/gatk/issues/1094. The idea is to remove the single reducer sort (which doesn't scale), by performing a totally ordered parallel sort on the reads, then writing each partition as a (headerless) BAM file. Finally, the BAM files are concatenated together after writing an initial header. This is very similar to the approach that Hadoop-BAM takes, but adapted to work on Spark. I haven't done extensive benchmarking, but when I ran MD on a ~75MB BAM the runtime dropped from >30 mins to around 8 mins. This is still worse than the walker equivalent for small files, but it's an improvement that means many jobs that didn't finish before now do. Note that this includes the changes from https://github.com/broadinstitute/gatk/pull/1127. I'll rebase once that is committed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1174:453,adapt,adapted,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174,1,['adapt'],['adapted']
Modifiability,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608:260,variab,variable,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608,1,['variab'],['variable']
Modifiability,"This adds a hard filter for low variant allele fraction calls. We will not turn this on by default in any of our pipelines, but it will give users an easy option to filter everything below a certain VAF that they don't care about. It also adds a hard filter for low alt depth calls based on a threshold from the median autosomal coverage (that must be supplied as an argument). It takes the cutoff from a Poisson with a mean of 1.5 * median coverage (to account for NuMTs with 3 copies in the autosome) and is tuned to catch 99% of the false positives (which we know will also catch lots of true positives). . It also removes the Polymorphic NUMT annotation (since that's basically what's going into the filter at this point).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842:630,Polymorphi,Polymorphic,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842,1,['Polymorphi'],['Polymorphic']
Modifiability,This allows `PrintReads` and other classes that inherit from `ReadWalker` to split files over an arbitrary number of intervals and not get any repeated reads across all of the split data files. - Added reads-must-start-within-intervals flag to ReadWalker to allow for; splitting of files over an interval set without seeing repeats. - Added a new iterator type to filter reads by this interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6054:48,inherit,inherit,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6054,1,['inherit'],['inherit']
Modifiability,This also means the `GeneListOutputRenderer` will need to accept a config file as a parameter. `SimpleTsvOutputRenderer` already does this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5962#issuecomment-494907060:67,config,config,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962#issuecomment-494907060,1,['config'],['config']
Modifiability,This branch does 2 things. ; 1. It makes ProgressMeter async #; 2. It makes ProgressMeter and interface so it could be made more flexible for non-locatables. This could be a first step to making it more flexible for https://github.com/broadinstitute/gatk/issues/6390 and https://github.com/broadinstitute/gatk/issues/5178,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6484:129,flexible,flexible,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6484,2,['flexible'],['flexible']
Modifiability,"This bug has become part of a bigger effort to address how configure the gatk. We're working on a general solution to avoid this sort of issue in the future. We haven't addressed this specific subcase yet though. For now the workaround I described above should work for you. If it doesn't, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565:59,config,configure,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565,1,['config'],['configure']
Modifiability,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4659:28,config,configurations,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659,1,['config'],['configurations']
Modifiability,"This code (building off of Louis' fixes) adds the following:; - AuthHolder, a replacement for the PipelineOptions. It stores the authentication info we need for GCS and supports both API_KEY and client-secrets.json. I adapted a few classes to accept an AuthHolder.; - BaseRecalibratorOptimizedSpark, a port of the ""shard"" approach I first did on the Dataflow side. Note that currently this code only performs reasonably for small inputs if you specify -L on the command line (for large inputs it doesn't matter).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987:218,adapt,adapted,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987,1,['adapt'],['adapted']
Modifiability,"This code comes with no tests and it duplicates the code we have in BQSR, which is not the right way to do it I think. . I think there are 2 ways to proceed. One is to refactor the current BQSR code to use an abstraction like the Recalibration Table builder (which may not be a bad idea by itself) - then you can reuse existing tests. Another way it to treat this a from-scratch reimplementation - but then we need all this new code fully tested. back to @jean-philippe-martin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101329709:168,refactor,refactor,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-101329709,1,['refactor'],['refactor']
Modifiability,"This codec should check for the existence of a config file next to the specified file which will inform the parser of the columns from which to parse the `contig`, `start`, and `end` locations from columns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3898:47,config,config,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3898,1,['config'],['config']
Modifiability,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2532:84,plugin,plugin,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532,3,['plugin'],['plugin']
Modifiability,"This contains the streaming Python executor, implemented by a (Python-independent) StreamingProcessController. The StreamingProcessController, and the existing ProcessController are refactored to use a shared ProcessControllerBase, and changed from using raw Threads and Runnables to using an ExecutorService with Futures and Callables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3872:182,refactor,refactored,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3872,1,['refactor'],['refactored']
Modifiability,"This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM). As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via `gcloud auth`, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on. The message is useful, for if we *were* running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095:419,variab,variable,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095,1,['variab'],['variable']
Modifiability,"This extends Variant Eval to compare AFs between variants in binned AF buckets based on Thousand Genomes VCF, between the expected AF from Thousand Genomes and the seen one in the actual VCF, to be used as a QC metric for our arrays pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6039:5,extend,extends,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6039,1,['extend'],['extends']
Modifiability,"This feature was initially opened in this PR: https://github.com/broadinstitute/gatk/pull/8750, after which @lbergelson and @droazen made comments here; https://github.com/broadinstitute/gatk/pull/8752. . The driving use-case is that we took over the GATK3 MergeVariantsAndGenotypes tool at DISCVRseq and users have been requesting the older behavior on VCF merges, such as: https://github.com/BimberLab/DISCVRSeq/issues/313. . The original PR has been languishing since March and I'm hoping to finalize this feature. Because I cant write to the GATK repo and b/c @lbergelson made some suggestions on a GATK-based branch I am going to put every together into one clean PR, which responds to the code review from the thread above. . To recap background: . - In GATK3, when merging variants, the IDs of all the source VCFs were retained. The GATK4 code path seems like it intended to do this, since the variantSources set is generated, but that variant isnt used for anything (I assume GATK3 code was partially carried forward to incompletely refactored?). . - This PR is designed to allow code to opt-in to the old GATK behavior of retaining the IDs of source VCFs in the ID field. It will not change the default behavior for existing code. - I dont think I can kick off the test suite, but these tests did pass here: https://github.com/broadinstitute/gatk/pull/8752. Again, @lbergelson and @droazen both reviewed the original PR and seemed fine with it in principle. The primary concern raised by @droazen was to avoid changing the current defaults and to not create additional burden (such as adding sorts). I believe this addresses both of those concerns. @jamesemery commented on the thread at one point as well. . Is there anything I can do to help move this forward? Thanks for your time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9032:1041,refactor,refactored,1041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9032,1,['refactor'],['refactored']
Modifiability,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787:12,Inherit,Inherited,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787,4,"['Inherit', 'extend', 'inherit']","['Inherited', 'extends', 'inherit', 'inherited']"
Modifiability,"This fixes several bugs in CompareSAMs, and adds test coverage:; - Bug 1: Program couldn't handle multiple reads with the same read name + start coordinate. To get around this, we add a UniquePrimaryAlignmentKey class and use that. ; - As a consequence of this, some unit tests were incorrect (they relied on the existence of the aforementioned bug). A test file was modified to fix this.; - Bug 2: When comparing unsorted files, if the read names stop being equal at any point, the program would throw an exception. Instead, it should finish but return ""false"".; - Bug 3: The case of queryname-sorted or unsorted inputs were not tested. At all. That counts as a bug, IMO.; - Added unit tests for all these cases. ; - Did some refactoring (I had initially hoped to abstract out the traversal method, but that will have to wait).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/183:727,refactor,refactoring,727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/183,1,['refactor'],['refactoring']
Modifiability,"This happens for me also. I will modify and set the environment variable SPARK_LOCAL_IP=""127.0.0.1"". Thanks @cwhelan and @SHuang-Broad. The environmental variable fixes the error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-356030921:64,variab,variable,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534#issuecomment-356030921,2,['variab'],['variable']
Modifiability,"This includes wrappers to present `SAMRecords` to the tools; Also adding 4 simple tools as examples; `FlagStatsDataflow`; It makes use of dataflow's built in hierarchical aggregation; `CountBasesDataflow`; Simple walker that makes use of the SAMRecord conversion; `CountReadsDataflow`; Does what it says; `PrintReadsDataflow`; This is a very limited version of our print reads walker; It prints `SAMRecords` as strings to an unordered text file; It could potentially be useful as method for examining bam output before we have a proper bam writer. These tools exist in two parts:; A transform extending from `PTransformSAM` (A subclass of `PTransform<Read,O>` which facilitates conversion to `SAMRecord`; A command line tool implementing a complete pipeline; These pipelines can apply arbitrary `ReadFilter`s/ `ReadTransformer`s which are applied before the main transform; (a list of transforms and a list of filters can be applied, it's currently not handled very efficiently though, better to pre-comine them into a single meta transform). Currently, only tests which use local files are running on travis.; There is code included to run on files in buckets, but the tests for it are currently disabled due to travis configuration issues (will be resolved in a seperate ticket). Some changes were made to existing classes to make them Serialize properly; Some test files were moved to help normalize test data locations (although not all tests are normalized, should be done in separate ticket); the new storage locations are based on the complete package name rather than just the tool name",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/443:593,extend,extending,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/443,2,"['config', 'extend']","['configuration', 'extending']"
Modifiability,This is a bit of a pain because the sam files produce by htsjdk now all say they're version 1.5. We need to rewrite our sam/bam files to be compliant with the new version (or at the very least update the version strings. ) Alternatively we could change the comparator to ignore versions when comparing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/758:108,rewrite,rewrite,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/758,1,['rewrite'],['rewrite']
Modifiability,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:161,refactor,refactoring,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,4,"['evolve', 'extend', 'refactor']","['evolve', 'extend', 'refactoring']"
Modifiability,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8737:90,refactor,refactoring,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737,1,['refactor'],['refactoring']
Modifiability,This is a strict copy-paste job. I'll do further refactoring after this. I'm doing this in two steps so it's easier to read the diffs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759:49,refactor,refactoring,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759,1,['refactor'],['refactoring']
Modifiability,This is a suggested refactor of PrintSVEvidence to use a single output stream and parameterize the type of evidence.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7045:20,refactor,refactor,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7045,2,"['parameteriz', 'refactor']","['parameterize', 'refactor']"
Modifiability,"This is a useful class when strictly dealing with pileups. However, it only supports point mutations. Extend to indels....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3641:102,Extend,Extend,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3641,1,['Extend'],['Extend']
Modifiability,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122:364,extend,extending,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122,1,['extend'],['extending']
Modifiability,"This is because the GencodeFuncotationFactory will force the datasource name (`getName()`) to return ""Gencode"". However, some areas of the code (e.g. `Funcotator.java`) will query the name from the config file. If these do not match, confusion ensues. You will get IGRs for everything, since all queries into the datasource will yield no found features/transcripts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4791:198,config,config,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4791,1,['config'],['config']
Modifiability,This is fixed for now with a travis environment variable. I'm testing lb_add_region_to_dataproc to make sure that the fix works before removing that variable and merging a change to the dataproc code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6129#issuecomment-525941840:48,variab,variable,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6129#issuecomment-525941840,2,['variab'],['variable']
Modifiability,"This is meant as a group discussion that could happen several places, and here is as good as any. We know that shipping around the header is has a _huge_ cost. So, we need to find a way to effectively strip it from the `SAMRecord` without breaking it. I propose the following.; - Modify `SAMRecord` to use getter methods for the header; - Create a `HeaderSAMRecord` that extends `SAMRecord` and that has a static field for the header. This class would override `getHeader` to return the static; - Use `Broadcast` with `mapPartitions` to set the static on each worker. An alternative would be audit the field usage and do a combination of performing all necessary calls that require the header to when we load the reads and, if possible, making the still offending methods inaccessible. So, @tomwhite , @akiezun , @droazen , @lbergelson , @jean-philippe-martin , what do you all think?. I know @lbergelson previous expressed he didn't like the usage of statics for this purpose.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900:371,extend,extends,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900,1,['extend'],['extends']
Modifiability,"This is not ready for merge -- I just want to see if tests pass with this configuration. There are still some unresolved vulnerabilities:. ```; [1/7] - pkg:maven/com.google.protobuf/protobuf-java@4.0.0-rc-2 - 3 vulnerabilities found!; [2/7] - pkg:maven/log4j/log4j@1.2.17 - 6 vulnerabilities found!; [3/7] - pkg:maven/org.codehaus.janino/janino@3.1.9 - 1 vulnerability found!; [4/7] - pkg:maven/net.minidev/json-smart@2.4.7 - 1 vulnerability found!; [5/7] - pkg:maven/org.codehaus.jettison/jettison@1.1 - 3 vulnerabilities found!; [6/7] - pkg:maven/org.eclipse.jetty/jetty-util@9.4.48.v20220622 - 1 vulnerability found!; [7/7] - pkg:maven/org.eclipse.jetty/jetty-http@9.4.48.v20220622 - 1 vulnerability found!; ```. Some of these we may be unable to resolve. Eg., the `protobuf-java` version in this branch appears to be the most recent one, but still has open vulnerabilities filed against it. The ancient log4j 1.x version is used by two of our dependencies (`hdf5-java-bindings` and `spark-mllib_2.12`), and is the most recent version. Note that this is completely unrelated to the infamous log4j 2.x vulnerability, which was patched in GATK a long time ago.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581408853:74,config,configuration,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581408853,1,['config'],['configuration']
Modifiability,This is now configurable via Owner -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-358072659:12,config,configurable,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-358072659,1,['config'],['configurable']
Modifiability,"This is only changing codepaths related with the help. So this change will change the usage to say that all filters are valid for disabeFilter to say that only the available ones are valid. The only problem could be in the docgen code, but not in the behavior of the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845,1,['plugin'],['plugin']
Modifiability,"This is ready for review again, @droazen. I split the walkers in two: `ReadSliderWalker` and `VariantSliderWalker`, both based on `ReadWalker` and `VariantWalker` to traverse in a sliding-window approach. I also implemented some `ArgumentCollectionDefinition` for the three parameters (window-size, window-step and window-padding). There are some changes that I would like to discuss with you:; - `ShardSource<T>`: a generic class for lazily load sources of certain type. Now `ReadShard` extends this class, but there is actually no change in the definitions. I wonder if it is worthy to maintain `ReadShard` as a separate class, because it seems that the only usage is in `AssemblyRegionWalker` and it could be easily changed by a `ShardSource<GATKRead>`.; - `FilteringIterator<T>`: another generic class for filter on iteration records. Now `ReadFilteringIterator` extends this class, but again I don't find any usage that requires an specific case instead of a generic.; - There is some repetition in the test code: `ShardSourceUnitTest` and `ReadShardUnitTest` are exactly the same. I didn't want to remove the later, but I think that it is redundant; the same for the `FilteringIteratorUnitTest` adn `ReadFilteringIterator.; - Because of the inclusion of `ArgumentCollectionDefinition`s for window-related parameters, I just want to know if I should include them in `AssemblyRegionWalker` to maintain consistency in the user options. Back to you @droazen, and thanks again for all the help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995:488,extend,extends,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-224348995,2,['extend'],['extends']
Modifiability,"This is rebased off of https://github.com/broadinstitute/gatk/pull/3716, since it depends on code there. Hence, only the second commit needs to be reviewed in this PR. The code and tests are quite similar to that for PlotSegmentedCopyRatio/PlotACNVResults. However, I've changed the R scripts to be more efficient (WGS plots no longer take several hours). Furthermore, PlotModeledSegments is more flexible than PlotACNVResults in that it plots CR, AF, or both on the fly depending on the available inputs. I've also added some more input validation, changed some terminology, and moved over to data.table for reading TSVs in R.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3729:397,flexible,flexible,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3729,1,['flexible'],['flexible']
Modifiability,"This is related to #7287. . By default MultiVariantWalkerGroupedOnStart will iterate over any variant that spans the user-provided intervals. This is not what one would typically want when running scatter/gather jobs, since variants spanning interval borders would be included in both jobs. There is a variable/argument for IGNORE_VARIANTS_THAT_START_OUTSIDE_INTERVAL, but it's private and therefore subclasses cant read it. I would like our MultiVariantWalkerGroupedOnStart to view this value and at least log a warning if the current job hasUserSuppliedIntervals(), and ignoreIntervalsOutsideStart=false. In this PR I just make that variable protected, but I could also add formal getter/setters if you prefer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7301:302,variab,variable,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7301,2,['variab'],['variable']
Modifiability,"This is resolved -- we are going to take in the Picard tools as a dependency, and refactor both projects to depend on a shared arg parsing/documentation system (https://github.com/broadinstitute/barclay).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1210#issuecomment-265904560:82,refactor,refactor,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1210#issuecomment-265904560,1,['refactor'],['refactor']
Modifiability,"This is the set of fixes for the filter plugin, @cmnbroad. The first commit is the change introduced in #2385 for less verbose test output, so it should be drop once it is accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-278899912:40,plugin,plugin,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-278899912,1,['plugin'],['plugin']
Modifiability,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4835:887,refactor,refactored,887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835,1,['refactor'],['refactored']
Modifiability,"This likely has to do with your spark configuration. Check on the Spark job's progress through the web interface, which should be something like http://<driver_address>:4040 (see https://spark.apache.org/docs/latest/monitoring.html). . If your BAM is very small, you can also try increasing the number of partitions by reducing --bamPartitionSize.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312316932:38,config,configuration,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312316932,1,['config'],['configuration']
Modifiability,This list was generated using a not-yet merged version of my CRAM metadata tool that uses my not-yet-merged refactored CRAM code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6018#issuecomment-505925995:108,refactor,refactored,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018#issuecomment-505925995,1,['refactor'],['refactored']
Modifiability,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:132,extend,extended,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['extend'],['extended']
Modifiability,"This moves all logging to log4j (previously we had a mix of log4j and SAMTools logging). . Questions:There are about 35 places where we were using the SAMTools ProgressLogger, which assumes SAMTools logging. I reproduced that class in hellbender, but implemented the SAMTools ProgressLoggerInterface in order to retain compatibility with SAMWriters, since we use those in a couple of places. The hellbender ProgressLogger class is in utils.runtime, not sure if there is a better place for it. Also I'm not sure how to handle the source code attribution of it since it was lifted from SAMTools but slightly modified for hellbender ? Can I do that or do I need to rewrite it from scratch ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/612:662,rewrite,rewrite,662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/612,1,['rewrite'],['rewrite']
Modifiability,This must be the most discussed tool in the GATK. I am relieved to close the book on this PR. Very nice work @mwalker174 -- thanks for all the refactoring!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243#issuecomment-936636793:143,refactor,refactoring,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243#issuecomment-936636793,1,['refactor'],['refactoring']
Modifiability,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:254,refactor,refactoring,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501,1,['refactor'],['refactoring']
Modifiability,"This new PathSeq WDL redesigns the workflow for improved performance in the cloud. Downsampling can be applied to BAMs with high microbial content (ie >10M reads) that normally cause performance issues. . Other improvements include:. * Removed microbial fasta input, as only the sequence dictionary is needed.; * Broke pipeline down to into smaller tasks. This helps reduce costs by a) provisioning fewer resources at the filter and score phases of the pipeline and b) reducing job wall time to minimize the likelihood of VM preemption.; * Filter-only option, which can be used to cheaply estimate the number of microbial reads in the sample.; * Metrics are now parsed so they can be fed as output to the Terra data model.; * CRAM-to-BAM capability; * Updated WDL readme; * Deleted unneeded WDL json configuration, as the configuration can be provided in Terra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6536:800,config,configuration,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6536,2,['config'],['configuration']
Modifiability,This optimizes the defaults in mitochondria-mode for WGS mitochondria calling. It changes the `pruning-lod-threshold` in adaptive pruning and the `lod-divided-by-depth` threshold in `FilterMutectCalls`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544:121,adapt,adaptive,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544,1,['adapt'],['adaptive']
Modifiability,"This pull request is focused on resolving occurrences of Sonar rule squid:S1197 - Array designators ""[]"" should be on the type, not the variable. You can find more information about the issue here: https://dev.eclipse.org/sonar/coding_rules#q=squid:S1197. Please let me know if you have any questions. M-Ezzat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1915:136,variab,variable,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1915,1,['variab'],['variable']
Modifiability,"This replaces a secret that requires a pr to fix, and updates the name of one of the others.; Requires 1 more step after this.; * Switch travis variable name from DOCKER_SERVICE_PASS -> DOCKER_SERVICE_TOKEN for clarity; * Replace gcloud encrypted key",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7521:144,variab,variable,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7521,1,['variab'],['variable']
Modifiability,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7448:1180,extend,extending,1180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448,1,['extend'],['extending']
Modifiability,"This requires also a finer control for the codecs, once the configuration-code is implemented, to ignore default packages, and include/exclude single classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-324272895:60,config,configuration-code,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-324272895,1,['config'],['configuration-code']
Modifiability,"This script will look for a small input that trips BaseRecalibrator. However, it can be adapted for debugging pretty much anything else, so long as you have two versions of the code: a ""known good"" one to compare against, and a ""under test"" one that has the bug you're trying to generate a minimal input for.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/913:88,adapt,adapted,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/913,1,['adapt'],['adapted']
Modifiability,"This seems like a consequence of the fact that we use `java.nio.file.Path`for a lot of things in gatk. This requires a custom `java.nio.file.spi.FileSystemProvider` to be available for each type of path you want to be able to resolve. Spark native uses `org.apache.hadoop.fs.Path` for a lot of things. It's seems likely that that maprfs provides a hadoop file system plugin, which many spark applications can consume, but it's unlikely that it also provides a java.nio.file.Path implementation. ; ; I don't think we'd be able to implement a provider for maprfs ourselves. We don't have any systems with maprfs and don't have the bandwidth to take it on right now. Implementing a file system provider isn't a terribly complicated project, but it's not a trivial one either. However, there's an implementation for hadoop here https://github.com/damiencarol/jsr203-hadoop which is sufficient for what gatk does. If maprfs provides a hadoop file system, it would probably not be too difficult to take that project as a template and modify it to use the maprfs implementation. . I think the only things you'd have to implement for the spark tools to work are the basic Path operations that support the simple operations like `Paths.get()`,`Files.exists()`, and `Path.resolve()`. (although that's not a complete list. . If you are interested in writing a plugin like that, you can add it to the gatk class path at runtime. We might also be open to packaging such a plugin with the gatk if there was wide demand for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555:367,plugin,plugin,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555,3,['plugin'],['plugin']
Modifiability,This seems like a good idea. I have never liked the -1 magic values. It's definitely a non-trivial refactoring effort though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1445#issuecomment-174612131:99,refactor,refactoring,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1445#issuecomment-174612131,1,['refactor'],['refactoring']
Modifiability,"This seems like a lot of machinery (introducing two new types and a new method) just to hide the config file argument. What if we just mark it `@Hidden` (I know thats prohibited, but this is kind of a special case). The only reason it even exists is because we wanted it to appear in the command lines we display on output and embed in output files. If its `@Hidden` it will still be reflected there when it's used, but it wouldn't be displayed in tool help/usage. Its already always displayed in help as an arg for the gatk wrapper.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371570897:97,config,config,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371570897,1,['config'],['config']
Modifiability,"This seems to be a regression with GATK 4.1.0.0. The code does check for compatible versions before beginning traversal. However, the following log was reported using the M2 WDL:; ```; Runtime.totalMemory()=58851328; ***********************************************************************. A USER ERROR has occurred: Bad input: Config file for datasource (file:///cromwell_root/funcotator_dataSources.v1.4.20180615/gencode/hg19/gencode.config) does not contain required key: ""ncbi_build_version"". ***********************************************************************; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005:328,Config,Config,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"This seems to happen in the cloud auth layers, which I don't control. . One potential workaround would be to add a command-line option to disable GCS support. This would only help the original reporter if they don't use GCS paths, of course. Is this something we think may be worth doing at all?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427413074:39,layers,layers,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427413074,1,['layers'],['layers']
Modifiability,This seems to have already been refactored at some point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2102#issuecomment-590488457:32,refactor,refactored,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2102#issuecomment-590488457,1,['refactor'],['refactored']
Modifiability,"This set of optimizations brings the GATK4 HaplotypeCaller performance into line; with GATK3.x performance. Note that HaplotypeCallerSpark is not touched by this PR (that is for a future PR). Summary of changes:. * AssemblyRegionWalker: query all intervals on each contig simultaneously, rather than individually; * GATKRead: Cache adaptor boundary, soft start/end, and cigar length; * GATKRead: add getBasesNoCopy() / getBaseQualitiesNoCopy(); * ReadPileup: speed up stratified constructor; * LIBS.lazyLoadNextAlignmentContext(): don't keep pileup elements unnecessarily separated by sample during pileup creation; * Restore faster GATK3 version of ReferenceConfidenceModel.sumMismatchingQualities(); * RefVsAnyResult: nest within ReferenceConfidenceModel, and allow direct field access; * Remove redundant getBases() call in ReadThreadingGraph; * Fix BaseGraph Utils.validateArg() call; * ReadPileup: replace Collections.unmodifiableList(pileupElements).iterator() with direct return of an iterator that forbids removal; * Kill expensive bounds checking in GATKRead getBase()/getBaseQuality()/getCigarElement(); * Kill nonNull checks in PileupElement; * Kill expensive PileupElement and ReadPileup arg validation; * GATKRead adapter: clear cached values upon mutation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031:332,adapt,adaptor,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031,2,['adapt'],"['adapter', 'adaptor']"
Modifiability,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3173:316,extend,extend,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173,2,['extend'],"['extend', 'extended']"
Modifiability,"This ticket is just to make the codec packages configurable, which would be resolved by https://github.com/broadinstitute/gatk/pull/3447. If you need more fine-grained control than this, we could discuss as part of a separate ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337651849:47,config,configurable,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337651849,1,['config'],['configurable']
Modifiability,"This ticket is silly -- these are argument collections, not actual code that needs tests. Each of these classes just contains an annotated argument variable. Each class also has comments explaining when a tool would want to use each argument collection. Closing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/451#issuecomment-97182591:148,variab,variable,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/451#issuecomment-97182591,1,['variab'],['variable']
Modifiability,This ties into the URI class design meeting we're having next week -- I'd say wait until then before starting any refactor of this part of the code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4480#issuecomment-369943177:114,refactor,refactor,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480#issuecomment-369943177,1,['refactor'],['refactor']
Modifiability,"This tool should be a ReadWalker, but because of the way it uses the reference it may require a bit of refactoring to port it to the ReadWalker interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/123:103,refactor,refactoring,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/123,1,['refactor'],['refactoring']
Modifiability,This uses the new defaults with adaptive pruning in version 4.1.0.0 in Mutect and removes the old ad hoc pruning argument. @ldgauthier can you please take a look when you get a chance?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5669:32,adapt,adaptive,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669,1,['adapt'],['adaptive']
Modifiability,"This version introduces a change that (at least on my machine) fixes the mysterious ""happens only on the command line"" test failure. Also uses a newer version of genomics-dataflow because I had to fix a bug there for API_KEY to work in our setting. Finally, this version also moves the files around so they match the local tree, and changes the environment variables naming scheme to be a little more consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535:357,variab,variables,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535,1,['variab'],['variables']
Modifiability,This was an issue with propagating polymorphic std::exception code from the native library's logger utility and has been fixed in the [1.3.2 release ](https://mvnrepository.com/artifact/org.genomicsdb/genomicsdb/1.3.2) of the GenomicsDB library. Also note that using java option `GATK_STACKTRACE_ON_USER_EXCEPTION` with gatk will also output a C/C++ limited stacktrace as requested by @lbergelson.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6852:35,polymorphi,polymorphic,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6852,1,['polymorphi'],['polymorphic']
Modifiability,"This worked until we kebabified. The Freemarker template looks for the hardcoded string ""readFilter"", but the doc system populates the Freemarker map using the display name self-reported by the (ReadFilter) plugin, which is in turn derived from the standard argument name for read filters. These matched when the arg was ""readFilter"". But after kebabification, its now ""read-filter"". We should probably change the plugins to use a fixed display.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4387:207,plugin,plugin,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4387,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"Those sound like issues with the IndelRealigner tool from GATK3, which is; not part of our pipeline anymore. Is this still a problem with 4.1.4.1?. On Wed, Feb 19, 2020, 1:00 AM Dario Strbenac <notifications@github.com>; wrote:. > In the news file of a structural variant software I use, I read; >; > Added FIX_SA and FIX_MISSING_HARD_CLIP; > FIX_SA: rewrites split read SA tags; > corrects GATK indel realignment SA tag data inconsistency; > FIX_MISSING_HARD_CLIP: infers missing hard clipping if split read records; > have different lengths; > corrects for GATK indel realignment stripping hard clipping when realigning; >; > Could such issues perhaps be resolved in an update to GATK?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6459?email_source=notifications&email_token=ABSGC5E7CIUF53HYCPS76FDRDTDHBA5CNFSM4KXSMK22YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4IOQ3U6A>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABSGC5DYKL5KH6EZS5ZU66DRDTDHBANCNFSM4KXSMK2Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6459#issuecomment-588488049:351,rewrite,rewrites,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6459#issuecomment-588488049,1,['rewrite'],['rewrites']
Modifiability,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8049:310,inherit,inherited,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049,1,['inherit'],['inherited']
Modifiability,To add to this ticket. In #7876 we have had to expand the JumboAnnotations to work in the HaplotypeCaller as well. Unfortunately this has created problems since there aren't evidences objects in the HC so we have had to change the erasure of the annotate() methods somewhat and some hacky code is now part of the `VariantAnnotatorEngine` which currently has some code in the `addInfoAnnotations()` method that has to resolve the complicated spiderwebs of which likelihoods objects do or don't exist at any given time and then cast them to what they likely are. This really needs to be revisited and refactored to handle the extra annotation inputs more gracefully.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7543#issuecomment-1191802150:599,refactor,refactored,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7543#issuecomment-1191802150,1,['refactor'],['refactored']
Modifiability,To allow implementation of other `ReadFilter` plugins with the same parameter names.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2352:46,plugin,plugins,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352,1,['plugin'],['plugins']
Modifiability,"To clarify what needs to be done here:. -Add a new `--javaOptions` argument to `gatk-launch`. -When running with a packaged local jar, the value of `--javaOptions` should be injected into the command line built by `formatLocalJarCommand()`. -When running with the ""wrapper script"" (as a result of building with `./gradlew installDist` instead of `./gradlew localJar`), propagate the value of `--javaOptions` to the `JAVA_OPTS` environment variable the wrapper script expects. You can inspect the wrapper script itself by running `./gradlew installDist` and then examining `build/install/gatk/bin/gatk`. -When running on Spark, you'll need to add the `--javaOptions` to `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2694#issuecomment-305007868:439,variab,variable,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2694#issuecomment-305007868,1,['variab'],['variable']
Modifiability,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:75,config,configured,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551,1,['config'],['configured']
Modifiability,"To provide some more background, the idea is to generate output as generated by [CollectAllelicCounts](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_copynumber_CollectAllelicCounts.php) for a pool of normals so that we can correct allelic biases in tumor-only. Would it be possible that CreateSomaticPanelofNormals is extended to cover the CollectAllelicCounts ""special case""? . @samuelklee @davidbenjamin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5649#issuecomment-462058940:380,extend,extended,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5649#issuecomment-462058940,1,['extend'],['extended']
Modifiability,Tools be able to override the behavior of options they have inherited?. How should this work? Should you be able to change a required field to an optional one? Or only the other way around? . If you override the a field that has an `@Option` annotation does it effectively replace it?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/149:60,inherit,inherited,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/149,1,['inherit'],['inherited']
Modifiability,"TrainVariantAnnotationsModel:. Trains a model for scoring variant calls based on site-level annotations. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use extracted annotations that contain both SNP and INDEL variants as input); * positive (training with *.annot.hdf5) vs. positive-unlabeled (training with *.annot.hdf5 and *.unlabeled.annot.hdf5); * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:177,config,configurations,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,2,"['Refactor', 'config']","['Refactor', 'configurations']"
Modifiability,"Turns out a typo prevents running the ""manage_sv_pipeline"" script, saying GATK_DIR is an unbound variable. Please fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318746083:97,variab,variable,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318746083,1,['variab'],['variable']
Modifiability,Unbound variable bug fixed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318751586:8,variab,variable,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318751586,1,['variab'],['variable']
Modifiability,"Unit tests for tool X should not rely on the behavior of instanceMain or doWork in tool Y. . In particular, unit tests that involve comparing/validating outputs should not reference CLPs like CompareSAMs or ValidateSamFile directly. Instead, these CLPs should just be thin wrappers around other classes that have the actual logic. This is already the case for ValidateSamFile, which is just a wrapper for SamFileValidator in HTSJDK. CompareSAMs should be refactored to match this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/145:455,refactor,refactored,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/145,1,['refactor'],['refactored']
Modifiability,Update to latest version of VQSR Lite; Refactor GvsCreateFilterSet.wdl to move VQSR Classic code to its own WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8269:39,Refactor,Refactor,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8269,1,['Refactor'],['Refactor']
Modifiability,Update variable names in mutect2_pon.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259:7,variab,variable,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259,1,['variab'],['variable']
Modifiability,Update variable names in mutect2_pon.wdl to reflect changes in mutect2.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259:7,variab,variable,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259,1,['variab'],['variable']
Modifiability,Updates to `gatk` launch script to fix properties and config file definitions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653:54,config,config,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653,1,['config'],['config']
Modifiability,"Upgrade to Barclay 3.0.0, with changes for FeatureInput discovery based on Barclay refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4523:83,refactor,refactoring,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4523,1,['refactor'],['refactoring']
Modifiability,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7609:90,plugin,plugins,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"Use SampleLocatableMetadata if you want CombineSegmentBreakpoints to only operate on segment files from a single sample. It's conceivable that you want it to be more flexible, in which case I would use LocatableMetadata. Also, go ahead and move the collection class into the collection package, rather than expose the abstract classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352762883:166,flexible,flexible,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352762883,1,['flexible'],['flexible']
Modifiability,"Use argument for execution project id for queries, specifically in the sample query. This was a gap since the project id was already used for Storage API usage. I think the SampleList class could also use some refactoring, but wanted to keep this PR small",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7136:210,refactor,refactoring,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7136,1,['refactor'],['refactoring']
Modifiability,Use default plugin instances (i.e. readFilters) as the default value for args in gatkDoc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6646:12,plugin,plugin,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6646,1,['plugin'],['plugin']
Modifiability,Use portable Python arg parsing from Docker build script [VS-995],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8380:4,portab,portable,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8380,1,['portab'],['portable']
Modifiability,Use the correct variable when freeing github runner space: AGENT_TOOLSDIRECTORY.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8373:16,variab,variable,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8373,1,['variab'],['variable']
Modifiability,"User Reported this issue as shown below: . I used GenomicsDBImport and GenotypeGVCFs to call SNPs in GATK4. Due to large reference genome (1G) and sample size (100). I want to separate the work for each chromosome. It seems that GenomicsDBImport works for all chromosomes, but GenotypeGVCFs only works for chromosome1. Could you please give me some suggestions. Below are commands and log information for chromosome 2. Look forward to hearing from you soon.; Best regards,; Baosheng. ### command, all variables are defined before command lines.; $GATK --java-options ""-Xmx24g"" \; GenomicsDBImport \; ${InputVCF} \; --genomicsdb-workspace-path ${OUTDIR}/chr02 \; -L Qrob_Chr02. $GATK --java-options ""-Xmx48g"" \; GenotypeGVCFs \; -R ${REF} \; -V gendb://${OUTDIR}/chr02 \; -all-sites \; -O ${OUTDIR}/chr02.vcf. ## log file; 23:41:40.274 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:42:41.990 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.990 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 23:42:41.990 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:42:41.991 INFO GenomicsDBImport - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 23:42:41.991 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:42:41.991 INFO GenomicsDBImport - Start Date/Time: January 26, 2019 11:41:40 PM CST; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865:501,variab,variables,501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865,1,['variab'],['variables']
Modifiability,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3301:731,sandbox,sandbox,731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301,1,['sandbox'],['sandbox']
Modifiability,"User writes: . > It's great that the GATK walkers can follow symlinks to bam files. But I've never understood, why GATK UG and HC can't look for the index file in the same location as the bam. Instead one has to also create symlinks to the index files in the same location as the symlinks to the bam files. This prevents one from using as input a file with bam symlinks. There are several ways to work around this, but it's just such an annoying obstacle, which I run into every 3 or 6 months, because I forget about this behaviour. Please please fix and you can come and raid my mini fridge at work full of chocolate (currently also holds LEGO). If you choose to ignore this enhancement request, then GATK is still a pretty good tool :). Sounds legit to me. Not something I think is worth spending time on in GATK3, but sounds like a reasonable feature request for 4. This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/5944/symlinks-to-bam-files/p1)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/781:676,enhance,enhancement,676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/781,1,['enhance'],['enhancement']
Modifiability,"Users are supposed to enable/disable async I/O options by passing java option flags to the gatk launch shell script:. `; ./gatk-4.0.1.2/gatk --java-options ""-Dsamjdk.use_async_io_read_samtools=true -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=true"" <Tool and its flags>; `. It appears that no matter how these flags are passed, and even if one hard codes the variables in the actual shell script (change the defaults within the script), they are ignored. **Three examples follow:**. (Irrelevant sections are omitted with "". . ."" in their place for readability) ; (These examples use BaseRecalibrator as an example tool). **Example 1: No Flags Passed (but -Xmx12G)** ; `; ./gatk-4.0.1.2/gatk --java-options ""-Xmx12G"" <Tool specific commands>; `. ```; Using GATK jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar ; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx12G -jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar BaseRecalibrator -I /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/hg38_aligned.sorted.dedupped.bam --known-sites /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/dbsnp_hg38/dbsnp_146.hg38.vcf -O /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/recal.table -R /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/reference_hg38/Homo_sapiens_assembly38.fasta; 10:45:49.790 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:45:49.887 INFO BaseRecalibrator - ------------------------------------------------------------ ; 10:45:49.888 INFO BaseRecalibrator - The Genome Analysis Toolkit (GAT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435:392,variab,variables,392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435,1,['variab'],['variables']
Modifiability,Using adaptive pruning in mitochondria pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5669:6,adapt,adaptive,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669,1,['adapt'],['adaptive']
Modifiability,Utils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.021 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.092 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 15:41:50.093 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.093 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.config; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; WARNING 2020-08-19 15:41:50 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:50.159 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hg19_All_20180423.vcf.gz -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.159 INFO Data,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:14753,config,config,14753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['config'],['config']
Modifiability,"VCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4115,Config,ConfigFactory,4115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,VS-1159 - Enhance GVSWithdrawSamples,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8599:10,Enhance,Enhance,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8599,1,['Enhance'],['Enhance']
Modifiability,VS-1490 - Refactor python code from extract dir into a scripts directory.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9017:10,Refactor,Refactor,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9017,1,['Refactor'],['Refactor']
Modifiability,"Valentin & I discussed this in person just now, with the following results:. -The various *Context objects should probably be refactored to return empty lists upon lack of input, as Valentin suggested, instead of being `Optional`. -There may be a need to allow tools to request additional context around the current locus/interval, but tools should probably not be performing arbitrary queries as a general rule, since it would be difficult or impossible to optimize a traversal in which the access pattern is random. If a tool needs to group disparate data items together (eg., mates on different contigs), there should be an initial grouping step to prepare the required data for the main analysis, instead of random queries within the main analysis. -apply()/map() should take its inputs as parameters instead of directly accessing member variables into which input data has been injected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76805735:126,refactor,refactored,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/242#issuecomment-76805735,2,"['refactor', 'variab']","['refactored', 'variables']"
Modifiability,Variable depth bins in Mutect2 autoval,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4339:0,Variab,Variable,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4339,1,['Variab'],['Variable']
Modifiability,Variant context and genotype comparison have been heavily refactored for readability and to provide more easily customized comparisons,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6417:58,refactor,refactored,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6417,1,['refactor'],['refactored']
Modifiability,VariantFiltration - port and a ~half-rewrite for GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/614:37,rewrite,rewrite,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/614,1,['rewrite'],['rewrite']
Modifiability,"VariantQC is a tool we made that is somewhat analogous to FastQC. Given an input VCF, it runs VariantEval to generate various summary tables of data, and then makes an HTML report (borrowing a lot from the tool MultiQC) summarizing that VCF. . I wrote this originally by forking GATK3 and wrote a new walker that internally called and run VariantEval. That was never the final plan. I dont know what this will need to look like in GATK4 yet. I'm fine with the expectation that GATK4 VariantEval will evolve and we'd need to update our code wrapping it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440806347:500,evolve,evolve,500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440806347,1,['evolve'],['evolve']
Modifiability,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2737:402,refactor,refactor,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737,1,['refactor'],['refactor']
Modifiability,"VariantsSpark - Done initializing engine; 19/02/18 16:58:10 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/02/18 16:58:10 INFO org.spark_project.jetty.util.log: Logging initialized @8431ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: Started @8536ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 19/02/18 16:58:11 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 19/02/18 16:58:12 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m/10.240.0.11:8032; 19/02/18 16:58:13 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m/10.240.0.11:10200; 19/02/18 16:58:15 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1550508751046_0004; WARNING	2019-02-18 16:58:23	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; WARNING	2019-02-18 16:58:23	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 19/02/18 16:58:25 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 19/02/18 16:58",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:4890,config,configuration,4890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,Very simple implementation of #2297 using a custom `GATKConf` class to allow both promatically (`GATKConfBuilder`) and by commons-configuration API (constructor). Only includes:. * Packages/Classes to include in the CLP on startup.; * Packages to look for codecs on startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322:130,config,configuration,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322,1,['config'],['configuration']
Modifiability,"WIth the 4.2.2.0 ReblockGVCF it is running fine. This was without rerunning the HaplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:256,variab,variable,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['variab'],['variable']
Modifiability,"WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Request",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4075,Config,ConfigFactory,4075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"We build GATK-SV docker images on GitHub runners. We use the following to set up the environment to use `--squash` flag. https://github.com/broadinstitute/gatk-sv/blob/52813222b64bf2d15fb9a1aae068590bee184511/.github/workflows/sv_pipeline_docker.yml#L199-L204. I am unsure if you can configure the runtime env on Google Cloud build, but if you can, hopefully, the above can hint some directions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1979099671:284,config,configure,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1979099671,1,['config'],['configure']
Modifiability,We can either add `to*LegacySegmentCollection` methods to ModeledSegmentCollection or add static utility methods to ModelSegments. No real preference here as the coupling is minimal.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407170543:162,coupling,coupling,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407170543,1,['coupling'],['coupling']
Modifiability,We could combine the filters and transformers into 1 plugin that can intersperse them. It would be more complicated but be maximally expressive.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246002976:53,plugin,plugin,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246002976,1,['plugin'],['plugin']
Modifiability,We currently use a confusing mix of system properties and environment variables for settings not controlled by command-line arguments. We should choose one or the other.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1666:70,variab,variables,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1666,1,['variab'],['variables']
Modifiability,"We generate indices on output files, but we decided not to auto-generate indices on input files in GATK4. We included tools `IndexFeatureFile` and `BuildBamIndex` that can generate these indices on-demand. Indexing inputs automatically is inherently racy/dangerous in the face of multiple processes sharing inputs unless you do things like file locking, which comes with all sorts of portability issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2647#issuecomment-298768350:384,portab,portability,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2647#issuecomment-298768350,1,['portab'],['portability']
Modifiability,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998:32,extend,extends,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998,3,"['adapt', 'extend', 'refactor']","['adapt', 'extends', 'refactor']"
Modifiability,"We haven't resolved the issue with our project configuration yet, so we need to publish this ourselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4008:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4008,1,['config'],['configuration']
Modifiability,"We might want to add a separate mechanism for this as well that doesn't involve extending `Main`, since using a different main class makes packaging slightly awkward. We could add an editable config file that `Main`reads from to find the list of packages to search.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1396#issuecomment-166960036:80,extend,extending,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396#issuecomment-166960036,2,"['config', 'extend']","['config', 'extending']"
Modifiability,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3275:126,config,configurable,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275,1,['config'],['configurable']
Modifiability,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5396:348,config,configurations,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396,1,['config'],['configurations']
Modifiability,We ported a bunch of code that is marked as deprecated. We should delete it and refactor anything that relies on it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/162:80,refactor,refactor,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/162,1,['refactor'],['refactor']
Modifiability,"We probably can't remove `ReferenceWindowFunction`, as it's being used by tools in gatk-protected. How can we guarantee that `JoinStrategy.OVERLAPS_PARTITIONER` will provide at least as much reference context as requested by the configured `ReferenceWindowFunction`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2254:229,config,configured,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2254,1,['config'],['configured']
Modifiability,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4990:130,refactor,refactored,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990,1,['refactor'],['refactored']
Modifiability,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706:425,config,configured,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706,1,['config'],['configured']
Modifiability,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2622:174,variab,variable,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622,2,['variab'],['variable']
Modifiability,"We recently updated (PR #4858) the default Smith-Waterman parameters for realigning reads to their best haplotype. Although there is no reason for the alignment of haplotypes to the reference to use the same parameters, it seems like we are similarly favoring indels too much. There is a forum discussion to this effect:. https://gatkforums.broadinstitute.org/gatk/discussion/23230/gatk-haplotypecaller-mnp-output-problem. Here are the parameters we use:. * match: 200; * substitution: -150; * indel start: -260; * indel extend: -11. These parameters, which are essentially a prior on biological variation, prefer an indel, with a cost of 260, to a SNP, with a cost of 350. This does not seem correct. It almost never comes up because the correct alignment is usually unambiguous, but when it does, shouldn't we break the tie in favor of the SNP?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564:521,extend,extend,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564,1,['extend'],['extend']
Modifiability,"We recommend backing up data just because it is the ""cleanest"" way to roll back. If backing up data is really such a pain point, you could skip doing that. Just back up the callset.json file, and don't turn on `--consolidate` when you're doing incremental import. If a failure happens, just roll back the callset.json and re-do the import. The downside is that the failed import will hang around and take up disk space, but hopefully it is a rare enough occurrence that it doesn't matter - and you will have saved yourself backing up the data. In response to 2) - I guess you're implying that the overhead of cluster/job scheduling won't amortize any benefits from parallelism there? I suppose that could be true, but doesn't seem to be worth optimizing towards that. What I'm asking is whether split and merge are purely an instrument to allow you to choose the granularity of parallelism you want to use? Or is there something else? As I said before, we are considering enabling other ways to do distributed import which would work for the former. It might go something like:; - Create a workspace/initialize configuration+intervals to be imported; - Actually do the import by kicking off (multiple) import(s). User can pick the number of intervals each import is responsible for. User must ensure that no interval gets specified in multiple import processes. P.S: regarding 1000s of small contigs - the current GenomicsDBImport doesn't so so well with large number of contigs (unless you do concatenate the contigs into fewer groups). We hope to have some changes coming soon that will help with that by adding an option for the tool to merge multiple contigs into a single folder in the workspace.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548:1111,config,configuration,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548,1,['config'],['configuration']
Modifiability,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377:20,plugin,plugin,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377,1,['plugin'],['plugin']
Modifiability,We should definitely try to centralize setting of the system properties if possible (perhaps using a master config file) -- though I vote that we put in a quick fix for this first.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267123755:108,config,config,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267123755,1,['config'],['config']
Modifiability,"We should probably adapt the IntervalTree from htsjdk to work for us. We've run into a number of cases where this is needed, in Valentine's exome code and in Tom's hadoop reader. We could use both. `boolean overlaps(Locatable locatable, IntervalTree<Locatable> locatables)`. and . `Set<Locatable> getOverlapping(Locatable locatable, IntervalTree<Locatable> locatables)`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/559:19,adapt,adapt,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/559,1,['adapt'],['adapt']
Modifiability,"We should rely on the GATK config file for the default values for these system properties, rather than hardcoding them into the `gatk` frontend script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4484:27,config,config,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4484,1,['config'],['config']
Modifiability,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3317:372,adapt,adaptive,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317,1,['adapt'],['adaptive']
Modifiability,"We use the GATKGCSOptions class to hold GCP authentication. It inherits from the Dataflow hierarchy and works well there, but since it doesn't implement Serializable it's cumbersome to work with in Spark. We've created AuthHolder as a replacement. It can do all the things GATKGCSOptions can do, and more (well, except for holding Dataflow debug options but we don't need that anymore). Once #978 is merged in, we need to migrate the code from GATKGCSOptions to AuthHolder. One benefit is that this will allow the Spark code to support client-secrets.json (for access to private GCS files, unlike the API key which only grants access to world-readable GCS files).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1002:63,inherit,inherits,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1002,1,['inherit'],['inherits']
Modifiability,"We're seeing messages like the following when running `GenomicsDBImport`:. ```; Column 948660 has too many alleles in the combined VCF record : 61 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; Column 948710 has too many alleles in the combined VCF record : 83 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; ```. Is this limit of 50 configurable, if we wanted to raise it, and if not, could it be made configurable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687:489,config,configurable,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687,2,['config'],['configurable']
Modifiability,"We've filed a ticket with github support -- however, the branch has been cleared to merge in its current state, as it's had more than enough reviews. We can file tickets to improve/refactor once it's in master.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625:181,refactor,refactor,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625,1,['refactor'],['refactor']
Modifiability,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:827,Config,Configuration,827,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['Config'],['Configuration']
Modifiability,"Weird, I would have that that forcing the htsjdk version like we already do would have done it... I don't see anything wrong with adding that exclusion, but I'm confused why we need it. ` force 'com.github.samtools:htsjdk:' + htsjdkVersion`. It sounds like a gradle bug in building the final pom file. I wonder if switching to the javaLibrary plugin would fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292579154:343,plugin,plugin,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292579154,1,['plugin'],['plugin']
Modifiability,"Well, I guess that part of the contract for GATKTool is that GATK tools should report progress as they go. I agree that the ProgressMeter should be made more flexible and allow reporting of progress in terms of things other than genomic location.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577262636:158,flexible,flexible,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577262636,1,['flexible'],['flexible']
Modifiability,"Well, that explains that, sort of. The code snippet you're providing looks like it ought to do what you say it does (i.e., the mates have to be paired, not unmapped, mapped to the same contig, and have a difference in their start positions that is at least `mateTooDistantLength`). . But there are two problems with this:. 1) This filter's behavior is unexpected wrt HaplotypeCaller. It seems to me that an inclusive filter (i.e., process only paired-end mappings whose TLEN falls within a specified range) would be more usable. That would imply a filter implementation that accepts a pair of integers, but the expected behavior would be more obvious and in line with GATK's other range-limited parameterizations (e.g., `MappingQualityReadFilter` comes immediately to mind). 2) I can't tell from where I sit, but the code snippet looks correct only if `getStart()` and `getMateStart()` return a zero-based start position of each mate relative to the start of the strand to which the mate is mapped. If the code is just computing the difference between POS for the mates, the computation is incorrect for forward + reverse-complement (Illumina-style) pairs. In addition, computing TLEN requires not only that you consider the orientation of the individual mate mappings, but also that you make an arbitrary decision about how to handle soft-clipped reads. I hate to say this, but I think this parameter needs some attention. Its potential utility with HaplotypeCaller seems evident to me (i.e., it would be good to be able to exclude outliers with unreasonable TLENs) but its implementation and frugal documentation make it unusable in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220:695,parameteriz,parameterizations,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220,1,['parameteriz'],['parameterizations']
Modifiability,What is the progress on this @cmnbroad? Is this waiting for the new Barclay plugin interface?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-282995363:76,plugin,plugin,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-282995363,1,['plugin'],['plugin']
Modifiability,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:25,portab,portable,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363,1,['portab'],['portable']
Modifiability,"What is your cluster configuration? . That's a lot of memory for one executor, it may be having trouble allocating workers with that much memory, or using all the memory on 1 very large executor.; Have you tried setting executor cores as well? I would usually set it to something like `--executor-cores 4 --executor-memory 16G` . You want to design your executors so they fit evenly into the worker nodes on your cluster but don't have too many cores per executor. . An aside, you *should* be able to create a bam index as part of SortSamSpark now, we have support for generating it in parallel and merging the indexes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547077382:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547077382,1,['config'],['configuration']
Modifiability,"What the rules for when a tool is allowed to be a `CommandLineProgram`? Most of the CNV tools extend `CommandLineProgram` rather than `GATKTool` for various reasons, including: 1) they use sequence-dictionary input in a way that requires custom argument documentation, 2) they use `-I` to specify non-BAM/SAM/CRAM input, and 3) they don't really make use of the argument collections available in `GATKTool` or otherwise fall under the walker paradigm. These reasons are admittedly minor, but they do make the tools a bit nicer to use in the end. Otherwise, whenever it makes sense for a tool to extend `GATKTool`, it does (4 out of 12 of the CNV tools). (A bit of a tangent: in all the cases where we do extend `GATKTool` to e.g. make use of the `-L` functionality, we still have to jump through some extra hoops to make sure we don't get tripped up. For example, the default `--interval-merging-rule` behavior is incorrect for most CNV analyses, so the user has to set this to `OVERLAPPING_ONLY` manually, otherwise we throw an exception---which is quite awkward. Ideally, we'd have some option to not modify the incoming intervals at all, as well.). So I'm comfortable with closing this issue, but we can discuss the pros and cons of moving more of the tools over if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358038497:94,extend,extend,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358038497,3,['extend'],['extend']
Modifiability,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2320:182,refactor,refactoring,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320,1,['refactor'],['refactoring']
Modifiability,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2282:538,extend,extending,538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282,1,['extend'],['extending']
Modifiability,"When running Spark tools with GCS files on Dataproc the [GCS connector](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) is set up and configured for you, but this isn't the case when running with local Spark, even on a GCP VM. We should make the experience easier through documentation and/or configuration improvements.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996:158,config,configured,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996,2,['config'],"['configuration', 'configured']"
Modifiability,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _Gonçalo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4434:1481,rewrite,rewrite,1481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434,1,['rewrite'],['rewrite']
Modifiability,"When we make enhancements to the walker engine (eg., modify the `GATKTool` base class to support CRAM, or to validate the sequence dictionaries of the inputs), it would be good if Spark tools could also reap the benefits of these changes automatically. We may need to unify (or better integrate) the `GATKTool` and `SparkCommandLineProgram` base classes somehow to make this possible, as well as classes like `ReadsDataSource` (for walkers) and `ReadsSparkSource` (for Spark tools).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/680:13,enhance,enhancements,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/680,1,['enhance'],['enhancements']
Modifiability,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4989:8,refactor,refactored,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989,2,"['config', 'refactor']","['configuration', 'refactored']"
Modifiability,"While @ldgauthier was presenting LobSTR, an STR indel caller, @fleharty and I were discussing how their probabilistic model was in some sense trying to convey a gap opening penalty and a gap continuation penalty for PCR slippage and related errors. This sort of suggests that one could go whole hog and build this into an enhanced pair-HMM that is aware of multiple error modes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1869:322,enhance,enhanced,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869,1,['enhance'],['enhanced']
Modifiability,"While we normally don't recommend ignoring that wrapper, this seems like a good reason to do so. . The wrapper is pretty simple, most of what it's doing is some munging of the input to allow it to be more standardized in several different gatk use cases. The only thing I can think of that you would want to be sure to copy is that it sets a number of properties. . We set these spark `--conf` properties with the wrapper. I don't actually know how important some of them are anymore. If it works without them then you're probably good.; ```; ""spark.kryoserializer.buffer.max"" : ""512m"",; ""spark.driver.maxResultSize"" : ""0"",; ""spark.driver.userClassPathFirst"" : ""false"",; ""spark.io.compression.codec"" : ""lzf"",; ""spark.executor.memoryOverhead"" : ""600"",; ""spark.driver.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK,; ""spark.executor.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK; ```. These are htsjdk properties we want to set for spark. ; ```; EXTRA_JAVA_OPTIONS_SPARK= ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "" \; ""-Dsamjdk.use_async_io_read_samtools=false "" \; ""-Dsamjdk.use_async_io_write_samtools=false "" \; ""-Dsamjdk.use_async_io_write_tribble=false "" \; ""-Dsamjdk.compression_level=2 ""; ```. If you can get this value into your spark environment variables it prevents and anying warning output. `SUPPRESS_GCLOUD_CREDS_WARNING=true`. Let us know how it works for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054:1251,variab,variables,1251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054,1,['variab'],['variables']
Modifiability,"While working on validating #5607 I noticed that at the top of the method `isReadInformativeOfIndelOfSize()` that there is the following breakout condition:; ```; if( read.getLength() - readStart < maxIndelSize || refBases.length - refStart < maxIndelSize ) {; return false;; }; ```; This says that if the readStart is too close to the read.getLenght() then it will break out and not calculate the informativeness of a read. Unfortunately readStart isn't the readbase indexed readStart, its actually the ""IGV view"" offset for the read generated by the pileup for a particular reference position. The actual length that matters to us is: `AlignmentUtils.getBasesAlignedOneToOne(read).length` which is computed later when we realign the read bases to the reference. What this means is that if a read happens to have a long deletion in it then we will end up prematurely marking the read bases as being non-informative despite there being more than enough bases to work with when doing computations. Furthermore, since we realign the read bases later in the codepath, these bases in the gap between the realigned length and `read.getLength()` are still used to compute mismatch likelihood for bases before that point in the read. An example of this issue: I have a read with the cigar ""77M10D24M"", at position 92 of the read (the igv offset so in reality the 5th base into the last element of the cigar) the code returns false due to this condition. In reality `AlignmentUtils.getBasesAlignedOneToOne(read).length - readStart` value is 19, and thus comparable since there are >10 bases left in the read to test. . I have duplicated this behavior in #5607, perhaps it would be easiest to get that branch in first before tackling this issue just so validation for that refactor is easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5646:1764,refactor,refactor,1764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5646,1,['refactor'],['refactor']
Modifiability,Will add a variable to our Protobuf configuration object - the JSON already an option to set this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300298863:11,variab,variable,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300298863,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"Will proposed a novel approach to filtering that could prove useful: instead of running a sequence of filters and discarding items as soon as they fail a filter, use `com.google.cloud.dataflow.sdk.transforms.Partition<T>` to group items into `PCollections` according to which filters they fail, and then allow a configurable transform to be run on each partition. By default we might just count the number of items failing each filter, but we could have the option of doing things like saving the failing items somewhere for debugging purposes, or outputting for each item the list of ALL filters that it fails.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/303:312,config,configurable,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/303,1,['config'],['configurable']
Modifiability,Will refactor and re-open as a different PR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8642#issuecomment-1937109021:5,refactor,refactor,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8642#issuecomment-1937109021,1,['refactor'],['refactor']
Modifiability,"With a service account key set, it worked like a charm:. ```; $ ./gatk-launch PrintReadsSpark -I gs://jpmartin-testing-project/hellbender-test-inputs/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O gs://jpmartin-testing-project/test-output/readcount --shardedOutput true -- --sparkRunner GCS --cluster jps-test-cluster; (...); [November 20, 2017 6:17:08 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=670040064; Job [13c93a62-96d0-456e-91d1-ef7b20f1236b] finished successfully.; ```. Though I understand that [this is expected](https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894). So next I tried it without any `HELLBEND*` environment variable and it worked as well!. ```; Job [6e2f2c6b-921a-4fdf-a42e-0706216b2098] finished successfully.; (...); $ gsutil ls -lh gs://jpmartin-testing-project/test-output/readcount/; 0 B 2017-11-20T18:28:27Z gs://jpmartin-testing-project/test-output/readcount/; 0 B 2017-11-20T18:28:52Z gs://jpmartin-testing-project/test-output/readcount/_SUCCESS; 120.25 MiB 2017-11-20T18:28:51Z gs://jpmartin-testing-project/test-output/readcount/part-r-00000.bam; ```. This is with `GOOGLE_APPLICATION_CREDENTIALS` set, as I believe is part of the GATK README instructions. Next I went to my repro code and tried it again with v30. It failed (`StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account.`) I'm not sure why but the new version is certainly an improvement over the previous one since it fixes `PrintReadsSpark`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-345788205:745,variab,variable,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-345788205,1,['variab'],['variable']
Modifiability,"With gnomAD support, we will need to be able to match an HG19 interval against the B37 data source. This can be done by adding a field to the data source config files `dataAreB37` (or similar). Then in the code, `DataSourceFuncotationFactory` sets that to an internal variable based on the config file and uses that setting to call either `getValues(mainSourceFileAsFeatureInput)` or `getValues(mainSourceFileAsFeatureInput, equivalentB37SimpleInterval)` where `equivalentHg19SimpleInterval` is the Hg19 equivalent interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5456:154,config,config,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5456,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"With the addition of the Owner configuration mechanism to GATK, we're going to need to fold (most of) `gatk-launch` into Java, so that the Spark settings can be loaded from Owner before `spark-submit` or `gcloud` are invoked. Note that we'll still need to have a thin GATK launcher script for the sake of the tab completion, which requires that we invoke GATK using a unique command name.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3503:31,config,configuration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3503,1,['config'],['configuration']
Modifiability,"With the exception of the HMM package, all of our R dependencies are available through the conda R or bioconda channels; the HMM package is available only through a user's custom channel. However, the HMM package is only used to generate truth for testing the Java HMM code by @vruano (which is currently unused, but we thought was worth keeping around). I'm sure we could easily rewrite the tests to load the truth from a file. I think we should get rid of the install_R_packages.R script altogether and just roll all of these dependencies into the conda environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920:380,rewrite,rewrite,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920,1,['rewrite'],['rewrite']
Modifiability,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7830:176,config,configuration,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830,1,['config'],['configuration']
Modifiability,"Works with local files, and a PR for cloud functionality (complete with passing test) is under review (it's PR #595). The one thing left to do is to adapt to the skeleton once that's merged in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/426#issuecomment-114635562:149,adapt,adapt,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/426#issuecomment-114635562,1,['adapt'],['adapt']
Modifiability,Would also be good to identify what changes we'd need to make to the `broad-dsde-dev` firewall config to make this work there so that we can file a request with IT.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/975#issuecomment-148461723:95,config,config,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/975#issuecomment-148461723,1,['config'],['config']
Modifiability,"Would be nice if common, rarely-changing dataflow options like the project and client secret could be specified in a config file rather than as part of every hellbender command.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508:117,config,config,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508,1,['config'],['config']
Modifiability,"Would it be helpful to discuss this in person? Filter behavior is an important design decision and I'm not sure I have a good handle on the current state + context. On Mon, Feb 23, 2015 at 9:22 PM, Louis Bergelson notifications@github.com wrote: Some always throw, some always filter, some depend on the arguments you; pass to the filter constructor.; On Feb 23, 2015 9:19 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > Ah -- maybe I'm mistaken. Or can it be a difference in how they're; > applied/invoked?; > ; > Maybe there's some inconsistency in behavior. Would be nice to iron this; > all out.; > ; > On Mon, Feb 23, 2015 at 5:17 PM, Louis Bergelson <notifications@github.com; > ; > > wrote:; > > ; > > I'm pretty sure they all do... or at least all can depending on how you; > > configure your MalformedReadFilter.; > > ; > > example:; > > ; > > private static boolean checkHasReadGroup(final SAMRecord read) {; > > if ( read.getReadGroup() == null ) {; > > // there are 2 possibilities: either the RG tag is missing or it is not; > > defined in the header; > > final String rgID =; > > (String)read.getAttribute(SAMTagUtil.getSingleton().RG);; > > if ( rgID == null ); > > throw new UserException.ReadMissingReadGroup(read);; > > throw new UserException.ReadHasUndefinedReadGroup(read, rgID);; > > }; > > return true;; > > }; > > ; > > —; > > Reply to this email directly or view it on GitHub; > > <; > > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75649333; > > ; > > .; > ; > ## ; > ; > Geraldine A. Van der Auwera, Ph.D.; > Bioinformatics Scientist II; > GATK Support & Outreach; > Broad Institute; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/issues/193#issuecomment-75686467; > . —Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/193#issuecomment-76679907:808,config,configure,808,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/193#issuecomment-76679907,1,['config'],['configure']
Modifiability,"Wow, thanks for the detailed comments so far, @davidbenjamin! But perhaps let's quickly chat before you go any further?. There are a lot of things you commented on---temporary integration tests using local files, lots of code/arguments/etc. intentionally copied verbatim over from VQSR/tranches, and entire tools (the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain)---that are rather in flux or will be scrapped/cleaned up shortly. That said, the comments on the code inherited from VQSR will certainly be useful in this process!. But it might save you some time if we could chat so I can give you a rough orientation and perhaps point out where the vestigial VQSR code remains. I think focusing discussion on the high level design of the tools that are likely to stay would also be most useful at this stage. Feel free to throw something on my calendar!. In the end, I think we will probably just retain the BGMM backend + the versions of the tools in the ""scalable"" package. I left the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain tools in this branch so I could do one round of tieout. That tieout came out OK, so I think we'll abandon the monolithic tools, along with all the associated code outside of the scalable package. If it helps, I can go ahead and remove that stuff from this draft PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942:478,inherit,inherited,478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942,1,['inherit'],['inherited']
Modifiability,XsvLocatableTableCodec and AnnotatedIntervalCodec should inherit from the same abstract class,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580:57,inherit,inherit,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580,1,['inherit'],['inherit']
Modifiability,"Y : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2930,Config,ConfigFactory,2930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,YXZh) | `0% <0%> (-100%)` | `0% <0%> (-8%)` | |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `3.448% <0%> (-94.253%)` | `2% <0%> (-8%)` | |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `0% <0%> (-91.429%)` | `0% <0%> (-23%)` | |; | [.../org/broadinstitute/hellbender/utils/IGVUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JR1ZVdGlscy5qYXZh) | `0% <0%> (-88.889%)` | `0% <0%> (-3%)` | |; | [...alkers/mutect/filtering/PolymorphicNuMTFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvUG9seW1vcnBoaWNOdU1URmlsdGVyLmphdmE=) | `0% <0%> (-88.235%)` | `0% <0%> (-9%)` | |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `0.431% <0%> (-87.5%)` | `2% <0%> (-85%)` | |; | [...alkers/mutect/SomaticReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9Tb21hdGljUmVmZXJlbmNlQ29uZmlkZW5jZU1vZGVsLmphdmE=) | `12.5% <0%> (-84.375%)` | `1% <0%> (-7%)` | |; | [...tils/variant/writers/SomaticGVCFBlockCombiner.ja,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5803#issuecomment-473417970:2816,Polymorphi,PolymorphicNuMTFilter,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5803#issuecomment-473417970,1,['Polymorphi'],['PolymorphicNuMTFilter']
Modifiability,"Y_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2997,Config,ConfigFactory,2997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,adapt,adapter,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,1,['adapt'],['adapter']
Modifiability,"Yeah, I expect to need to rewrite tests for the final port. Thanks for any help on the validation data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358036823:26,rewrite,rewrite,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358036823,1,['rewrite'],['rewrite']
Modifiability,"Yeah, it would be useful (see https://github.com/broadinstitute/gatk/issues/2582). Not sure if/when we'll ever get around to the Barclay changes though. Another simple option that wouldn't require Barclay changes would be to implement it as just another (plugin descriptor) command line argument that could be sued alongside `--read-filter`'. So if you wanted a `ReadNameFilter` and an inverted `ReadLengthFilter`, the syntax would be:. `--read-filter ReadNameFilter --invert-read-filter ReadLengthReadFilter`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306:255,plugin,plugin,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306,1,['plugin'],['plugin']
Modifiability,"Yeah, refactoring this class is pretty essential to our ability to write good tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/641#issuecomment-146230791:6,refactor,refactoring,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/641#issuecomment-146230791,1,['refactor'],['refactoring']
Modifiability,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:197,adapt,adapting,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431,2,"['adapt', 'config']","['adapting', 'configuration']"
Modifiability,"Yep, that will definitely cause a crash - variable length fields need a length value stored while fixed length fields don't. I can add checks for this scenario in GenomicsDB. Please let me know if the following make sense:; Header says that the field F is a fixed length field with length = N. In the data section, if; * length(F) < N - pad with missing values, no error message, continue; * length(F) > N - error, throw exception and print descriptive error message",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684:42,variab,variable,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684,1,['variab'],['variable']
Modifiability,"Yes it is. Honestly not sure on the u/g config, as an end user I'd really rather not have to care about that 😅 ; This is the only tool causing this kind of issue so it's got to be the tool itself, no?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078145966:40,config,config,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078145966,1,['config'],['config']
Modifiability,"Yes, Hadoop-BAM uses the NIO API to do file merging, whereas in GATK we were using the Hadoop APIs (and therefore the GCS<->HDFS adapter) to do it. It looks like there are a couple of things needed in GCS-NIO to use the NIO API for this.; 1. https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1450 so that we don't have to special-case `gs` URIs to remove everything except the scheme and host when looking up the filesystem (see https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L40); 2. https://github.com/GoogleCloudPlatform/google-cloud-java/issues/813 to support path matching (https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L90). There may be more, as I stopped there. The best way forward is probably to go back to the old code in GATK while the deficiencies in GCS-NIO are fixed and then released. The stacktrace I got for 1 was:. ```; java.lang.IllegalArgumentException: GCS FileSystem URIs mustn't have: port, userinfo, path, query, or fragment: gs://gatk-demo-tom/TEST/markdups.parts/_SUCCESS; 	at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:54); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); ```. And for 2:. ```; java.lang.UnsupportedOperationException; 	at com.google.cloud.s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265132050:129,adapt,adapter,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265132050,1,['adapt'],['adapter']
Modifiability,"Yes, thank you for jogging my memory @bbimber. @davidbenjamin adding something to that extend in the BadArgumentException message would be helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6263#issuecomment-558740872:87,extend,extend,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6263#issuecomment-558740872,1,['extend'],['extend']
Modifiability,"Yes, with GATK4 we made the decision early on that all test data must be publicly-shareable and checked into the repo. This has often meant that when we port a tool from GATK3 to GATK4, we need to rewrite the tests to use different test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358035278:197,rewrite,rewrite,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-358035278,1,['rewrite'],['rewrite']
Modifiability,YnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zdi9jbHVzdGVyL1NWQ2x1c3RlckVuZ2luZS5qYXZh) | `93.269% <0.000%> (-1.002%)` | :arrow_down: |; | [...stitute/hellbender/tools/walkers/sv/SVCluster.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3Rlci5qYXZh) | `89.773% <0.000%> (-0.881%)` | :arrow_down: |; | [...tools/walkers/sv/JointGermlineCNVSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L0pvaW50R2VybWxpbmVDTlZTZWdtZW50YXRpb24uamF2YQ==) | `86.047% <0.000%> (-0.752%)` | :arrow_down: |; | [...der/tools/walkers/sv/SVClusterIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3RlckludGVncmF0aW9uVGVzdC5qYXZh) | `99.496% <0.000%> (-0.004%)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <0.000%> (+0.035%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7858#issuecomment-1130438520:4985,Adapt,AdaptiveChainPruner,4985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7858#issuecomment-1130438520,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"You can currently specify additional spark configuration with the --conf argument, so you could override the registrator that way. I think you'd have to update gatk-launch as well as build.gradle to get it to work, and currently gatk-launch is shared with gatk-protected. Probably the best solution is going to be to extract all the hardcoded configurations into a configuration file that can be changed on a per project basis. There's some movement to this in gatk public at the moment, but we haven't settled on a solution yet I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272484533:43,config,configuration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272484533,3,['config'],"['configuration', 'configurations']"
Modifiability,"You can detect whether a tool has failed in bash by checking whether the exit status code is non-zero. In bash, the exit status code of the last command run is stored in the variable `$?`; ; In general, you should ask questions like these on the GATK forum (https://gatkforums.broadinstitute.org/gatk) instead of here, however -- this is for bug reports rather than support requests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349:174,variab,variable,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349,1,['variab'],['variable']
Modifiability,You have a compile error btw @jamesemery:. ```; :compileTestJava/home/travis/build/broadinstitute/gatk/src/test/java/org/broadinstitute/hellbender/engine/spark/SparkCommandLineProgramUnitTest.java:20: warning: [serial] serializable class TestSparkCommandLineProgram has no definition of serialVersionUID; private static class TestSparkCommandLineProgram extends SparkCommandLineProgram {; ^; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-228763924:354,extend,extends,354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1947#issuecomment-228763924,1,['extend'],['extends']
Modifiability,You would not have access to docker container options when using the Google backend because the running of your image is all controlled by Pipelines API. You would be able to set that value when running on a local backend but thats probably not portable enough for your workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357313468:245,portab,portable,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357313468,1,['portab'],['portable']
Modifiability,"You'd better verify the consistency of the results step by step, then you can find which step's result become diffierent, finally fix it.; For example, you want to see if the `JavaRDD<Shard<GATKRead>> readShards`results are always same. You can add debug info in `callVariantsWithHaplotypeCaller` just like this:. ```; ...... final JavaRDD<Shard<GATKRead>> readShards = SparkSharder.shard ....; //Debug; String path = ""./dubug/readShardsTest1"";; java.io.File debug = new File(path);; if (!debug.getParentFile().exists()); debug.getParentFile().mkdir();; try (PrintStream printStream = new PrintStream(debug)) {; printStream.println(""List<ShardBoundary> shardBoundaries size : "" + shardBoundaries.size());; printStream.printf(""NumPartitions : %d\n"", readShards.getNumPartitions());. List<ShardDebug> shardDebugs = readShards.mapToPair(shard -> new Tuple2<>(new ShardDebug(shard), null)); .sortByKey((Comparator<ShardDebug> & Serializable) (o1, o2) ->; IntervalUtils.compareLocatables(o1, o2, header.getSequenceDictionary()); ).keys().collect();; printStream.printf(""NumShard : %d\n"", shardDebugs.size());; for (ShardDebug shardDebug : shardDebugs) {; printStream.println(shardDebug.toString());; }; }catch (Exception e){; e.printStackTrace();; }; ```. ```; static class ShardDebug extends ShardBoundary{; int size;. public ShardDebug(Shard<GATKRead> shard) {; super(shard.getInterval(),shard.getPaddedInterval());; size = Iterators.size(shard.iterator());; }. @Override; public String toString() {; return this.getInterval().toString() + ""\t"" + size;; }; }; ```; ; Run twice and compare the differences, do this step by step, you will find the bug. Gook Luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371415511:1280,extend,extends,1280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371415511,1,['extend'],['extends']
Modifiability,"Your solution doesn't address your third listed drawback to the current; approach, though I'm not sure there's any way to do that that wouldn't; require a pretty dramatic change. It's not obvious to me why we wanted the given alleles in the graph; originally. Maybe the use case was variants from UG that we didn't; necessarily believe were aligned properly?. I don't have any objections, but I'd feel better if we had a better guess; at what the original method was trying to do. On Wed, Apr 3, 2019 at 9:56 PM David Benjamin <notifications@github.com>; wrote:. > In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them; > into the ref haplotype, then threading these constructed haplotypes into; > the assembly graph with a large edge weight. There are several drawbacks to; > this approach:; >; > - The strange edge weights interfere with the AdaptiveChainPruner.; > - The large edge weights may not be large enough to avoid pruning when; > depth is extremely high.; > - The alleles may be lost if assembly fails.; > - If the alleles actually exist but are in phase with another variant; > we end up putting an enormous amount of weight on a false haplotype.; >; > We can get around these issue with the following method:; >; > - assemble haplotypes without regard to the force-called alleles.; > - if an allele is present in these haplotypes, do nothing further.; > - otherwise, add a haplotype in which the allele is injected into the; > reference haplotype.; >; > @LeeTL1220 <https://github.com/LeeTL1220> I prototyped this and it seems; > to resolve the missed forced alleles that Ziao found.; >; > @ldgauthier <https://github.com/ldgauthier> Can you think of any; > objections to making this change in HaplotypeCaller?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5857>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMcaTJg47gn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767:862,Adapt,AdaptiveChainPruner,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"[![Coverage Status](https://coveralls.io/builds/6585478/badge)](https://coveralls.io/builds/6585478). Coverage decreased (-0.5%) to 81.546% when pulling **f80168755f991400a5028c9525468650f253751b on DevFactory:release/array-designators-should-be-on-the-type,not-the-variable-fix-1** into **f4a4a6f1f13886d05a03c4b13244c3339145a6e5 on broadinstitute:master**.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1915#issuecomment-225816392:266,variab,variable-fix-,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1915#issuecomment-225816392,1,['variab'],['variable-fix-']
Modifiability,"[Broadcast data presentation.pdf](https://github.com/broadinstitute/gatk/files/369684/Broadcast.data.presentation.pdf). Here is a link to the presentation I gave on the broadcasting profiling data that I collected. The big takeaways are:; - Broadcasting is relatively time efficient and appears to scale linearly with filesize; - Reference broadcasting takes longer and is much more variable compared to the variants which get broadcast immediately before; - There doesn't seem to be any memory shared between executors for broadcast variables, making it very memory inefficient when there are many executors ; - The brodcast block size is best kept small, (in the order of 4-10MB) as it can explode the broadcast time to be very slow; - Having files in HDFS is significantly faster than using the GCS adapter for large file sizes",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-233396177:383,variab,variable,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-233396177,3,"['adapt', 'variab']","['adapter', 'variable', 'variables']"
Modifiability,"[Compiler daemon](https://docs.gradle.org/current/userguide/performance.html#compiler_daemon). The Gradle Java plugin allows you to run the compiler as a separate process by setting `options.fork = true`. This feature can lead to much less garbage collection and make Gradle’s infrastructure faster. This project has more than 1000 source files. We can consider enabling this feature. =====================; If there are any inappropriate modifications in this PR, please give me a reply and I will change them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7561:111,plugin,plugin,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561,1,['plugin'],['plugin']
Modifiability,"[bug report] GermlineCNVCaller (gatk 4.2.0.0) ""H5DreadVL_str: failed to read variable length strings 	at ncsa.hdf.hdf5lib.H5.H5DreadVL""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7202:77,variab,variable,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202,1,['variab'],['variable']
Modifiability,"] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] WorkflowManagerActor Starting workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] WorkflowManagerActor Successfully started WorkflowActor-c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-10-01 02:53:02,55] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-10-01 02:53:02,64] [info] MaterializeWorkflowDescriptorActor [c55a06f3]: Parsing workflow as WDL draft-2; [2019-10-01 02:53:03,80] [info] WorkflowManagerActor Workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to evaluate input 'padding' (reason 1 of 1): For input string: ""Int? (optional)""; Failed to evaluate input 'minimum_interval_median_percentile' (reaso",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:2709,config,configured,2709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configured']
Modifiability,] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3387,config,configuration,3387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:2279,variab,variable,2279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['variab'],['variable']
Modifiability,"_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3392,variab,variable,3392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['variab'],['variable']
Modifiability,"_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3669,Config,ConfigFactory,3669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"_MODE(_MM_FLUSH_ZERO_ON);. //Profiling: times for compute and transfer (either bytes copied or pointers copied); m_compute_time = 0;; diff --git i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc w/src/main/cpp/VectorLoglessPairH; index f45153e..70cf54f 100644; --- i/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; +++ w/src/main/cpp/VectorLoglessPairHMM/org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc; @@ -6,7 +6,7 @@. using namespace std;. -bool use_double = false;; +bool use_double = true;. //Should be called only once for the whole Java process - initializes field ids for the classes JNIReadDataHolderClass; //and JNIHaplotypeDataHolderClass; diff --git i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java w/src/main/java/org/broadinstitute/hellbender/utils; index 18370b0..74a1dad 100644; --- i/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java; +++ w/src/main/java/org/broadinstitute/hellbender/utils/pairhmm/VectorLoglessPairHMM.java; @@ -1,6 +1,7 @@; package org.broadinstitute.hellbender.utils.pairhmm;. -import org.apache.log4j.Logger;; +import org.apache.logging.log4j.LogManager;; +import org.apache.logging.log4j.Logger;; import org.broadinstitute.hellbender.exceptions.UserException;; import org.broadinstitute.hellbender.utils.genotyper.ReadLikelihoods;; import org.broadinstitute.hellbender.utils.genotyper.LikelihoodMatrix;; @@ -20,7 +21,7 @@ import java.util.Map;; */; public final class VectorLoglessPairHMM extends LoglessPairHMM {. - final static Logger logger = Logger.getLogger(VectorLoglessPairHMM.class);; + private static final Logger logger = LogManager.getLogger(VectorLoglessPairHMM.class);; final static Boolean runningOnMac = System.getProperty(""os.name"", ""unknown"").toLowerCase().startsWith(""mac"");; long threadLocalSetupTimeDiff = 0;; long pairHMMSetupTime = 0;; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083:2741,extend,extends,2741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1572#issuecomment-195496083,1,['extend'],['extends']
Modifiability,"_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3999,Config,ConfigFactory,3999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/baseline.cc:1:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/cpp/LoadTimeInitializer.cc:1:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/utils.h:4:; In file included from /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/common_data_structure.h:4:; /Users/louisb/Workspace/gatk/src/VectorLoglessPairHMM/headers/headers.h:16:10: fatal error: 'omp.h' file not found; #include <omp.h>; ^; 1 error generated. :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp'.; > Multiple build operations failed.; C++ compiler failed while compiling avx_function_instantiations.cc.; C++ compiler failed while compiling utils.cc.; C++ compiler failed while compiling org_broadinstitute_hellbender_utils_pairhmm_VectorLoglessPairHMM.cc.; C++ compiler failed while compiling baseline.cc.; C++ compiler failed while compiling LoadTimeInitializer.cc.; See the complete log at: file:///Users/louisb/Workspace/gatk/build/tmp/compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp/output.txt; ```. I'm using clang 6.0. Would you expect it to build on that?. ```; Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 6.0 (clang-600.0.57) (based on LLVM 3.5svn); Target: x86_64-apple-darwin13.4.0; Thread model: posix; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068:2599,Config,Configured,2599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504#issuecomment-185809068,1,['Config'],['Configured']
Modifiability,"_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.50: 0%| | 2/5000 [00:03<2:40:21, 1.93s/it]; 14:14:16.609 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3698,variab,variables,3698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['variab'],['variables']
Modifiability,"_expectation_mode=hybrid --num_samples_copy_ratio_approx=200 --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC cu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:5829,variab,variable,5829,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['variab'],['variable']
Modifiability,_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5420,Config,ConfigFactory,5420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5486,Config,ConfigFactory,5486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.82",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4383,Config,ConfigFactory,4383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4491,Config,ConfigFactory,4491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4015,Config,ConfigFactory,4015,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"`AFCalculator` has a couple of methods that don't belong: `reduceScope()`, which is called _before_ the AF calculation to reduce its computational burden, and `subsetAlleles()`, which is called _after_ the calculation to eliminate alleles that don't exist in called genotypes (in conformance with the dubious VCF spec). These methods are chronologically distinct from the rest of `AFCalculator` and do not appear to use any private variables. Thus they could easily be turned into static methods and removed from `AFCalculator`. Furthermore, `reduceScope()` and `subsetAlleles()` each have two implementations, in `DiploidExactAFCalculator` and `GeneralPloidyExactAFCalculator`. Since these two cases are complementary, they could easily be merged in a single method with an `if (ploidy == 2) . . .`. Finally, the general ploidy code is more complicated than it needs to be and needs editing. Beyond general housekeeping, the main motivation here is to untangle the AF/qual code as much as possible _without_ changing behavior before introducing the new model into the mix (issue #1697).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1891:432,variab,variables,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1891,1,['variab'],['variables']
Modifiability,`DataSourceUtils` contains string constants for fields in the config file (e.g. `CONFIG_FILE_FIELD_NAME_NAME`). These should be rolled into an enum together. This will facilitate file validation by enabling them to be iterated over automatically using the enum's built in methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5465:62,config,config,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5465,1,['config'],['config']
Modifiability,"`FilterMutectCalls` has always and continues to filter multiallelics by default, except in mitochondria mode. You may adjust this with the `max-alt-allele-count` argument. `-max-alternate-alleles` is a `HaplotypeCaller` argument that appeared in `Mutect2` due to excess inheritance in the class hierarchy. I don't believe it ever had any effect in `Mutect2`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6603#issuecomment-629897864:270,inherit,inheritance,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6603#issuecomment-629897864,1,['inherit'],['inheritance']
Modifiability,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5963:63,config,configuration,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963,2,['config'],['configuration']
Modifiability,"`HaplotypeCaller` and `Mutect2` share an argument `min-base-quality-score` that restricts low-quality bases from assembly. This argument gets passed to `ReadThreadingAssembler`, but take a look at the following code in that class:. ```java; private static final byte MIN_BASE_QUALITY_TO_USE_IN_ASSEMBLY = DEFAULT_MIN_BASE_QUALITY_TO_USE;; protected byte minBaseQualityToUseInAssembly = DEFAULT_MIN_BASE_QUALITY_TO_USE;; ```; The latter variable is set from the command line argument, but only the `static` constant is ever used, in particular in line 447:. ```java; final ReadThreadingGraph rtgraph = new ReadThreadingGraph(kmerSize, debugGraphTransformations, MIN_BASE_QUALITY_TO_USE_IN_ASSEMBLY, numPruningSamples);; ```. The fix is extremely simple: just replace `MIN_BASE_QUALITY_TO_USE_IN_ASSEMBLY` with `minBaseQualityToUseInAssembly`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4126:436,variab,variable,436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4126,1,['variab'],['variable']
Modifiability,"`Tribble` codecs can only read data from `Locatable` data sources (those with contig + start + end position). Recently we have found a need for reading in files that do not have locatable data (e.g. tabular data that has a `Gene Name` and a set of attributes, but no start/stop location). Tribble should be updated to have a baseline `Interface` that is generic (and not necessarily `Locatable`). Our current interface / infrastructure can inherit from that for data sources that are `Locatable`. Then a new `Codec` can be created for data sources that are not Locatable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3760:440,inherit,inherit,440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3760,1,['inherit'],['inherit']
Modifiability,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:225,config,config,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183,1,['config'],['config']
Modifiability,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:77,config,configuration,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,3,"['config', 'plugin']","['configuration', 'plugins']"
Modifiability,```; > ls -1 /tmp | grep config$; ...; tmp_read_resource_9070459585787683374.config; tmp_read_resource_9128464625731709220.config; tmp_read_resource_9145440961585524679.config; tmp_read_resource_9164235676580024644.config; tmp_read_resource_959850395283914212.config; tmp_read_resource_979816073947827397.config; tmp_read_resource_983369287551636047.config; tmp_read_resource_993654349410744404.config; ...; ```. Anybody else seeing these files being created but not deleted. I'm running thousands of samples via HaplotypeCaller/GenotypeGVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6771:25,config,config,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6771,9,['config'],['config']
Modifiability,"```; ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; ```. shows up at the top of every run, we should fix this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/216:34,config,configuration,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/216,2,['config'],['configuration']
Modifiability,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2707:2952,refactor,refactor,2952,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707,1,['refactor'],['refactor']
Modifiability,"aJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1272,config,configuration,1272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,"able.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:341); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:261); ... 18 more; ```. java version:; ```; java -version; openjdk version ""1.8.0_222""; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1~deb9u1-b10); OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode); ```; I added the cadd folder into data source folder like the structure mentioned in document:; ```; cadd; |- hg19; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; |; |- hg38; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; ```; The config file (cadd.config); ```; name = CADD; version = v1.4; src_file = InDels_inclAnno.tsv; origin_location =; preprocessing_script = UNKNOWN. Whether this data source is for the b37 reference.; Required and defaults to false.; isB37DataSource = false. Supported types:; simpleXSV -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript IDlocatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome locationgencode -- Custom datasource class for GENCODEcosmic -- Custom datasource class for COSMIC vcf -- Custom datasource class for Variant Call Format (VCF) files; type = locatableXSV; Required field for GENCODE files.Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path =. Required field for GENCODE files.; NCBI build version (either hg19 or hg38):; ncbi_build_version =. Required field for simpleXSV files.; Valid values:; GENE_NAME; TRANSCRIPT_ID; xsv_key = GENE_NAME. Required field for simpleXSV files.; The 0-based index of the column c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:4551,config,config,4551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"ace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5911,Config,ConfigFactory,5911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,ache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3405,config,config,3405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,added log4j2.xml to configure logging and remove warning at runtime,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/295:20,config,configure,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/295,1,['config'],['configure']
Modifiability,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7184:283,refactor,refactored,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184,1,['refactor'],['refactored']
Modifiability,"adds ""extends GATKBaseTest"" to all SV unit tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3789:6,extend,extends,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3789,1,['extend'],['extends']
Modifiability,"age](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1405,variab,variables,1405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,ah_var_store - ngs_cohort_extract - add backticks to table name variable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7050:64,variab,variable,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7050,1,['variab'],['variable']
Modifiability,"al String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; >; > binWidth would be a more readable variable name. There's nothing wrong; > with the command line argument and the variable being identical.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646097>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > This tool extends GATKTool, which means that it inherits an; > IntervalArgumentCollection that already includes a padding argument. A; > new one is not needed. BTW @samuelklee <https://github.com/samuelklee>; > does this come up elsewhere in the CNV code? It could be a holdover from; > the days of porting ReCapSeg when I feel we used to write more; > CommandLinePrograms.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646119>:; >; > > + createBins();; > + }; > +; > + /**; > + * Generates binning coverage in the intervals given by the user.; > + * The width of bins, the intervals and the output file's path are given by the user.; > + */; > + public void createBin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:2816,extend,extends,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,2,"['extend', 'inherit']","['extends', 'inherits']"
Modifiability,al.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.Default,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3936,config,configureHierarchy,3936,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configureHierarchy']
Modifiability,allerSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:01:43.730 INFO HaplotypeCallerSpark - Deflater: IntelDeflater; 19:01:43.730 INFO HaplotypeCallerSpark - Inflater: IntelInflater; 19:01:43.730 INFO HaplotypeCallerSpark - GCS max retries/reopens: 20; 19:01:43.730 INFO HaplotypeCallerSpark - Requester pays: disabled; 19:01:43.730 WARN HaplotypeCallerSpark - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:4062,config,configuration,4062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['config'],['configuration']
Modifiability,allowing variant walkers to configure their caching behavior,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:28,config,configure,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,1,['config'],['configure']
Modifiability,"am/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipeline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5391,Config,ConfigFactory,5391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"am/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6030,Config,ConfigFactory,6030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,anding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:16628,Extend,Extended,16628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,angling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10278,Extend,Extended,10278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,another executor: ; `Reading broadcast variable 3 took 34917 ms`; `Reading broadcast variable 4 took 121334 ms`. and another one:; `Reading broadcast variable 3 took 35354 ms`; `Reading broadcast variable 4 took 76535 ms`. Changing `spark.broadcast.blockSize` to 400M may slow it down:; `Reading broadcast variable 3 took 16235 ms`; `Reading broadcast variable 4 took 140846 ms`. And some with 10MB. ```; Reading broadcast variable 3 took 29083 ms; Reading broadcast variable 4 took 104675 ms; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208658975:39,variab,variable,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675#issuecomment-208658975,8,['variab'],['variable']
Modifiability,appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3425,Plugin,PluginRegistry,3425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginRegistry']
Modifiability,apted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:14055,adapt,adapted,14055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"ark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:3591,config,configuration,3591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"ark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --num-executors 20 --executor-cores 6 --executor-memory 6g /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 23:10:10.737 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 23:10:10.965 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 23:10:12.679 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.680 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:10:12.680 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:10:12.680 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 23:10:12.681 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 23:10:12.681 INFO CountReadsSpark - Start Date/Time: February 5, 2019 11:10:10 PM EST; 23:10:12.681 INFO CountReadsSpark - -------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:3169,variab,variables,3169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3464,Plugin,PluginRegistry,3464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginRegistry']
Modifiability,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857:931,Refactor,Refactoring,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857,1,['Refactor'],['Refactoring']
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5680,Config,ConfigFactory,5680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6319,Config,ConfigFactory,6319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5497,Config,ConfigFactory,5497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"ation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:7293,variab,variable,7293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['variab'],['variable']
Modifiability,"aults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3406,Config,ConfigFactory,3406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5324,Config,ConfigFactory,5324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5963,Config,ConfigFactory,5963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"aults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3379,Config,ConfigFactory,3379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"aults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGerm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4846,Config,ConfigFactory,4846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,ava:648); 	at org.gradle.api.internal.file.copy.DefaultCopySpec$DefaultCopySpecResolver.walk(DefaultCopySpec.java:650); 	at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:458); 	at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:38); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$Static,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:4713,plugin,plugins,4713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"b.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30,55] [info] Unspecified type (Unspecified version) workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 submitted; [2020-07-14 05:09:30,66] [info] SingleWorkflowRunnerActor: Workflow submitted 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,68] [info] WorkflowManagerActor Starting workflow 968be82c-eef3-4bdb-a1ab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:2371,config,configured,2371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configured']
Modifiability,b3e464b642?src=pr&el=desc) will **increase** coverage by `0.036%`.; > The diff coverage is `84.211%`. ```diff; @@ Coverage Diff @@; ## master #4960 +/- ##; ==============================================; + Coverage 80.784% 80.82% +0.036% ; - Complexity 17957 17967 +10 ; ==============================================; Files 1095 1095 ; Lines 64587 64600 +13 ; Branches 10392 10394 +2 ; ==============================================; + Hits 52176 52210 +34 ; + Misses 8388 8372 -16 ; + Partials 4023 4018 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4960?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `86.364% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `77.64% <100%> (+1.242%)` | `45 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `90.556% <81.25%> (+4.927%)` | `53 <7> (+6)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.491% <0%> (+0.546%)` | `170% <0%> (+2%)` | :arrow_up: |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242:1267,config,config,1267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"back to @tomwhite - some small refactor+code duplication, build and testing suggestions. I ran it and it works. Let's put it in soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219536914:31,refactor,refactor,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219536914,1,['refactor'],['refactor']
Modifiability,"bble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-6-amd64 amd64; 21:02:08.890 INFO PrintReadsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-1~bpo8+1-b11; 21:02:08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:3833,variab,variables,3833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"bi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4639,Config,ConfigFactory,4639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:2576,refactor,refactoring,2576,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['refactor'],['refactoring']
Modifiability,broadinstitute) (9aa31e4) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/72684d0fae3326398c80e2f47d78eeff1fcc14fe?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (72684d0) will **decrease** coverage by `0.001%`.; > The diff coverage is `100.000%`. ```diff; @@ Coverage Diff @@; ## master #7851 +/- ##; ===============================================; - Coverage 86.948% 86.947% -0.001% ; Complexity 36927 36927 ; ===============================================; Files 2219 2219 ; Lines 173673 173674 +1 ; Branches 18755 18755 ; ===============================================; - Hits 151006 151005 -1 ; + Misses 16055 16054 -1 ; - Partials 6612 6615 +3 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/7851?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <100.000%> (+0.035%)` | :arrow_up: |; | [.../hellbender/utils/python/PythonUnitTestRunner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9weXRob24vUHl0aG9uVW5pdFRlc3RSdW5uZXIuamF2YQ==) | `75.410% <0.000%> (-3.279%)` | :arrow_down: |; | [...itute/hellbender/tools/LocalAssemblerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7851#issuecomment-1126424538:1373,Adapt,AdaptiveChainPruner,1373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7851#issuecomment-1126424538,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553:3481,variab,variable,3481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553,1,['variab'],['variable']
Modifiability,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2634:324,config,configuration,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634,1,['config'],['configuration']
Modifiability,buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4120,config,configure,4120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configure']
Modifiability,builds are now failing due to the changes in the artifical reads generator...I'll make them more flexible so that I can still get more random reads without breaking the other tests....,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6280#issuecomment-559244663:97,flexible,flexible,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6280#issuecomment-559244663,1,['flexible'],['flexible']
Modifiability,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595:180,refactor,refactoring,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595,1,['refactor'],['refactoring']
Modifiability,"c 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11083,refactor,refactored,11083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,1,['refactor'],['refactored']
Modifiability,c=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `86.42%`. ```diff; @@ Coverage Diff @@; ## master #5462 +/- ##; ===============================================; + Coverage 87.075% 87.087% +0.012% ; + Complexity 31334 31225 -109 ; ===============================================; Files 1921 1915 -6 ; Lines 144602 144079 -523 ; Branches 15951 15891 -60 ; ===============================================; - Hits 125912 125474 -438 ; + Misses 12896 12834 -62 ; + Partials 5794 5771 -23; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5462?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/walkers/haplotypecaller/graphs/PathUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvUGF0aFVuaXRUZXN0LmphdmE=) | `93.258% <ø> (-0.22%)` | `7 <0> (ø)` | |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `95.349% <100%> (ø)` | `16 <0> (ø)` | :arrow_down: |; | [...ller/readthreading/ReadThreadingGraphUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9yZWFkdGhyZWFkaW5nL1JlYWRUaHJlYWRpbmdHcmFwaFVuaXRUZXN0LmphdmE=) | `95.238% <100%> (+0.018%)` | `55 <0> (ø)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/ChainPrunerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQ2hhaW5QcnVuZXJVbml0VGVzdC5qYXZh) | `99.194% <100%> (-0.006%)` | `40 <0> (ø)` | |; | [...der/t,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5462#issuecomment-450062027:1281,Adapt,AdaptiveChainPruner,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462#issuecomment-450062027,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2739:3419,plugin,plugin,3419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739,4,"['extend', 'plugin']","['extends', 'plugin']"
Modifiability,"c_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implemen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5200,Config,ConfigFactory,5200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,ce for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:3142,Adapt,AdaptedCallable,3142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['Adapt'],['AdaptedCallable']
Modifiability,ch 52/65; 19:36:40.808 INFO  GenomicsDBImport - Done importing batch 53/65; 20:18:42.274 INFO  GenomicsDBImport - Done importing batch 54/65; 21:01:51.304 INFO  GenomicsDBImport - Done importing batch 55/65; 21:36:00.458 INFO  GenomicsDBImport - Done importing batch 56/65; 22:08:38.587 INFO  GenomicsDBImport - Done importing batch 57/65; 22:40:44.082 INFO  GenomicsDBImport - Done importing batch 58/65; 23:14:11.202 INFO  GenomicsDBImport - Done importing batch 59/65; 23:48:23.805 INFO  GenomicsDBImport - Done importing batch 60/65; 00:20:35.869 INFO  GenomicsDBImport - Done importing batch 61/65; 00:51:47.408 INFO  GenomicsDBImport - Done importing batch 62/65; 01:25:23.587 INFO  GenomicsDBImport - Done importing batch 63/65; 01:59:03.103 INFO  GenomicsDBImport - Done importing batch 64/65; Using GATK jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) defined in environment variable GATK_LOCAL_JAR; Running:;     java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx150g -Xms16g -jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) GenomicsDBImport --sample-name-map sample_map.chr3 --genomicsdb-workspace-path genomicsDB.rb.chr3 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --L chr3 --batch-size 50 --bypass-feature-reader --reader-threads 5 --merge-input-intervals --overwrite-existing-genomicsdb-workspace --consolidate; [farrell@scc-hadoop genomicsdb]$ ls genomicsDB.rb.chr3; __tiledb_workspace.tdb  chr3$1$198295559  vcfheader.vcf  vidmap.json. ```; It never indicates that it imported batch 65/65. No error and the  callset.json is missing which we found in chr4 to chr22. ;   ; ls genomicsDB.rb.chr4. __tiledb_workspace.tdb  callset.json  chr4$1$1902145,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:3638,variab,variable,3638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['variab'],['variable']
Modifiability,"ck broadcast_0 stored as values in memory (estimated size 285.6 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.131.101.159:34044 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.131.101.159:34044 (size: 2.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 1 from broadcast at ReadsSparkSink.java:195; 17/10/11 14:19:18 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir; 17/10/11 14:19:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1; 17/10/11 14:19:18 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 17/10/11 14:19:18 INFO spark.SparkContext: Starting job: saveAsNewAPIHadoopFile at ReadsSparkSink.java:203; 17/10/11 14:19:18 INFO input.FileInputFormat: Total input paths to process : 1; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Registering RDD 5 (mapToPair at SparkUtils.java:157); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Got job 0 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) with 1 output partitions; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Parents of fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:10468,Config,Configuration,10468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['Config'],['Configuration']
Modifiability,clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:1361,variab,variables,1361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,closing - refactoring to one WDL,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7112#issuecomment-786760463:10,refactor,refactoring,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112#issuecomment-786760463,1,['refactor'],['refactoring']
Modifiability,code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2468,config,configuration,2468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"commend for all experiments going forward. One concern I have is that our cluster memory is already far larger than; the input size. In fact here each machine can fit the whole input; comfortably in RAM. This is not going to be the case for the full input.; Since it would take too long to iterate using the full input, it seems wise; instead to reduce both the input size and to keep a close eye on the amount; of memory we're using to make sure we're not going down a path that would; not be able to cope with the full input. On Wed, Nov 18, 2015 at 11:08 AM, droazen notifications@github.com wrote:. > I did some additional runs on the Broad cluster on a 14 GB bam, twice as; > large as the bam used in the plot above. This was with 60 cores, 4 cores; > per executor, and 16 GB of memory per executor. Results:; > ; > Broadcast (3 runs): 10m52.020s, 11m46.975s, 10m17.274s; > Sharded (3 runs): 19m33.310s, 13m36.466s, (died from out-of-memory error); > ; > Not sure what's going on here, but something about this configuration is; > favorable to Broadcast and unfavorable to Sharded. We should try to; > understand what and why.; > ; > Below are the commands I used to run each implementation on dataflow01,; > for reference:; > ; > spark-submit \; > --master yarn-client \; > --driver-memory 8G \; > --num-executors 16 \; > --executor-cores 4 \; > --executor-memory 16G \; > --conf spark.driver.maxResultSize=0 \; > --conf spark.driver.userClassPathFirst=true \; > --conf spark.executor.userClassPathFirst=true \; > --conf spark.io.compression.codec=lzf \; > --conf spark.yarn.executor.memoryOverhead=600 \; > $JAR BaseRecalibratorSpark \; > --input hdfs:///user/droazen/bqsr/CEUTrio.HiSeq.WGS.b37.NA12878.1m-130m.bam \; > --output bqsr_out_${1}.bam \; > -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; > --knownSites hdfs:///user/droazen/bqsr/dbsnp_138.b37.1m-130m.vcf \; > --joinStrategy BROADCAST \; > --apiKey $API_KEY \; > --sparkMaster yarn-client; > ; > spark-submit \; > --master yarn-c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734:2104,config,configuration,2104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/995#issuecomment-157838734,1,['config'],['configuration']
Modifiability,"compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar PrintReadsSpark -I /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam -O /home/centos/storage/output.bam --sparkMaster spark://192.168.1.110:7077; 05:27:50.924 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 05:27:51.034 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 3, 2017 5:27:51 AM UTC] PrintReadsSpark --output /home/centos/storage/output.bam --input /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam --sparkMaster spark://192.168.1.110:7077 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 3, 2017 5:27:51 AM UTC] Executing as centos@master.novalocal on Lin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:1429,variab,variables,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"config file is good for convenience, but I find that they cause lots of confusion and errors because people forget their existence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100312587:0,config,config,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508#issuecomment-100312587,1,['config'],['config']
Modifiability,coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:11354,Extend,Extended,11354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,"cotationFactory.createDefaultFuncotationsOnVariant(GencodeFuncotationFactory.java:499); 22 Jun 2023 14:54:27,163 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:217); 22 Jun 2023 14:54:27,164 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182); 22 Jun 2023 14:54:27,166 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:152); 22 Jun 2023 14:54:27,167 DEBUG: 		at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); 22 Jun 2023 14:54:27,168 DEBUG: 		at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179); 22 Jun 2023 14:54:27,170 DEBUG: 		at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625); 22 Jun 2023 14:54:27,171 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); 22 Jun 2023 14:54:27,172 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499); 22 Jun 2023 14:54:27,174 DEBUG: 		at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921); 22 Jun 2023 14:54:27,175 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 22 Jun 2023 14:54:27,177 DEBUG: 		at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682); 22 Jun 2023 14:54:27,178 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:162); 22 Jun 2023 14:54:27,180 DEBUG: 		at com.github.discvrseq.walkers.ExtendedFuncotator.enqueueAndHandleVariant(ExtendedFuncotator.java:209); 22 Jun 2023 14:54:27,181 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:878); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226:3423,Extend,ExtendedFuncotator,3423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226,2,['Extend'],['ExtendedFuncotator']
Modifiability,create gradle plugin that automatically configures sparkJar correctly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1453:14,plugin,plugin,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453,2,"['config', 'plugin']","['configures', 'plugin']"
Modifiability,create variable for java xmx size in picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8395:7,variab,variable,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8395,1,['variab'],['variable']
Modifiability,created variable for user to specify java heap size in picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8406:8,variab,variable,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8406,1,['variab'],['variable']
Modifiability,"cription; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:404); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5855:1152,variab,variable,1152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855,1,['variab'],['variable']
Modifiability,"ctory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run.; CollectIlluminaLaneMetrics (Picard) Collects Illumina lane metrics for the given BaseCalling analysis directory.; ExtractIlluminaBarcodes (Picard) Tool determines the barcode for each read in an Illumina lane.; IlluminaBasecallsToFastq (Picard) Generate FASTQ file(s) from Illumina basecall read data. ...; ```. With this change it instead prints the gatk launcher help, which is not the intended result. ; ```; Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after --; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the; java JVM at runtime.; Java options MUST be passed inside a single string with space-separated values.; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030:1429,Config,Configuration,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030,2,"['Config', 'config']","['Configuration', 'config-file']"
Modifiability,"ctory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4288,Config,ConfigFactory,4288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"ctory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3812,Config,ConfigFactory,3812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"ctory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5356,Config,ConfigFactory,5356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ctory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4253,Config,ConfigFactory,4253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"ctory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_convert",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5547,Config,ConfigFactory,5547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"cuting as myname@ln14 on Linux 3.10.0-514.16.1.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b15; Version: 4.alpha.2-1125-g27b5190-SNAPSHOT; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4504,variab,variable,4504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,cy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL0FzeW5jaHJvbm91c1N0cmVhbVdyaXRlci5qYXZh) | `81.633% <0.000%> (-2.041%)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `96.396% <0.000%> (-1.305%)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `73.750% <0.000%> (-1.250%)` | :arrow_down: |; | [...ools/walkers/annotator/VariantAnnotatorEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9WYXJpYW50QW5ub3RhdG9yRW5naW5lLmphdmE=) | `86.260% <0.000%> (-0.999%)` | :arrow_down: |; | [...org/broadinstitute/hellbender/utils/MathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:4557,config,config,4557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"d I think things look good from this perspective, at least. This tieout uses a subset of the Pf7 samples containing 300 cohort and 1683 case samples (which were indeed treated as a cohort-case cluster in the original Pf7 CNV genotyping analysis). ~4k genomic bins are covered. We compare this branch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1085,variab,variables,1085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['variab'],['variables']
Modifiability,"d be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all of your ""minimal"" changes for the port, including the real, new tests. This should compile, and tests should",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1888,plugin,plugins,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,1,['plugin'],['plugins']
Modifiability,"d evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvalua",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2273,config,configuration,2273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,d on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using these work arounds (change the max number of haplotypes or kmersize) but those are not good solutions in general as he would pay a CPU and sensitivity penalty in other places. . ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265551340). I can see how the current best-path selection algorithm may fail to produce a good coverage of events across the active region depending on the weights on edges .... for some configurations the algorithm may dedicate too much time in exploring alternatives in one section of the graph because these are nearly equaly likely disregarding other possibilities other section just because they can only result in a relatively larger drop in the likelihood of the path. . I quick but elegant solution would be to simulate passes across the graph... first iterations would produce a quickly growing set of haplotypes but eventually repeated sampling would not produce new haplotypes. if after 100 subsequent simulations there is no new discovery or we have reached a limit (128?) we would stop there. This simulation approach could be implemented only in situations the graphs are too complex for an analytical solution. We can determine the maximum number of paths in a graph with a quick deep first traversal to decide whether to use the analytical-exact or the simulation-proximate approach. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-287813366). To be done in GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2954:4673,config,configurations,4673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954,1,['config'],['configurations']
Modifiability,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1644,flexible,flexible,1644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,1,['flexible'],['flexible']
Modifiability,"da_read_pipeline is the right spot. If it's OK, I suggest holding off for a little bit. That branch is next in queue to get merged. I plan to have it merged by next Friday. Until then, I'll be doing (hopefully small) cleanup. I don't expect any serious refactors, but it hasn't gone for review, so I can't promise anything. (Read: it probably won't change much, but I can't guarantee it, so using it may waste some of your time). Right after that goes in, I want to work with you Tom on a refactor that'll work well for everyone. There many also be other places where we're overly tied to Cloud Dataflow right now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120037353:253,refactor,refactors,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120037353,2,['refactor'],"['refactor', 'refactors']"
Modifiability,default configuration rather than the path. Fixes #1324.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1433:8,config,configuration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1433,1,['config'],['configuration']
Modifiability,delete PluginManager,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/158:7,Plugin,PluginManager,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/158,1,['Plugin'],['PluginManager']
Modifiability,der.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3530,plugin,plugins,3530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['plugin'],['plugins']
Modifiability,"e --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 17:31:06.046 INFO cohort_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 17:31:16.537 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 17:31:23.180 INFO cohort_denoising_calling - Loading 44 read counts file(s)...; 17:34:27.362 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 17:40:48.713 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)... Stderr:; at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:340); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:13931,variab,variable,13931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['variab'],['variable']
Modifiability,"e GATK/Picard (`IndexFeatureFile `), but they would print inconsistent logs with the rest of my toolkits and they aren't overridable because the classes are final; thus, I would use a decorator over this tools to print the proper startup messages. After a while, I might implement a `VariantWalker`, which will require that I implement another layer (`MyVariantWalker`). Thus, I end up with a lot of naive classes implemented on top of the base walkers and wrappers around bundled GATK/Picard tools. This is very difficult to maintain, because if a change is done at the `CommandLineProgram` abstract class for the logging output (a new method, for example), I will need to update every naive class and wrapper if I bump the GATK version. In addition, extensions of my own toolkit (if any) would need to do the same, making the class-dependency tree so deep that it is difficult to follow (with GATK3, this problem was really driving me crazy when I tried to implement custom tools). On the other hand, there is another use case for the GATK itself: once barclay has a common class for CLP, GATK would be able to run directly Picard tools without the decorator; nevertheless, they will still need it for the log output. This also gives me the impression that the configuration for the CLP output should be at the barclay level, to be shared between Picard/GATK/downstream toolkits to be able to combine them. I think that a way of managing that woul be a new field in the CLP consisting on an interface/abstract class, `CommandLineStartupFormatter`, with the same CLP methods for this kind of operations, that will be passed to the CLP on construction (in `Main`) and defaults to whatever base class is chosen. This will allow custom toolkits to override in their `Main` the formatter and thus make consistent the output of every tool. Another option is to use directly something like the Spring framework, but I think that it is quite complicated for API users without knowledge of Spring (like me).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646:2071,config,configuration,2071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646,1,['config'],['configuration']
Modifiability,"e VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1066,Config,ConfigurationContainerInternal,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Config'],['ConfigurationContainerInternal']
Modifiability,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:2175,variab,variable,2175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['variab'],['variable']
Modifiability,"e interpreted and location inferred by studying local assembly contig alignment signatures, it is time to clean up the corresponding package in the pipeline and make the switch to the updated implementation, which now outputs not only insertion, deletion, small tandem duplication, and inversions, but also novel adjacencies (BND records whose meanings cannot be fully resolved solely from assembly alignment signatures) as well as complex variants that theoretically could be arbitrarily complex (`<CPX>`, as long as we have assembled across the full event). . ## Planed organization. the `discovery` package could be divided roughly now into. ### interface. `SvDiscoveryDataBundle`, `SvDiscoverFromLocalAssemblyContigAlignmentsSpark`, `SvType`, `AnnotatedVariantProducer`. ### alignment prep (sub package). `AlignmentInterval`, `AlignedContig` (refactor `AssemblyContigWithFineTunedAlignments` into `AlignedContig`), `AlignedContigGenerator`, `AlignedAssembly`, `ContigAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:917,refactor,refactor,917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,2,['refactor'],['refactor']
Modifiability,"e typically used to indicate adapter sequence. See reply to jhess in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/35120#Comment_35120>:. > That's correct, Q2 bases are considered to be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic workflows that call down to the lowest allele fraction variants. It seems this ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:1145,adapt,adapter,1145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,1,['adapt'],['adapter']
Modifiability,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:3065,config,configuration,3065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,1,['config'],['configuration']
Modifiability,"e; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5002,Config,ConfigFactory,5002,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,eCNVCaller - HTSJDK Defaults.CREATE_MD5 : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3197,Config,ConfigFactory,3197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"eCaller.java b/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; index cf34b1fb4e..2ee5752f04 100644; --- a/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; +++ b/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; @@ -497,10 +497,11 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; protected boolean mergeVariantsViaLD = false;; ; @Advanced; - @Argument(fullName=""tryPhysicalPhasing"", shortName=""tryPhysicalPhasing"", doc=""If specified, we will add physical (read-based) phasing information"", required = false); - protected boolean tryPhysicalPhasing = false;; + @Argument(fullName=""doNotRunPhysicalPhasing"", shortName=""doNotRunPhysicalPhasing"", doc=""If specified, we will not try to add physical (read-based) phasing information"", required = false); + protected boolean doNotRunPhysicalPhasing = false;; ; - public static final String HAPLOTYPE_CALLER_PHASING_KEY = ""HCP"";; + public static final String HAPLOTYPE_CALLER_PHASING_ID_KEY = ""PID"";; + public static final String HAPLOTYPE_CALLER_PHASING_GT_KEY = ""PGT"";; ; // -----------------------------------------------------------------------------------------------; // arguments for debugging / developing the haplotype caller; @@ -634,12 +635,11 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if ( emitReferenceConfidence() ) {; ; if (SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES); - throw new UserException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and Genotyping Giving Alleles at the same time"");; + throw new UserException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and GENOTYPE_GIVEN_ALLELES at the same time"");; ; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:1232,extend,extends,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,2,['extend'],['extends']
Modifiability,"eProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz; ```. #### Steps to reproduce; gatk --java-options -Xms100g VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/ch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:10517,polymorphi,polymorphic,10517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['polymorphi'],['polymorphic']
Modifiability,"e_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - Executing as root@gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m on Linux v4.9.0-8-amd64 amd64; 16:58:10.114 INFO PrintVariantsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-2~deb9u1-b13; 16:58:10.114 INFO PrintVariantsSpark - Start Date/Time: February 18, 2019 4:58:09 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1514,variab,variables,1514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"eano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:269); at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5142,config,configdefaults,5142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configdefaults']
Modifiability,ed[m[50D[1B[1m> :testOnPackagedReleaseJar > Executing test org...help.DocumentationGeneration[m[79D[1B[3A src/main/java/org/broadinstitute/hellbender/utils/variant/writers/SomaticGVCFWriter.java:4: error: package com.google.common.collect does not exist[0K; 2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; 2022-08-16T00:09:07.4436380Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4436641Z location: class CommandLineProgram; 2022-08-16T00:09:07.4436930Z src/main/java/org/broadinstitute/hellbender/engine/FeatureInput.java:120: error: cannot find symbol; 2022-08-16T00:09:07.4437094Z @VisibleForTesting; 2022-08-16T00:09:07.4437369Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4437519Z location: class FeatureInput<T>; 2022-08-16T00:09:07.4437725Z where T is a type-variable:; 2022-08-16T00:09:07.4437925Z T extends Feature declared in class FeatureInput; 2022-08-16T00:09:07.4438276Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:251: error: cannot find symbol; 2022-08-16T00:09:07.4438417Z @VisibleForTesting; 2022-08-16T00:09:07.4438677Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4438873Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4439223Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:271: error: cannot find symbol; 2022-08-16T00:09:07.4439362Z @VisibleForTesting; 2022-08-16T00:09:07.4439618Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4439806Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4465668Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/DefaultGATKVariantAnnotationArgumentCollection.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4466113Z [done in 2417 ms]; 2022-08-16T00:09:07.4466222Z ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:20885,extend,extends,20885,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:1384,config,config,1384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,12,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"ed for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1380,config,configuration,1380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['config'],['configuration']
Modifiability,"ed(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for pon; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 151, column 69:. gatk GenomicsDBImport --genomicsdb-workspace-path pon_db -R ~{ref_fasta} -V ~{sep=' -V ' input_vcfs} -L ~{intervals}; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. Am I using womtool wrong, or is there a bug with it, or is this an issue with wdls?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:4916,adapt,adapted,4916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['adapt'],['adapted']
Modifiability,"ed, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of PGEN files and then merges them by chromosome. ### Part 1: PGEN-JNI; The PGEN-JNI library was written by Chris Norman of the GATK Engine Team and lives [here](https://github.com/broadinstitute/pgen-jni). It is written primarily in C++ for performance purposes and also compatibility with the pgenlib library (part of the [plink repo](https://github.com/chrchang/plink-ng/tree/master)). It builds on top of pgenlib to provide a writer for creating PGEN files and writing to them from HTSJDK VariantContext objects. PGEN-JNI is compatible with Linux and macOS. A build of this library is currently hosted on the Broad's artifactory repo, and that is being used as a dependency for GATK. Ownership of the PGEN-JNI library will stay with the GATK Engine Team and we will provide support for it if y'all encounter any issues with it. ### Part 2: ExtractCohortToPgen; ExtractCohortToPgen is a GATK tool that inherits from ExtractCohort and is based very closely on ExtractCohortToVcf. It produces 3-4 files:. 1. A `.pgen` file, which contains a mapping of samples and sites to variants,; 2. A `.psam` file, which contains a list of sample names,; 3. A `.pvar.zst` file, which is a zstd compressed list of sites with alleles, similar to a sites-only VCF, and; 4. Optionally (if specified by setting `write-mode` to `WRITE_SEPARATE_INDEX`), a `.pgi` file, which contains an index for the `.pgen` file. It has a few arguments that are specific to it that warrant explanation. #### pgen-chromosome-code; Plink defines a set of [chromosome codes](https://www.cog-genomics.org/plink/2.0/data#irreg_output) that correspond to different sets of contig names of chromosomes. This tool supports two of those chromosome code options: `chrM` and `MT`, which correspond to hg38 and hg19 contig naming, respectively. This argument is required. #### write-mode; The PGEN writer defined in PGEN-JNI defines two different writ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:2902,inherit,inherits,2902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['inherit'],['inherits']
Modifiability,ed-websocket\9.0.35\tomcat-embed-websocket-9.0.35.jar;E:\repository\org\springframework\spring-web\5.2.6.RELEASE\spring-web-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-webmvc\5.2.6.RELEASE\spring-webmvc-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-aop\5.2.6.RELEASE\spring-aop-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-context\5.2.6.RELEASE\spring-context-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-expression\5.2.6.RELEASE\spring-expression-5.2.6.RELEASE.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-starter\2.1.2\mybatis-spring-boot-starter-2.1.2.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-autoconfigure\2.1.2\mybatis-spring-boot-autoconfigure-2.1.2.jar;E:\repository\org\mybatis\mybatis\3.5.4\mybatis-3.5.4.jar;E:\repository\org\mybatis\mybatis-spring\2.0.4\mybatis-spring-2.0.4.jar;E:\repository\mysql\mysql-connector-java\8.0.20\mysql-connector-java-8.0.20.jar;E:\repository\org\springframework\boot\spring-boot-configuration-processor\2.3.0.RELEASE\spring-boot-configuration-processor-2.3.0.RELEASE.jar;E:\repository\org\springframework\spring-core\5.2.6.RELEASE\spring-core-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-jcl\5.2.6.RELEASE\spring-jcl-5.2.6.RELEASE.jar;E:\repository\com\google\firebase\firebase-admin\6.8.1\firebase-admin-6.8.1.jar;E:\repository\com\google\api-client\google-api-client\1.25.0\google-api-client-1.25.0.jar;E:\repository\com\google\oauth-client\google-oauth-client\1.25.0\google-oauth-client-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-jackson2\1.25.0\google-http-client-jackson2-1.25.0.jar;E:\repository\com\google\api-client\google-api-client-gson\1.25.0\google-api-client-gson-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-gson\1.25.0\google-http-client-gson-1.25.0.jar;E:\repository\com\google\code\gson\gson\2.8.6\gson-2.8.6.jar;E:\repository\com\google\http-client\google-http-client\1.25.0\google-http-,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:5458,config,configuration-processor,5458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration-processor']
Modifiability,"egion, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(hcArgs.maxMnpDistance);. // same things as Mutect2 — we trim on the unmodified region; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(region, allVariationEvents, referenceContext);. // same as Mutect2; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. In addition to the proposed simple fix, this brings up a few code smells:. * One would expect assembly not to modify its input reads, but it does through the side effect of `finalizeRegion`.; * Assembly has both the permanent changes of finalize region and the temporary changes of read error correction.; * `AssemblyResultSet` stores the reads but so does `AssemblyRegion`. Without doing any serious refactoring, perhaps `finalizeRegion` could at least be split off from assembly so that the latter does not stealthily modify reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:2833,refactor,refactoring,2833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['refactor'],['refactoring']
Modifiability,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8158:3679,Config,Config,3679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158,7,"['Config', 'config']","['Config', 'configuration']"
Modifiability,"ellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:805); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:789). the deletion that is causing the error is 141 base pairs, and I noticed the length of the contig Funcotator is trying to retrieve (895) is equal to the UTR length + deletion length + 1, 753 + 141 + 1. When I looked at the source code around where the error occurs, I see where the length of the retrieved interval is defined (line 738): . > final SimpleInterval transcriptInterval = new SimpleInterval(; > transcriptMapIdAndMetadata.mapKey,; > transcriptMapIdAndMetadata.fivePrimeUtrStart,; > transcriptMapIdAndMetadata.fivePrimeUtrEnd + extraBases; > );. and the logic for how large that extraBases should be (line 1566):. >final int numExtraTrailingBases = variant.getReference().length() < defaultNumTrail ingBasesForUtrAnnotationSequenceConstruction ? defaultNumTrailingBasesForUtrAnnotationSequenceConst ruction : variant.getReference().length() + 1;. I believe line 1566 is the source of the problem; there is no check that UTR-end + deletion length extends past the end of the transcript. #### Steps to reproduce. download funcotator_dataSources.v1.6.20190124s from Broad FTP server. run funcotator using:. `Funcotator -R /tmp/GRCh38.fa -V broken.vcf -O broken.out.vcf --data-sources-path funcotator_dataSources.v1.6.20190124s/ --output-file-format VCF --ref-version hg38`. on a vcf with a single variant:. >chr17 7241460 . ACTGCAAAAGATACAAGATGCAAGAAAGTCACAGAGGTCAAAAATGCCCTCAAAAGAACAGCTGCTAGGTGGAGCCTCCTCCCGCAGAGACTGCACTCCCACCCACAGGAAGCAAGCCTGAGTCTTGGATCAGGTTCCCAC A . #### Expected behavior; Funcotator should not attempt to retrieve a sequence that extends past the end of a transcript. #### Actual behavior; Funcotator crashes because it attempts to retrieve sequence past the end of a transcript",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345:2961,extend,extends,2961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345,2,['extend'],['extends']
Modifiability,"ellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:341); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:261); ... 18 more; ```. java version:; ```; java -version; openjdk version ""1.8.0_222""; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1~deb9u1-b10); OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode); ```; I added the cadd folder into data source folder like the structure mentioned in document:; ```; cadd; |- hg19; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; |; |- hg38; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; ```; The config file (cadd.config); ```; name = CADD; version = v1.4; src_file = InDels_inclAnno.tsv; origin_location =; preprocessing_script = UNKNOWN. Whether this data source is for the b37 reference.; Required and defaults to false.; isB37DataSource = false. Supported types:; simpleXSV -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript IDlocatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome locationgencode -- Custom datasource class for GENCODEcosmic -- Custom datasource class for COSMIC vcf -- Custom datasource class for Variant Call Format (VCF) files; type = locatableXSV; Required field for GENCODE files.Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path =. Required field for GENCODE files.; NCBI build version (either hg19 or hg38):; ncbi_build_version =. Required field for simpleXSV files.; Valid values:; GENE_NAME; TRANSCRIPT_ID; xsv_key = GENE_NAME. Required field for simpleXSV files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:4533,config,config,4533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4089,Config,ConfigFactory,4089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"enameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] WorkflowManagerActor Starting workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] WorkflowManage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:1474,config,configured,1474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configured']
Modifiability,"encing data. Given that a major source of coverage variance in targetted sequencing stems from the variance in bait efficiencies, the most reasonable read-depth calculation scheme is to associate **inserts to baits** rather than **single reads to targets**. No more arbitrary interval padding (which was a hacky way to get away not thinking about inserts). There is a subtle problem, though: inserts often overlap with more than one bait. In such cases, we need to have a model for estimating the probability that the insert is captured by either of the overlapping baits. The modeling can be done in the following semi-empirical fashion (thanks @yfarjoun), which needs to be done only once for each capture technology (Agilent, ICE):; - We locate isolated baits (i.e. those that are separated from one another by a few standard deviations of the average insert size); - We take a number of BAMs and calculate the empirical distribution of inserts around the isolated baits; - We fit a simple parametric distribution to the obtained empirical distributions, parameterized by bait length and insert length; we probably don't need to go all-in here, though the reference context of the bait is also likely to be an important covariate. Once these distributions are known, we can easily calculate the membership share of each bait in ambiguous cases and give each bait the appropriate share. Bonus:; -------. The empirical distribution of inserts around baits also allows us to associate a more reasonable GC content to each bait. Since GC bias is a property of the fragments that are pulled by the baits, a reasonable measure of ""GC content"" of each bait has to be calculated from the expected value of the GC content of the fragments that the bait pulls (not the GC content of the baits or targets), and this can be easily calculated from the previously obtained empirical distributions. ---. @mbabadi commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2947:1447,parameteriz,parameterized,1447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947,1,['parameteriz'],['parameterized']
Modifiability,enhance ReadsPipelineSpark with metrics,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1237:0,enhance,enhance,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1237,1,['enhance'],['enhance']
Modifiability,"ent overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whether the in cases whether variants ar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1431,config,configuration,1431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['config'],['configuration']
Modifiability,"equester pays: disabled; 10:33:05.865 WARN SortSamSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: SortSamSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:33:05.865 INFO SortSamSpark - Initializing engine; 10:33:05.865 INFO SortSamSpark - Done initializing engine; 10:33:06.134 WARN Utils - Your hostname, gs2040t resolves to a loopback address: 127.0.1.1; using 172.20.19.130 instead (on interface bond0); 10:33:06.134 WARN Utils - Set SPARK_LOCAL_IP if you need to bind to another address; 10:33:06.242 INFO SparkContext - Running Spark version 3.3.0; 10:33:06.403 WARN SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).; 10:33:06.427 INFO ResourceUtils - ==============================================================; 10:33:06.427 INFO ResourceUtils - No custom resources configured for spark.driver.; 10:33:06.428 INFO ResourceUtils - ==============================================================; 10:33:06.428 INFO SparkContext - Submitted application: SortSamSpark; 10:33:06.446 INFO ResourceProfile - Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 600, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0); 10:33:06.454 INFO ResourceProfile - Limiting resource is cpu; 10:33:06.455 INFO ResourceProfileManager - Added ResourceProfile id: 0; 10:33:06.500 INFO SecurityManager - Changing view acls to: root; 10:33:06.501 INFO SecurityManager - Changing modify acls to: root; 10:33:06.501 INFO SecurityManager - Changing view acls groups to:; 10:33:06.502 INFO SecurityManager - Changing modify acls groups to:;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:41041,config,configured,41041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['config'],['configured']
Modifiability,"er and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job go",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1474,variab,variable,1474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,1,['variab'],['variable']
Modifiability,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1982,variab,variables,1982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,7,"['config', 'variab']","['configuration', 'variables']"
Modifiability,erArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWFkVGhyZWFkaW5nQXNzZW1ibGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `94.118% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `76.923% <ø> (-0.946%)` | `34 <0> (-1)` | |; | [...walkers/haplotypecaller/HaplotypeCallerEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmUuamF2YQ==) | `78.425% <100%> (ø)` | `76 <0> (ø)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `95.349% <100%> (+0.111%)` | `16 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/walkers/mutect/Mutect2Engine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRW5naW5lLmphdmE=) | `90.173% <100%> (ø)` | `65 <0> (ø)` | :arrow_down: |; | [...otypecaller/HaplotypeCallerArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <100%> (ø)` | `3 <1> (+1)` | :arrow_up: |; | [...r/tools/walkers/mutect/Mutect2Integrati,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544#issuecomment-449424951:2272,Adapt,AdaptiveChainPruner,2272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544#issuecomment-449424951,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,erators$ConcatenatedIterator.getTopMetaIterator(Iterators.java:1379); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$ConcatenatedIterator.hasNext(Iterators.java:1395); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.fillCache(PushToPullIterator.java:71); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.(PushToPullIterator.java:37); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiningIterator.(GVCFBlockCombiningIterator.java:14); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.lambda$writeVariantsSingle$516343c4$1(VariantsSparkSink.java:127); 	at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitions$1(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858); 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:56); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:56); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:56); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93); 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166); 	at org.apache.spark.scheduler.Task.run(Task.scala:141); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620); 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961:3230,adapt,adapted,3230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961,1,['adapt'],['adapted']
Modifiability,erce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40); 	at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:174); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar.copy(ShadowJar.java:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:46); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26); 	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:788); 	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:755); 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:6739,plugin,plugins,6739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,ering/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTXV0ZWN0MkZpbHRlcmluZ0VuZ2luZS5qYXZh) | `97.115% <100%> (+0.057%)` | `43 <0> (ø)` | :arrow_down: |; | [.../mutect/filtering/M2FiltersArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTTJGaWx0ZXJzQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `93.75% <100%> (+0.417%)` | `6 <0> (ø)` | :arrow_down: |; | [...kers/mutect/filtering/MinAlleleFractionFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTWluQWxsZWxlRnJhY3Rpb25GaWx0ZXIuamF2YQ==) | `100% <100%> (ø)` | `7 <7> (?)` | |; | [...e/hellbender/utils/variant/GATKVCFHeaderLines.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `94.886% <100%> (+0.029%)` | `11 <0> (ø)` | :arrow_down: |; | [...alkers/mutect/filtering/PolymorphicNuMTFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvUG9seW1vcnBoaWNOdU1URmlsdGVyLmphdmE=) | `88.235% <88.235%> (ø)` | `9 <9> (?)` | |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `87.309% <0%> (-0.306%)` | `244% <0%> (-2%)` | |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477635851:3450,Polymorphi,PolymorphicNuMTFilter,3450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477635851,1,['Polymorphi'],['PolymorphicNuMTFilter']
Modifiability,"erval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkernel.tasks.inference_task_base - (denoising) starting...: 0%| | 0/1000 [00:00<?, ?it/s]; 15:10:03.410 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -1038.498 +/- 431.707, SNR: 71.3, T: 1.98: 8%|8 | 83/1000 [00:00<00:01, 826.53it/s]; 15:10:03.522 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -821.262 +/- 327.042, SNR: 38.9, T: 1.97: 17%|#6 | 166/1000 [00:00<00:01, 776.56it/s]; 15:10:03.636 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -727.432 +/- 277.971, SNR: 29.4, T: 1.95: 24%|##4 | 244/1000 [00:00<00:01, 732.12it/s]; 15:10:03.754 INFO gcnvkernel.tasks.inference_task_base - (deno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:7364,variab,variables,7364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variables']
Modifiability,"es can be specified as:; ./. .. completely missing (""."" or ""./."", depending on ploidy); ./x .. partially missing (e.g., ""./0"" or "".|1"" but not ""./.""); . .. partially or completely missing; a .. all genotypes; b .. heterozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascript or python plugins to extend some of the functionality where developers may not reach immediately. Personally I use htsjdk extensively (and sometimes p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:1030,Plugin,Plugin,1030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,1,['Plugin'],['Plugin']
Modifiability,"es/theano/__init__.py"", line 124, in <module>; from theano.scan_module import (scan, map, reduce, foldl, foldr, clone,; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4147,config,config,4147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['config']
Modifiability,"es:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4359,Config,ConfigFactory,4359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,eshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-read-to-haplotype-gap-extend-penalty -5 --flow-assembly-collapse-hmer-size 0 --flow-assembly-collapse-partial-mode; false --flow-filter-alleles false --flow-filter-alleles-qual-threshold 30.0 --flow-filter-alleles-sor-threshold 3.0 --flow-filter-lone-alleles false --flow-filter-alleles-debug-graphs fal; se --min-assembly-region-size 50 --max-assembly-region-size 300 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --force-active false --assembly-region-padding 100 -; -padding-around-indels 75 --padding-around-snps 20 --padding-around-strs 75 --max-extension-into-assembly-region-pa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7749,extend,extend-penalty,7749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['extend'],['extend-penalty']
Modifiability,est/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:61901,variab,variable,61901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['variab'],['variable']
Modifiability,expose DEFAULT_FEATURE_CACHE_LOOKAHEAD as a configurable option,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3489:44,config,configurable,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3489,1,['config'],['configurable']
Modifiability,"faults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4795,Config,ConfigFactory,4795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"fdbd7fc000-7ffdbd7fe000 r-xp 00000000 00:00 0 [vdso]; ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall]. VM Arguments:; jvm_args: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx16g ; java_command: /usr/bin/gatk-package-4.0.0.0-local.jar HaplotypeCaller -I CMT_Project04172015_20150062801_S_2.bam -R /beegfs/work/zxmai83/Reference/genome/b37/human_g1k_v37.fasta -O CMT_Project04172015_20150062801_S_2_variants.vcf -ERC GVCF --create-output-variant-index --annotation MappingQualityRankSumTest --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation FisherStrand --annotation Coverage --dbsnp /beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf --verbosity INFO; java_class_path (initial): /usr/bin/gatk-package-4.0.0.0-local.jar; Launcher Type: SUN_STANDARD. Environment Variables:; PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; LD_LIBRARY_PATH=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/.singularity.d/libs. Signal Handlers:; SIGSEGV: [libjvm.so+0x632f3c], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGBUS: [libjvm.so+0x632f3c], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGFPE: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGPIPE: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGXFSZ: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGILL: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGUSR1: SIG_DFL, sa_mask[0]=00000000000000000000000000000000, sa_flags=none; SIGUSR2: [libjvm.so+0x562499], sa_mask[0]=000000000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:39585,Variab,Variables,39585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Variab'],['Variables']
Modifiability,feCycle.doStart(ContainerLifeCycle.java:105); at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61); at org.eclipse.jetty.server.Server.doStart(Server.java:394); at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68); at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1155); at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:181); at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:885); at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:707); at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953); at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:926); at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1692); at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1314); at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1083); at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:958); at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:890); at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518); at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477); at org.broadinstitute.hellbender.testutils.MiniClusterUtils.getMiniCluster(MiniClusterUtils.java:30); at org.broadinstitute.hellbender.testutils.MiniClusterUtils.getMiniCluster(MiniClusterUtils.java:38); at org.broadinstitute.hellbender.metrics.MetricsUtilsTest.setupMiniCluster(MetricsUtilsTest.java:24). Caused by:; java.lang.IllegalArgumentException: Invalid Java version 11.0.16.1; at org.eclipse.jetty.util.JavaVersion.parseJDK9(JavaVersion.java:71); at org.eclipse.jetty.util.JavaVersion.parse(JavaVersion.java:49); at org.eclipse.jetty.util.JavaVersion.<clinit>(JavaVersion.java:[43](https://github.com/broadinstitute/gatk/action,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8098#issuecomment-1320505279:2385,config,configureNameService,2385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8098#issuecomment-1320505279,1,['config'],['configureNameService']
Modifiability,"find the string 'entrainScore=0.7203;HW=4.306476E-6' in the vcf file, and Line 104 is part of the header (I think Line 104 refers to tribble source though). Gatks ValidateVariants does not have any issues with the vcf file, and looking visually with bcftools I cannot see an issue either. . Can anyone suggest further diagnosis steps? Should I take this to GATK issue tracker?. The VCF file is here: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf. Here is the gatk command line: ; ```; gatk --java-options ""-Xmx100g -Xms100g"" \; VariantRecalibrator \; -V /share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SitesOnlyGatherVcf/execution/NA12878.sites_only.vcf.gz \; -O NA12878.snps.recal \; --tranches-file NA12878.snps.tranches \; -trust-all-polymorphic \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ -an SOR -an DP \; -mode SNP \; -sample-every 10 \; --output-model NA12878.snps.model.report \; --max-gaussians 6 \; -resource hapmap,known=false,training=true,truth=true,prior=15:/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/hapmap_3.3.b37.vcf \; -resource omni,known=false,training=true,truth=true,prior=12:/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/1000G_omni2.5.b37.vcf \; -resource 1000G,known=false,training=true,truth=fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4761:1583,polymorphi,polymorphic,1583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761,1,['polymorphi'],['polymorphic']
Modifiability,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1450:468,config,configurations,468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450,1,['config'],['configurations']
Modifiability,fixes #1484 ; I also refactored the code duplication between ExcessHet and InbreedingCoef . @droazen please have a look,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1487:21,refactor,refactored,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1487,1,['refactor'],['refactored']
Modifiability,"fixes #1653. ok, results are in, with asyncIO GATK4 out of the box beats GATK3 4.53x on PrintReads and 1.48x on BaseRecalibrator!. PrintReads; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1.32x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.68x baseline ; syncTribble, syncSamtools, JDKDeflater: 2.32x baseline. For comparison, GATK3 out of the box is 4.53x baseline on PrintReads. BaseRecalibrator; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1x baseline; asyncTribble, asyncSamtools, IntelDeflater: 2.14x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.04x baseline; syncTribble, syncSamtools, JDKDeflater: 1.03x baseline; asyncTribble, asyncSamtools, JDKDeflater: 2.12x baseline. For comparison, GATK3 out of the box is 1.48x baseline on BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1695:232,config,config,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1695,2,['config'],['config']
Modifiability,fixes and refactoring methods in SparkUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4765:10,refactor,refactoring,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765,1,['refactor'],['refactoring']
Modifiability,g.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:1299,Config,ConfigCache,1299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,1,['Config'],['ConfigCache']
Modifiability,"g.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; ^C; ####################### Ctrl-C after 16 hours ##############; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6298,variab,variable,6298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,2,['variab'],['variable']
Modifiability,"gAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (for ins, del, small dup, inversion only) by delegating to `ChimericAlignment.parseOneContig` and `NovelAdjacencyReferenceLocations(ChimericAlignment chimericAlignment, byte[] contigSequence, SAMSequenceDictionary)`; this should be deprecated; * exact variant type inference (delegated to `SvTypeInference.inferFromNovelAdjacency()`) and annotation (delegated to `AnnotatedVariantProducer.produceAnnotatedVcFromInferredTypeAndRefLocations()`); this should be deprecated; * imprecise variants detection; this should be kept and factored out. --------; --------. ## Planed steps. 1. repackaging & refactoring (no logic change, see #3934 ); 2. bring in some valuable changes made in PR #3668; 3. **more test coverage** (ticket #3431); 4. switch; make `StructuralVariationDiscoveryPipelineSpark` call into `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` by default and optionally into `DiscoverVariantsFromContigAlignmentsSAMSpark`, i.e. opposite of what we currently do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:2665,refactor,refactoring,2665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,1,['refactor'],['refactoring']
Modifiability,gCNV in the CASE mode now fills in all hidden DenoisingModelConfig and CopyNumberCallingConfig arguments from the input model configuration. . This addresses issue #6994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7464:126,config,configuration,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7464,1,['config'],['configuration']
Modifiability,gCNV refactoring and code improvement,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2979:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979,1,['refactor'],['refactoring']
Modifiability,"gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1377,config,configparser,1377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4813,Config,ConfigFactory,4813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,ge Δ | Complexity Δ | |; |---|---|---|---|; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `80.743% <0%> (-4.581%)` | `89% <0%> (ø)` | |; | [...ute/hellbender/utils/test/FuncotatorTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Z1bmNvdGF0b3JUZXN0VXRpbHMuamF2YQ==) | `95.161% <0%> (-3.084%)` | `7% <0%> (+1%)` | |; | [...GATKPlugin/GATKReadFilterPluginDescriptorTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yVGVzdC5qYXZh) | `88.62% <0%> (-1.76%)` | `48% <0%> (+1%)` | |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `88.235% <0%> (-1.43%)` | `58% <0%> (+1%)` | |; | [.../tools/walkers/haplotypecaller/RefVsAnyResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZWc0FueVJlc3VsdC5qYXZh) | `100% <0%> (ø)` | `3% <0%> (+1%)` | :arrow_up: |; | [...ools/walkers/annotator/VariantAnnotatorEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9WYXJpYW50QW5ub3RhdG9yRW5naW5lLmphdmE=) | `91.304% <0%> (ø)` | `70% <0%> (ø)` | :arrow_down: |; | [...line/GATKPlugin/testpluggables/TestAnnotation.java](https:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5566#issuecomment-452843310:1862,Plugin,Plugin,1862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5566#issuecomment-452843310,1,['Plugin'],['Plugin']
Modifiability,"genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:2273,Refactor,Refactored,2273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,1,['Refactor'],['Refactored']
Modifiability,ging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:4275,config,config,4275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,ging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3412,plugin,plugins,3412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['plugin'],['plugins']
Modifiability,git-blame says that this is quite old code by MdP 2013... and is difficult to recover the history since is before the maven refactoring. Here I would just apply amnesty and remove one of the tests; let's the lack of coverage do the talking. . Of the two test method names `testStartInMiddleWithBubble` seems the most plausible one given the code but I'm not 100% sure about that.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1081#issuecomment-166013743:124,refactor,refactoring,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1081#issuecomment-166013743,1,['refactor'],['refactoring']
Modifiability,google-cloud-nio: implement configurable retries/reopens,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5306:28,config,configurable,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5306,1,['config'],['configurable']
Modifiability,"gradle uploadArchives will perform a maven release (currently a snapshot release). Unfortunately the maven plugin has an ""install"" task which installs to the local repo, so it's now necessary to specify `installApp` or `installDist` instead of just `gradle install`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/482:107,plugin,plugin,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/482,1,['plugin'],['plugin']
Modifiability,gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5197,plugin,plugins,5197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,hadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40); 	at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:174); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar.copy(ShadowJar.java:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73); 	at org.gradle.api.internal.project.taskfactory.StandardTask,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:6215,plugin,plugins,6215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,hc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <ø> (ø)` | `19 <0> (ø)` | :arrow_down: |; | [...tute/hellbender/tools/AnnotatePairOrientation.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Bbm5vdGF0ZVBhaXJPcmllbnRhdGlvbi5qYXZh) | `96.429% <ø> (ø)` | `8 <0> (ø)` | :arrow_down: |; | [...nder/tools/copynumber/utils/TagGermlineEvents.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL1RhZ0dlcm1saW5lRXZlbnRzLmphdmE=) | `100% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...t/java/org/broadinstitute/hellbender/MainTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluVGVzdC5qYXZh) | `85.714% <90.909%> (+2.787%)` | `15 <9> (+9)` | :arrow_up: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ender/tools/walkers/annotator/PolymorphicNuMT.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9Qb2x5bW9ycGhpY051TVQuamF2YQ==) | `92.593% <0%> (-3.704%)` | `8% <0%> (-1%)` | |; | [...r/tools/walkers/mutect/Mutect2IntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QySW50ZWdyYXRpb25UZXN0LmphdmE=) | `87.586% <0%> (-0.517%)` | `89% <0%> (-2%)` | |; | ... and [13 more](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5551#issuecomment-450184780:3447,Polymorphi,PolymorphicNuMT,3447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5551#issuecomment-450184780,1,['Polymorphi'],['PolymorphicNuMT']
Modifiability,"he 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz; ```. #### Steps to reproduce; gatk --java-options -Xms100g VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz. The input VCF was generated with the dragen-gatk HaplotypeCaller with Allele Specific annotations. . #### Expected behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:11605,polymorphi,polymorphic,11605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['polymorphi'],['polymorphic']
Modifiability,"hila.sorted.bam \; -O hdfs://master2:9000/Drosophila/output/Drosophila.sorted.markdup.bam \; -M; hdfs://master2:9000/Drosophila/output/Drosophila.sorted.markdup_metrics.txt; \; -- \; --spark-runner SPARK --spark-master spark://master2:7077; ```; **error logs:**. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError:; scala/Product$class; at; org.bdgenomics.adam.serialization.InputStreamWithDecoder.<init>(ADAMKryoRegistrator.scala:35); at; org.bdgenomics.adam.serialization.AvroSerializer.<init>(ADAMKryoRegistrator.scala:45); at; org.bdgenomics.adam.models.VariantContextSerializer.<init>(VariantContext.scala:94); at; org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:179); at; org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:78); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$8(KryoSerializer.scala:170); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$8$adapted(KryoSerializer.scala:170); at scala.Option.foreach(Option.scala:407); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:170); at; scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); at; org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:221); at; org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:161); at; org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102); at; com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48); at; org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109); at; org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:336); at; org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:256); at; org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:422); at; org.apache.spark.broadcast.TorrentBroadcast$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644:1308,adapt,adapted,1308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644,1,['adapt'],['adapted']
Modifiability,"his environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Succe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:2580,config,config,2580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,1,['config'],['config']
Modifiability,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2092:867,refactor,refactor,867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092,1,['refactor'],['refactor']
Modifiability,"http://docs.travis-ci.com/user/build-timeouts/#Build-times-out-because-no-output-was-received; use that to extend the waiting time.; On Aug 18, 2015 9:13 PM, ""JP Martin"" notifications@github.com wrote:. > Marking cloud test as ""todo"" for now so I can merge.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hellbender/pull/812#issuecomment-132408527; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132409715:107,extend,extend,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812#issuecomment-132409715,1,['extend'],['extend']
Modifiability,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable ‘supportedFieldNames’ with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376:494,variab,variable,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376,1,['variab'],['variable']
Modifiability,"icsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4935,Config,ConfigFactory,4935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ied to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in che",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1125,config,configparser,1125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"ies = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6149,Config,ConfigFactory,6149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"ies = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6788,Config,ConfigFactory,6788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"ies.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6123,config,config,6123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['config']
Modifiability,"if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:6301,refactor,refactored,6301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['refactor'],['refactored']
Modifiability,"igFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5130,Config,ConfigFactory,5130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"igPloidy - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4077,Config,ConfigFactory,4077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"igarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). This looks similar to issue #8232, but I've not been able to solve this using any of the fixes suggested on that page. I'm using Java version 8, the input BAMs were mapped using STAR via nfcore rnaseq without issue and I have plenty of space on my disk drive, even when specifying a temp directory. It also doesn't matter if I run the tool independently using the command above or as part of a pre-configured pipeline, and I get the same issue with SplitNCigarReads acting as if it has running out of space. . How do I fix this? I need this step to run variant calling on my rnaseq samples (I'm using the GATK best practices pipeline).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:10545,config,configured,10545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,1,['config'],['configured']
Modifiability,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356:3935,config,configuration,3935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356,2,"['Config', 'config']","['Configuration', 'configuration']"
Modifiability,"ility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/48570e53-024c-44b5-8835-3fd40b4c5866); ![image](https://github.com/broadinstitute/gatk/assets/11076296/99100e5d-05e2-4a5c-9d68-57db1b734029); ![image](https://github.com/broadinstitute/gatk/assets/11076296/abae09e1-70a5-4213-95a2-0cb10f9db192); ![image](https://github.com/broadinstitute/gatk/assets/11076296/ef68d0da-90df-4c4b-9802-97988a498280). 2) ... Will update more later!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1571,variab,variables,1571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,3,['variab'],['variables']
Modifiability,"inaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Did you mean this?; FixVcfHeader; ```. The same happens with unknown commands. The code that should be changed for that is the following, where the `setupConfigAndExtractProgram` call should be also inside the try block:. https://github.com/broadinstitute/gatk/blob/8ac2f102b303f343c4787ad4e3359335641c5121/src/main/java/org/broadinstitute/hellbender/Main.java#L190-L212",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:1646,adapt,adapters,1646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['adapt'],['adapters']
Modifiability,"ine.collect(ReferencePipeline.java:566); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:341); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:261); ... 18 more; ```. java version:; ```; java -version; openjdk version ""1.8.0_222""; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1~deb9u1-b10); OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode); ```; I added the cadd folder into data source folder like the structure mentioned in document:; ```; cadd; |- hg19; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; |; |- hg38; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; ```; The config file (cadd.config); ```; name = CADD; version = v1.4; src_file = InDels_inclAnno.tsv; origin_location =; preprocessing_script = UNKNOWN. Whether this data source is for the b37 reference.; Required and defaults to false.; isB37DataSource = false. Supported types:; simpleXSV -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript IDlocatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome locationgencode -- Custom datasource class for GENCODEcosmic -- Custom datasource class for COSMIC vcf -- Custom datasource class for Variant Call Format (VCF) files; type = locatableXSV; Required field for GENCODE files.Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path =. Required field for GENCODE files.; NCBI build version (either hg19 or hg38):; ncbi_build_version =. Required field for simpleXSV files.; Valid values:; GENE_NAME; TRANSCR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:4457,config,config,4457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"ineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4704,Config,ConfigFactory,4704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"ineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1658,config,config,1658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['config'],['config']
Modifiability,"ingframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; 	at com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:131); 	at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); 	at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100); 	at com.luz.push.utils.GcmUtils.init(GcmUtils.java:31); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:389); 	at org.springf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:28788,variab,variable,28788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['variab'],['variable']
Modifiability,institute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:288); at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:225); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.getPileupsOverReference(AssemblyBasedCallerUtils.java:443); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.calculateRefConfidence(ReferenceConfidenceModel.java:195); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:645); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```; This hypothesis is further evidenced by the fact that one user at least claims that their input file validates and that they couldn't find the problem reads by looking at the input files manually. We probably will want to look at an example file in the debugger to catch what is happening at this site. We have refactored a bunch of code adjacent to this function recently so its possible this is a recent regression.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490:3216,refactor,refactored,3216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490,1,['refactor'],['refactored']
Modifiability,"involving ""zombie"" likelihoods from past removed evidences being assigned to new appended evidences in AlleleLikelihoods. Refactor and clean some code as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7154:122,Refactor,Refactor,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7154,1,['Refactor'],['Refactor']
Modifiability,"ional coverages, e.g. a value of 0.499 rounds to zero. The QC step does not transform data per se but does then throw out data points (targets, samples) from consideration and inclusion in the PoN. VR's debugging traces this QC step to HDF5PCACoveragePoNCreationUtils.java, line 212. It appears the current workaround in the official WDL scripts towards preventing data from being thrown out is to disable the QC step altogether, with the `--noQC true` option. However, this appears to me a hack that does not allow us to create a more refined PoN that the QC is meant to enable. **Can someone explain the impact of skipping QC?**. @mbabadi has said for me to go ahead and fix this line of code by changing the `roundToInteger` to true. I am placing this issue here for further discussion and to separate the discussion from the PR. ---; ### Information is lacking in our repo regards to certain settings; Our understanding of the CNV workflow was to use proportional coverage (PCOV) for CalculateTargetCoverage, whether for the PoN or samples. However, @LeeTL1220, there isn't any example INPUTS JSON or discussion of settings for such for the somatic WDL variables pertaining to `gatk/scripts/cnv_wdl/cnv_common_tasks.wdl`'s CalculateTargetCoverage task, where `transform` is the variable that defines whether counts ought to be raw or proportional. . If I search the repo for the WDL variable ""transform PCOV json"", then I get no hits. However, if I search the repo for ""transform RAW json"", then I get germline calling workflows that show '""transform = ""RAW""'. For example, '""transform = ""RAW""' is in `scripts/cnv_wdl/germline/cnv_germline_single_sample_calling_workflow.wdl` and `scripts/cnv_wdl/germline/cnv_germline_panel_creation_workflow.wdl`. Please correct me if I am wrong but It seems to me that this setting shouldn't be different between the binned targets of the germline workflow and a somatic WES workflow. What is the reason we use proportional (PCOV) counts instead of RAW counts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3163:11684,variab,variables,11684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163,3,['variab'],"['variable', 'variables']"
Modifiability,"is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second attempt, now running, scattered the genome into 150 shards, and is running 12 jobs at a time with 8 cores each and the environment variables set. On the second attempt it looks like the jobs will finish the first denoising epoch in < 1 hour each. That's far faster than the 6x reduction in runtime you might expect if a) runtime is linear in the number of bins and b) runtime is proportional to 1/cpus used. Without doing a lot more experiments it's hard to tell whether the better runtime is due to less fighting over resources (I can imagine 24 jobs each running 96 threads could degrade performance) or because runtime is super-linear vs. number of bins. I'm not asking for total precision, but the current docs are not really enough for anyone outside the GATK team to get the CNV caller up and running in an efficient manner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:2133,variab,variables,2133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,1,['variab'],['variables']
Modifiability,"ission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3556,variab,variables,3556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['variab'],['variables']
Modifiability,"ist; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8432:1244,Config,Configure,1244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432,1,['Config'],['Configure']
Modifiability,it was coming from the adam project - log4j was picking up that config file because it was the first log4j.properties file it could find. Fixed by providing our own,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1123#issuecomment-185537487:64,config,config,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1123#issuecomment-185537487,1,['config'],['config']
Modifiability,"ite_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 4G --num-executors 4 --executor-cores 6 --executor-memory 16G --conf spark.dynamicAllocation.enabled=false /opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img -I hdfs:///user/sun/1982.unmapped.bam -R hdfs:///user/sun/ucsc.hg19.fasta -O hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam --sparkMaster yarn; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running spark-class from user-defined location.; 18:30:33.354 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:30:33.534 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [January 9, 2018 6:30:33 PM CST] BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img --output hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam --reference hdfs:///user/sun/ucsc.hg19.fasta --input hdfs:///user/sun/1982.unmapped.bam --sparkMaster yarn --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater fal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:2806,variab,variables,2806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ite_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://973f3a3a3407:7077; 13:47:29.376 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:47:29.548 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDupl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:18403,variab,variables,18403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:4532,config,config,4532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,2,['config'],['config']
Modifiability,java:650); 	at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:458); 	at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:38); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:4822,plugin,plugins,4822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,jdk1.8.0_121\jre\lib\ext\cldrdata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\dnsns.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jaccess.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jfxrt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\localedata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\nashorn.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunec.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunjce_provider.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunmscapi.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunpkcs11.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\zipfs.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\javaws.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jce.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfr.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfxswt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jsse.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\management-agent.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\plugin.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\resources.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\rt.jar;C:\project\push\target\classes;E:\repository\org\springframework\boot\spring-boot-starter-jdbc\2.3.0.RELEASE\spring-boot-starter-jdbc-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter\2.3.0.RELEASE\spring-boot-starter-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot\2.3.0.RELEASE\spring-boot-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-autoconfigure\2.3.0.RELEASE\spring-boot-autoconfigure-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter-logging\2.3.0.RELEASE\spring-boot-starter-logging-2.3.0.RELEASE.jar;E:\repository\ch\qos\logback\logback-classic\1.2.3\logback-classic-1.2.3.jar;E:\repository\ch\qos\logback\logback-core\1.2.3\logback-core-1.2.3.jar;E:\repository\org\apache\logging\log4j\log4j-to-slf4j\2.13.2\log4j-to-slf4j-2.13.2.jar;E:\repository\org\apache\logging\log,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:1669,plugin,plugin,1669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['plugin'],['plugin']
Modifiability,"k - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-small-m.c.broad-dsde-methods.internal, exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:7126,config,configuration,7126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"k.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).ty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5481,Config,ConfigFactory,5481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,k/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1467,config,configuration,1467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['config'],['configuration']
Modifiability,"k/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3536,Config,ConfigFactory,3536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"k/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkernel.tasks.inference_task_base - (denoising) starting...: 0%| | 0/1000 [00:00<?, ?it/s]; 15:10:03.410 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -1038.498 +/- 431.707, SNR: 71.3, T: 1.98: 8%|8 | 83/1000 [00:00<00:01, 826.53it/s]; 15:10:03.522 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -821.262 +/- 327.042, SNR: 38.9, T: 1.97: 17%|#6 | 166/1000 [00:00<00:01, 776.56it/s]; 15:10:03.636 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -727.432 +/- 277.971, SNR: 29.4, T: 1.95: 24%|##4 | 244/1000 [00:00<00:01, 732.12it/s]; 15:10:03.754 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -662.875 +/- 251.403, SNR: 23.4, T: 1.94: 32%|###1 | 318/1000 [00:00<00:00, 689.38it/s]; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:7492,variab,variables,7492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variables']
