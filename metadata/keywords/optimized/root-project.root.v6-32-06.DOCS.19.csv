quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"ange things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics of the IA64 stop bit?. Basically, the IA64 writes instructions like this:; mov ...; add ...; sub ...; op xxx; op xxx; ;;; mov ...; add ...; sub ...; op xxx; op xxx; ;;. Where the ;; delimits a group of instruction with no dependencies between; them, which can all be executed concurrently (to the limits of the; available functional units). The ;; gets translated into a bit set in one; of the opcodes. The advantages of this representation is that you don't have to do some; kind of 'thread id scheduling' pass by having to specify ahead of time how; many threads to use, and the representation doesn't have a per instruction; overhead... > And finally, another thought about the syntax for arrays :-); > Although this syntax:; > array <dimension-list> of <type>; > is verbose, it will be used only in the human-readable assembly code so; > size should not matter. I think we should consider it because I find it; > to be the clearest syntax. It could even make arrays of function; > pointers somewhat readable. My only comment will be to give you an example of why this is a bad; idea. :). Here is an example of using the switch statement (with my recommended; syntax):. switch uint %val, label %otherwise, ; [%3 x {uint, label}] [ { uint %57, label %l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:7440,concurren,concurrently,7440,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,1,['concurren'],['concurrently']
Performance,"anges. Support for multiple, parallel OpenGL views that can show different; projections of the same event. Provide object selection and feedback highlight across all GL-views and; list-trees. New classes for visualization of calorimeter data,; TEveCaloXYZ, see tutorials/eve/cms_calo.C. Available; representations: 3D-cylindrical view, projected views r-phi and rho-z,; and lego-view (with dedicated event handler allowing detailed; inspection of the data). Support for compound objects in view of selection, highlight and; color managament (see class TEveCompound). Optimize updates of GL-scenes by introducing change-stamping bits; into TEveElement. See methods AddStamp() and; StampXyzz(). Added support for central management of visualization parameters; of objects. Instead of specifying visual attributes individually by; set-methods a single string tag can be used to retrieve all of them; with a single command, e.g.,; track->ApplyVizTag(""MuonTrack""). The parameter-database can; be saved as a CINT script, edited manually and loaded. This provides more; flexibility as different users can share the same code to; instantiate visualziation objects but still override visualization; parameters independently. See TEveElement::CopyVizParams(); and TEveManager::*VizDB() methods for more information. Minor changes, fixes and improvements. Improved handling of projected elements. For fish-eye projections, allow fixing of compression scale; beyond given distance from the center. Add support for step-function scaling of 2D-projections. This; allows arbitrary magnification of concentric regions in r-phi and; rho-z views. See tutorial; tutorials/eve/projection_test_prescale.C. Path-mark type representing 2D clusters was added for; TEveTrack and TEveTrackPropagator classes. Add support for representing a set of arbitrary cones to; TEveBoxSet (see tutorials/eve/cone_test.C). TEveFrameBox now supports arbitrary 2d frame shapes -; user specifies individual points via SetQuadByPoints() functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html:2423,load,loaded,2423,graf3d/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html,2,['load'],['loaded']
Performance,"anging *all* of ``foo`` (or its base region) will; *definitely* change ``foo[i]``. This logic could be improved by using the current constraints on ``i``, at the; cost of speed. The latter case could also be improved by matching region kinds,; i.e. changing ``foo[0].a`` is unlikely to affect ``foo[i].b``, no matter what; ``i`` is. For more detail, read through ``RegionStoreManager::removeSubRegionBindings`` in; RegionStore.cpp. ObjCIvarRegions; ---------------. Objective-C instance variables require a bit of special handling. Like struct; fields, they are not base regions, and when their parent object region is; invalidated, all the instance variables must be invalidated as well. However,; they have no concrete compile-time offsets (in the modern, ""non-fragile""; runtime), and so cannot easily be represented as an offset from the start of; the object in the analyzer. Moreover, this means that invalidating a single; instance variable should *not* invalidate the rest of the object, since unlike; struct fields or array elements there is no way to perform pointer arithmetic; to access another instance variable. Consequently, although the base region of an ObjCIvarRegion is the entire; object, RegionStore offsets are computed from the start of the instance; variable. Thus it is not valid to assume that all bindings with non-symbolic; offsets start from the base region!. Region Invalidation; -------------------. Unlike binding invalidation, region invalidation occurs when the entire; contents of a region may have changed---say, because it has been passed to a; function the analyzer can model, like memcpy, or because its address has; escaped, usually as an argument to an opaque function call. In these cases we; need to throw away not just all bindings within the region itself, but within; its entire cluster, since neighboring regions may be accessed via pointer; arithmetic. Region invalidation typically does even more than this, however. Because it; usually represents the co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/RegionStore.rst:3253,perform,perform,3253,interpreter/llvm-project/clang/docs/analyzer/developer-docs/RegionStore.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/RegionStore.rst,1,['perform'],['perform']
Performance,"anguage [:ref:`HIP <amdgpu-dwarf-HIP>`], which is supported; by the AMDGPU, is added. See :ref:`amdgpu-dwarf-language-names-table`. 2.19 Support for Source Language Optimizations that Result in Concurrent Iteration Execution; --------------------------------------------------------------------------------------------. A compiler can perform loop optimizations that result in the generated code; executing multiple iterations concurrently. For example, software pipelining; schedules multiple iterations in an interleaved fashion to allow the; instructions of one iteration to hide the latencies of the instructions of; another iteration. Another example is vectorization that can exploit SIMD; hardware to allow a single instruction to execute multiple iterations using; vector registers. Note that although this is similar to SIMT execution, the way a client debugger; uses the information is fundamentally different. In SIMT execution the debugger; needs to present the concurrent execution as distinct source language threads; that the user can list and switch focus between. With iteration concurrency; optimizations, such as software pipelining and vectorized SIMD, the debugger; must not present the concurrency as distinct source language threads. Instead,; it must inform the user that multiple loop iterations are executing in parallel; and allow the user to select between them. In general, SIMT execution fixes the number of concurrent executions per target; architecture thread. However, both software pipelining and SIMD vectorization; may vary the number of concurrent iterations for different loops executed by a; single source language thread. It is possible for the compiler to use both SIMT concurrency and iteration; concurrency techniques in the code of a single source language thread. Therefore, a DWARF operation is required to denote the current concurrent; iteration instance, much like ``DW_OP_push_object_address`` denotes the current; object. See ``DW_OP_LLVM_push_itera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:33496,concurren,concurrent,33496,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['concurren'],['concurrent']
Performance,"anguage configuration, rather than once per translation unit that uses the module. Modules maintain references to each of the headers that were part of the module build. If any of those headers changes, or if any of the modules on which a module depends change, then the module will be (automatically) recompiled. The process should never require any user intervention. Command-line parameters; -----------------------; ``-fmodules``; Enable the modules feature. ``-fbuiltin-module-map``; Load the Clang builtins module map file. (Equivalent to ``-fmodule-map-file=<resource dir>/include/module.modulemap``). ``-fimplicit-module-maps``; Enable implicit search for module map files named ``module.modulemap`` and similar. This option is implied by ``-fmodules``. If this is disabled with ``-fno-implicit-module-maps``, module map files will only be loaded if they are explicitly specified via ``-fmodule-map-file`` or transitively used by another module map file. ``-fmodules-cache-path=<directory>``; Specify the path to the modules cache. If not provided, Clang will select a system-appropriate default. ``-fno-autolink``; Disable automatic linking against the libraries associated with imported modules. ``-fmodules-ignore-macro=macroname``; Instruct modules to ignore the named macro when selecting an appropriate module variant. Use this for macros defined on the command line that don't affect how modules are built, to improve sharing of compiled module files. ``-fmodules-prune-interval=seconds``; Specify the minimum delay (in seconds) between attempts to prune the module cache. Module cache pruning attempts to clear out old, unused module files so that the module cache itself does not grow without bound. The default delay is large (604,800 seconds, or 7 days) because this is an expensive operation. Set this value to 0 to turn off pruning. ``-fmodules-prune-after=seconds``; Specify the minimum time (in seconds) for which a file in the module cache must be unused (according to access t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:14931,cache,cache-path,14931,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"annor(sigmat,sigmas);; Int_t ntrack = Int_t(600 + 600 *sigmat/120.);; Float_t random = gRandom->Rndm(1);; sprintf(etype,""type%d"",ev%5);; event->SetType(etype);; event->SetHeader(ev, 200, 960312, random);; event->SetNseg(Int_t(10*ntrack+20*sigmas));; event->SetNvertex(Int_t(1+20*gRandom->Rndm()));; event->SetFlag(UInt_t(random+0.5));; event->SetTemperature(random+20.);; for(UChar_t m = 0; m < 10; m++) {; event->SetMeasure(m, Int_t(gRandom->Gaus(m,m+1)));; }; // continued...; // fill the matrix; for(UChar_t i0 = 0; i0 < 4; i0++) {; for(UChar_t i1 = 0; i1 < 4; i1++) {; event->SetMatrix(i0,i1,gRandom->Gaus(i0*i1,1));; }; }; // create and fill the Track objects; for (Int_t t = 0; t < ntrack; t++) event->AddTrack(random);; t4.Fill(); // Fill the tree; event->Clear(); // Clear before reloading event; }; f.Write(); // Write the file header; t4.Print(); // Print the tree contents; }; ```. ### Reading the Tree. First, we check if the shared library with the class definitions is; loaded. If not we load it. Then we read two branches, one for the number; of tracks and one for the entire event. We check the number of tracks; first, and if it meets our condition, we read the entire event. We show; the fist entry that meets the condition. ``` {.cpp}; void tree4r() {; // check if the event class is in the dictionary; // if it is not load the definition in libEvent.so; if (!TClassTable::GetDict(""Event"")) {; gSystem->Load(""$ROOTSYS/test/libEvent.so"");; }; // read the tree generated with tree4w. // note that we use ""new"" to create the TFile and TTree objects, because we; // want to keep these objects alive when we leave this function.; TFile *f = new TFile(""tree4.root"");; TTree *t4 = (TTree*)f->Get(""t4"");. // create a pointer to an event object for reading the branch values.; Event *event = new Event();; // get two branches and set the branch address; TBranch *bntrack = t4->GetBranch(""fNtrack"");; TBranch *branch = t4->GetBranch(""event_split"");; branch->SetAddress(&event);. Int_t nevent ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:63954,load,load,63954,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"ansparently used by ``clang`` when generating PCH files. The; resulting PCH file contains the serialized form of the compiler's internal; representation after it has completed parsing and semantic analysis. The PCH; file can then be used as a prefix header with the `-include-pch`; option:. .. code-block:: bash. $ clang -cc1 -include-pch test.h.pch test.c -o test.s. Design Philosophy; -----------------. Precompiled headers are meant to improve overall compile times for projects, so; the design of precompiled headers is entirely driven by performance concerns.; The use case for precompiled headers is relatively simple: when there is a; common set of headers that is included in nearly every source file in the; project, we *precompile* that bundle of headers into a single precompiled; header (PCH file). Then, when compiling the source files in the project, we; load the PCH file first (as a prefix header), which acts as a stand-in for that; bundle of headers. A precompiled header implementation improves performance when:. * Loading the PCH file is significantly faster than re-parsing the bundle of; headers stored within the PCH file. Thus, a precompiled header design; attempts to minimize the cost of reading the PCH file. Ideally, this cost; should not vary with the size of the precompiled header file. * The cost of generating the PCH file initially is not so large that it; counters the per-source-file performance improvement due to eliminating the; need to parse the bundled headers in the first place. This is particularly; important on multi-core systems, because PCH file generation serializes the; build when all compilations require the PCH file to be up-to-date. Modules, as implemented in Clang, use the same mechanisms as precompiled; headers to save a serialized AST file (one per module) and use those AST; modules. From an implementation standpoint, modules are a generalization of; precompiled headers, lifting a number of restrictions placed on precompiled; headers. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:1742,perform,performance,1742,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performance']
Performance,"ant adding a release note (this list is not; exhaustive):. * Adding, removing, or modifying command-line options.; * Adding, removing, or regrouping a diagnostic.; * Fixing a bug that potentially has significant user-facing impact (please link; to the issue fixed in the bug database).; * Adding or removing optimizations that have widespread impact or enables new; programming paradigms.; * Modifying a C stable API.; * Notifying users about a potentially disruptive change expected to be made in; a future release, such as removal of a deprecated feature. In this case, the; release note should be added to a ``Potentially Breaking Changes`` section of; the notes with sufficient information and examples to demonstrate the; potential disruption. Additionally, any new entries to this section should be; announced in the `Announcements <https://discourse.llvm.org/c/announce/>`_; channel on Discourse. See :ref:`breaking` for more details. Code reviewers are encouraged to request a release note if they think one is; warranted when performing a code review. Quality; -------. The minimum quality standards that any change must satisfy before being; committed to the main development branch are:. #. Code must adhere to the `LLVM Coding Standards <CodingStandards.html>`_. #. Code must compile cleanly (no errors, no warnings) on at least one platform. #. Bug fixes and new features should `include a testcase`_ so we know if the; fix/feature ever regresses in the future. #. Code must pass the ``llvm/test`` test suite. #. The code must not cause regressions on a reasonable subset of llvm-test,; where ""reasonable"" depends on the contributor's judgement and the scope of; the change (more invasive changes require more testing). A reasonable subset; might be something like ""``llvm-test/MultiSource/Benchmarks``"". #. Ensure that links in source code and test files point to publicly available; resources and are used primarily to add additional information rather than; to supply critical context.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst:12271,perform,performing,12271,interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DeveloperPolicy.rst,1,['perform'],['performing']
Performance,"ant to be able to do all this, and, provide; feedback to the user. This is where pass registration comes into play. The fundamental mechanisms for pass registration are the; ``MachinePassRegistry`` class and subclasses of ``MachinePassRegistryNode``. An instance of ``MachinePassRegistry`` is used to maintain a list of; ``MachinePassRegistryNode`` objects. This instance maintains the list and; communicates additions and deletions to the command line interface. An instance of ``MachinePassRegistryNode`` subclass is used to maintain; information provided about a particular pass. This information includes the; command line name, the command help string and the address of the function used; to create an instance of the pass. A global static constructor of one of these; instances *registers* with a corresponding ``MachinePassRegistry``, the static; destructor *unregisters*. Thus a pass that is statically linked in the tool; will be registered at start up. A dynamically loaded pass will register on; load and unregister at unload. Using existing registries; -------------------------. There are predefined registries to track instruction scheduling; (``RegisterScheduler``) and register allocation (``RegisterRegAlloc``) machine; passes. Here we will describe how to *register* a register allocator machine; pass. Implement your register allocator machine pass. In your register allocator; ``.cpp`` file add the following include:. .. code-block:: c++. #include ""llvm/CodeGen/RegAllocRegistry.h"". Also in your register allocator ``.cpp`` file, define a creator function in the; form:. .. code-block:: c++. FunctionPass *createMyRegisterAllocator() {; return new MyRegisterAllocator();; }. Note that the signature of this function should match the type of; ``RegisterRegAlloc::FunctionPassCtor``. In the same file add the ""installing""; declaration, in the form:. .. code-block:: c++. static RegisterRegAlloc myRegAlloc(""myregalloc"",; ""my register allocator help string"",; createMyRegisterAlloca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:49200,load,loaded,49200,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,2,['load'],"['load', 'loaded']"
Performance,"anywhere in the program. Example of program with no call to `vset{i}vl{i}`:. .. code-block:: none. # LLVM-MCA-RISCV-LMUL M2; vadd.vv v2, v2, v2. Example of program with call to `vset{i}vl{i}`:. .. code-block:: none. vsetvli zero, a0, e8, m1, tu, mu; # LLVM-MCA-RISCV-LMUL M1; vadd.vv v2, v2, v2. Example of program with multiple calls to `vset{i}vl{i}`:. .. code-block:: none. vsetvli zero, a0, e8, m1, tu, mu; # LLVM-MCA-RISCV-LMUL M1; vadd.vv v2, v2, v2; vsetvli zero, a0, e8, m8, tu, mu; # LLVM-MCA-RISCV-LMUL M8; vadd.vv v2, v2, v2. Example of program with call to `vsetvl`:. .. code-block:: none. vsetvl rd, rs1, rs2; # LLVM-MCA-RISCV-LMUL M1; vadd.vv v12, v12, v12; vsetvl rd, rs1, rs2; # LLVM-MCA-RISCV-LMUL M4; vadd.vv v12, v12, v12. HOW LLVM-MCA WORKS; ------------------. :program:`llvm-mca` takes assembly code as input. The assembly code is parsed; into a sequence of MCInst with the help of the existing LLVM target assembly; parsers. The parsed sequence of MCInst is then analyzed by a ``Pipeline`` module; to generate a performance report. The Pipeline module simulates the execution of the machine code sequence in a; loop of iterations (default is 100). During this process, the pipeline collects; a number of execution related statistics. At the end of this process, the; pipeline generates and prints a report from the collected statistics. Here is an example of a performance report generated by the tool for a; dot-product of two packed float vectors of four elements. The analysis is; conducted for target x86, cpu btver2. The following result can be produced via; the following command using the example located at; ``test/tools/llvm-mca/X86/BtVer2/dot-product.s``:. .. code-block:: bash. $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=300 dot-product.s. .. code-block:: none. Iterations: 300; Instructions: 900; Total Cycles: 610; Total uOps: 900. Dispatch Width: 2; uOps Per Cycle: 1.48; IPC: 1.48; Block RThroughput: 2.0. Instruction Info:; [1]: #uOps; [",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:13562,perform,performance,13562,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,"anywhere near as strong at reasoning about the various intrinsics. If; profitable for code generation purposes, the optimizer will likely form the; intrinsics itself late in the optimization pipeline. It is *very* rarely; profitable to emit these directly in the language frontend. This item; explicitly includes the use of the :ref:`overflow intrinsics <int_overflow>`. #. Avoid using the :ref:`assume intrinsic <int_assume>` until you've; established that a) there's no other way to express the given fact and b); that fact is critical for optimization purposes. Assumes are a great; prototyping mechanism, but they can have negative effects on both compile; time and optimization effectiveness. The former is fixable with enough; effort, but the later is fairly fundamental to their designed purpose. Describing Language Specific Properties; =======================================. When translating a source language to LLVM, finding ways to express concepts; and guarantees available in your source language which are not natively; provided by LLVM IR will greatly improve LLVM's ability to optimize your code.; As an example, C/C++'s ability to mark every add as ""no signed wrap (nsw)"" goes; a long way to assisting the optimizer in reasoning about loop induction; variables and thus generating more optimal code for loops. The LLVM LangRef includes a number of mechanisms for annotating the IR with; additional semantic information. It is *strongly* recommended that you become; highly familiar with this document. The list below is intended to highlight a; couple of items of particular interest, but is by no means exhaustive. Restricted Operation Semantics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; #. Add nsw/nuw flags as appropriate. Reasoning about overflow is; generally hard for an optimizer so providing these facts from the frontend; can be very impactful. #. Use fast-math flags on floating point operations if legal. If you don't; need strict IEEE floating point semantics, there are a numb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:10089,optimiz,optimize,10089,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimize']
Performance,"ap`` section is emitted which includes address; offsets for each basic block in the program, relative to the parent function; address. With the ``list=<arg>`` option, a file containing the subset of basic blocks; that need to placed in unique sections can be specified. The format of the; file is as follows. For example, ``list=spec.txt`` where ``spec.txt`` is the; following:. ::. !foo; !!2; !_Z3barv. will place the machine basic block with ``id 2`` in function ``foo`` in a; unique section. It will also place all basic blocks of functions ``bar``; in unique sections. Further, section clusters can also be specified using the ``list=<arg>``; option. For example, ``list=spec.txt`` where ``spec.txt`` contains:. ::. !foo; !!1 !!3 !!5; !!2 !!4 !!6. will create two unique sections for function ``foo`` with the first; containing the odd numbered basic blocks and the second containing the; even numbered basic blocks. Basic block sections allow the linker to reorder basic blocks and enables; link-time optimizations like whole program inter-procedural basic block; reordering. Profile Guided Optimization; ---------------------------. Profile information enables better optimization. For example, knowing that a; branch is taken very frequently helps the compiler make better decisions when; ordering basic blocks. Knowing that a function ``foo`` is called more; frequently than another function ``bar`` helps the inliner. Optimization; levels ``-O2`` and above are recommended for use of profile guided optimization. Clang supports profile guided optimization with two different kinds of; profiling. A sampling profiler can generate a profile with very low runtime; overhead, or you can build an instrumented version of the code that collects; more detailed profile information. Both kinds of profiles can provide execution; counts for instructions in the code and information on branches taken and; function invocation. Regardless of which kind of profiling you use, be careful to collect profi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:89483,optimiz,optimizations,89483,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"appen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any fol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:301166,load,load,301166,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"aque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic loads. The optional constant ``align`` argument specifies the alignment of the; operation (that is, the alignment of the memory address). It is the; responsibility of the code emitter to ensure that the alignment information is; correct. Overestimating the alignment results in undefined behavior.; Underestimating the alignment may produce less efficient code. An alignment of; 1 is always safe. The maximum possible alignment is ``1 << 32``. An alignment; value higher than the size of the loaded type implies memory up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:413864,load,loads,413864,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"aquePointerElementType()`` can be used as a marker in; code-paths where opaque pointers have been explicitly excluded.; * To get the type of a byval argument, use ``getParamByValType()``. Similar; method exists for other ABI-affecting attributes that need to know the; element type, such as byref, sret, inalloca and preallocated.; * Some intrinsics require an ``elementtype`` attribute, which can be retrieved; using ``getParamElementType()``. This attribute is required in cases where; the intrinsic does not naturally encode a needed element type. This is also; used for inline assembly. Note that some of the methods mentioned above only exist to support both typed; and opaque pointers at the same time, and will be dropped once the migration; has completed. For example, ``isOpaqueOrPointeeTypeEquals()`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. F",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:8776,load,load,8776,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['load'],['load']
Performance,"ar(); }; 3 |; 4 | void Test(int *res, int *c, int *d, int *p, int n) {; 5 | int i;; 6 |; 7 | #pragma clang loop vectorize(assume_safety); 8 V4,1 | for (i = 0; i < 1600; i++) {; 9 | res[i] = (p[i] == 0) ? res[i] : res[i] + d[i];; 10 | }; 11 |; 12 U16 | for (i = 0; i < 16; i++) {; 13 | res[i] = (p[i] == 0) ? res[i] : res[i] + d[i];; 14 | }; 15 |; 16 I | foo();; 17 |; 18 | foo(); bar(); foo();; I | ^; I | ^; 19 | }; 20 |. Symbols printed on the left side of the program indicate what kind of optimization was performed.; The meanings of the symbols are as follows:. - I: The function is inlined.; - U: The loop is unrolled. The following number indicates the unroll factor.; - V: The loop is vectorized. The following numbers indicate the vector length and the interleave factor. .. note:: . If a specific line of code is output twice, it means that the same optimization pass was applied to that ; line of code twice, and the pass was able to further optimize the code on the second iteration. OPTIONS; -------. If ``input`` is ""``-``"" or omitted, :program:`llvm-opt-report` reads from standard; input. Otherwise, it will read from the specified filename. If the :option:`-o` option is omitted, then :program:`llvm-opt-report` will send its output; to standard output. If the :option:`-o` option specifies ""``-``"", then the output will also; be sent to standard output. .. option:: --help. Display available options. .. option:: --version. Display the version of this program. .. option:: --format=<string>. The format of the optimization record file.; The Argument is one of the following:. - yaml; - yaml-strtab; - bitstream. .. option:: --no-demangle. Do not demangle function names. .. option:: -o=<string>. Output file. .. option:: -r=<string>. Root for relative input paths. .. option:: -s. Do not include vectorization factors, etc. EXIT STATUS; -----------. :program:`llvm-opt-report` returns 0 on success. Otherwise, an error message is printed; to standard error, and the tool returns 1. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst:2483,optimiz,optimization,2483,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst,1,['optimiz'],['optimization']
Performance,"arameter Estimation #. After going through the previous chapters, you already know how to use; analytical functions (class `TF1`), and you got some insight into the; graph (`TGraphErrors`) and histogram classes (`TH1F`) for data; visualisation. In this chapter we will add more detail to the previous; approximate explanations to face the fundamental topic of parameter; estimation by fitting functions to data. For graphs and histograms, ROOT; offers an easy-to-use interface to perform fits - either the fit panel; of the graphical interface, or the `Fit` method. The class `TFitResult`; allows access to the detailed results. Very often it is necessary to study the statistical properties of; analysis procedures. This is most easily achieved by applying the; analysis to many sets of simulated data (or ""pseudo data""), each; representing one possible version of the true experiment. If the; simulation only deals with the final distributions observed in data, and; does not perform a full simulation of the underlying physics and the; experimental apparatus, the name ""Toy Monte Carlo"" is frequently used; [^5]. Since the true values of all parameters are known in the; pseudo-data, the differences between the parameter estimates from the; analysis procedure w.r.t. the true values can be determined, and it is; also possible to check that the analysis procedure provides correct; error estimates. ## Fitting Functions to Pseudo Data ##. In the example below, a pseudo-data set is produced and a model fitted; to it. ROOT offers various minimisation algorithms to minimise a chi2 or a; negative log-likelihood function. The default minimiser is MINUIT, a; package originally implemented in the FORTRAN programming language. A; C++ version is also available, MINUIT2, as well as Fumili [@Fumili] an; algorithm optimised for fitting. The; minimisation algorithms can be selected using the static functions of; the `ROOT::Math::MinimizerOptions` class. Steering options for the; minimiser, such as t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md:995,perform,perform,995,documentation/primer/functions_and_parameter_estimation.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md,1,['perform'],['perform']
Performance,"arameter. Arguments:; """""""""""""""""""". The '``llvm.vp.fpext``' intrinsic takes a value to cast as its first operand.; The return type is the type to cast the value to. Both types must be vector of; :ref:`floating-point <t_floating>` type. The bit size of the value must be; smaller than the bit size of the return type. This implies that; '``llvm.vp.fpext``' cannot be used to make a *no-op cast*. The second operand; is the vector mask. The return type, the value to cast, and the vector mask have; the same number of elements. The third operand is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fpext``' intrinsic extends the ``value`` from a smaller; :ref:`floating-point <t_floating>` type to a larger :ref:`floating-point; <t_floating>` type. The '``llvm.vp.fpext``' cannot be used to make a; *no-op cast* because it always changes bits. Use ``bitcast`` to make a; *no-op cast* for a floating-point cast.; The conversion is performed on lane positions below the explicit vector length; and where the vector mask is true. Masked-off lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x double> @llvm.vp.fpext.v4f64.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fpext <4 x float> %a to <4 x double>; %also.r = select <4 x i1> %mask, <4 x double> %t, <4 x double> poison. .. _int_vp_fptoui:. '``llvm.vp.fptoui.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.fptoui.v16i32.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.fptoui.nxv4i32.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.fptoui.v256i64.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.fptoui``' intrinsic converts the :ref:`floatin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:804460,perform,performed,804460,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"arameters_8C.html); and [arxiv:2012.02746](https://arxiv.org/abs/2012.02746). ### Modernised RooDataHist. RooDataHist was partially modernised to improve const-correctness, to reduce side effects as well as its memory footprint, and to make; it ready for RooFit's faster batch evaluations.; Derived classes that directly access protected members might need to be updated. This holds especially for direct accesses to `_curWeight`,; `_curWeightErrLo`, etc, which have been removed. (It doesn't make sense to write to these members from const functions when the same information; can be retrieved using an index access operator of an array.) All similar accesses in derived classes should be replaced by the getters `get_curWeight()`; or better `get_wgt(i)`, which were also supported in ROOT \<v6.24. More details on what happened:. - Reduced side effects. This code produces undefined behaviour because the side effect of `get(i)`, i.e., loading the new weight into `_curWeight`; is not guaranteed to happen before `weight()` is called:; ```; processEvent(dataHist.get(i), dataHist.weight()); // Dangerous! Order of evaluation is not guaranteed.; ```; With the modernised interface, one would use:; ```; processEvent(dataHist.get(i), dataHist.weight(i));; ```; To modernise old code, one should replace patterns like `h.get(i); h.func()` by `h.func(i);`. One may `#define R__SUGGEST_NEW_INTERFACE` to switch on; deprecation warnings for the functions in question.; Similarly, the bin content can now be set using an index, making prior loading of a certain coordinate unnecessary:; ```; for (int i=0 ; i<hist->numEntries() ; i++) {; - hist->get(i) ;; - hist->set(hist->weight() / sum);; + hist->set(i, hist->weight(i) / sum, 0.);; }; ```; - More const correctness. `calcTreeIndex()` doesn't rely on side effects, any more. Instead of overwriting the internal; coordinates with new values:; ```; // In a RooDataHist subclass:; _vars = externalCoordinates;; auto index = calcTreeIndex();. // Or from the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:21451,load,loading,21451,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['load'],['loading']
Performance,"arbyint|; +-----+-----+---------+; | | | fmuladd |; +-----+-----+---------+. Note that the optimizer may not be able to vectorize math library functions; that correspond to these intrinsics if the library calls access external state; such as ""errno"". To allow better optimization of C/C++ math library functions,; use ""-fno-math-errno"". The loop vectorizer knows about special instructions on the target and will; vectorize a loop containing a function call that maps to the instructions. For; example, the loop below will be vectorized on Intel x86 if the SSE4.1 roundps; instruction is available. .. code-block:: c++. void foo(float *f) {; for (int i = 0; i != 1024; ++i); f[i] = floorf(f[i]);; }. Partial unrolling during vectorization; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Modern processors feature multiple execution units, and only programs that contain a; high degree of parallelism can fully utilize the entire width of the machine.; The Loop Vectorizer increases the instruction level parallelism (ILP) by; performing partial-unrolling of loops. In the example below the entire array is accumulated into the variable 'sum'.; This is inefficient because only a single execution port can be used by the processor.; By unrolling the code the Loop Vectorizer allows two or more execution ports; to be used simultaneously. .. code-block:: c++. int foo(int *A, int n) {; unsigned sum = 0;; for (int i = 0; i < n; ++i); sum += A[i];; return sum;; }. The Loop Vectorizer uses a cost model to decide when it is profitable to unroll loops.; The decision to unroll the loop depends on the register pressure and the generated code size. Epilogue Vectorization; ^^^^^^^^^^^^^^^^^^^^^^. When vectorizing a loop, often a scalar remainder (epilogue) loop is necessary; to execute tail iterations of the loop if the loop trip count is unknown or it; does not evenly divide the vectorization and unroll factors. When the; vectorization and unroll factors are large, it's possible for loops with smaller; tr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:10592,perform,performing,10592,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['perform'],['performing']
Performance,"ards the interface. ### VariableMetricMinimizer() ###. The VariableMetricMinimizer is instantiated using default constructor. ### minimize(const FCNBase&, ...) ###. The VariableMetricMinimizer provides several overloaded methods minimize; with return value FunctionMinimum. Together with the user; $\mbox{FCN}$ (either an implementation of FCNBase or; FCNGradientBase) the user has to give as input the parameters with; starting values in one of the defined formats (std::vector$<$double$>$,; MnUserParameters or MnUserParameterState). ## MnMinimize and CombinedMinimizer ##. [api:minimize]. Causes minimization of the function by the method of; $\mbox{MIGRAD}$, as does the MnMigrad class, but switches to the; $\mbox{SIMPLEX}$ method if $\mbox{MIGRAD}$ fails to converge.; Constructor arguments, methods arguments and names of methods are the; same as for MnMigrad or MnSimplex and VariableMetricMinimizer or; SimplexMinimizer. ## MnMinos ##. [api:minos] Causes a $\mbox{MINOS}$ error analysis to be performed; on the parameter whose number is specified. $\mbox{MINOS}$ errors; may be expensive to calculate, but are very reliable since they take; account of non-linearities in the problem as well as parameter; correlations, and are in general asymmetric. The optional argument; $\mbox{maxcalls}$ specifies the (approximate) maximum number of; function calls **per parameter requested**, after which the calculation; will be stopped for that parameter. ### MnMinos(const FCNBase&, const FunctionMinimum&) ###. Construct an MnMinos object from the user's $\mbox{FCN}$ and a valid; FunctionMinimum. Additional constructors for user specific MnStrategy; settings are provided. ### operator() ###. MnMinos::operator()(unsigned int n, unsigned int maxcalls) causes a; $\mbox{MINOS}$ error analysis for external parameter $\mbox{n}$.; The return value is a std::pair$<$double,double$>$ with the lower and; upper bounds of parameter $\mbox{n}$. ### minos(unsigned int n, unsigned int maxcalls) ###. MnMin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:45074,perform,performed,45074,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['perform'],['performed']
Performance,"are deserialized from the AST file, the data; within the record is used to build and populate a new instance of the; corresponding ``Decl`` node. As with types, each declaration node has a; numeric ID that is used to refer to that declaration within the AST file. In; addition, a lookup table provides a mapping from that numeric ID to the offset; within the precompiled header where that declaration is described. Declarations in Clang's abstract syntax trees are stored hierarchically. At; the top of the hierarchy is the translation unit (``TranslationUnitDecl``),; which contains all of the declarations in the translation unit but is not; actually written as a specific declaration node. Its child declarations (such; as functions or struct types) may also contain other declarations inside them,; and so on. Within Clang, each declaration is stored within a :ref:`declaration; context <DeclContext>`, as represented by the ``DeclContext`` class.; Declaration contexts provide the mechanism to perform name lookup within a; given declaration (e.g., find the member named ``x`` in a structure) and; iterate over the declarations stored within a context (e.g., iterate over all; of the fields of a structure for structure layout). In Clang's AST file format, deserializing a declaration that is a; ``DeclContext`` is a separate operation from deserializing all of the; declarations stored within that declaration context. Therefore, Clang will; deserialize the translation unit declaration without deserializing the; declarations within that translation unit. When required, the declarations; stored within a declaration context will be deserialized. There are two; representations of the declarations within a declaration context, which; correspond to the name-lookup and iteration behavior described above:. * When the front end performs name lookup to find a name ``x`` within a given; declaration context (for example, during semantic analysis of the expression; ``p->x``, where ``p``'s type is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:14353,perform,perform,14353,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['perform']
Performance,"are included for testing and performance evaluation). Method No Fisher Fisher, Mahalanobis Discrimination method. Configuration options for MVA method :. Configuration options reference for MVA method: PDERS. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). VolumeRangeMode No Adaptive Unscaled, MinMax, RMS, Adaptive, kNN Method to determine volume size. KernelEstimator No Box Box, Sphere, Teepee, Gauss, Sinc3, Sinc5, Sinc7, Sinc9, Sinc11, Lanczos2, Lanczos3, Lanczos5, Lanczos8, Trim Kernel estimation function. DeltaFrac No 3  nEventsMin/Max for minmax and rms volume range. NEventsMin No 100  nEventsMin for adaptive volume range. NEventsMax No 200  nEventsMax for adaptive volume range. MaxVIterations No 150  MaxVIterations for adaptive volume range. InitialScale No 0.99  InitialScale for adaptive volume range. GaussSigma No 0.1  Width (wrt volume size) of Gaussian kernel estimator. NormTree No False  Normalize binary search tree. Configuration options for MVA method :. Configuration options reference for MVA method: FDA. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:4342,perform,performance,4342,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4814,cache,cache,4814,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['cache'],['cache']
Performance,"are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18249,Load,Load,18249,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,1,['Load'],['Load']
Performance,"arely have to explicitly tell the system what type parts of your patterns; are. In the ``FMADDS`` case above, we didn't have to tell ``tblgen`` that all; of the nodes in the pattern are of type 'f32'. It was able to infer and; propagate this knowledge from the fact that ``F4RC`` has type 'f32'. * Targets can define their own (and rely on built-in) ""pattern fragments"".; Pattern fragments are chunks of reusable patterns that get inlined into your; patterns during compiler-compile time. For example, the integer ""``(not; x)``"" operation is actually defined as a pattern fragment that expands as; ""``(xor x, -1)``"", since the SelectionDAG does not have a native '``not``'; operation. Targets can define their own short-hand fragments as they see fit.; See the definition of '``not``' and '``ineg``' for examples. * In addition to instructions, targets can specify arbitrary patterns that map; to one or more instructions using the 'Pat' class. For example, the PowerPC; has no way to load an arbitrary integer immediate into a register in one; instruction. To tell tblgen how to do this, it defines:. ::. // Arbitrary immediate support. Implement in terms of LIS/ORI.; def : Pat<(i32 imm:$imm),; (ORI (LIS (HI16 imm:$imm)), (LO16 imm:$imm))>;. If none of the single-instruction patterns for loading an immediate into a; register match, this will be used. This rule says ""match an arbitrary i32; immediate, turning it into an ``ORI`` ('or a 16-bit immediate') and an ``LIS``; ('load 16-bit immediate, where the immediate is shifted to the left 16 bits'); instruction"". To make this work, the ``LO16``/``HI16`` node transformations; are used to manipulate the input immediate (in this case, take the high or low; 16-bits of the immediate). * When using the 'Pat' class to map a pattern to an instruction that has one; or more complex operands (like e.g. `X86 addressing mode`_), the pattern may; either specify the operand as a whole using a ``ComplexPattern``, or else it; may specify the components o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:50394,load,load,50394,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"arest, ties away from zero is not a supported; mode. The raw rounding mode values in the MODE; register do not exactly match the FLT_ROUNDS values,; so a conversion is performed. llvm.amdgcn.wave.reduce.umin Performs an arithmetic unsigned min reduction on the unsigned values; provided by each lane in the wavefront.; Intrinsic takes a hint for reduction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.wave.reduce.umax Performs an arithmetic unsigned max reduction on the unsigned values; provided by each lane in the wavefront.; Intrinsic takes a hint for reduction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.udot2 Provides direct access to v_dot2_u32_u16 across targets which; support such instructions. This performs unsigned dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output. llvm.amdgcn.udot4 Provides direct access to v_dot4_u32_u8 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.udot8 Provides direct access to v_dot8_u32_u4 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 8 4bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.sdot2 Provides direct access to v_dot2_i32_i16 across targets whi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:39042,perform,performed,39042,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"argets can define their own (and rely on built-in) ""pattern fragments"".; Pattern fragments are chunks of reusable patterns that get inlined into your; patterns during compiler-compile time. For example, the integer ""``(not; x)``"" operation is actually defined as a pattern fragment that expands as; ""``(xor x, -1)``"", since the SelectionDAG does not have a native '``not``'; operation. Targets can define their own short-hand fragments as they see fit.; See the definition of '``not``' and '``ineg``' for examples. * In addition to instructions, targets can specify arbitrary patterns that map; to one or more instructions using the 'Pat' class. For example, the PowerPC; has no way to load an arbitrary integer immediate into a register in one; instruction. To tell tblgen how to do this, it defines:. ::. // Arbitrary immediate support. Implement in terms of LIS/ORI.; def : Pat<(i32 imm:$imm),; (ORI (LIS (HI16 imm:$imm)), (LO16 imm:$imm))>;. If none of the single-instruction patterns for loading an immediate into a; register match, this will be used. This rule says ""match an arbitrary i32; immediate, turning it into an ``ORI`` ('or a 16-bit immediate') and an ``LIS``; ('load 16-bit immediate, where the immediate is shifted to the left 16 bits'); instruction"". To make this work, the ``LO16``/``HI16`` node transformations; are used to manipulate the input immediate (in this case, take the high or low; 16-bits of the immediate). * When using the 'Pat' class to map a pattern to an instruction that has one; or more complex operands (like e.g. `X86 addressing mode`_), the pattern may; either specify the operand as a whole using a ``ComplexPattern``, or else it; may specify the components of the complex operand separately. The latter is; done e.g. for pre-increment instructions by the PowerPC back end:. ::. def STWU : DForm_1<37, (outs ptr_rc:$ea_res), (ins GPRC:$rS, memri:$dst),; ""stwu $rS, $dst"", LdStStoreUpd, []>,; RegConstraint<""$dst.reg = $ea_res"">, NoEncode<""$ea_res"">;. def : P",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:50701,load,loading,50701,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['loading']
Performance,"args*. Pass all arguments specified after **--opt-args** to the invocation of **opt**. **--disable-{dce,simplifycfg}**. Do not run the specified passes to clean up and reduce the size of the test; program. By default, **bugpoint** uses these passes internally when attempting to; reduce test programs. If you're trying to find a bug in one of these passes,; **bugpoint** may crash. **--enable-valgrind**. Use valgrind to find faults in the optimization phase. This will allow; bugpoint to find otherwise asymptomatic problems caused by memory; mis-management. **-find-bugs**. Continually randomize the specified passes and run them on the test program; until a bug is found or the user kills **bugpoint**. **-help**. Print a summary of command line options. **--input** *filename*. Open *filename* and redirect the standard input of the test program, whenever; it runs, to come from that file. **--load** *plugin*. Load the dynamic object *plugin* into **bugpoint** itself. This object should; register new optimization passes. Once loaded, the object will add new command; line options to enable various optimizations. To see the new complete list of; optimizations, use the **-help** and **--load** options together; for example:. .. code-block:: bash. bugpoint --load myNewPass.so -help. **--mlimit** *megabytes*. Specifies an upper limit on memory usage of the optimization and codegen. Set; to zero to disable the limit. **--output** *filename*. Whenever the test program produces output on its standard output stream, it; should match the contents of *filename* (the ""reference output""). If you; do not use this option, **bugpoint** will attempt to generate a reference output; by compiling the program with the ""safe"" backend and running it. **--run-{int,jit,llc,custom}**. Whenever the test program is compiled, **bugpoint** should generate code for it; using the specified code generator. These options allow you to choose the; interpreter, the JIT compiler, the static native code compiler,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst:3405,optimiz,optimization,3405,interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,1,['optimiz'],['optimization']
Performance,"argument, N; return value function (where M is the number of values being; relocated + the original call arguments and N is the original return; value + each relocated value), but LLVM does not easily support such a; representation. Instead, the statepoint intrinsic marks the actual site of the; safepoint or statepoint. The statepoint returns a token value (which; exists only at compile time). To get back the original return value; of the call, we use the ``gc.result`` intrinsic. To get the relocation; of each pointer in turn, we use the ``gc.relocate`` intrinsic with the; appropriate index. Note that both the ``gc.relocate`` and ``gc.result`` are; tied to the statepoint. The combination forms a ""statepoint relocation; sequence"" and represents the entirety of a parseable call or 'statepoint'. When lowered, this example would generate the following x86 assembly:. .. code-block:: gas. 	 .globl	test1; 	 .align	16, 0x90; 	 pushq	%rax; 	 callq	foo; .Ltmp1:; 	 movq	(%rsp), %rax # This load is redundant (oops!); 	 popq	%rdx; 	 retq. Each of the potentially relocated values has been spilled to the; stack, and a record of that location has been recorded to the; :ref:`Stack Map section <stackmap-section>`. If the garbage collector; needs to update any of these pointers during the call, it knows; exactly what to change. The relevant parts of the StackMap section for our example are:. .. code-block:: gas. # This describes the call site; # Stack Maps: callsite 2882400000; 	 .quad	2882400000; 	 .long	.Ltmp1-test1; 	 .short	0; # .. 8 entries skipped ..; # This entry describes the spill slot which is directly addressable; # off RSP with offset 0. Given the value was spilled with a pushq,; # that makes sense.; # Stack Maps: Loc 8: Direct RSP [encoding: .byte 2, .byte 8, .short 7, .int 0]; 	 .byte	2; 	 .byte	8; 	 .short	7; 	 .long	0. This example was taken from the tests for the :ref:`RewriteStatepointsForGC`; utility pass. As such, its full StackMap can be easily examined with the; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:10274,load,load,10274,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['load'],['load']
Performance,"arguments are the same as they are in the; :ref:`@llvm.memmove <int_memmove>` intrinsic, with the added constraint that; ``len`` is required to be a positive integer multiple of the ``element_size``.; If ``len`` is not a positive integer multiple of ``element_size``, then the; behaviour of the intrinsic is undefined. ``element_size`` must be a compile-time constant positive power of two no; greater than a target-specific atomic access size limit. For each of the input pointers the ``align`` parameter attribute must be; specified. It must be a power of two no less than the ``element_size``. Caller; guarantees that both the source and destination pointers are aligned to that; boundary. Semantics:; """""""""""""""""""". The '``llvm.memmove.element.unordered.atomic.*``' intrinsic copies ``len`` bytes; of memory from the source location to the destination location. These locations; are allowed to overlap. The memory copy is performed as a sequence of load/store; operations where each access is guaranteed to be a multiple of ``element_size``; bytes wide and aligned at an ``element_size`` boundary. The order of the copy is unspecified. The same value may be read from the source; buffer many times, but only one write is issued to the destination buffer per; element. It is well defined to have concurrent reads and writes to both source; and destination provided those reads and writes are unordered atomic when; specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered loads from the source location and stores to the; destination. Lowering:; """""""""""""""""". In the most general case call to the; '``llvm.memmove.element.unordered.atomic.*``' is lowered to a call to the symbol; ``__llvm_memmove_element_unordered_atomic_*``. Where '*' is replaced with an; actual element size. See :ref:`RewriteStatepointsForGC intrinsic lowering; <RewriteStatepointsForGC_intrinsic_lowering>` for details on GC specific; lowering. The optimizer is allowe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:962094,perform,performed,962094,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['load', 'perform']","['load', 'performed']"
Performance,"arguments specified after **--gcc-tool-args** to the invocation of; **gcc**. **--opt-args** *opt args*. Pass all arguments specified after **--opt-args** to the invocation of **opt**. **--disable-{dce,simplifycfg}**. Do not run the specified passes to clean up and reduce the size of the test; program. By default, **bugpoint** uses these passes internally when attempting to; reduce test programs. If you're trying to find a bug in one of these passes,; **bugpoint** may crash. **--enable-valgrind**. Use valgrind to find faults in the optimization phase. This will allow; bugpoint to find otherwise asymptomatic problems caused by memory; mis-management. **-find-bugs**. Continually randomize the specified passes and run them on the test program; until a bug is found or the user kills **bugpoint**. **-help**. Print a summary of command line options. **--input** *filename*. Open *filename* and redirect the standard input of the test program, whenever; it runs, to come from that file. **--load** *plugin*. Load the dynamic object *plugin* into **bugpoint** itself. This object should; register new optimization passes. Once loaded, the object will add new command; line options to enable various optimizations. To see the new complete list of; optimizations, use the **-help** and **--load** options together; for example:. .. code-block:: bash. bugpoint --load myNewPass.so -help. **--mlimit** *megabytes*. Specifies an upper limit on memory usage of the optimization and codegen. Set; to zero to disable the limit. **--output** *filename*. Whenever the test program produces output on its standard output stream, it; should match the contents of *filename* (the ""reference output""). If you; do not use this option, **bugpoint** will attempt to generate a reference output; by compiling the program with the ""safe"" backend and running it. **--run-{int,jit,llc,custom}**. Whenever the test program is compiled, **bugpoint** should generate code for it; using the specified code generator. These ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst:3296,load,load,3296,interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,1,['load'],['load']
Performance,"ariable is set to a new; value. The first argument is the new value (wrapped as metadata). The second; argument is a `local variable <LangRef.html#dilocalvariable>`_ containing a; description of the variable. The third argument is a `complex expression; <LangRef.html#diexpression>`_. An `llvm.dbg.value` intrinsic describes the *value* of a source variable; directly, not its address. Note that the value operand of this intrinsic may; be indirect (i.e, a pointer to the source variable), provided that interpreting; the complex expression derives the direct value. ``llvm.dbg.assign``; ^^^^^^^^^^^^^^^^^^^; .. toctree::; :hidden:. AssignmentTracking. .. code-block:: llvm. void @llvm.dbg.assign(Value *Value,; DIExpression *ValueExpression,; DILocalVariable *Variable,; DIAssignID *ID,; Value *Address,; DIExpression *AddressExpression). This intrinsic marks the position in IR where a source assignment occurred. It; encodes the value of the variable. It references the store, if any, that; performs the assignment, and the destination address. The first three arguments are the same as for an ``llvm.dbg.value``. The fourth; argument is a ``DIAssignID`` used to reference a store. The fifth is the; destination of the store (wrapped as metadata), and the sixth is a `complex; expression <LangRef.html#diexpression>`_ that modifies it. The formal LLVM-IR signature is:. .. code-block:: llvm. void @llvm.dbg.assign(metadata, metadata, metadata, metadata, metadata, metadata). See :doc:`AssignmentTracking` for more info. Object lifetimes and scoping; ============================. In many languages, the local variables in functions can have their lifetimes or; scopes limited to a subset of a function. In the C family of languages, for; example, variables are only live (readable and writable) within the source; block that they are defined in. In functional languages, values are only; readable after they have been defined. Though this is a very obvious concept,; it is non-trivial to model in L",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:11316,perform,performs,11316,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['perform'],['performs']
Performance,"ariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriate; SSA form for the variable. ``loop-deletion``: Delete dead loops; ------------------------------------. This file implements the Dead Loop Deletion Pass. This pass is responsible for; eliminating loops with non-infinite computable trip counts that have no side; effects or volatile instructions, and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24388,load,load,24388,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['load']
Performance,"aries and headers on which it depends. The section `Modularizing a Platform`_ describes the steps one must take to write these module maps. One can use module maps without modules to check the integrity of the use of header files. To do this, use the ``-fimplicit-module-maps`` option instead of the ``-fmodules`` option, or use ``-fmodule-map-file=`` option to explicitly specify the module map files to load. Compilation model; -----------------; The binary representation of modules is automatically generated by the compiler on an as-needed basis. When a module is imported (e.g., by an ``#include`` of one of the module's headers), the compiler will spawn a second instance of itself [#]_, with a fresh preprocessing context [#]_, to parse just the headers in that module. The resulting Abstract Syntax Tree (AST) is then persisted into the binary representation of the module that is then loaded into translation unit where the module import was encountered. The binary representation of modules is persisted in the *module cache*. Imports of a module will first query the module cache and, if a binary representation of the required module is already available, will load that representation directly. Thus, a module's headers will only be parsed once per language configuration, rather than once per translation unit that uses the module. Modules maintain references to each of the headers that were part of the module build. If any of those headers changes, or if any of the modules on which a module depends change, then the module will be (automatically) recompiled. The process should never require any user intervention. Command-line parameters; -----------------------; ``-fmodules``; Enable the modules feature. ``-fbuiltin-module-map``; Load the Clang builtins module map file. (Equivalent to ``-fmodule-map-file=<resource dir>/include/module.modulemap``). ``-fimplicit-module-maps``; Enable implicit search for module map files named ``module.modulemap`` and similar. This option is i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:13722,cache,cache,13722,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache']
Performance,aries}); ROOT_ADD_TEST(TMVA-DNN-Loss-Functions-Cpu COMMAND testLossFunctionsCpu). # DNN - Derivatives CPU; ROOT_EXECUTABLE(testDerivativesCpu TestDerivativesCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Derivatives-Cpu COMMAND testDerivativesCpu). # DNN - Backpropagation CPU; ROOT_EXECUTABLE(testBackpropagationCpu TestBackpropagationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-Cpu COMMAND testBackpropagationCpu). # DNN - BackpropagationDL CPU; ROOT_EXECUTABLE(testBackpropagationDLCpu TestBackpropagationDLCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-DL-Cpu COMMAND testBackpropagationDLCpu). # DNN - Batch normalization; ROOT_EXECUTABLE(testBatchNormalizationCpu TestBatchNormalizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cpu COMMAND testBatchNormalizationCpu). # DNN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RM,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:5501,Optimiz,Optimization-Cpu,5501,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cpu']
Performance,"arly, the annotation may apply to type of a function; parameter declaration which precedes the parameter count in the same function.; This means parsing the argument of bounds annotations must be done after the; parser has the whole context of a struct or a function declaration. Clang has; late parsing logic for C++ declaration attributes that require late parsing,; while the C declaration attributes and C/C++ type attributes do not have the; same logic. This requires introducing late parsing logic for C/C++ type; attributes. Internal bounds annotations; ===========================. ``__indexable`` and ``__bidi_indexable`` alter pointer representations to be; equivalent to a struct with the pointer and the corresponding bounds fields.; Despite this difference in their representations, they are still pointers in; terms of types of operations that are allowed and their semantics. For instance,; a pointer dereference on a ``__bidi_indexable`` pointer will return the; dereferenced value same as plain C pointers, modulo the extra bounds checks; being performed before dereferencing the wide pointer. This means mapping the; wide pointers to struct types with equivalent layout wont be sufficient. To; represent the wide pointers in Clang AST, we add an extra field in the; PointerType class to indicate the internal bounds of the pointer. This ensures; pointers of different representations are mapped to different canonical types; while they are still treated as pointers. In LLVM IR, wide pointers will be emitted as structs of equivalent; representations. Clang CodeGen will handle them as Aggregate in; ``TypeEvaluationKind (TEK)``. ``AggExprEmitter`` was extended to handle pointer; operations returning wide pointers. Alternatively, a new ``TEK`` and an; expression emitter dedicated to wide pointers could be introduced. Default bounds annotations; ==========================. The model may implicitly add ``__bidi_indexable`` or ``__single`` depending on; the context of the decla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst:2662,perform,performed,2662,interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,1,['perform'],['performed']
Performance,"arrying information. .. code-block:: llvm. @glb = global i8 0. define void @f(ptr %a) {; %c = icmp eq ptr %a, @glb; br i1 %c, label %BB_EXIT, label %BB_CONTINUE ; escapes %a; BB_EXIT:; call void @exit(); unreachable; BB_CONTINUE:; ret void; }. 4. The pointer is used in a volatile access as its address. .. _volatile:. Volatile Memory Accesses; ------------------------. Certain memory accesses, such as :ref:`load <i_load>`'s,; :ref:`store <i_store>`'s, and :ref:`llvm.memcpy <int_memcpy>`'s may be; marked ``volatile``. The optimizers must not change the number of; volatile operations or change their order of execution relative to other; volatile operations. The optimizers *may* change the order of volatile; operations relative to non-volatile operations. This is not Java's; ""volatile"" and has no cross-thread synchronization behavior. A volatile load or store may have additional target-specific semantics.; Any volatile operation can have side effects, and any volatile operation; can read and/or modify state which is not accessible via a regular load; or store in this module. Volatile operations may use addresses which do; not point to memory (like MMIO registers). This means the compiler may; not use a volatile operation to prove a non-volatile access to that; address has defined behavior. The allowed side-effects for volatile accesses are limited. If a; non-volatile store to a given address would be legal, a volatile; operation may modify the memory at that address. A volatile operation; may not modify any other memory accessible by the module being compiled.; A volatile operation may not call any code in the current module. In general (without target specific context), the address space of a; volatile operation may not be changed. Different address spaces may; have different trapping behavior when dereferencing an invalid; pointer. The compiler may assume execution will continue after a volatile operation,; so operations which modify memory or may have undefined behavi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:146516,load,load,146516,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"art of clang as part of a continuous integration; framework, ensuring updates to the compiler that reduce CFI coverage on; indirect control flow instructions are identified. Background; ==========. This tool will continuously validate that CFI directives are properly; implemented around all indirect control flows by analysing the output machine; code. The analysis of machine code is important as it ensures that any bugs; present in linker or compiler do not subvert CFI protections in the final; shipped binary. Unprotected indirect control flow instructions will be flagged for manual; review. These unexpected control flows may simply have not been accounted for in; the compiler implementation of CFI (e.g. indirect jumps to facilitate switch; statements may not be fully protected). It may be possible in the future to extend this tool to flag unnecessary CFI; directives (e.g. CFI directives around a static call to a non-polymorphic base; type). This type of directive has no security implications, but may present; performance impacts. Design Ideas; ============. This tool will disassemble binaries and DSO's from their machine code format and; analyse the disassembled machine code. The tool will inspect virtual calls and; indirect function calls. This tool will also inspect indirect jumps, as inlined; functions and jump tables should also be subject to CFI protections. Non-virtual; calls (``-fsanitize=cfi-nvcall``) and cast checks (``-fsanitize=cfi-*cast*``); are not implemented due to a lack of information provided by the bytecode. The tool would operate by searching for indirect control flow instructions in; the disassembly. A control flow graph would be generated from a small buffer of; the instructions surrounding the 'target' control flow instruction. If the; target instruction is branched-to, the fallthrough of the branch should be the; CFI trap (on x86, this is a ``ud2`` instruction). If the target instruction is; the fallthrough (i.e. immediately succeeds) of a co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CFIVerify.rst:2174,perform,performance,2174,interpreter/llvm-project/llvm/docs/CFIVerify.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CFIVerify.rst,1,['perform'],['performance']
Performance,"articularly on older hardware. Comparing baseline results with this mode; enabled can help determine the effects of the frontend and can be used to; improve latency and throughput estimates. .. option:: --repetition-mode=[duplicate|loop|min]. Specify the repetition mode. `duplicate` will create a large, straight line; basic block with `num-repetitions` instructions (repeating the snippet; `num-repetitions`/`snippet size` times). `loop` will, optionally, duplicate the; snippet until the loop body contains at least `loop-body-size` instructions,; and then wrap the result in a loop which will execute `num-repetitions`; instructions (thus, again, repeating the snippet; `num-repetitions`/`snippet size` times). The `loop` mode, especially with loop; unrolling tends to better hide the effects of the CPU frontend on architectures; that cache decoded instructions, but consumes a register for counting; iterations. If performing an analysis over many opcodes, it may be best to; instead use the `min` mode, which will run each other mode,; and produce the minimal measured result. .. option:: --num-repetitions=<Number of repetitions>. Specify the target number of executed instructions. Note that the actual; repetition count of the snippet will be `num-repetitions`/`snippet size`.; Higher values lead to more accurate measurements but lengthen the benchmark. .. option:: --loop-body-size=<Preferred loop body size>. Only effective for `-repetition-mode=[loop|min]`.; Instead of looping over the snippet directly, first duplicate it so that the; loop body contains at least this many instructions. This potentially results; in loop body being cached in the CPU Op Cache / Loop Cache, which allows to; which may have higher throughput than the CPU decoders. .. option:: --max-configs-per-opcode=<value>. Specify the maximum configurations that can be generated for each opcode.; By default this is `1`, meaning that we assume that a single measurement is; enough to characterize an opcode. This m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:12998,perform,performing,12998,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['perform'],['performing']
Performance,"as de facto disactivating the cache; the; other was causing a std::bad_alloc exception to be thrown on workers; when opening a remote file after a local one.  ; Fix several problems in TChain::Draw including. drawing into an existing histogram, i.e.; chain->Draw(""var>>myhist"");. treatment of histogram merging in case of small; statistics, i.e. when; the autobinning is not or only partially active;. usage of existing canvases when different histogram; names are specified;. Fix a problem causing a duplication of the final feedback; object. Fix problem with determining the subdir name in; TFileMerger::MergeRecursive on Windows; Make sure that the default sandbox is under $HOME/.proof; Fix a problem with dataset validation in multi-level; master setups; Fix a problem with ordinal numbers in multi-master setups; Fix a problem with defining the internal paths for; executables when configuring with '--prefix'; Fix backward-incompatibility issue giving the error; message ""unknown action code: 5112""; Fix a few problems with file retrieval from the cache; Fix a problem with iteration of a std::list occasionally; causing seg-violations in TXSocket; Fix a few problems preventing correct usage of entry; lists in PROOF; Fix a problem with the permissions of the credentials; files created under <sandbox>/.creds; Fix a potential problem while determining the log paths; in log retrieval. Do not use vnsprintf in the XrdProofd plug-in, potential; source of deadlocks.; Fix a problem overwriting the local environment settings; for the xrootd sec modules; In XrdProofdProofServMgr::Destroy, fix segv in message; creation when all sessions are destroyed at once; Fix a problem determining the relative time order of old; sessions for log retrieval; In TProof::HandleInputMessage, fix possible double delete; after kPROOF_STOPPROCESS; Fix a couple of issues on reconnection to a running; session (some dialog buttons not in the correct state; logs not; correctly redirected); Fix a problem creati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html:9777,cache,cache,9777,proof/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html,2,['cache'],['cache']
Performance,"as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visible when building later submodules in that module. This can lead to fragile modules that depend on the build order used for the submodules of the module, and should not be relied upon. This behavior is subject to change. As an example, in C, this implies that if two structs are defined in different submodules with the same name, those two types are distinct types (but may be *compatible* types if their definitions match). In C++, two structs defined with the same name in different submodules are the *same* type, and must be equivalent under C++'s One Definition Rule. .. note::. Clang currently only performs minimal checking for violations of the One Definition Rule. If any submodule of a module is imported into any part of a program, the entire top-level module is considered to be part of the program. As a consequence of this, Clang may diagnose conflicts between an entity declared in an unimported submodule and an entity declared in the current translation unit, and Clang may inline or devirtualize based on knowledge from unimported submodules. Macros; ------. The C and C++ preprocessor assumes that the input text is a single linear buffer, but with modules this is not the case. It is possible to import two modules that have conflicting definitions for a macro (or where one ``#define``\s a macro and the other ``#undef``\ines it). The rules for handling macro definitions in the presence of modules are as follows:. * Each definition and undefinition of a macro is considered to be a distinct entity.; * Such entities are *visible* if they are from the current submodule or translation unit, or if they were",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:25114,perform,performs,25114,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['perform'],['performs']
Performance,"as pointers. If you really need this functionality, you can do the arithmetic with explicit; integer instructions, and use inttoptr to convert the result to an address. Most; of GEP's special aliasing rules do not apply to pointers computed from ptrtoint,; arithmetic, and inttoptr sequences. Can I compute the distance between two objects, and add that value to one address to compute the other address?; ---------------------------------------------------------------------------------------------------------------. As with arithmetic on null, you can use GEP to compute an address that way, but; you can't use that pointer to actually access the object if you do, unless the; object is managed outside of LLVM. Also as above, ptrtoint and inttoptr provide an alternative way to do this which; do not have this restriction. Can I do type-based alias analysis on LLVM IR?; ----------------------------------------------. You can't do type-based alias analysis using LLVM's built-in type system,; because LLVM has no restrictions on mixing types in addressing, loads or stores. LLVM's type-based alias analysis pass uses metadata to describe a different type; system (such as the C type system), and performs type-based aliasing on top of; that. Further details are in the; `language reference <LangRef.html#tbaa-metadata>`_. What happens if a GEP computation overflows?; --------------------------------------------. If the GEP lacks the ``inbounds`` keyword, the value is the result from; evaluating the implied two's complement integer computation. However, since; there's no guarantee of where an object will be allocated in the address space,; such values have limited meaning. If the GEP has the ``inbounds`` keyword, the result value is ``poison``; if the GEP overflows (i.e. wraps around the end of the address space). As such, there are some ramifications of this for inbounds GEPs: scales implied; by array/vector/pointer indices are always known to be ""nsw"" since they are; signed values ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:17056,load,loads,17056,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['loads']
Performance,"as the llvm.amdgcn.is.shared, llvm.amdgcn.is.private, llvm.trap, and; llvm.debug intrinsics. ""amdgpu-no-hostcall-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to the hostcall buffer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-heap-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to an initialized memory buffer; that conforms to the requirements of the malloc/free device library V1; version implementation. If this attribute is absent, then the; amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-multigrid-sync-arg"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the multigrid synchronization pointer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-default-queue"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the default queue pointer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-completion-action"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the completion action pointer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-lds-size""=""min[,max]"" Min is the minimum number of bytes that will be allocated in the Local; Data Store at address zero. Variables are allocated within this frame; using absolute symbol metadata, primarily by the AMDGPULowerModuleLDS; pass. Optional max is the maximum number of bytes that will be allocated.; Note that min==max indicates that no further variables can be added to; the frame. This is an internal detail of how LDS variables are lowered,; language front ends should not set this attribute. ======================================= ==============================================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:50890,queue,queue,50890,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],['queue']
Performance,"as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 -1, i32 -1, i32 -1, i32 -1>; %reduction = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %masked.a); %also.r = and i32 %reduction, %start. .. _int_vp_reduce_or:. '``llvm.vp.reduce.or.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.or.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.or.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``OR`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.or``' intrinsic performs the integer ``OR`` reduction; (:ref:`llvm.vector.reduce.or <int_vector_reduce_or>`) of the vector operand; ``val`` on each enabled lane, performing an '``or``' of that with the scalar; ``start_value``. Disabled lanes are treated as containing the neutral value; ``0`` (i.e. having no effect on the reduction operation). If the vector length; is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.or.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:759418,perform,performed,759418,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"as well as `CommitCluster()` and `FlushCluster()`.; For `FlushColumns()` and `FillNoFlush()`, the sequential writer assumes exclusive access only if buffered writing is turned off.; The parallel writer assumes exclusive access to the underlying file during all operations on the writer (e.g. construction and destruction) and all operations on any created fill context (e.g. `Fill()` and `FlushCluster()`).; Notable exceptions are `FlushColumns()` and `FillNoFlush()` which are guaranteed to never access the underlying `TFile` during parallel writing (which is always buffered). A `TFile` does not take ownership of any `RNTuple` objects. When reading data, RNTuple uses the `RMiniFile` and `RRawFile` classes to open a given storage path and find the `RNTuple` anchor.; When creating a `RNTupleReader` from an existing anchor object, RNTuple uses `RRawFile` only for files of dynamic type `TFile`, `TDavixFile`, and `TNetXNGFile`.; In either case, the `RRawFile` owns its own file descriptor and does not interfere with `TFile` objects concurrently reading the file.; For anchors from files of other dynamic type, including all other `TFile` subclasses, the file is wrapped in a `RRawFileTFile` and access is shared. On-Disk Encoding; ----------------. ### Writing Case; The following steps are taken to write RNTuple data to disk:. 1. On creation of the RNTupleWriter, the header is written to disk; 2. Upon `RNTupleWriter::Fill()`, the RField<T> class _serializes_ the object into its column representation.; To this end, it uses the `RColumn` class to append elements to the columns page buffer (`RPage`); 3. When a page buffer is full (cf. tuning.md), it is sent to the page sink for writing it to disk.; Note that page boundaries do _not_ need to align with entry boundaries,; e.g. information from a single entry can span multiple pages.; 1. The page is _packed_:; depending on the type of the page, a light encoding is applied to facilitate compression, e.g., byte splitting (`RColumnElement",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:18935,concurren,concurrently,18935,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['concurren'],['concurrently']
Performance,"ase classes.; - Added a TStatusBitsChecker to avoid Status Bits overlap in class hierarchy deriving from TObject (and resolved a handful of conflicts).; - Introduced support for type safe range-for-loop for ROOT collection. The typical use is:. ```; for(auto bcl : TRangeDynCast<TBaseClass>( * cl->GetListOfBases() )) {; if (!bcl) continue;; ... use bcl as a TBaseClass*; }; for(auto bcl : TRangeDynCast<TBaseClass>( cl->GetListOfBases() )) {; if (!bcl) continue;; ... use bcl as a TBaseClass*; }; ```; - ClassDefInline has been enhanced even for some compiled class (without a dictionary). ClassDefInline can still not be used for class template instance using Double32_t or Float16_t as a template parameter or for class or class template that do not have a public default constructor.; - ROOT's backport of `std::string_view` has been updated to follow what's available in C++17, notably its `to_string` member function has been removed. ### Thread safety. Resolved the race conditions inherent to the use of the RecursiveRemove mechanism. - Introduced ```ROOT::TReentrantRWLock```, an implementation of a reentrant read-write lock with a configurable internal mutex/lock and a condition variable to synchronize readers and writers when necessary. The implementation allows a single reader to take the write lock without releasing the reader lock. It also allows the writer to take a read lock. In other word, the lock is re-entrant for both reading and writing. The implementation tries to make faster the scenario when readers come and go but there is no writer. In that case, readers will not pay the price of taking the internal lock.; Moreover, this RW lock tries to be fair with writers, giving them the possibility to claim the lock and wait for only the remaining readers, thus preventing starvation. - Switched the ROOT global to be a ```ROOT::TReentrantRWLock``` and renamed it ROOT::gCoreMutex. The old name ```gROOTMutex``` and ```gInterpreterMutex``` are deprecated and may be removed ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:4314,race condition,race conditions,4314,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['race condition'],['race conditions']
Performance,"ase structures with; a single node: A sequential execution of basic blocks, acyclic conditional; branches (or switches), and a basic block looping on itself.; `Wikipedia <https://en.wikipedia.org/wiki/Control-flow_graph#Reducibility>`_; has a more formal definition, which basically says that every cycle has; a dominating header. * Irreducible control-flow can occur at any level of the loop nesting.; That is, a loop that itself does not contain any loops can still have; cyclic control flow in its body; a loop that is not nested inside; another loop can still be part of an outer cycle; and there can be; additional cycles between any two loops where one is contained in the other.; However, an LLVM :ref:`cycle<cycle-terminology>` covers both, loops and; irreducible control flow. * The `FixIrreducible <https://llvm.org/doxygen/FixIrreducible_8h.html>`_; pass can transform irreducible control flow into loops by inserting; new loop headers. It is not included in any default optimization pass; pipeline, but is required for some back-end targets. * Exiting edges are not the only way to break out of a loop. Other; possibilities are unreachable terminators, [[noreturn]] functions,; exceptions, signals, and your computer's power button. * A basic block ""inside"" the loop that does not have a path back to the; loop (i.e. to a latch or header) is not considered part of the loop.; This is illustrated by the following code. .. code-block:: C. for (unsigned i = 0; i <= n; ++i) {; if (c1) {; // When reaching this block, we will have exited the loop.; do_something();; break;; }; if (c2) {; // abort(), never returns, so we have exited the loop.; abort();; }; if (c3) {; // The unreachable allows the compiler to assume that this will not rejoin the loop.; do_something();; __builtin_unreachable();; }; if (c4) {; // This statically infinite loop is not nested because control-flow will not continue with the for-loop.; while(true) {; do_something();; }; }; }. * There is no requirement for the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:5727,optimiz,optimization,5727,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimization']
Performance,"ased. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:268184,load,load,268184,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ased. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:272023,load,load,272023,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"asic block). cmpGEP; ------; Compares two GEPs (``getelementptr`` instructions). It differs from regular operations comparison with the only thing: possibility; to use ``accumulateConstantOffset`` method. So, if we get constant offset for both left and right *GEPs*, then compare it as; numbers, and return comparison result. Otherwise treat it like a regular operation (see previous paragraph). cmpOperation; ------------; Compares instruction opcodes and some important operation properties. 1. Compare opcodes, if it differs return the result. 2. Compare number of operands. If it differs  return the result. 3. Compare operation types, use *cmpType*. All the same  if types are; different, return result. 4. Compare *subclassOptionalData*, get it with ``getRawSubclassOptionalData``; method, and compare it like a numbers. 5. Compare operand types. 6. For some particular instructions, check equivalence (relation in our case) of; some significant attributes. For example, we have to compare alignment for; ``load`` instructions. O(log(N)); ---------; Methods described above implement order relationship. And latter, could be used; for nodes comparison in a binary tree. So we can organize functions set into; the binary tree and reduce the cost of lookup procedure from; O(N*N) to O(log(N)). Merging process, mergeTwoFunctions; ==================================; Once *MergeFunctions* detected that current function (*G*) is equal to one that; were analyzed before (function *F*) it calls ``mergeTwoFunctions(Function*,; Function*)``. Operation affects ``FnTree`` contents with next way: *F* will stay in; ``FnTree``. *G* being equal to *F* will not be added to ``FnTree``. Calls of; *G* would be replaced with something else. It changes bodies of callers. So,; functions that calls *G* would be put into ``Deferred`` set and removed from; ``FnTree``, and analyzed again. The approach is next:. 1. Most wished case: when we can use alias and both of *F* and *G* are weak. We; make both of th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst:25724,load,load,25724,interpreter/llvm-project/llvm/docs/MergeFunctions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MergeFunctions.rst,1,['load'],['load']
Performance,"asic support for hidden enums; * Support function pointer returns and optimize function point variables; * Fix reuse of CPPOverload proxies in vector calls from different threads; * Use `-march=native` instead of checking the cpu for avx; * Workaround for handling exceptions from JITed code on ARM; * Drop ``from cppyy.interactive import *`` from CPython 3.11; * Fix regression in converting `std::vector<T*` to `list`; * Update to the latest patch version of Cling (from 6.26.04). 2022-04-03: 2.3.1; -----------------; * Use portable type Py_ssize_t instead of ssize_t. 2022-03-08: 2.3.0; -----------------. * CUDA support (up to version 10.2); * Allow `std::string_view<char>` initialization from Python `str` (copies); * Provide access to extern ""C"" declared functions in namespaces; * Support for (multiple and nested) anonymous structs; * Pull forward upstream patch for PPC; * Only apply system_dirs patch (for asan) on Linux; * Add not-yet loaded classes to namespaces in dir(); * Fix lookup of templates of function with template args; * Fix lookup of templates types with << in name; * Fix regression for accessing `char16_t` data member arrays; * Add custom `__reshape__` method to CPPInstance to allow array cast; * Prioritize callee exceptions over bindings exceptions; * Prevent infinite recursion when instantiating class with no constructors. 2021-11-14: 2.2.0; -----------------. * Migrated repos to github/wlav; * Properly resolve enum type of class enums; * Get proper shape of ``void*`` and enum arrays; * Fix access to (const) ref data members; * Fix sometimes PCH uninstall issue; * Fix argument passing of fixed arrays of pointers; * Include all gcc system paths (for asan); * Initial support for Apple M1. 2021-07-17: 2.1.0; -----------------. * Support for vector calls with CPython 3.8 and newer; * Support for typed C++ literals as defaults when mixing with keywords; * Enable reshaping of multi-dim LowLevelViews; * Refactored multi-dim arrays and support for multi-dim ass",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst:4732,load,loaded,4732,bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,1,['load'],['loaded']
Performance,"ask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.ptrtoint``' intrinsic takes a value to cast as its first operand; , which must be a vector of pointers, and a type to cast it to return type,; which must be a vector of :ref:`integer <t_integer>` type.; The second operand is the vector mask. The return type, the value to cast, and; the vector mask have the same number of elements.; The third operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.ptrtoint``' intrinsic converts value to return type by; interpreting the pointer value as an integer and either truncating or zero; extending that value to the size of the integer type.; If ``value`` is smaller than return type, then a zero extension is done. If; ``value`` is larger than return type, then a truncation is done. If they are; the same size, then nothing is done (*no-op cast*) other than a type; change.; The conversion is performed on lane positions below the explicit vector length; and where the vector mask is true. Masked-off lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i8> @llvm.vp.ptrtoint.v4i8.v4p0i32(<4 x ptr> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = ptrtoint <4 x ptr> %a to <4 x i8>; %also.r = select <4 x i1> %mask, <4 x i8> %t, <4 x i8> poison. .. _int_vp_inttoptr:. '``llvm.vp.inttoptr.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x ptr> @llvm.vp.inttoptr.v16p0.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x ptr> @llvm.vp.inttoptr.nxv4p0.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x ptr> @llvm.vp.inttoptr.v256p0.v256i32 (<256 x i32> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.inttoptr``' intrinsic converts its integer value to the poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:814397,perform,performed,814397,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ask>, i32 <vector_length>); declare <256 x double> @llvm.vp.maximum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point maximum of two vectors of floating-point values,; propagating NaNs and treating -0.0 as less than +0.0. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.maximum``' intrinsic performs floating-point maximum (:ref:`maximum <i_maximum>`); of the first and second vector operand on each enabled lane, the result being ; NaN if either operand is a NaN. -0.0 is considered to be less than +0.0 for this; intrinsic. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. ; The operation is performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.maximum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.maximum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fadd:. '``llvm.vp.fadd.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fadd.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fadd.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fadd.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:731789,perform,performed,731789,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ask>, i32 <vector_length>); declare <256 x double> @llvm.vp.minimum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point minimum of two vectors of floating-point values,; propagating NaNs and treating -0.0 as less than +0.0. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.minimum``' intrinsic performs floating-point minimum (:ref:`minimum <i_minimum>`); of the first and second vector operand on each enabled lane, the result being ; NaN if either operand is a NaN. -0.0 is considered to be less than +0.0 for this; intrinsic. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. ; The operation is performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.minimum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.minimum.v4f32(<4 x float> %a, <4 x float> %b); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_maximum:. '``llvm.vp.maximum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.maximum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maximum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.maximum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:729968,perform,performed,729968,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fmul.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fmul.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point multiplication of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fmul``' intrinsic performs floating-point multiplication (:ref:`fmul <i_fmul>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fmul.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fmul <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fdiv:. '``llvm.vp.fdiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fdiv.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fdiv.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fdiv.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point division of two vectors ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:736633,perform,performed,736633,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"askell Compiler (GHC) <http://www.haskell.org/ghc>`_.; It passes everything in registers, going to extremes to achieve this; by disabling callee save registers. This calling convention should; not be used lightly but only for specific situations such as an; alternative to the *register pinning* performance technique often; used when implementing functional programming languages. At the; moment only X86, AArch64, and RISCV support this convention. The ; following limitations exist:. - On *X86-32* only up to 4 bit type parameters are supported. No; floating-point types are supported.; - On *X86-64* only up to 10 bit type parameters and 6; floating-point parameters are supported.; - On *AArch64* only up to 4 32-bit floating-point parameters,; 4 64-bit floating-point parameters, and 10 bit type parameters; are supported.; - *RISCV64* only supports up to 11 bit type parameters, 4; 32-bit floating-point parameters, and 4 64-bit floating-point; parameters. This calling convention supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; both the caller and callee are using it.; ""``cc 11``"" - The HiPE calling convention; This calling convention has been implemented specifically for use by; the `High-Performance Erlang; (HiPE) <http://www.it.uu.se/research/group/hipe/>`_ compiler, *the*; native code compiler of the `Ericsson's Open Source Erlang/OTP; system <http://www.erlang.org/download.shtml>`_. It uses more; registers for argument passing than the ordinary C calling; convention and defines no callee-saved registers. The calling; convention properly supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; that both the caller and the callee use it. It uses a *register pinning*; mechanism, similar to GHC's convention, for keeping frequently; accessed runtime components pinned to specific hardware registers.; At the moment only X86 supports this convention (both 32 and 64; bit).; ""``anyregcc``"" - Dynam",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:14740,optimiz,optimization,14740,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"ass Stuff {}; def thing : Stuff {}; ```. ------------- Classes -----------------; class Stuff {; }; ------------- Defs -----------------; def thing {	// Stuff; }. By default cells are connected. Meaning that we cache the code and magic directives from the previously run cells. This means that the next cell still sees the `Stuff` class. ```tablegen; def other_thing : Stuff {}; ```. ------------- Classes -----------------; class Stuff {; }; ------------- Defs -----------------; def other_thing {	// Stuff; }; def thing {	// Stuff; }. You can use the magic `%reset` to clear this cache and start fresh. ```tablegen; %reset; def other_thing : Stuff {}; ```. <stdin>:1:19: error: Couldn't find class 'Stuff'; def other_thing : Stuff {}; ^. You can also configure the default reset behaviour using the `%config` magic. ```tablegen; %config cellreset on; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; // The cache is reset here so this is an error.; def AThing: Thing {}; ```. <stdin>:2:13: error: Couldn't find class 'Thing'; def AThing: Thing {}; ^. The default value is `off`, meaning cells are connected. If you want to override the default for one cell only, use the `%reset` or `%noreset` magic. These always override the default. ```tablegen; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; %noreset; // This works because of the noreset above.; def AThing: Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------; def AThing {	// Thing; }. ```tablegen; // This does not because we're not changing the default.; def AnotherThing: Thing {}; ```. <stdin>:2:19: error: Couldn't find class 'Thing'; def AnotherThing: Thing {}; ^. ```tablegen; %config cellreset off; %reset; // Here we have an empty cache and default reset behaviour.; ```. ------------- Classes ---------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md:1438,cache,cache,1438,interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,1,['cache'],['cache']
Performance,"ass of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Formula No (0)  The discrimination formula. ParRanges No ()  Parameter ranges. FitMethod No MINUIT MC, GA, SA, MINUIT Optimisation Method. Converger No None None, MINUIT FitMethod uses Converger to improve result. Configuration options for MVA method :. Configuration options reference for MVA method: LD. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: SVM. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before trainin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:6632,perform,performed,6632,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"ass(ID) {}. bool runOnFunction(Function &F) override {; errs() << ""Hello: "";; errs().write_escaped(F.getName()) << '\n';; return false;; }; }; // end of struct Hello; } // end of anonymous namespace. char Hello::ID = 0;; static RegisterPass<Hello> X(""hello"", ""Hello World Pass"",; false /* Only looks at CFG */,; false /* Analysis Pass */);. Now that it's all together, compile the file with a simple ""``gmake``"" command; from the top level of your build directory and you should get a new file; ""``lib/LLVMHello.so``"". Note that everything in this file is; contained in an anonymous namespace --- this reflects the fact that passes; are self contained units that do not need external interfaces (although they; can have them) to be useful. Running a pass with ``opt``; ---------------------------. Now that you have a brand new shiny shared object file, we can use the; :program:`opt` command to run an LLVM program through your pass. Because you; registered your pass with ``RegisterPass``, you will be able to use the; :program:`opt` tool to access it, once loaded. To test it, follow the example at the end of the :doc:`GettingStarted` to; compile ""Hello World"" to LLVM. We can now run the bitcode file (hello.bc) for; the program through our transformation like this (or course, any bitcode file; will work):. .. code-block:: console. $ opt -load lib/LLVMHello.so -hello < hello.bc > /dev/null; Hello: __main; Hello: puts; Hello: main. The :option:`-load` option specifies that :program:`opt` should load your pass; as a shared object, which makes ""``-hello``"" a valid command line argument; (which is one reason you need to :ref:`register your pass; <writing-an-llvm-pass-registration>`). Because the Hello pass does not modify; the program in any interesting way, we just throw away the result of; :program:`opt` (sending it to ``/dev/null``). To see what happened to the other string you registered, try running; :program:`opt` with the :option:`-help` option:. .. code-block:: console. $ opt -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:7963,load,loaded,7963,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['loaded']
Performance,"assStreamer` bit is set, the automatically generated; `Streamer `can call directly the method **`TClass::WriteBuffer`**.; Bypassing the `Streamer` improves the performance when writing/reading; the objects in the **`TClonesArray`**. However, the drawback is when a; **`TClonesArray`** is written with `split=0` bypassing the `Streamer`,; the `StreamerInfo `of the class in the array being optimized, one cannot; later use the **`TClonesArray`** with `split > 0`. For example, there is; a problem with the following scenario: a class `Foo` has a; **`TClonesArray`** of `Bar` objects the `Foo` object is written with; `split=0` to `Tree` `T1`. In this case the `StreamerInfo` for the class; `Bar` is created in optimized mode in such a way that data members of; the same type are written as an array improving the I/O performance. In; a new program, `T1` is read and a new `Tree` `T2` is created with the; object `Foo` in `split > 1`. When the `T2 `branch is created, the `StreamerInfo` for the class `Bar`; is created with no optimization (mandatory for the split mode). The; optimized Bar `StreamerInfo` is going to be used to read the; **`TClonesArray`** in `T1`. The result will be `Bar` objects with data; member values not in the right sequence. The solution to this problem is; to call `BypassStreamer(kFALSE)` for the **`TClonesArray`**. In this; case, the normal `Bar::Streamer` function will be called. The; `Bar::Streamer` function works OK independently if the `Bar`; `StreamerInfo `had been generated in optimized mode or not. ## Pointers and References in Persistency. An object pointer as a data member presents a challenge to the streaming; software. If the object pointed to is saved every time, it could create; circular dependencies and consume a large amount of disk space. The; network of references must be preserved on disk and recreated upon; reading the file. If you use independent I/O operations for pointers and their referenced; objects you can use the **`TRef`** class. Lat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:54895,optimiz,optimization,54895,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['optimiz'],['optimization']
Performance,"associated **`TClass`**:. ``` {.cpp}; myobject.IsA().Destructor(myobject); ```. which will send out the deletion notification to the system (thus you do; not need to care anymore at this point about Python reference counting,; the object will go, even if it's reference count it non-zero), and free; the memory. ### Performance. The performance of `PyROOT` when programming with ROOT in Python is; similar to that of Cling. Differences occur mainly because of differences; in the respective languages: C++ is much harder to parse, but once; parsed, it is much easier to optimize. Consequently, individual calls to; ROOT are typically faster from `PyROOT`, whereas loops are typically; slower. When programming in Python, the modus operandi is to consider; performance generally ""good enough"" on the outset, and when it turns out; that, it is not good enough; the performance critical part is converted; into C/C++ in an extension module. The school of thought where; pre-mature optimization is the root of all evil should find this way of; working very satisfying. In addition, if you look at their history, you; will see that many of the standard Python modules have followed this; path. Your code should always make maximum use of ROOT facilities; such that; most of the time is spending in compiled code. This goes even for very; simple things: e.g. do not compute invariant masses in Python, use; **`TLorentzVector`** instead. Moreover, before you start optimizing,; make sure that you have run a profiler to find out where the bottlenecks; are. Some performance, without cost in terms of programmer effort, may; be gained by using `psyco`, see the next link:; <http://psyco.sourceforge.net>, a Python just in time compiler (JIT).; Note, however, that `psyco` is limited to Intel i386 CPUs. Since `psyco`; optimizes Python, not `PyROOT` calls; it generally does not improve; performance that much if most of your code consists of ROOT API calls.; Mathematical computations in Python, on the other ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:22976,optimiz,optimization,22976,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['optimiz'],['optimization']
Performance,"assume(i1 true) [""align""(ptr %val, i32 %align)]. If the operand bundle value violates any requirements on the attribute value,; the behavior is undefined, unless one of the following exceptions applies:. * ``""align""`` operand bundles may specify a non-power-of-two alignment; (including a zero alignment). If this is the case, then the pointer value; must be a null pointer, otherwise the behavior is undefined. In addition to allowing operand bundles encoding function and parameter; attributes, an assume operand bundle my also encode a ``separate_storage``; operand bundle. This has the form:. .. code-block:: llvm. separate_storage(<val1>, <val2>)``. This indicates that no pointer :ref:`based <pointeraliasing>` on one of its; arguments can alias any pointer based on the other. Even if the assumed property can be encoded as a boolean value, like; ``nonnull``, using operand bundles to express the property can still have; benefits:. * Attributes that can be expressed via operand bundles are directly the; property that the optimizer uses and cares about. Encoding attributes as; operand bundles removes the need for an instruction sequence that represents; the property (e.g., `icmp ne ptr %p, null` for `nonnull`) and for the; optimizer to deduce the property from that instruction sequence.; * Expressing the property using operand bundles makes it easy to identify the; use of the value as a use in an :ref:`llvm.assume <int_assume>`. This then; simplifies and improves heuristics, e.g., for use ""use-sensitive""; optimizations. .. _ob_preallocated:. Preallocated Operand Bundles; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Preallocated operand bundles are characterized by the ``""preallocated""``; operand bundle tag. These operand bundles allow separation of the allocation; of the call argument memory from the call site. This is necessary to pass; non-trivially copyable objects by value in a way that is compatible with MSVC; on some targets. There can be at most one ``""preallocated""`` operand bundl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:125929,optimiz,optimizer,125929,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"at here to; avoid needing to retain arguments across a large number of calls. The remainder of this section describes exceptions to these rules, how those; exceptions are detected, and what those exceptions imply semantically. .. _arc.objects.operands.consumed:. Consumed parameters; ^^^^^^^^^^^^^^^^^^^. A function or method parameter of retainable object pointer type may be marked; as :arc-term:`consumed`, signifying that the callee expects to take ownership; of a +1 retain count. This is done by adding the ``ns_consumed`` attribute to; the parameter declaration, like so:. .. code-block:: objc. void foo(__attribute((ns_consumed)) id x);; - (void) foo: (id) __attribute((ns_consumed)) x;. This attribute is part of the type of the function or method, not the type of; the parameter. It controls only how the argument is passed and received. When passing such an argument, ARC retains the argument prior to making the; call. When receiving such an argument, ARC releases the argument at the end of the; function, subject to the usual optimizations for local values. .. admonition:: Rationale. This formalizes direct transfers of ownership from a caller to a callee. The; most common scenario here is passing the ``self`` parameter to ``init``, but; it is useful to generalize. Typically, local optimization will remove any; extra retains and releases: on the caller side the retain will be merged with; a +1 source, and on the callee side the release will be rolled into the; initialization of the parameter. The implicit ``self`` parameter of a method may be marked as consumed by adding; ``__attribute__((ns_consumes_self))`` to the method declaration. Methods in; the ``init`` :ref:`family <arc.method-families>` are treated as if they were; implicitly marked with this attribute. It is undefined behavior if an Objective-C message send to a method with; ``ns_consumed`` parameters (other than self) is made with a null receiver. It; is undefined behavior if the method to which an Objective-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:17336,optimiz,optimizations,17336,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizations']
Performance,"at is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:358363,perform,performing,358363,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"at it is unspecified; whether rounding will be performed between the multiplication and addition; steps. Fusion is not guaranteed, even if the target platform supports it.; If a fused multiply-add is required, the corresponding; :ref:`llvm.fma <int_fma>` intrinsic function should be used instead.; This never sets errno, just as '``llvm.fma.*``'. Examples:; """""""""""""""""". .. code-block:: llvm. %r2 = call float @llvm.fmuladd.f32(float %a, float %b, float %c) ; yields float:r2 = (a * b) + c. Hardware-Loop Intrinsics; ------------------------. LLVM support several intrinsics to mark a loop as a hardware-loop. They are; hints to the backend which are required to lower these intrinsics further to target; specific instructions, or revert the hardware-loop to a normal loop if target; specific restriction are not met and a hardware-loop can't be generated. These intrinsics may be modified in the future and are not intended to be used; outside the backend. Thus, front-end and mid-level optimizations should not be; generating these intrinsics. '``llvm.set.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare void @llvm.set.loop.iterations.i32(i32); declare void @llvm.set.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.set.loop.iterations.*``' intrinsics are used to specify the; hardware-loop trip count. They are placed in the loop preheader basic block and; are marked as ``IntrNoDuplicate`` to avoid optimizers duplicating these; instructions. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.set.loop.iterations.*``' intrinsics do not perform any arithmetic; on their operand. It's a hint to the backend that can use this to set up the; hardware-loop count with a target specific instruction, usually a move of this; value to a special register or a hardware-loop ins",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:643029,optimiz,optimizations,643029,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"at require chaining a call to a super class's implementation of a method.; Accepts -arch arm64 (which may be passed by Xcode 5.0), but for the time being analyzes code in such cases as -arch armv7s.; Many sundry fixes, improvements to C++ support, etc. checker-275; built: May 23, 2013; download: checker-275.tar.bz2; highlights:. Xcode: Includes a new arrow layout algorithm for issue presentation within Xcode. The goal is for interprocedural bug reports to look cleaner and less busy (and easier to read). Feedback appreciated.; Xcode: Bugs that occur within header code (e.g., C++) are now reported within the callers in the main source file. For example, if you misuse a C++ function declared in a header the primary diagnostic will be in the caller (in the main source file). The full expanded path, however, will show the bug in the header code as well. These kind of cross-file issues are currently only support by Xcode, not the HTML output.; This build is built with LLVM's Link-Time Optimization (LTO), which should make it slightly faster.; LTO also reduces the download size (about 19% smaller than checker-274).; Many sundry fixes. checker-274; built: April 23, 2013; download: checker-274.tar.bz2; highlights:. Improved use-after-free and mismatched deallocator checking.; Diagnostic polish.; Fixes crashes found in checker-273. checker-273; built: April 8, 2013; download: checker-273.tar.bz2; highlights:. Additional checks for misuse of Foundation collection APIs.; New C++ checker for attempting to create a reference to null.; New use-after-free checker for C++ 'delete'.; New checker for simple cases of mismatched allocators and deallocators, e.g. ""delete malloc(4);""; Support for basic interprocedural analysis of C++ destructors.; Additional heuristics for suppressing null pointer false positives.; Misc. bug fixes and performance enhancements. checker-272; built: March 1, 2013; highlights:. Better modeling of C++ constructors:; ; Interprocedural analysis support for constr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html:4255,Optimiz,Optimization,4255,interpreter/llvm-project/clang/www/analyzer/release_notes.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html,1,['Optimiz'],['Optimization']
Performance,"at runtime. JIT-compiling traces of mixed Python/bound C++ code reduces, and in some; cases removes, the overhead of boxing/unboxing native data into their Python; proxies and vice versa.; It can also reduce or remove temporaries, especially for template; expressions.; Thus, there can be significant speedups for mixed code, beyond the Numba; compilation of Python code itself.; The current implementation integrates compiled C++ through function pointers,; object pointers, and pointer offsets, into the intermediate representation; (IR) as generated by Numba.; A future version may integrate Cling-generated IR directly into Numba IR (or; vice versa), e.g. if the C++ code is exposed from (precompiled) headers.; This would allow inlining of C++ code into Numba traces, for further; expected speedups. Why Numba?; ----------. The advertised premise of Numba is that it ""makes Python code fast.""; However, there is a much more compelling reason: Numba allows developers to; stay in their chosen ecosystem, be it Python or C++, in mixed environments,; without paying for their choice in lost performance.; For example, a Python developer using Numba does not need to rewrite a kernel; into C++ just to run performantly in a C++ framework.; Similarly, a C++ developer can use Numba to compile and create function; pointers to Python code for easy, performant, access.; This becomes even more compelling if the deployment target is a GPU, which; would otherwise certainly require a rewrite of the Python code.; Add that Numba, as a JIT-compiler, is fully run-time just like ``cppyy``,; and the use case for integration is clear.; (Numba does not currently provide support for C++.). Usage; -------. ``cppyy`` does not use Numba extension hooks to minimize accidental; dependencies.; Instead, it requires that the extensions are loaded explicitly by any code; that uses it::. import cppyy.numba_ext. After that, Numba is able to trace ``cppyy`` bound code when applying the; usual ``numba.njit`` decorat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:1645,perform,performance,1645,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['perform'],['performance']
Performance,"at the user knows what the normalization of their; $\mbox{FCN}$ means, and also that they are interested in parameter; errors, the user can change the error definition which allows them to; define what they means by one ""error"", in terms of the change in the; $\mbox{FCN}$ value which should be caused by changing one parameter; by one ""error"". If the $\mbox{FCN}$ is the usual chisquare function; (defined below) and if the user wants the usual one-standard-deviation; errors, then the error definition (return value of the FCNBase::up(); method, see [howto:errordef]) should be $1.0$. If the $\mbox{FCN}$; is a negative-log-likelihood function, then the one-standard-deviation; value for FCNBase::up() to return is $0.5$. If the $\mbox{FCN}$ is a; chisquare, but the user wants two-standard-deviation errors, then; FCNBase::up() should return $= 4.0$, etc. Note that in the usual case where M is being used to perform a fit to; some experimental data, the parameter errors will be proportional to the; uncertainty in the data, and therefore meaningful parameter errors; cannot be obtained unless the measurement errors of the data are known.; In the common case of a least-squares fit, $\mbox{FCN}$ is usually; defined as a chisquare:. $$\chi^2 (\alpha) = \sum_{i=1}^{n} (\frac{f(x_i,\alpha) - m_i)^2}{\sigma_i^2}$$. where $\alpha$ is the vector of free parameters being fitted, and the; $\sigma_i$ are the uncertainties in the individual measurements $m_i$.; If these uncertainties are not known, and are simply left out of the; calculation, then the fit may still have meaning, but not the; quantitative values of the resulting parameter errors. (Only the; relative errors of different parameters with respect to each other may; be meaningful.). If the $\sigma_i$ are all overestimated by a factor $\beta$, then the; resulting parameter errors from the fit will be overestimated by the; same factor $\beta$. ### The error matrix ###. The M processors $\mbox{MIGRAD}$ (MnMigrad, see [api:migrad]) a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:14367,perform,perform,14367,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['perform'],['perform']
Performance,"at uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; pass",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2078,optimiz,optimizations,2078,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,3,"['optimiz', 'perform']","['optimization', 'optimizations', 'performed']"
Performance,"at> <float QNAN, float QNAN, float QNAN, float QNAN>; %reduction = call float @llvm.vector.reduce.fmax.v4f32(<4 x float> %masked.a); %also.r = call float @llvm.maxnum.f32(float %reduction, float %start). .. _int_vp_reduce_fmin:. '``llvm.vp.reduce.fmin.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare float @llvm.vp.reduce.fmin.v4f32(float <start_value>, <4 x float> <val>, <4 x i1> <mask>, float <vector_length>); declare double @llvm.vp.reduce.fmin.nxv8f64(double <start_value>, <vscale x 8 x double> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point ``MIN`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; floating-point type equal to the result type. The second operand is the vector; on which the reduction is performed and must be a vector of floating-point; values whose element type is the result/start type. The third operand is the; vector mask and is a vector of boolean values with the same number of elements; as the vector operand. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.fmin``' intrinsic performs the floating-point ``MIN``; reduction (:ref:`llvm.vector.reduce.fmin <int_vector_reduce_fmin>`) of the; vector operand ``val`` on each enabled lane, taking the minimum of that and the; scalar ``start_value``. Disabled lanes are treated as containing the neutral; value (i.e. having no effect on the reduction operation). If the vector length; is zero, the result is the start value. The neutral value is dependent on the :ref:`fast-math flags <fastmath>`. If no; flags are set, the neutral value is ``+QNAN``. If ``nnan`` and ``ninf`` are; both set, then the neutral value is the largest floating-point value for the; result type. If only ``nnan`` ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:774634,perform,performed,774634,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ata and then; it is used to evaluate the likelihood. ; ; This class implements the HypoTestCalculatorGeneric interface and can be used as an alternative Hypothesis test; calculator in the HypoTestInverter class. It can then plugged in the HypoTestInverter for computing asymptotic CLs and CLs+b; limits. In this way the limits will be computed by just performing a fit for each test parameter value and without; generating any toys. . The class can be used via the StandardHypothesisTest.C tutorial passing a value of 2 for the; calculator type. . RooStats Utils. Add a utility function (from G. Petrucciani), RooStats::MakeNuisancePdf, which given a model configuration (or the global pdf and the; observables), factorizes from the model pdf the constraint probability density functions for the nuisance parameters; and builds a global nuisance pdf. This function can then be used in the HybridCalculator or in the BayesianCalculator; with the option ""TOYMC"".; . HypotestInverter and HypoTestInverterResult. Several improvements and bug fixes in merging results and in computing the observed and expected limits.; Provide support now for using the AsympoticCalculator. MCMCCalculator. Add now possibility to store in the chain only the parameter of interested via the method MCMCCalculator::SetChainParameters. This saves memory in case of models with a; large number of nuisance parameters. . Test Statistics classes. Make a more robust evaluation of the ProfileLikelihoodTestStat. Use RooMinimizer and give possibility to use; different minimizer, via ProfileLikelihoodTestStat::SetMinimizer. The print level of minimization can also be; controlled via ProfileLikelihoodTestStat::SetPrintLevel. Activate also the RooFit cache optimizations when; evaluating the NLL ; The same optimizations are applied also to the RatioOfProfilesLikelihood test statistic; Fix a bug in reusing the NLL object in the SimpleLikelihoodCalculator. This makes now the evaluation of this test; statistics much faster. . ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:9237,cache,cache,9237,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,6,"['cache', 'optimiz']","['cache', 'optimizations']"
Performance,"ata from r1 into r3; // of threads id r2.; r3 = my_sub_group_shuffle(r1, r2);; if (r0) r3 = computeB();. with non-SPMD semantics this is optimized to the following equivalent code:. .. code-block:: c. r1 = ...; if (!r0); // Incorrect functionality! The data in r1; // have not been computed by all threads yet.; r3 = my_sub_group_shuffle(r1, r2);; else {; r1 = computeA();; r3 = my_sub_group_shuffle(r1, r2);; r3 = computeB();; }. Declaring the function ``my_sub_group_shuffle`` with the convergent attribute; would prevent this:. .. code-block:: c. my_sub_group_shuffle() __attribute__((convergent));. Using ``convergent`` guarantees correct execution by keeping CFG equivalence; wrt operations marked as ``convergent``. CFG ``G`` is equivalent to ``G`` wrt; node ``Ni`` : ``iff  Nj (ij)`` domination and post-domination relations with; respect to ``Ni`` remain the same in both ``G`` and ``G``. noduplicate; ^^^^^^^^^^^. ``noduplicate`` is more restrictive with respect to optimizations than; ``convergent`` because a convergent function only preserves CFG equivalence.; This allows some optimizations to happen as long as the control flow remains; unmodified. .. code-block:: c. for (int i=0; i<4; i++); my_sub_group_shuffle(). can be modified to:. .. code-block:: c. my_sub_group_shuffle();; my_sub_group_shuffle();; my_sub_group_shuffle();; my_sub_group_shuffle();. while using ``noduplicate`` would disallow this. Also ``noduplicate`` doesn't; have the same safe semantics of CFG as ``convergent`` and can cause changes in; CFG that modify semantics of the original program. ``noduplicate`` is kept for backwards compatibility only and it considered to be; deprecated for future uses. .. _cxx_for_opencl:. C++ for OpenCL; --------------. Starting from clang 9 kernel code can contain C++17 features: classes, templates,; function overloading, type deduction, etc. Please note that this is not an; implementation of `OpenCL C++; <https://www.khronos.org/registry/OpenCL/specs/2.2/pdf/OpenCL_C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:151468,optimiz,optimizations,151468,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"ata. This new designs; allows for a much faster RooAbsTestStatistic::setData() implementation, which changes the dataset in; an existing likelihood object. This will speed up RooStats tools based on 'simple' likelihood models; substantially. Automatic detections of 'binned' pdfs and automatic generation of binned data in generate(). RooFit will; now automatically generate binned pdf shapes. Binned pdfs shapes are fundamentally RooHistPdf and RooHistFuncs; (with interpolation order set to zero). Products and sums of exclusively binned shapes are also recognized; as binned shapes. For such binned shapes generate() will now by default follow the 'binned' strategy ; -- that is, take the expectation value in each bin and sample a Poisson distribution from that -- rather; than follow the unbinned strategy. The rationale is that such datasets result in much faster likelihood; calculations (for nbin smaller than nevent). The optimization is also exact: the likelihood of a binned ; data using a binned pdf is identical to that of an unbinned dataset with a binned pdf. Nevertheless you can ; switch off this feature by passing AutoBinned(false) to RooAbsPdf::generate(). Mixed binned/unbinned generation from simultaneous pdf. For a RooSimultaneous consisting of exclusively; extended terms it is now possible to generate a mixed binned/unbinned datasets. Components defined; by a binned pdf at the top level are automatically generated binned (unless AutoBinned(false) is set); but it is also possible to generate other component pdfs forcibly binned by adding GenBinned(tagname); to generate(). In that case all component pdfs labeled with pdf->setAttribute(tagname) will be generated; binned. To generate all component binned, the shorthand method AllBinned() can be used. All binned; datasets made by generate are represented as weighted unbinned datasets (of type RooDataSet) rather; than binned datasets of type RooDataHist so that mixed binned/unbinned data is always represented; throug",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:5073,optimiz,optimization,5073,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"ate serial; numbers in CRL handling; support for an external function for DN-to-username; mapping function; provide example for an LDAP based search; fixed a few problem with return code checking. netx. TXNetFile:; . Enable dynamic cache size synchronization; ; Enable per-instance control of the cache parameters; also for RAW files; by; default cache is OFF for these files, but there maybe cases in which the cache can; improve performances.; Remove call to XrdClient::Sync in SysStat. Correctly honor the create/recreate options coming from TFile::Open(); Allow the size of the (written) file to be retrieved after the Close (solves several reported file size mismatches).; . TXNetSystem:; ; Fix problem with GetDirEntry: the entry object was; going out-of-scope so; that the returned string was meaningless.; Reset; the list if dir entries in FreeDirectory.; Fix problem affecting repeated calls. The implementation of TFile throughput and info sending was; just sending 'regular' samples about the activity of the single TFile; instance that happened to trigger an activity in the right moment.; Now TMonaLisaWriter keeps internally track of every; activity; and regularly sends summaries valid for all the files which had; activity in the last time interval.; Additionally, it's now finalized the infrastructure able to; measure; and keep track of the file Open latency. A packet is sent for each; successful Open, sending the measures of the latencies for the; various phases of the open. Currently exploited fully by TAlienFile; and TXNetFile. Easy to report from other TFiles too.; Now, the hook for the Close() func triggers sending of a; packet containing various information about the performance related to; that file only.; Added support also for performance monitoring when writing. RGLITE: A ROOT GRID interface. RGLite plug-in - a ROOT plug-in module, which implements the ROOT Grid; interface and offers to ROOT users possibilities to perform a number of; operations using gLite mid",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html:2165,throughput,throughput,2165,net/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html,2,['throughput'],['throughput']
Performance,"ate()`; called after the last entry has been processed. Typically,; initialization like booking of histograms is performed in; `SlaveBegin()`, the analysis, i.e. the selection of entries,; calculations and filling of histograms, is done in `Process()`, and; final operations like plotting and storing of results happen in; `SlaveTerminate()` or `Terminate()`. The entry points `SlaveBegin()` and `SlaveTerminate()` are called on; so-called slave nodes only if parallel processing via `PROOF` or; `PROOF lite` is enabled, as will be explained below. A simple example of a selector class is shown in the macro; `MySelector.C`. The example is executed with the following sequence of; commands:. ``` {.cpp}; > TChain *ch=new TChain(""cond_data"", ""Chain for Example N-Tuple"");; > ch->Add(""conductivity_experiment*.root"");; > ch->Process(""MySelector.C+"");; ```. As usual, the ""`+`"" appended to the name of the macro to be executed; initiates the compilation of the `MySelector.C` with the system compiler; in order to improve performance. The code in `MySelector.C`, shown in the listing below, books some; histograms in `SlaveBegin()` and adds them to the instance `fOutput`,; which is of the class `TList` [^6]. The final processing in; `Terminate()` allows to access histograms and store, display or save; them as pictures. This is shown in the example via the `TList`; `fOutput`. See the commented listing below for more details; most of the; text is actually comments generated automatically by; `TTree::MakeSelector`. ``` {.cpp}; @ROOT_INCLUDE_FILE macros/MySelector.C; ```. ### *For power-users:* Multi-core processing with `PROOF lite` ###. The processing of n-tuples via a selector function of type `TSelector`; through `TChain::Process()`, as described at the end of the previous; section, offers an additional advantage in particular for very large; data sets: on distributed systems or multi-core architectures, portions; of data can be processed in parallel, thus significantly reducing the; exe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md:8289,perform,performance,8289,documentation/primer/filio.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md,1,['perform'],['performance']
Performance,"ate. Modules, as implemented in Clang, use the same mechanisms as precompiled; headers to save a serialized AST file (one per module) and use those AST; modules. From an implementation standpoint, modules are a generalization of; precompiled headers, lifting a number of restrictions placed on precompiled; headers. In particular, there can only be one precompiled header and it must; be included at the beginning of the translation unit. The extensions to the; AST file format required for modules are discussed in the section on; :ref:`modules <pchinternals-modules>`. Clang's AST files are designed with a compact on-disk representation, which; minimizes both creation time and the time required to initially load the AST; file. The AST file itself contains a serialized representation of Clang's; abstract syntax trees and supporting data structures, stored using the same; compressed bitstream as `LLVM's bitcode file format; <https://llvm.org/docs/BitCodeFormat.html>`_. Clang's AST files are loaded ""lazily"" from disk. When an AST file is initially; loaded, Clang reads only a small amount of data from the AST file to establish; where certain important data structures are stored. The amount of data read in; this initial load is independent of the size of the AST file, such that a; larger AST file does not lead to longer AST load times. The actual header data; in the AST file --- macros, functions, variables, types, etc. --- is loaded; only when it is referenced from the user's code, at which point only that; entity (and those entities it depends on) are deserialized from the AST file.; With this approach, the cost of using an AST file for a translation unit is; proportional to the amount of code actually used from the AST file, rather than; being proportional to the size of the AST file itself. When given the `-print-stats` option, Clang produces statistics; describing how much of the AST file was actually loaded from disk. For a; simple ""Hello, World!"" program that includes t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:3411,load,loaded,3411,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance,"ated by the `HLT` instruction and are handled by a signal handler. Attribute; ---------. HWASAN uses its own LLVM IR Attribute `sanitize_hwaddress` and a matching; C function attribute. An alternative would be to re-use ASAN's attribute; `sanitize_address`. The reasons to use a separate attribute are:. * Users may need to disable ASAN but not HWASAN, or vise versa,; because the tools have different trade-offs and compatibility issues.; * LLVM (ideally) does not use flags to decide which pass is being used,; ASAN or HWASAN are being applied, based on the function attributes. This does mean that users of HWASAN may need to add the new attribute; to the code that already uses the old attribute. Comparison with AddressSanitizer; ================================. HWASAN:; * Is less portable than :doc:`AddressSanitizer`; as it relies on hardware `Address Tagging`_ (AArch64).; Address Tagging can be emulated with compiler instrumentation,; but it will require the instrumentation to remove the tags before; any load or store, which is infeasible in any realistic environment; that contains non-instrumented code.; * May have compatibility problems if the target code uses higher; pointer bits for other purposes.; * May require changes in the OS kernels (e.g. Linux seems to dislike; tagged pointers passed from address space:; https://www.kernel.org/doc/Documentation/arm64/tagged-pointers.txt).; * **Does not require redzones to detect buffer overflows**,; but the buffer overflow detection is probabilistic, with roughly; `1/(2**TS)` chance of missing a bug (6.25% or 0.39% with 4 and 8-bit TS; respectively).; * **Does not require quarantine to detect heap-use-after-free,; or stack-use-after-return**.; The detection is similarly probabilistic. The memory overhead of HWASAN is expected to be much smaller; than that of AddressSanitizer:; `1/TG` extra memory for the shadow; and some overhead due to `TG`-aligning all objects. Supported architectures; =======================; HWASAN reli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:9473,load,load,9473,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,1,['load'],['load']
Performance,"ated cycle. For processors with an; in-order backend, *DispatchWidth* is the maximum number of micro opcodes issued; to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total; number of cycles. Field *Block RThroughput* is the reciprocal of the block throughput. Block; throughput is a theoretical quantity computed as the maximum number of blocks; (i.e. iterations) that can be executed per simulated clock cycle in the absence; of loop carried dependencies. Block throughput is superiorly limited by the; dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the observed IPC tends to a; theoretical maximum which can be computed by dividing the number of instructions; of a single iteration by the `Block RThroughput`. Field 'uOps Per Cycle' is computed dividing the total number of simulated micro; opcodes by the total number of cycles. A delta between Dispatch Width and this; field is an indicator of a performance issue. In the absence of loop-carried; data dependencies, the observed 'uOps Per Cycle' should tend to a theoretical; maximum throughput which can be computed by dividing the number of uOps of a; single iteration by the `Block RThroughput`. Field *uOps Per Cycle* is bounded from above by the dispatch width. That is; because the dispatch width limits the maximum size of a dispatch group. Both IPC; and 'uOps Per Cycle' are limited by the amount of hardware parallelism. The; availability of hardware resources affects the resource pressure distribution,; and it limits the number of instructions that can be executed in parallel every; cycle. A delta between Dispatch Width and the theoretical maximum uOps per; Cycle (computed by dividing the number of uOps of a single iteration by the; `Block RThroughput`) is an indicator of a performance bottleneck caused by the; lack of hardware resources.; In general, the lower the Block RThroughput, the better. In thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:17048,perform,performance,17048,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,"ated path; that bit is always fixed at either 0 or 1 regardless of the value of the bit; loaded. The most obvious implementation uses either an `and` instruction with; an all-zero mask along misspeculated paths and an all-one mask along correct; paths, or an `or` instruction with an all-one mask along misspeculated paths; and an all-zero mask along correct paths. Other options become less appealing; such as multiplying by zero, or multiple shift instructions. For reasons we; elaborate on below, we end up suggesting you use `or` with an all-ones mask,; making the x86 instruction sequence look like the following:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; movl (%rsi), %edi # Load potentially secret data from %rsi.; orl %eax, %edi; ```. Other useful patterns may be to fold the load into the `or` instruction itself; at the cost of a register-to-register copy. There are some challenges with deploying this approach:; 1. Many loads on x86 are folded into other instructions. Separating them would; add very significant and costly register pressure with prohibitive; performance cost.; 1. Loads may not target a general purpose register requiring extra instructions; to map the state value into the correct register class, and potentially more; expensive instructions to mask the value in some way.; 1. The flags registers on x86 are very likely to be live, and challenging to; preserve cheaply.; 1. There are many more values loaded than pointers & indices used for loads. As; a consequence, hardening the result of a load requires substantially more; instructions than hardening the address of the load (see below). Despite these challenges, hardening the result of the load critically allows; the load to proceed and thus has dramatically less impact on the total; speculative / out-of-order potential of the execution. There are also several; interesting techniques to try and mitigate these challenges and make hardening; the results of loads ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:23867,load,loads,23867,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"ath::tgamma, which again uses Cephes.; The incomplete gamma function, TMath::Gamma(a,x) it is implemented using ROOT::Math::inc_gamma based on a corresponding Cephes function.; TMath::Prob (the upper chi2 probability) is implemented also using ROOT::Math::chisquared_cdf_c which uses ROOT::Math::inc_gamma_c based on a corresponding Cephes function. Now the implementation does not suffer anymore from large numerical error present when the result of TMath::Prob was approaching zero.; TMath::LnGamma(z) is implemented using ROOT::Math::lgamma. This affects also TMath::Beta which is implemented using the log of the gamma function.; TMath::BetaIncomplete is implemented using ROOT::Math::beta_inc.; TMath::GammaDist is implemented using ROOT::Math::gamma_pdf.; TMath::LogNormal is implemented using ROOT::Math::lognormal_pdf.; TMath::PoissonI: fixed a problem for large values and is implemented using directly TMath::Poisson with integer values. Fit; Mathcore include now new classes for performing fits and minimization of multi-dimensional functions. The aim of these classes is to extend and improve the fitting functionality provided in ROOT via the TVirtualFitter classes and the fitting methods present in many data analysis object, such as TH1::Fit.; ; The fit data are decoupled from the fitter class and described by the dedicated fit data classes like the ROOT::Fit::BinData for bin data containing coordinate values of any dimensions, bin content values and optionally errors in coordinate and bin content, and ROOT::Fit::UnBinData classes for any dimension un-bin data.; The fitter class, ROOT::Fit::Fitter, provides the functionality for fitting those data with any model function implementing the parametric function interface, ROOT::Math::IParamMultiFunction. Fit methods such as least square, bin and un-bin likelihood are supported. The fit solution is then found by using the ROOT::Math::Minimizer interface class and the results are stored in the ROOT::Fit::FitResult class. Fit p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html:4027,perform,performing,4027,math/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html,2,['perform'],['performing']
Performance,"ath; [other options] SRC_ROOT. Compiling the LLVM Suite Source Code; ------------------------------------. Unlike with autotools, with CMake your build type is defined at configuration.; If you want to change your build type, you can re-run cmake with the following; invocation:. .. code-block:: console. % cmake -G ""Unix Makefiles"" -DCMAKE_BUILD_TYPE=<type> SRC_ROOT. Between runs, CMake preserves the values set for all options. CMake has the; following build types defined:. Debug. These builds are the default. The build system will compile the tools and; libraries unoptimized, with debugging information, and asserts enabled. Release. For these builds, the build system will compile the tools and libraries; with optimizations enabled and not generate debug info. CMakes default; optimization level is -O3. This can be configured by setting the; ``CMAKE_CXX_FLAGS_RELEASE`` variable on the CMake command line. RelWithDebInfo. These builds are useful when debugging. They generate optimized binaries with; debug information. CMakes default optimization level is -O2. This can be; configured by setting the ``CMAKE_CXX_FLAGS_RELWITHDEBINFO`` variable on the; CMake command line. Once you have LLVM configured, you can build it by entering the *OBJ_ROOT*; directory and issuing the following command:. .. code-block:: console. % make. If the build fails, please `check here`_ to see if you are using a version of; GCC that is known not to compile LLVM. If you have multiple processors in your machine, you may wish to use some of the; parallel build options provided by GNU Make. For example, you could use the; command:. .. code-block:: console. % make -j2. There are several special targets which are useful when working with the LLVM; source code:. ``make clean``. Removes all files generated by the build. This includes object files,; generated C/C++ files, libraries, and executables. ``make install``. Installs LLVM header files, libraries, tools, and documentation in a hierarchy; under ``$",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:29377,optimiz,optimized,29377,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimized']
Performance,"ating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5328,optimiz,optimizations,5328,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance,"ating register aliases,; and therefore consuming physical registers). Table *Average Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; The ``-bottleneck-analysis`` command line option enables the analysis of; performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in; backend pressure (caused by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product example on btver2. .. code-block:: none. Cycles with backend pressure increase [ 48.07% ]; Throughput Bottlenecks:; Resource Pressure [ 47.77% ]; - JFPA [ 47.77% ]; - JFPU0 [ 47.77% ]; Data Dependencies: [ 0.30% ]; - Register Dependencies [ 0.30% ]; - Memory Dependencies [ 0.00% ]. Critical sequence based on the simulation:. Instruction Dependency Information; +----< 2. vhaddps %xmm3, %xmm3, %xmm4; |; | < loop carried >; |;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:26379,queue,queue,26379,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"ating-point environment. This is a; comma separated pair. The elements may be one of ``""ieee""``,; ``""preserve-sign""``, ``""positive-zero""``, or ``""dynamic""``. The; first entry indicates the flushing mode for the result of floating; point operations. The second indicates the handling of denormal inputs; to floating point instructions. For compatibility with older; bitcode, if the second value is omitted, both input and output; modes will assume the same mode. If this is attribute is not specified, the default is ``""ieee,ieee""``. If the output mode is ``""preserve-sign""``, or ``""positive-zero""``,; denormal outputs may be flushed to zero by standard floating-point; operations. It is not mandated that flushing to zero occurs, but if; a denormal output is flushed to zero, it must respect the sign; mode. Not all targets support all modes. If the mode is ``""dynamic""``, the behavior is derived from the; dynamic state of the floating-point environment. Transformations; which depend on the behavior of denormal values should not be; performed. While this indicates the expected floating point mode the function; will be executed with, this does not make any attempt to ensure; the mode is consistent. User or platform code is expected to set; the floating point mode appropriately before function entry. If the input mode is ``""preserve-sign""``, or ``""positive-zero""``,; a floating-point operation must treat any input denormal value as; zero. In some situations, if an instruction does not respect this; mode, the input may need to be converted to 0 as if by; ``@llvm.canonicalize`` during lowering for correctness. ``""denormal-fp-math-f32""``; Same as ``""denormal-fp-math""``, but only controls the behavior of; the 32-bit float type (or vectors of 32-bit floats). If both are; are present, this overrides ``""denormal-fp-math""``. Not all targets; support separately setting the denormal mode per type, and no; attempt is made to diagnose unsupported uses. Currently this; attribute is respected by ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:107203,perform,performed,107203,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ation description. This can be extended to allow a default address space; memory location description to be implicitly converted back to its address; value. This allows all DWARF Version 5 expressions to retain their same meaning,; while enabling the ability to explicitly create memory location descriptions in; non-default address spaces and generalizing the power of composite location; descriptions to any kind of location description. For those familiar with the definition of location descriptions in DWARF Version; 5, the definitions in these extensions are presented differently, but does in; fact define the same concept with the same fundamental semantics. However, it; does so in a way that allows the concept to extend to support address spaces,; bit addressing, the ability for composite location descriptions to be composed; of any kind of location description, and the ability to support objects located; at multiple places. Collectively these changes expand the set of architectures; that can be supported and improves support for optimized code. Several approaches were considered, and the one presented, together with the; extensions it enables, appears to be the simplest and cleanest one that offers; the greatest improvement of DWARF's ability to support debugging optimized GPU; and non-GPU code. Examining the GDB debugger and LLVM compiler, it appears only; to require modest changes as they both already have to support general use of; location descriptions. It is anticipated that will also be the case for other; debuggers and compilers. GDB has been modified to evaluate DWARF Version 5 expressions with location; descriptions as stack entries and with implicit conversions. All GDB tests have; passed, except one that turned out to be an invalid test case by DWARF Version 5; rules. The code in GDB actually became simpler as all evaluation is done on a; single stack and there was no longer a need to maintain a separate structure for; the location description results. T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:6911,optimiz,optimized,6911,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimized']
Performance,"ation of global variables and local ``static`` variables. Floating-point operations in these contexts will be rounded using ``FE_TONEAREST``. - The option ``-fno-rounding-math`` allows the compiler to assume that the rounding mode is set to ``FE_TONEAREST``. This is the default.; - The option ``-frounding-math`` forces the compiler to honor the dynamically-set rounding mode. This prevents optimizations which might affect results if the rounding mode changes or is different from the default; for example, it prevents floating-point operations from being reordered across most calls and prevents constant-folding when the result is not exactly representable. .. option:: -ffp-model=<value>. Specify floating point behavior. ``-ffp-model`` is an umbrella; option that encompasses functionality provided by other, single; purpose, floating point options. Valid values are: ``precise``, ``strict``,; and ``fast``.; Details:. * ``precise`` Disables optimizations that are not value-safe on; floating-point data, although FP contraction (FMA) is enabled; (``-ffp-contract=on``). This is the default behavior. This value resets; ``-fmath-errno`` to its target-dependent default.; * ``strict`` Enables ``-frounding-math`` and; ``-ffp-exception-behavior=strict``, and disables contractions (FMA). All; of the ``-ffast-math`` enablements are disabled. Enables; ``STDC FENV_ACCESS``: by default ``FENV_ACCESS`` is disabled. This option; setting behaves as though ``#pragma STDC FENV_ACCESS ON`` appeared at the; top of the source file.; * ``fast`` Behaves identically to specifying both ``-ffast-math`` and; ``ffp-contract=fast``. Note: If your command line specifies multiple instances; of the ``-ffp-model`` option, or if your command line option specifies; ``-ffp-model`` and later on the command line selects a floating point; option that has the effect of negating part of the ``ffp-model`` that; has been selected, then the compiler will issue a diagnostic warning; that the override has occurred. .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:62360,optimiz,optimizations,62360,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"ation that produce higher; precision analysis results. The trick is to keep exploring the execution paths; separately and delay joining until later. However, we won't discuss those; variations here.). To make a conclusion about all paths through the program, we repeat this; computation on all basic blocks until we reach a fixpoint. In other words, we; keep propagating information through the CFG until the computed sets of values; stop changing. If the lattice has a finite height and transfer functions are monotonic the; algorithm is guaranteed to terminate. Each iteration of the algorithm can; change computed values only to larger values from the lattice. In the worst; case, all computed values become ``, which is not very useful, but at least the; analysis terminates at that point, because it can't change any of the values. Fixpoint iteration can be optimised by only reprocessing basic blocks which had; one of their inputs changed on the previous iteration. This is typically; implemented using a worklist queue. With this optimisation the time complexity; becomes `O(m * |L|)`, where `m` is the number of basic blocks in the CFG and; `|L|` is the size of lattice used by the analysis. ## Symbolic execution: a very short informal introduction. ### Symbolic values. In the previous example where we tried to figure out what values a variable can; have, the analysis had to be seeded with a concrete value. What if there are no; assignments of concrete values in the program? We can still deduce some; interesting information by representing unknown input values symbolically, and; computing results as symbolic expressions:. ```c++; void PrintAbs(int x) {; int result;; if (x >= 0) {; result = x; // result is {x}; } else {; result = -x; // result is {-x}; }; print(result); // result is {x; -x}; }; ```. We can't say what specific value gets printed, but we know that it is either `x`; or `-x`. Dataflow analysis is an instance of abstract interpretation, and does not dictate; how ex",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md:10852,queue,queue,10852,interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,1,['queue'],['queue']
Performance,"ation"");; TTask *tracker = new MyTracker(""tracker"",""Tracker manager"");; TTask *tpc = new MyRecTPC(""tpc"",""TPC Reconstruction"");; TTask *its = new MyRecITS(""its"",""ITS Reconstruction"");; TTask *muon = new MyRecMUON(""muon"",""MUON Reconstruction"");; TTask *phos = new MyRecPHOS(""phos"",""PHOS Reconstruction"");; TTask *rich = new MyRecRICH(""rich"",""RICH Reconstruction"");; TTask *trd = new MyRecTRD(""trd"",""TRD Reconstruction"");; TTask *global = new MyRecGlobal(""global"",""Global Reconstruction"");. // Create a hierarchy by adding sub tasks; run->Add(geomInit);; run->Add(matInit);; run->Add(event);; event->Add(tracker);; event->Add(global);; tracker->Add(tpc);; tracker->Add(its);; tracker->Add(muon);; tracker->Add(phos);; tracker->Add(rich);; tracker->Add(trd);. // Add the top level task; gROOT->GetListOfTasks()->Add(run);. // Add the task to the browser; gROOT->GetListOfBrowsables()->Add(run);; new TBrowser;; }; ```. ![Tasks in the ROOT browser](pictures/030000E5.png). Note that the first line loads the class definitions in `MyTasks.cxx`; with ACLiC. ACLiC builds a shared library and adds the classes to the; Cling dictionary. See ""Adding a Class with ACLiC"". To execute a **`TTask`**, you call the `ExecuteTask` method.; `ExecuteTask` will recursively call:. - the `TTask::Exec `method of the derived class;. - the `TTask::ExecuteTasks` to execute for each task the list of its; subtasks;. If the top level task is added to the list of ROOT browsable objects,; the tree of tasks can be seen in the ROOT browser. To add it to the; browser, get the list of browsable objects first and add it to the; collection. ``` {.cpp}; gROOT->GetListOfBrowsables()->Add(run);; ```. The first parameter of the `Add` method is a pointer to a **`TTask`**,; the second parameter is the string to show in the browser. If the string; is left out, the name of the task is used. After executing, the script above the browser will look like in this; figure. ## Execute and Debug Tasks. The browser can be used to start a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FoldersTasks.md:9545,load,loads,9545,documentation/users-guide/FoldersTasks.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FoldersTasks.md,1,['load'],['loads']
Performance,"ation, we switch; representations to an explicit form. The exact location chosen for lowering; is an implementation detail. Note that most of the value of the abstract machine model comes for collectors; which need to model potentially relocatable objects. For a compiler which; supports only a non-relocating collector, you may wish to consider starting; with the fully explicit form. Warning: There is one currently known semantic hole in the definition of; non-integral pointers which has not been addressed upstream. To work around; this, you need to disable speculation of loads unless the memory type; (non-integral pointer vs anything else) is known to unchanged. That is, it is; not safe to speculate a load if doing causes a non-integral pointer value to; be loaded as any other type or vice versa. In practice, this restriction is; well isolated to isSafeToSpeculate in ValueTracking.cpp. Explicit Representation; ^^^^^^^^^^^^^^^^^^^^^^^. A frontend could directly generate this low level explicit form, but; doing so may inhibit optimization. Instead, it is recommended that; compilers with relocating collectors target the abstract machine model just; described. The heart of the explicit approach is to construct (or rewrite) the IR in a; manner where the possible updates performed by the garbage collector are; explicitly visible in the IR. Doing so requires that we:. #. create a new SSA value for each potentially relocated pointer, and; ensure that no uses of the original (non relocated) value is; reachable after the safepoint,; #. specify the relocation in a way which is opaque to the compiler to; ensure that the optimizer can not introduce new uses of an; unrelocated value after a statepoint. This prevents the optimizer; from performing unsound optimizations.; #. recording a mapping of live pointers (and the allocation they're; associated with) for each statepoint. At the most abstract level, inserting a safepoint can be thought of as; replacing a call instruction with a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:6297,optimiz,optimization,6297,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['optimiz'],['optimization']
Performance,"ation. Clearly, LLVM must not move; stores across a stack map. However, loads must also be handled; conservatively. If the load may trigger an exception, hoisting it; above a stack map could be invalid. For example, the runtime may; determine that a load is safe to execute without a type check given; the current state of the type system. If the type system changes while; some activation of the load's function exists on the stack, the load; becomes unsafe. The runtime can prevent subsequent execution of that; load by immediately patching any stack map location that lies between; the current call site and the load (typically, the runtime would; simply patch all stack map locations to invalidate the function). If; the compiler had hoisted the load above the stack map, then the; program could crash before the runtime could take back control. To enforce these semantics, stackmap and patchpoint intrinsics are; considered to potentially read and write all memory. This may limit; optimization more than some clients desire. This limitation may be; avoided by marking the call site as ""readonly"". In the future we may; also allow meta-data to be added to the intrinsic call to express; aliasing, thereby allowing optimizations to hoist certain loads above; stack maps. Direct Stack Map Entries; ^^^^^^^^^^^^^^^^^^^^^^^^. As shown in :ref:`stackmap-section`, a Direct stack map location; records the address of frame index. This address is itself the value; that the runtime requested. This differs from Indirect locations,; which refer to a stack locations from which the requested values must; be loaded. Direct locations can communicate the address if an alloca,; while Indirect locations handle register spills. For example:. .. code-block:: none. entry:; %a = alloca i64...; llvm.experimental.stackmap(i64 <ID>, i32 <shadowBytes>, ptr %a). The runtime can determine this alloca's relative location on the; stack immediately after compilation, or at any time thereafter. This; differs from R",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:18887,optimiz,optimization,18887,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimization']
Performance,"atisfy all cases, ``cppyy`` aims to maximize; functionality and minimum surprises based on common use.; Thus, for example, ``std::vector`` grows a pythonistic ``__len__`` method,; but does not lose its C++ ``size`` method.; Passing a Python container through a const reference to a ``std::vector``; will trigger automatic conversion, but such an attempt through a non-const; reference will fail since a non-temporary C++ object is required [#f1]_ to; return any updates/changes. ``std::string`` is almost always converted to Python's ``str`` on function; returns (the exception is return-by-reference when assigning), but not when; its direct use is more likely such as in the case of (global) variables or; when iterating over a ``std::vector<std::string>``. The rest of this section shows examples of how STL containers can be used in; a natural, pythonistic, way. `std::vector`; -------------. A ``std::vector`` is the most commonly used C++ container type because it is; more efficient and performant than specialized types such as ``list`` and; ``map``, unless the number of elements gets very large.; Python has several similar types, from the builtin ``tuple`` and ``list``,; the ``array`` from builtin module ``array``, to ""as-good-as-builtin""; ``numpy.ndarray``.; A vector is more like the latter two in that it can contain only one type,; but more like the former two in that it can contain objects.; In practice, it can interplay well with all these containers, but e.g.; efficiency and performance can differ significantly. A vector can be instantiated from any sequence, including generators, and; vectors of objects can be recursively constructed.; If the template type is to be inferred from the argument to the constructor,; the first element needs to be accessible, which precludes generators. .. code-block:: python. >>> from cppyy.gbl.std import vector, pair; >>> v = vector[int](range(10)) # from generator; >>> len(v); 10; >>> v = vector([x for x in range(10)]) # type inferred; >",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst:2119,perform,performant,2119,bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,1,['perform'],['performant']
Performance,"atment of subnormal input values (such as indicated; by the ``""denormal-fp-math""`` attribute), a subnormal value will be; observed (will not be implicitly treated as zero). General Intrinsics; ------------------. This class of intrinsics is designed to be generic and has no specific; purpose. '``llvm.var.annotation``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.var.annotation(ptr <val>, ptr <str>, ptr <str>, i32 <int>). Overview:; """""""""""""""""". The '``llvm.var.annotation``' intrinsic. Arguments:; """""""""""""""""""". The first argument is a pointer to a value, the second is a pointer to a; global string, the third is a pointer to a global string which is the; source file name, and the last argument is the line number. Semantics:; """""""""""""""""""". This intrinsic allows annotation of local variables with arbitrary; strings. This can be useful for special purpose optimizations that want; to look for these annotations. These have no other defined use; they are; ignored by code generation and optimization. '``llvm.ptr.annotation.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use '``llvm.ptr.annotation``' on a; pointer to an integer of any width. *NOTE* you must specify an address space for; the pointer. The identifier for the default address space is the integer; '``0``'. ::. declare ptr @llvm.ptr.annotation.p0(ptr <val>, ptr <str>, ptr <str>, i32 <int>); declare ptr @llvm.ptr.annotation.p1(ptr addrspace(1) <val>, ptr <str>, ptr <str>, i32 <int>). Overview:; """""""""""""""""". The '``llvm.ptr.annotation``' intrinsic. Arguments:; """""""""""""""""""". The first argument is a pointer to an integer value of arbitrary bitwidth; (result of some expression), the second is a pointer to a global string, the; third is a pointer to a global string which is the source file name, and the; last argument is the line number. It returns the value of the first argument. Semantics:; """""""""""""""""""". This intrinsic all",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:924839,optimiz,optimization,924839,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic loads. The optional constant ``align`` argument specifies the alignment of the; operation (that is, the alignment of the memory address). It is the; responsibility of the code emitter to ensure that the alignment information is; correct. Overestimating the alignment results in undefined behavior.; Underestimating the alignment may produce less efficient code. An alignment of;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:413263,load,load,413263,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"atomic with respect to calls to ``objc_storeWeak`` on ``object``. .. _arc.runtime.objc_moveWeak:. ``void objc_moveWeak(id *dest, id *src);``; ------------------------------------------. *Precondition:* ``src`` is a valid pointer which either contains a null pointer; or has been registered as a ``__weak`` object. ``dest`` is a valid pointer; which has not been registered as a ``__weak`` object. ``dest`` is initialized to be equivalent to ``src``, potentially registering it; with the runtime. ``src`` may then be left in its original state, in which; case this call is equivalent to :ref:`objc_copyWeak; <arc.runtime.objc_copyWeak>`, or it may be left as null. Must be atomic with respect to calls to ``objc_storeWeak`` on ``src``. .. _arc.runtime.objc_release:. ``void objc_release(id value);``; --------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a; release operation exactly as if the object had been sent the ``release``; message. .. _arc.runtime.objc_retain:. ``id objc_retain(id value);``; -----------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation exactly as if the object had been sent the ``retain`` message. Always returns ``value``. .. _arc.runtime.objc_retainAutorelease:. ``id objc_retainAutorelease(id value);``; ----------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation followed by an autorelease operation. Equivalent to the following; code:. .. code-block:: objc. id objc_retainAutorelease(id value) {; return objc_autorelease(objc_retain(value));; }. Always returns ``value``. .. _arc.runtime.objc_retainAutoreleaseReturnValue:. ``id objc_retainAutoreleaseReturnValue(id value);``; ------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:113300,perform,performs,113300,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance,"atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If O",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:269409,load,load,269409,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object files and the standard linker tool. ``lto_module_t``; ----------------. A non-native object file is handled via an ``lto_module_t``. The following; functions allow the linker to check if a file (on disk or in a memory buffer) is; a file which libLTO can process:. .. code-block:: c. lto_module_is_object_file(const char*); lto_module_is_object_file_for_target(const char*, const char*); lto_module_is_object_file_in_memory(const void*, size_t); lto_module_is_object_file_in_memory_for_target(const void*, size_t, const char*). If the object file can be processed by ``libLTO``, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:8208,optimiz,optimizer,8208,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"atory for atomic loads and stores. Other Things to Consider; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Use ptrtoint/inttoptr sparingly (they interfere with pointer aliasing; analysis), prefer GEPs. #. Prefer globals over inttoptr of a constant address - this gives you; dereferencability information. In MCJIT, use getSymbolAddress to provide; actual address. #. Be wary of ordered and atomic memory operations. They are hard to optimize; and may not be well optimized by the current optimizer. Depending on your; source language, you may consider using fences instead. #. If calling a function which is known to throw an exception (unwind), use; an invoke with a normal destination which contains an unreachable; instruction. This form conveys to the optimizer that the call returns; abnormally. For an invoke which neither returns normally or requires unwind; code in the current function, you can use a noreturn call instruction if; desired. This is generally not required because the optimizer will convert; an invoke with an unreachable unwind destination to a call instruction. #. Use profile metadata to indicate statically known cold paths, even if; dynamic profiling information is not available. This can make a large; difference in code placement and thus the performance of tight loops. #. When generating code for loops, try to avoid terminating the header block of; the loop earlier than necessary. If the terminator of the loop header; block is a loop exiting conditional branch, the effectiveness of LICM will; be limited for loads not in the header. (This is due to the fact that LLVM; may not know such a load is safe to speculatively execute and thus can't; lift an otherwise loop invariant load unless it can prove the exiting; condition is not taken.) It can be profitable, in some cases, to emit such; instructions into the header even if they are not used along a rarely; executed path that exits the loop. This guidance specifically does not; apply if the condition which terminates the loo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:6916,optimiz,optimizer,6916,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizer']
Performance,"atrix.multiply.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare vectorty @llvm.matrix.multiply.*(vectorty %A, vectorty %B, i32 <OuterRows>, i32 <Inner>, i32 <OuterColumns>). Overview:; """""""""""""""""". The '``llvm.matrix.multiply.*``' intrinsics treat ``%A`` as a ``<OuterRows> x; <Inner>`` matrix, ``%B`` as a ``<Inner> x <OuterColumns>`` matrix, and; multiplies them. The result matrix is returned in the result vector. Arguments:; """""""""""""""""""". The first vector argument ``%A`` corresponds to a matrix with ``<OuterRows> *; <Inner>`` elements, and the second argument ``%B`` to a matrix with; ``<Inner> * <OuterColumns>`` elements. Arguments ``<OuterRows>``,; ``<Inner>`` and ``<OuterColumns>`` must be positive, constant integers. The; returned vector must have ``<OuterRows> * <OuterColumns>`` elements.; Vectors ``%A``, ``%B``, and the returned vector all have the same float or; integer element type. '``llvm.matrix.column.major.load.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare vectorty @llvm.matrix.column.major.load.*(; ptrty %Ptr, i64 %Stride, i1 <IsVolatile>, i32 <Rows>, i32 <Cols>). Overview:; """""""""""""""""". The '``llvm.matrix.column.major.load.*``' intrinsics load a ``<Rows> x <Cols>``; matrix using a stride of ``%Stride`` to compute the start address of the; different columns. The offset is computed using ``%Stride``'s bitwidth. This; allows for convenient loading of sub matrixes. If ``<IsVolatile>`` is true, the; intrinsic is considered a :ref:`volatile memory access <volatile>`. The result; matrix is returned in the result vector. If the ``%Ptr`` argument is known to; be aligned to some boundary, this can be specified as an attribute on the; argument. Arguments:; """""""""""""""""""". The first argument ``%Ptr`` is a pointer type to the returned vector type, and; corresponds to the start address to load from. The second argument ``%Strid",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:677605,load,load,677605,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"attributes described here. However, you should consider using proper naming; conventions or the objc_method_family; attribute, if applicable.; Attribute 'ns_returns_retained'; (Clang-specific); The GCC-style (Clang-specific) attribute 'ns_returns_retained' allows one to; annotate an Objective-C method or C function as returning a retained Cocoa; object that the caller is responsible for releasing (via sending a; release message to the object). The Foundation framework defines a; macro NS_RETURNS_RETAINED that is functionally equivalent to the; one shown below.; Placing on Objective-C methods: For Objective-C methods, this; annotation essentially tells the analyzer to treat the method as if its name; begins with ""alloc"" or ""new"" or contains the word; ""copy"".; Placing on C functions: For C functions returning Cocoa objects, the; analyzer typically does not make any assumptions about whether or not the object; is returned retained. Explicitly adding the 'ns_returns_retained' attribute to C; functions allows the analyzer to perform extra checking.; Example. $ cat test.m; #import <Foundation/Foundation.h>. #ifndef __has_feature // Optional.; #define __has_feature(x) 0 // Compatibility with non-clang compilers.; #endif. #ifndef NS_RETURNS_RETAINED; #if __has_feature(attribute_ns_returns_retained); #define NS_RETURNS_RETAINED __attribute__((ns_returns_retained)); #else; #define NS_RETURNS_RETAINED; #endif; #endif. @interface MyClass : NSObject {}; - (NSString*) returnsRetained NS_RETURNS_RETAINED;; - (NSString*) alsoReturnsRetained;; @end. @implementation MyClass; - (NSString*) returnsRetained {; return [[NSString alloc] initWithCString:""no leak here""];; }; - (NSString*) alsoReturnsRetained {; return [[NSString alloc] initWithCString:""flag a leak""];; }; @end. Running scan-build on this source file produces the following output:. Attribute 'ns_returns_not_retained'; (Clang-specific); The 'ns_returns_not_retained' attribute is the complement of 'ns_returns_retained'. Where a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html:4141,perform,perform,4141,interpreter/llvm-project/clang/www/analyzer/annotations.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html,2,['perform'],['perform']
Performance,"attributes*:sub:`opt` *config-macro-list*:sub:`opt`. *config-macro-list*:; *identifier* (',' *identifier*)*. Each *identifier* in the *config-macro-list* specifies the name of a macro. The compiler is required to maintain different variants of the given module for differing definitions of any of the named macros. A *config-macros-declaration* shall only be present on a top-level module, i.e., a module that is not nested within an enclosing module. The ``exhaustive`` attribute specifies that the list of macros in the *config-macros-declaration* is exhaustive, meaning that no other macro definition is intended to have an effect on the API of that module. .. note::. The ``exhaustive`` attribute implies that any macro definitions; for macros not listed as configuration macros should be ignored; completely when building the module. As an optimization, the; compiler could reduce the number of unique module variants by not; considering these non-configuration macros. This optimization is not; yet implemented in Clang. A translation unit shall not import the same module under different definitions of the configuration macros. .. note::. Clang implements a weak form of this requirement: the definitions; used for configuration macros are fixed based on the definitions; provided by the command line. If an import occurs and the definition; of any configuration macro has changed, the compiler will produce a; warning (under the control of ``-Wconfig-macros``). **Example:** A logging library might provide different API (e.g., in the form of different definitions for a logging macro) based on the ``NDEBUG`` macro setting:. .. parsed-literal::. module MyLogger {; umbrella header ""MyLogger.h""; config_macros [exhaustive] NDEBUG; }. Conflict declarations; ~~~~~~~~~~~~~~~~~~~~~; A *conflict-declaration* describes a case where the presence of two different modules in the same translation unit is likely to cause a problem. For example, two modules may provide similar-but-incompatible func",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:47439,optimiz,optimization,47439,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['optimiz'],['optimization']
Performance,"ault the ``Release`` option uses the ``-O3``; optimization level, and ``RelWithDebInfo`` uses ``-O2``. If you want to generate; debug information and use ``-O3`` you can override the; *CMAKE_<LANG>_FLAGS_RELWITHDEBINFO* option for C and CXX.; DistributionExample.cmake does this. Another easy to use option is Link-Time-Optimization. You can set the; *LLVM_ENABLE_LTO* option on your stage-2 build to ``Thin`` or ``Full`` to enable; building LLVM with LTO. These options will significantly increase link time of; the binaries in the distribution, but it will create much faster binaries. This; option should not be used if your distribution includes static archives, as the; objects inside the archive will be LLVM bitcode, which is not portable. The :doc:`AdvancedBuilds` documentation describes the built-in tooling for; generating LLVM profiling information to drive Profile-Guided-Optimization. The; in-tree profiling tests are very limited, and generating the profile takes a; significant amount of time, but it can result in a significant improvement in; the performance of the generated binaries. In addition to PGO profiling we also have limited support in-tree for generating; linker order files. These files provide the linker with a suggested ordering for; functions in the final binary layout. This can measurably speed up clang by; physically grouping functions that are called temporally close to each other.; The current tooling is only available on Darwin systems with ``dtrace(1)``. It; is worth noting that dtrace is non-deterministic, and so the order file; generation using dtrace is also non-deterministic. Options for Reducing Size; =========================. .. warning::; Any steps taken to reduce the binary size will come at a cost of runtime; performance in the generated binaries. The simplest and least significant way to reduce binary size is to set the; *CMAKE_BUILD_TYPE* variable to ``MinSizeRel``, which will set the compiler; optimization level to ``-Os`` which opti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:9283,perform,performance,9283,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['perform'],['performance']
Performance,"ausing several failures in the CMS code when inverting matrices.; ; Add the Cholesky decomposition method for symmetric positive defined matrices (thanks to Manuel Schiller). A class has been introduced,; ROOT::Math::CholeskyDecomp which provaids methods for decomposing or inverting a matrix and also for solving a linear system.; . New methods have also been added in SMatrix: bool SMatrix::InvertChol() and SMatrix & SMatrix::InverseChol(ifail) for the inversion of a symmetric positive defined matrix. New specialized implementation exists up to matrices with sizes 6x6. The speed is comparable to the Cramer method (SMatrix::InvertFast), but with much better accuracy. The new InvertChol method is in any case faster than the general inverter method for all symmetric matrices (SMatrix::Invert), which uses the Bunch-Kaufman decomposition.; Add also a new free function, ROOT::Math::SolveChol for solving a symmetric linear system. For users who need the solution, using this functions avoid for them performing the inversion and then a matrix multiplication. Add support in the SMatrix class for operator m[i][j]; Add in the dictionary the typedefs for some square and symmetrix matrices based on double and floats (up to size 7) defined in the file Math/SMatrixDfwd and Math/SMatrixFfwd; . Minuit. Apply various improvements in the TMInuitMInimizer class thanks to the feedback of Alfio Lazzaro:; ; implement Hess() and CovMatrixStatus();; add new method based on SEEK. The Tolerance() value can be used to specify the volume (in unit of sigma) for searching for the global minimum; fix some of the methods, like NCalls() and GlobalCC(); . Minuit2. Apply some fixes in MnHesse and MnPosDef classes to check correctly variables to not be zero.; (use same checks as in F77Minuit); ; Fix a bug introduced in DavidonErrorCalculator when checking for delgam. Negative values are allowed. This fixes a test problem given privately by A. Suter.; ; Uses also a tighter condition on edm when exiting th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html:4695,perform,performing,4695,math/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html,2,['perform'],['performing']
Performance,"ave independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 channel. Therefore, the vector; and scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different SAs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is; required to ensure synchronization between vector memory operations of; different SAs. It ensures a previous vector memory operation has completed; before executing a subsequent vector memory and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence.; * On GFX10.3 and GFX11 a memory attached last level (MALL) cache exists for GPU memory.; The MALL cache is fully coherent with GPU memory and has no impact on system; coherence. All agents (GPU and CPU) access GPU memory through the MALL cache. Scalar memory operations are o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:339005,queue,queue,339005,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"ave their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; local load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:377316,load,load,377316,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ave their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-trap-handler-abi:. Trap Handler ABI; ~~~~~~~~~~~~~~~~. For code objects generated by the AMDGPU backend for HSA ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:379279,load,load,379279,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ave this information; carried over to machine instructions. Asm printer (or JIT) can use this; information to add the ""lock"" prefix. //===---------------------------------------------------------------------===//. struct B {; unsigned char y0 : 1;; };. int bar(struct B* a) { return a->y0; }. define i32 @bar(%struct.B* nocapture %a) nounwind readonly optsize {; %1 = getelementptr inbounds %struct.B* %a, i64 0, i32 0; %2 = load i8* %1, align 1; %3 = and i8 %2, 1; %4 = zext i8 %3 to i32; ret i32 %4; }. bar: # @bar; # %bb.0:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. Missed optimization: should be movl+andl. //===---------------------------------------------------------------------===//. The x86_64 abi says:. Booleans, when stored in a memory object, are stored as single byte objects the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = xor i32 %b, %a ; <i32> [#uses=1]; %tmp6 = and i32 %tmp, 255 ; <i32> [#uses=1]; %cmp = icmp eq i32 %tmp6, 0 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	xorl	%esi, %edi; 	testb	$-1, %dil; 	sete	%al; 	movzbl	%al, %eax; 	ret. A cmpb instead of the xorl+testb would be one instruction shorter. //===---------------------------------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:35578,load,load,35578,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"ave your; scripts compiled, linked and dynamically loaded using the C++ compiler; and linker. The advantage of this is that your scripts will run with the; speed of compiled C++ and that you can use language constructs that are; not fully supported by Cling. On the other hand, you cannot use any Cling; shortcuts (see ""C++ Extensions To Ease Scripting"" above) and for small scripts, the; overhead of the compile/link cycle might be larger than just executing; the script in the interpreter. ACLiC will build a dictionary and a shared library from your C++; script, using the compiler and the compiler options that were used to; compile the ROOT executable. You do not have to write a Makefile; remembering the correct compiler options, and you do not have to exit; ROOT. ### Usage. Before you can compile your interpreted script you need to add include; statements for the classes used in the script. Once you did that, you; can build and load a shared library containing your script. To load it; use the command `.L` and append the file name with a `+`. ``` {.cpp}; root[] .L MyScript.C+; ```. The + option generates the shared library and names it by taking; the name of the file ""filename"" but replacing the dot before the; extension by an underscore and by adding the shared library extension; for the current platform. For example on most platforms, `hsimple.cxx`; will generate `hsimple_cxx.so`. The + command rebuild the library only if the script or any of the; files it includes are newer than the library. When checking the; timestamp, ACLiC generates a dependency file which name is the same as; the library name, just replacing the 'so' extension by the extension; 'd'. For example on most platforms, `hsimple.cxx` will generate; `hsimple_cxx.d`. To ensure that the shared library is rebuilt you can use the ++; syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. To build, load, and execute the function with the same name as the; file you can use the `.x` command. This is the same as exe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:13642,load,load,13642,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['load']
Performance,"avefront - local; - workgroup - generic; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214429,load,load,214429,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"avefront Offset to the queue base address in the V#. The; result is a V# with a base address pointing to the beginning of the wavefront; scratch backing memory. The Private Segment Buffer is always requested, but the Private Segment; Wavefront Offset is only requested if it is used (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). .. _amdgpu-amdhsa-memory-model:. Memory Model; ~~~~~~~~~~~~. This section describes the mapping of the LLVM memory model onto AMDGPU machine; code (see :ref:`memmodel`). The AMDGPU backend supports the memory synchronization scopes specified in; :ref:`amdgpu-memory-scopes`. The code sequences used to implement the memory model specify the order of; instructions that a single thread must execute. The ``s_waitcnt`` and cache; management instructions such as ``buffer_wbinvl1_vol`` are defined with respect; to other memory instructions executed by the same thread. This allows them to be; moved earlier or later which can allow them to be combined with other instances; of the same instruction, or hoisted/sunk out of loops to improve performance.; Only the instructions related to the memory model are given; additional; ``s_waitcnt`` instructions are required to ensure registers are defined before; being used. These may be able to be combined with the memory model ``s_waitcnt``; instructions as described above. The AMDGPU backend supports the following memory models:. HSA Memory Model [HSA]_; The HSA memory model uses a single happens-before relation for all address; spaces (see :ref:`amdgpu-address-spaces`).; OpenCL Memory Model [OpenCL]_; The OpenCL memory model which has separate happens-before relations for the; global and local address spaces. Only a fence specifying both global and; local address space, and seq_cst instructions join the relationships. Since; the LLVM ``memfence`` instruction does not allow an address space to be; specified the OpenCL fence has to conservatively assume both local and; global address space was specifi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:201494,perform,performance,201494,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performance']
Performance,"aw \; sourceClass=""TAxis"" \; source=""fXbins"" \; targetClass=""TAxis"" \; target=""fXbins"" \; version=""[-5]"" \; include=""TAxis.h"" \; code=""\; {\; Float_t * xbins=0; \; Int_t n = buffer.ReadArray( xbins ); \; fXbins.Set( xbins ); \; }"". * For REFLEX dictionaries:. <ioread sourceClass=""ClassA""; source=""double m_a; double m_b; double m_c""; version=""[4-5,7,9,12-]""; checksum=""[12345,123456]""; targetClass=""ClassB""; target=""m_x""; embed=""true""; include=""iostream,cstdlib"">; <![CDATA[; m_x = onfile.m_a * onfile.m_b * onfile.m_c;; ]] >; </ioread>. <ioreadraw sourceClass=""TAxis""; source=""fXbins""; targetClass=""TAxis""; target=""fXbins""; version=""[-5]""; include=""TAxis.h"">; <![CDATA[; Float_t *xbins = 0;; Int_t n = buffer.ReadArray( xbins ) ;; fXbins.Set( xbins );; ]] >; </ioreadraw>. The variables in the rules have the following meaning:. * sourceClass - The field defines the on-disk class that is the input for the rule.; * source - A semicolon-separated list of values defining the source class data members; that need to be cached and accessible via object proxy when the rule is; executed. The values are either the names of the data members or the type-name; pairs (separated by a space). If types are specified then the ondisk structure; can be generated and used in the code snippet defined by the user.; * version - A list of versions of the source class that can be an input for this rule.; The list has to be enclosed in a square bracket and be a comma-separated; list of versions or version ranges. The version is an integer number, whereas; the version range is one of the following:;  ""a-b"" - a and b are integers and the expression means all the numbers between; and including a and b;  ""-a"" - a is an integer and the expression means all the version numbers smaller; than or equal to a;  ""a-"" - a is an integer and the expression means all the version numbers greater; than or equal to a; * checksum - A list of checksums of the source class that can be an input for this; rule. The list h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:3060,cache,cached,3060,io/doc/DataModelEvolution.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt,2,['cache'],['cached']
Performance,"away during the compilation process. This meta information provides an LLVM; user a relationship between generated code and the original program source; code. Currently, there are two backend consumers of debug info: DwarfDebug and; CodeViewDebug. DwarfDebug produces DWARF suitable for use with GDB, LLDB, and; other DWARF-based debuggers. :ref:`CodeViewDebug <codeview>` produces CodeView,; the Microsoft debug info format, which is usable with Microsoft debuggers such; as Visual Studio and WinDBG. LLVM's debug information format is mostly derived; from and inspired by DWARF, but it is feasible to translate into other target; debug info formats such as STABS. It would also be reasonable to use debug information to feed profiling tools; for analysis of generated code, or, tools for reconstructing the original; source from generated code. .. _intro_debugopt:. Debug information and optimizations; -----------------------------------. An extremely high priority of LLVM debugging information is to make it interact; well with optimizations and analysis. In particular, the LLVM debug; information provides the following guarantees:. * LLVM debug information **always provides information to accurately read; the source-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means that, with effort, the LLVM; optimizers could optimize debug code just as well as non-debug code",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:3762,optimiz,optimizations,3762,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"ax # Conditionally update predicate state.; shrxq %rax, %rsi, %rsi # Shift away bits if misspeculating.; movl (%rsi), %edi; ```. This will collapse the register to zero or one, and everything but the offset; in the addressing mode to be less than or equal to 9. This means the full; address can only be guaranteed to be less than `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35011,optimiz,optimization,35011,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"ax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; indicating the index value that is associated with that optimization. To skip; optimizations, pass the value of the last optimization to be performed as the; opt-bisect-limit. All optimizations with a higher index value will be skipped. In order to use the -opt-bisect-limit option with a driver that provides a; wrapper around the LLVM core library, an additional prefix option may be; required, as defined by the driver. For example, to use this option with; clang, the ""-mllvm"" prefix must be used. A typical clang invocation would look; like this:. ::. clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:2179,optimiz,optimizations,2179,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,1,['optimiz'],['optimizations']
Performance,"axis as; requested [here](https://sft.its.cern.ch/jira/browse/ROOT-35).; For example, to accept 6 digits number like 900000 on the X axis of the; histogram `h` call:; ```{.cpp}; h->GetXaxis()->SetMaxDigits(6);; ```; - Auto-coloring for TF1 (drawing options PFC, PLC and PMC) is implemented. ## 3D Graphics Libraries; - When a LEGO plot was drawn with Theta=90, the X and Y axis were misplaced. ## Geometry Libraries; - Added system of units and physical constants matching the CLHEP port to Geant4, adapted to ROOT by Marko Petric.; - Computing radiation length and nuclear interaction length for mixtures as in Geant4 to have; numeric matching of average properties.; - Added support for reading region definition and production cuts for e+, e-, gamma, p; from GDML files; - Added support for reading/writing parts of the geometry tree to GDML (Markus Frank). ## Database Libraries. ## Networking Libraries. ## GUI Libraries. ## Montecarlo Libraries. ## Parallelism; - Fix issue which prevented nested TBB task execution without race conditions, e.g. in TDataFrame; - Fix race condition in TTreeProcessorMT due to TBB nested task execution; - The TTaskGroup class has been added to the ROOT::Experimental namespace. It allows to submit to the runtime; item of work which are dealt with in parallel;; - The Async template function has been added the ROOT::Experimental namespace. The template function is analogous; to *std::async* but without the possibility of specifying the execution policy and without creating a thread but; directly submitting the work to the runtime in order to use the same pool as any other item of work spawned by ROOT.; - The TFuture template has been added to the ROOT::Experimental namespace. It represents a future and is compatible; with the ROOT::Experimental::Async function. It has the same properties of an STL future and can be initialised by; one of these classes. For example, *TFuture<int> = std::async(myfunc,a,b,c);*; - Reintroduced greedy reduction in TProc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:23485,race condition,race conditions,23485,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['race condition'],['race conditions']
Performance,"axis of the; histogram `h` call:; ```{.cpp}; h->GetXaxis()->SetMaxDigits(6);; ```; - Auto-coloring for TF1 (drawing options PFC, PLC and PMC) is implemented. ## 3D Graphics Libraries; - When a LEGO plot was drawn with Theta=90, the X and Y axis were misplaced. ## Geometry Libraries; - Added system of units and physical constants matching the CLHEP port to Geant4, adapted to ROOT by Marko Petric.; - Computing radiation length and nuclear interaction length for mixtures as in Geant4 to have; numeric matching of average properties.; - Added support for reading region definition and production cuts for e+, e-, gamma, p; from GDML files; - Added support for reading/writing parts of the geometry tree to GDML (Markus Frank). ## Database Libraries. ## Networking Libraries. ## GUI Libraries. ## Montecarlo Libraries. ## Parallelism; - Fix issue which prevented nested TBB task execution without race conditions, e.g. in TDataFrame; - Fix race condition in TTreeProcessorMT due to TBB nested task execution; - The TTaskGroup class has been added to the ROOT::Experimental namespace. It allows to submit to the runtime; item of work which are dealt with in parallel;; - The Async template function has been added the ROOT::Experimental namespace. The template function is analogous; to *std::async* but without the possibility of specifying the execution policy and without creating a thread but; directly submitting the work to the runtime in order to use the same pool as any other item of work spawned by ROOT.; - The TFuture template has been added to the ROOT::Experimental namespace. It represents a future and is compatible; with the ROOT::Experimental::Async function. It has the same properties of an STL future and can be initialised by; one of these classes. For example, *TFuture<int> = std::async(myfunc,a,b,c);*; - Reintroduced greedy reduction in TProcessExecutor.; - Fix empty chunks in the result vector of TThreadExecutor::Map. If the integer partition of the data in nChunks causes ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:23528,race condition,race condition,23528,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['race condition'],['race condition']
Performance,"ay be both ``null`` and; ``dereferenceable(<n>)``). This attribute may only be applied to; pointer typed parameters. ``swiftself``; This indicates that the parameter is the self/context parameter. This is not; a valid attribute for return values and can only be applied to one; parameter. .. _swiftasync:. ``swiftasync``; This indicates that the parameter is the asynchronous context parameter and; triggers the creation of a target-specific extended frame record to store; this pointer. This is not a valid attribute for return values and can only; be applied to one parameter. ``swifterror``; This attribute is motivated to model and optimize Swift error handling. It; can be applied to a parameter with pointer to pointer type or a; pointer-sized alloca. At the call site, the actual argument that corresponds; to a ``swifterror`` parameter has to come from a ``swifterror`` alloca or; the ``swifterror`` parameter of the caller. A ``swifterror`` value (either; the parameter or the alloca) can only be loaded and stored from, or used as; a ``swifterror`` argument. This is not a valid attribute for return values; and can only be applied to one parameter. These constraints allow the calling convention to optimize access to; ``swifterror`` variables by associating them with a specific register at; call boundaries rather than placing them in memory. Since this does change; the calling convention, a function which uses the ``swifterror`` attribute; on a parameter is not ABI-compatible with one which does not. These constraints also allow LLVM to assume that a ``swifterror`` argument; does not alias any other memory visible within a function and that a; ``swifterror`` alloca passed as an argument does not escape. ``immarg``; This indicates the parameter is required to be an immediate; value. This must be a trivial immediate integer or floating-point; constant. Undef or constant expressions are not valid. This is; only valid on intrinsic declarations and cannot be applied to a; call s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:63567,load,loaded,63567,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"ay be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 channel. Therefore, the vector; and scalar memory operations performed by wavefront",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:338081,cache,cache,338081,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'queue']","['cache', 'queue']"
Performance,"ay be; replaced at link time (``linkonce``, ``weak``, ``extern_weak`` and ``common``; linkage types), the allocation size and alignment of the definition it resolves; to must be greater than or equal to that of the declaration or replaceable; definition, otherwise the behavior is undefined. Globals can also have a :ref:`DLL storage class <dllstorageclass>`,; an optional :ref:`runtime preemption specifier <runtime_preemption_model>`,; an optional :ref:`global attributes <glattrs>` and; an optional list of attached :ref:`metadata <metadata>`. Variables and aliases can have a; :ref:`Thread Local Storage Model <tls_model>`. Globals cannot be or contain :ref:`Scalable vectors <t_vector>` because their; size is unknown at compile time. They are allowed in structs to facilitate; intrinsics returning multiple values. Generally, structs containing scalable; vectors are not considered ""sized"" and cannot be used in loads, stores, allocas,; or GEPs. The only exception to this rule is for structs that contain scalable; vectors of the same type (e.g. ``{<vscale x 2 x i32>, <vscale x 2 x i32>}``; contains the same type while ``{<vscale x 2 x i32>, <vscale x 2 x i64>}``; doesn't). These kinds of structs (we may call them homogeneous scalable vector; structs) are considered sized and can be used in loads, stores, allocas, but; not GEPs. Syntax::. @<GlobalVarName> = [Linkage] [PreemptionSpecifier] [Visibility]; [DLLStorageClass] [ThreadLocal]; [(unnamed_addr|local_unnamed_addr)] [AddrSpace]; [ExternallyInitialized]; <global | constant> <Type> [<InitializerConstant>]; [, section ""name""] [, partition ""name""]; [, comdat [($name)]] [, align <Alignment>]; [, code_model ""model""]; [, no_sanitize_address] [, no_sanitize_hwaddress]; [, sanitize_address_dyninit] [, sanitize_memtag]; (, !name !N)*. For example, the following defines a global in a numbered address space; with an initializer, section, and alignment:. .. code-block:: llvm. @G = addrspace(5) constant float 1.0, section ""foo"", align ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:36329,scalab,scalable,36329,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"ay, that we will call *direct mapping*, is based on the use; of methods of the classes ``TargetRegisterInfo``, and ``MachineOperand``. The; second way, that we will call *indirect mapping*, relies on the ``VirtRegMap``; class in order to insert loads and stores sending and getting values to and from; memory. The direct mapping provides more flexibility to the developer of the register; allocator; however, it is more error prone, and demands more implementation; work. Basically, the programmer will have to specify where load and store; instructions should be inserted in the target function being compiled in order; to get and store values in memory. To assign a physical register to a virtual; register present in a given operand, use ``MachineOperand::setReg(p_reg)``. To; insert a store instruction, use ``TargetInstrInfo::storeRegToStackSlot(...)``,; and to insert a load instruction, use ``TargetInstrInfo::loadRegFromStackSlot``. The indirect mapping shields the application developer from the complexities of; inserting load and store instructions. In order to map a virtual register to a; physical one, use ``VirtRegMap::assignVirt2Phys(vreg, preg)``. In order to map; a certain virtual register to memory, use; ``VirtRegMap::assignVirt2StackSlot(vreg)``. This method will return the stack; slot where ``vreg``'s value will be located. If it is necessary to map another; virtual register to the same stack slot, use; ``VirtRegMap::assignVirt2StackSlot(vreg, stack_location)``. One important point; to consider when using the indirect mapping, is that even if a virtual register; is mapped to memory, it still needs to be mapped to a physical register. This; physical register is the location where the virtual register is supposed to be; found before being stored or after being reloaded. If the indirect strategy is used, after all the virtual registers have been; mapped to physical registers or stack slots, it is necessary to use a spiller; object to place load and store instruction",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:65023,load,load,65023,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"ayer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContex",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2169,optimiz,optimization,2169,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"ays converted to Python's ``str`` on function; returns (the exception is return-by-reference when assigning), but not when; its direct use is more likely such as in the case of (global) variables or; when iterating over a ``std::vector<std::string>``. The rest of this section shows examples of how STL containers can be used in; a natural, pythonistic, way. `std::vector`; -------------. A ``std::vector`` is the most commonly used C++ container type because it is; more efficient and performant than specialized types such as ``list`` and; ``map``, unless the number of elements gets very large.; Python has several similar types, from the builtin ``tuple`` and ``list``,; the ``array`` from builtin module ``array``, to ""as-good-as-builtin""; ``numpy.ndarray``.; A vector is more like the latter two in that it can contain only one type,; but more like the former two in that it can contain objects.; In practice, it can interplay well with all these containers, but e.g.; efficiency and performance can differ significantly. A vector can be instantiated from any sequence, including generators, and; vectors of objects can be recursively constructed.; If the template type is to be inferred from the argument to the constructor,; the first element needs to be accessible, which precludes generators. .. code-block:: python. >>> from cppyy.gbl.std import vector, pair; >>> v = vector[int](range(10)) # from generator; >>> len(v); 10; >>> v = vector([x for x in range(10)]) # type inferred; >>> type(v); <class cppyy.gbl.std.vector<int> at 0x12d226f00>; >>> len(v); 10; >>> vp = vector[pair[int, int]](((1, 2), (3, 4))); >>> len(vp); 2; >>> vp[1][0]; 3; >>>. To extend a vector in-place with another sequence object, use ``+=``, just as; for Python's ``list``:. .. code-block:: python. >>> v += range(10, 20); >>> len(v); 20; >>>. Indexing and slicing of a vector follows the normal Python slicing rules;; printing a vector prints all its elements:. .. code-block:: python. >>> v[1]; 1; >>> v[-1]; 1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst:2623,perform,performance,2623,bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,1,['perform'],['performance']
Performance,"b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===----------------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25378,optimiz,optimized,25378,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"bage collection intrinsics; <gc_intrinsics>` that offer support for a broad class of collector models. For; instance, the intrinsics permit:. * semi-space collectors. * mark-sweep collectors. * generational collectors. * incremental collectors. * concurrent collectors. * cooperative collectors. * reference counting. We hope that the support built into the LLVM IR is sufficient to support a; broad class of garbage collected languages including Scheme, ML, Java, C#,; Perl, Python, Lua, Ruby, other scripting languages, and more. Note that LLVM **does not itself provide a garbage collector** --- this should; be part of your language's runtime library. LLVM provides a framework for; describing the garbage collectors requirements to the compiler. In particular,; LLVM provides support for generating stack maps at call sites, polling for a; safepoint, and emitting load and store barriers. You can also extend LLVM -; possibly through a loadable :ref:`code generation plugins <plugin>` - to; generate code and data structures which conforms to the *binary interface*; specified by the *runtime library*. This is similar to the relationship between; LLVM and DWARF debugging info, for example. The difference primarily lies in; the lack of an established standard in the domain of garbage collection --- thus; the need for a flexible extension mechanism. The aspects of the binary interface with which LLVM's GC support is; concerned are:. * Creation of GC safepoints within code where collection is allowed to execute; safely. * Computation of the stack map. For each safe point in the code, object; references within the stack frame must be identified so that the collector may; traverse and perhaps update them. * Write barriers when storing object references to the heap. These are commonly; used to optimize incremental scans in generational collectors. * Emission of read barriers when loading object references. These are useful; for interoperating with concurrent collectors. There are add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:6028,load,loadable,6028,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['loadable']
Performance,"bal/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:301761,load,loads,301761,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"bal/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:224139,perform,performing,224139,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"bal; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_gl*_inv.;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:371664,load,loads,371664,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"be ""invisible"" at tracking time. Let us suppose that we need to group together two volumes `A` and `B`; into a structure and position this into several other volumes `D,E,` and; `F`. What we need to do is to create a virtual container volume `C`; holding `A` and `B`, then position `C` in the other volumes. Note that `C` is a volume having a determined medium. Since it is not a; real volume, we need to manually set its medium the same as that of; `D,E` or `F` in order to make it invisible' (same physics properties).; In other words, the limitation in proceeding this way is that `D,E,` and; `F` must point to the same medium. If this was not the case, we would; have to define different virtual volumes for each placement: `C`, `C'`; and `C""`, having the same shape but different media matching the; corresponding containers. This might not happen so often, but when it; does, it forces the creation of several extra virtual volumes. Other; limitation comes from the fact that any container is directly used by; navigation algorithms to optimize tracking. These must geometrically; contain their belongings (positioned volumes) so that these do not; extrude its shape boundaries. Not respecting this rule generally leads; to unpredictable results. Therefore `A` and `B` together must fit into; `C` that has to fit also into `D,E,` and `F`. This is not always; straightforward to accomplish, especially when instead of `A` and `B` we; have many more volumes. In order to avoid these problems, one can use for the difficult cases; the class **`TGeoVolumeAssembly`**, representing an assembly of volumes.; This behaves like a normal container volume supporting other volumes; positioned inside, but it has neither shape nor medium. It cannot be; used directly as a piece of the geometry, but just as a temporary; structure helping temporary assembling and positioning volumes. If we define now `C` as an assembly containing `A` and `B`, positioning; the assembly into `D,E` and `F` will actually p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:71073,optimiz,optimize,71073,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['optimiz'],['optimize']
Performance,"be an integer; type whose bit width is a power of two greater than or equal to eight; and less than or equal to a target-specific size limit. For xchg, this; may also be a floating point or a pointer type with the same size constraints; as integers. For fadd/fsub/fmax/fmin, this must be a floating point type. The; type of the '``<pointer>``' operand must be a pointer to that type. If; the ``atomicrmw`` is marked as ``volatile``, then the optimizer is not; allowed to modify the number or order of execution of this; ``atomicrmw`` with other :ref:`volatile operations <volatile>`. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. A ``atomicrmw`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``'; operand are atomically read, modified, and written back. The original; value at the location is returned. The modification is specified by the; operation argument:. - xchg: ``*ptr = val``; - add: ``*ptr = *ptr + val``; - sub: ``*ptr = *ptr - val``; - and: ``*ptr = *ptr & val``; - nand: ``*ptr = ~(*ptr & val)``; - or: ``*ptr = *ptr | val``; - xor: ``*ptr = *ptr ^ val``; - max: ``*ptr = *ptr > val ? *ptr : val`` (using a signed comparison); - min: ``*ptr = *ptr < val ? *ptr : val`` (using a signed comparison); - umax: ``*ptr = *ptr > val ? *ptr : val`` (using an unsigned comparison); - umin: ``*ptr = *ptr < val ? *ptr : val`` (using an unsigned comparison); - fadd: ``*ptr = *ptr + val`` (using floating point arithmetic); - fsub",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:430774,load,load,430774,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"be included manually using ``-include`` or ``-I`` followed by the path; to the header location. The header can be found in the clang source tree or; installation directory. .. code-block:: console. $ clang -I<path to clang sources>/lib/Headers/opencl-c.h test.cl; $ clang -I<path to clang installation>/lib/clang/<llvm version>/include/opencl-c.h/opencl-c.h test.cl. In this example it is assumed that the kernel code contains; ``#include <opencl-c.h>`` just as a regular C include. Because the header is very large and long to parse, PCH (:doc:`PCHInternals`); and modules (:doc:`Modules`) can be used internally to improve the compilation; speed. To enable modules for OpenCL:. .. code-block:: console. $ clang --target=spir-unknown-unknown -c -emit-llvm -Xclang -finclude-default-header -fmodules -fimplicit-module-maps -fmodules-cache-path=<path to the generated module> test.cl. Another way to circumvent long parsing latency for the OpenCL builtin; declarations is to use mechanism enabled by :ref:`-fdeclare-opencl-builtins; <opencl_fdeclare_opencl_builtins>` flag that is available as an alternative; feature. .. _opencl_fdeclare_opencl_builtins:. .. option:: -fdeclare-opencl-builtins. In addition to regular header includes with builtin types and functions using; :ref:`-finclude-default-header <opencl_finclude_default_header>`, clang; supports a fast mechanism to declare builtin functions with; ``-fdeclare-opencl-builtins``. This does not declare the builtin types and; therefore it has to be used in combination with ``-finclude-default-header``; if full functionality is required. **Example of Use**:. .. code-block:: console. $ clang -Xclang -fdeclare-opencl-builtins test.cl. .. _opencl_fake_address_space_map:. .. option:: -ffake-address-space-map. Overrides the target address space map with a fake map.; This allows adding explicit address space IDs to the bitcode for non-segmented; memory architectures that do not have separate IDs for each of the OpenCL; logical address spac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst:4232,latency,latency,4232,interpreter/llvm-project/clang/docs/OpenCLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OpenCLSupport.rst,1,['latency'],['latency']
Performance,"be stale in L2 due to; the memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:252285,load,loads,252285,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"be true or false. ; example with vectors; %v = <2 x i32> <i32 undef, i32 poison>; %a = extractelement <2 x i32> %v, i32 0 ; undef; %b = extractelement <2 x i32> %v, i32 1 ; poison; %add = add i32 %a, %a ; undef. %v.fr = freeze <2 x i32> %v ; element-wise freeze; %d = extractelement <2 x i32> %v.fr, i32 0 ; not undef; %add.f = add i32 %d, %d ; even number. ; branching on frozen value; %poison = add nsw i1 %k, undef ; poison; %c = freeze i1 %poison; br i1 %c, label %foo, label %bar ; non-deterministic branch to %foo or %bar. .. _i_call:. '``call``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = [tail | musttail | notail ] call [fast-math flags] [cconv] [ret attrs] [addrspace(<num>)]; <ty>|<fnty> <fnptrval>(<function args>) [fn attrs] [ operand bundles ]. Overview:; """""""""""""""""". The '``call``' instruction represents a simple function call. Arguments:; """""""""""""""""""". This instruction requires several arguments:. #. The optional ``tail`` and ``musttail`` markers indicate that the optimizers; should perform tail call optimization. The ``tail`` marker is a hint that; `can be ignored <CodeGenerator.html#tail-call-optimization>`_. The; ``musttail`` marker means that the call must be tail call optimized in order; for the program to be correct. This is true even in the presence of; attributes like ""disable-tail-calls"". The ``musttail`` marker provides these; guarantees:. #. The call will not cause unbounded stack growth if it is part of a; recursive cycle in the call graph.; #. Arguments with the :ref:`inalloca <attr_inalloca>` or; :ref:`preallocated <attr_preallocated>` attribute are forwarded in place.; #. If the musttail call appears in a function with the ``""thunk""`` attribute; and the caller and callee both have varargs, then any unprototyped; arguments in register or memory are forwarded to the callee. Similarly,; the return value of the callee is returned to the caller's caller, even; if a void return type is in use. Both markers imply that the callee ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:472618,optimiz,optimizers,472618,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['optimiz', 'perform']","['optimization', 'optimizers', 'perform']"
Performance,"be used to express the maximum acceptable error in the; result of that instruction, in ULPs, thus potentially allowing the; compiler to use a more efficient but less accurate method of computing; it. ULP is defined as follows:. If ``x`` is a real number that lies between two finite consecutive; floating-point numbers ``a`` and ``b``, without being equal to one; of them, then ``ulp(x) = |b - a|``, otherwise ``ulp(x)`` is the; distance between the two non-equal finite floating-point numbers; nearest ``x``. Moreover, ``ulp(NaN)`` is ``NaN``. The metadata node shall consist of a single positive float type number; representing the maximum relative error, for example:. .. code-block:: llvm. !0 = !{ float 2.5 } ; maximum acceptable inaccuracy is 2.5 ULPs. .. _range-metadata:. '``range``' Metadata; ^^^^^^^^^^^^^^^^^^^^. ``range`` metadata may be attached only to ``load``, ``call`` and ``invoke`` of; integer or vector of integer types. It expresses the possible ranges the loaded; value or the value returned by the called function at this call site is in. If; the loaded or returned value is not in the specified range, a poison value is; returned instead. The ranges are represented with a flattened list of integers.; The loaded value or the value returned is known to be in the union of the ranges; defined by each consecutive pair. Each pair has the following properties:. - The type must match the scalar type of the instruction.; - The pair ``a,b`` represents the range ``[a,b)``.; - Both ``a`` and ``b`` are constants.; - The range is allowed to wrap.; - The range should not represent the full or empty set. That is,; ``a!=b``. In addition, the pairs must be in signed order of the lower bound and; they must be non-contiguous. For vector-typed instructions, the range is applied element-wise. Examples:. .. code-block:: llvm. %a = load i8, ptr %x, align 1, !range !0 ; Can only be 0 or 1; %b = load i8, ptr %y, align 1, !range !1 ; Can only be 255 (-1), 0 or 1; %c = call i8 @foo(), !ra",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:284932,load,loaded,284932,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"been; omitted otherwise. .. option:: --minimize, -z. When used when creating a dSYM file, this option will suppress the emission of; the .debug_inlines, .debug_pubnames, and .debug_pubtypes sections since; dsymutil currently has better equivalents: .apple_names and .apple_types. When; used in conjunction with ``--update`` option, this option will cause redundant; accelerator tables to be removed. .. option:: --no-odr. Do not use ODR (One Definition Rule) for uniquing C++ types. .. option:: --no-output. Do the link in memory, but do not emit the result file. .. option:: --no-swiftmodule-timestamp. Don't check the timestamp for swiftmodule files. .. option:: --num-threads <threads>, -j <threads>. Specifies the maximum number (``n``) of simultaneous threads to use when; linking multiple architectures. .. option:: --object-prefix-map <prefix=remapped>. Remap object file paths (but no source paths) before processing. Use; this for Clang objects where the module cache location was remapped using; ``-fdebug-prefix-map``; to help dsymutil find the Clang module cache. .. option:: --oso-prepend-path <path>. Specifies a ``path`` to prepend to all debug symbol object file paths. .. option:: --out <filename>, -o <filename>. Specifies an alternate ``path`` to place the dSYM bundle. The default dSYM; bundle path is created by appending ``.dSYM`` to the executable name. .. option:: --remarks-drop-without-debug. Drop remarks without valid debug locations. Without this flags, all remarks are kept. .. option:: --remarks-output-format <format>. Specify the format to be used when serializing the linked remarks. .. option:: --remarks-prepend-path <path>. Specify a directory to prepend the paths of the external remark files. .. option:: --reproducer <mode>. Specify the reproducer generation mode. Valid options are 'GenerateOnExit',; 'GenerateOnCrash', 'Use', 'Off'. .. option:: --statistics. Print statistics about the contribution of each object file to the linked; debug info. This prints ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/dsymutil.rst:3489,cache,cache,3489,interpreter/llvm-project/llvm/docs/CommandGuide/dsymutil.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/dsymutil.rst,2,['cache'],['cache']
Performance,"before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Method No Fisher Fisher, Mahalanobis Discrimination method. Configuration options for MVA method :. Configuration options reference for MVA method: PDERS. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). VolumeRangeMode No Adaptive Unscaled, MinMax, RMS, Adaptive, kNN Method to determine volume size. KernelEstimator No Box Box, Sphere, Teepee, Gauss, Sinc3, Sinc5, Sinc7, Sinc9, Sinc11, Lanczos2, Lanczos3, Lanczos5, Lanczos8, Trim Kernel estimation function. DeltaFrac No 3  nEventsMin/Max for minmax and rms volume range. NEventsMin No 100  nEventsMin for adaptive volume range. NEventsMax No 200  nEventsMax for adap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:3801,perform,performed,3801,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:268749,load,load,268749,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"before; the following; buffer_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen bef",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:297676,load,load,297676,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"being built. If you are building a cross-compiler,; set it to the target triple of your desired architecture. **LLVM_DOXYGEN_QCH_FILENAME**:STRING; The filename of the Qt Compressed Help file that will be generated when; ``-DLLVM_ENABLE_DOXYGEN=ON`` and; ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON`` are given. Defaults to; ``org.llvm.qch``.; This option is only useful in combination with; ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON``;; otherwise it has no effect. **LLVM_DOXYGEN_QHELPGENERATOR_PATH**:STRING; The path to the ``qhelpgenerator`` executable. Defaults to whatever CMake's; ``find_program()`` can find. This option is only useful in combination with; ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON``; otherwise it has no; effect. **LLVM_DOXYGEN_QHP_CUST_FILTER_NAME**:STRING; See `Qt Help Project`_ for; more information. Defaults to the CMake variable ``${PACKAGE_STRING}`` which; is a combination of the package name and version string. This filter can then; be used in Qt Creator to select only documentation from LLVM when browsing; through all the help files that you might have loaded. This option is only; useful in combination with ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON``;; otherwise it has no effect. .. _Qt Help Project: http://qt-project.org/doc/qt-4.8/qthelpproject.html#custom-filters. **LLVM_DOXYGEN_QHP_NAMESPACE**:STRING; Namespace under which the intermediate Qt Help Project file lives. See `Qt; Help Project`_; for more information. Defaults to ""org.llvm"". This option is only useful in; combination with ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON``; otherwise; it has no effect. **LLVM_DOXYGEN_SVG**:BOOL; Uses .svg files instead of .png files for graphs in the Doxygen output.; Defaults to OFF. .. _llvm_enable_assertions:. **LLVM_ENABLE_ASSERTIONS**:BOOL; Enables code assertions. Defaults to ON if and only if ``CMAKE_BUILD_TYPE``; is *Debug*. **LLVM_ENABLE_BINDINGS**:BOOL; If disabled, do not try to build the OCaml bindings. **LLVM_ENABLE_DIA_SDK**:BOOL; Enable building with MSVC DIA SDK for PDB deb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:19530,load,loaded,19530,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['load'],['loaded']
Performance,"being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/gen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:318697,load,load,318697,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ber of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:7113,load,load,7113,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"best way to decrease your runtime would be transferring the data you need onto a faster; storage medium (ie. a faster disk/drive such as an SSD, or connecting to a faster network; for remote file access), or to use a compression algorithm with a higher compression ratio,; possibly at the cost of the decompression rate.; Changing the number of threads is unlikely to help, and in fact using too many threads may; degrade performance if they make requests to different regions of your local storage. ; * If no '--threads' argument was provided this is 1, otherwise it is the minimum of the value; provided and the number of threads your CPU can run in parallel. It is worth noting that -; on shared systems or if running other heavy applications - the number of your own threads; running at any time may be lower than the limit due to demand on the CPU.; - The 'Real Time' is similar to 'CPU Time / number of threads' AND 'Compressed Throughput' is lower than expected; for your storage medium: this would imply that your CPU threads aren't decompressing data as fast as your storage; medium can provide it, and so decompression is the bottleneck.; The best way to decrease your runtime would be to utilise a system with a faster CPU, or make use; use of more threads when running, or use a compression algorithm with a higher decompression rate such as LZ4,; possibly at the cost of some extra file size. ### A note on caching. If your data is stored on a local disk, the system may cache some/all of the file in memory after it is; first read. If this is realistic of how your analysis will run - then there is no concern. However, if; you expect to only read files once in a while - and as such the files are unlikely to be in the cache -; consider clearing the cache before running rootreadspeed.; On Linux this can be done by running 'echo 3 > /proc/sys/vm/drop_caches' as a superuser,; or a specific file can be dropped from the cache with; `dd of=<FILENAME> oflag=nocache conv=notrunc,fdatasyn",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md:3122,bottleneck,bottleneck,3122,tree/readspeed/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md,1,['bottleneck'],['bottleneck']
Performance,"between `Streamer` writers and readers are frequent; and increase as the number of classes increase. We recommend you use; `rootcling` generated `Streamers` whenever you can, and profit from the; automatic schema evolution. ### Building Class Definitions with the StreamerInfo. A ROOT file's `StreamerInfo `list contains the description of all; versions of all classes in the file. When a file is opened the; `StreamerInfo `is read into memory and it provides enough information to; make the file browsable. The `TStreamerInfo `enables us to recreate a; header file for the class in case the compiled class is not available.; This is done with the `TFile::MakeProject` method. It creates a; directory with the header files for the named classes and a `makefile`; to compile a shared library with the class definitions. ### Example: MakeProject. To explain the details, we use the example of the `ATLFast` project that; is a fast simulation for the ATLAS experiment. The complete source for; `ATLFast` can be down loaded at; <ftp://root.cern.ch/root/atlfast.tar.gz>. Once we compile and run; `ATLFast` we get a ROOT file called `atlfast.root`, containing the; `ATLFast` objects. When we open the file, we get a warning that the file; contains classes that are not in the dictionary. This is correct; since we did not load the class definitions. ``` {.cpp}; root[] TFile f(""atlfast.root""); Warning in <TClass::TClass>: no dictionary for class TMCParticle is available; Warning in <TClass::TClass>: no dictionary for class ATLFMuon available; ```. We can see the `StreamerInfo `for the classes:. ``` {.cpp}; root[] f.ShowStreamerInfo(); ...; StreamerInfo for class: ATLFMuon, version=1; BASE TObject offset= 0 type=66 Basic ROOT object; BASE TAtt3D offset= 0 type= 0 3D attributes; Int_t m_KFcode offset= 0 type= 3 Muon KF-code; Int_t m_MCParticle offset= 0 type= 3 Muon position in MCParticles list; Int_t m_KFmother offset= 0 type= 3 Muon mother KF-code; Int_t m_UseFlag offset= 0 type= 3 Muon energy u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:85057,load,loaded,85057,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['load'],['loaded']
Performance,"between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L0 and L1 caches at the start of each kernel dispatch.; * On dGPU the kernarg backing memory is accessed as MTYPE UC (uncached) to avoid; needing to invalidate the L2 cache.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC (non-coherent). Since the private address space is only accessed; by a single thread, and is always write-before-read, there is never a need to; invalidate these entries from the L0 or L1 caches. Wavefronts are executed in native mode with in-order reporting of loads and; sample instructions. In this mode vmcnt reports completion of load, atomic with; return and sample instructions in order, and the vscnt reports the completion of; store and atomic without return in order. See ``MEM_ORDERED`` field in; :ref:`amdgpu-amdhsa-compute_pgm_rsrc1-gfx6-gfx12-table`. Wavefronts can be executed in WGP or CU wavefront execution mode:. * In WGP wavefront execution mode the wavefronts of a work-group are executed; on the SIMDs of both CUs of the WGP. Therefore, explicit management of the per; CU L0 caches is required for work-group",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:341441,cache,cache,341441,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],['cache']
Performance,"between shader; stages.; 0x1000000B ViewId View id (32-bit unsigned integer) identifies a view of graphic; pipeline instancing.; 0x1000000C StreamOutTable 32-bit pointer to GPU memory containing the stream out target SRD table. This; can only appear for one shader stage per pipeline.; 0x1000000D PerShaderPerfData 32-bit pointer to GPU memory containing the per-shader performance data buffer.; 0x1000000F VertexBufferTable 32-bit pointer to GPU memory containing the vertex buffer SRD table. This can; only appear for one shader stage per pipeline.; 0x10000010 UavExportTable 32-bit pointer to GPU memory containing the UAV export SRD table. This can; only appear for one shader stage per pipeline (PS). These replace color targets; and are completely separate from any UAVs used by the shader. This is optional,; and only used by the PS when UAV exports are used to replace color-target; exports to optimize specific shaders.; 0x10000011 NggCullingData 64-bit pointer to GPU memory containing the hardware register data needed by; some NGG pipelines to perform culling. This value contains the address of the; first of two consecutive registers which provide the full GPU address.; 0x10000015 FetchShaderPtr 64-bit pointer to GPU memory containing the fetch shader subroutine.; ========== ================= ===============================================================================. .. _amdgpu-amdpal-code-object-metadata-user-data-per-shader-table-section:. Per-Shader Table; ################. Low 32 bits of the GPU address for an optional buffer in the ``.data``; section of the ELF. The high 32 bits of the address match the high 32 bits; of the shader's program counter. The buffer can be anything the shader compiler needs it for, and; allows each shader to have its own region of the ``.data`` section.; Typically, this could be a table of buffer SRD's and the data pointed to; by the buffer SRD's, but it could be a flat-address region of memory as; well. Its layout and usage are def",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:418034,perform,perform,418034,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['perform']
Performance,"binned pdf. Nevertheless you can ; switch off this feature by passing AutoBinned(false) to RooAbsPdf::generate(). Mixed binned/unbinned generation from simultaneous pdf. For a RooSimultaneous consisting of exclusively; extended terms it is now possible to generate a mixed binned/unbinned datasets. Components defined; by a binned pdf at the top level are automatically generated binned (unless AutoBinned(false) is set); but it is also possible to generate other component pdfs forcibly binned by adding GenBinned(tagname); to generate(). In that case all component pdfs labeled with pdf->setAttribute(tagname) will be generated; binned. To generate all component binned, the shorthand method AllBinned() can be used. All binned; datasets made by generate are represented as weighted unbinned datasets (of type RooDataSet) rather; than binned datasets of type RooDataHist so that mixed binned/unbinned data is always represented; through a uniform interface. Fix in the optimization strategy of likelihoods constructed from simultaneous pdf. In the parameter; dependency analysis of the components of a simultaneous pdfs parameters originating from 'irrelevant'; constraint terms (i.e. those that don't relate to any of the parameters of that likelihood component) were; not ignored, which could result in a substantial loss of computational efficiency as likelihood; terms were erroneously recalculated even if no relevant parameters was changed. General performance tuning of RooFit to reduce computational overhead. Extensive profiling of; CPU times in call graphas and analysis heap memory use have been performed and many small ; changes have been made to make the code more efficient and use less memory. RooStats Package; AsymptoticCalculator. New Class for doing an hypothesis tests using the asymptotic likelihood formulae, described in the paper from; G. Cowan, K. Cranmer, E. Gross and O. Vitells, Asymptotic formulae for likelihood- based tests of new physics,; Eur. Phys. J., C71 (1), 20",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:6176,optimiz,optimization,6176,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"bjCMessageExpr(hasKeywordSelector()) matches the generated setFrame; message expression in. UIWebView *webView = ...;; CGRect bodyFrame = webView.frame;; bodyFrame.size.height = self.bodyContentHeight;; webView.frame = bodyFrame;; // ^---- matches here. Matcher<ObjCMessageExpr>hasNullSelector; Matches when the selector is the empty selector. Matches only when the selector of the objCMessageExpr is NULL. This may; represent an error condition in the tree!. Matcher<ObjCMessageExpr>hasSelectorstd::string BaseName; Matches when BaseName == Selector.getAsString(). matcher = objCMessageExpr(hasSelector(""loadHTMLString:baseURL:""));; matches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>hasUnarySelector; Matches when the selector is a Unary Selector. matcher = objCMessageExpr(matchesSelector(hasUnarySelector());; matches self.bodyView in the code below, but NOT the outer message; invocation of ""loadHTMLString:baseURL:"".; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>isClassMessage; Returns true when the Objective-C message is sent to a class. Example; matcher = objcMessageExpr(isClassMessage()); matches; [NSString stringWithFormat:@""format""];; but not; NSString *x = @""hello"";; [x containsString:@""h""];. Matcher<ObjCMessageExpr>isInstanceMessage; Returns true when the Objective-C message is sent to an instance. Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose name contains; a substring matched by the given RegExp.; matcher = objCMessageExpr(matchesSelector(""loadHTMLStringmatches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTML",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:107153,load,loadHTMLString,107153,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,['load'],['loadHTMLString']
Performance,"bjects/subfolder/obj/root.json; ```. Then, its representation will look like:. ```json; {; ""_typename"" : ""TNamed"",; ""fUniqueID"" : 0,; ""fBits"" : 0,; ""fName"" : ""obj"",; ""fTitle"" : ""title""; }; ```. The following requests can be performed:. | Name | Description |; | :----------- | :---------------- |; | `root.bin` | binary data produced by object streaming with `TBufferFile` |; | `root.json` | ROOT JSON representation for object and objects members |; | `file.root` | Creates TMemFile with the only object, from ROOT 6.32 |; | `root.xml` | ROOT XML representation |; | `root.png` | PNG image (if object drawing implemented) |; | `root.gif` | GIF image |; | `root.jpeg` | JPEG image |; | `exe.json` | method execution in the object |; | `exe.bin` | method execution, return result in binary form |; | `cmd.json` | command execution |; | `item.json` | item (object) properties, specified on the server |; | `multi.json` | perform several requests at once |; | `multi.bin` | perform several requests at once, return result in binary form |. All data will be automatically zipped if '.gz' extension is appended. Like:. ```bash; [shell] wget http://localhost:8080/Objects/subfolder/obj/root.json.gz; ```. If the access to the server is restricted with htdigest, it is recommended to use the **curl** program since only curl correctly implements such authentication method. The command will look like:. ```bash; [shell] curl --user ""accout:password"" http://localhost:8080/Objects/subfolder/obj/root.json --digest -o root.json; ```. ### Objects data access in JSON format. Request `root.json` implemented with [TBufferJSON](https://root.cern/doc/master/classTBufferJSON.html) class. TBufferJSON generates such object representation, which could be directly used in [JSROOT](https://root.cern/js/) for drawing. `root.json` request returns either complete object or just object member like:. ```bash; [shell] wget http://localhost:8080/Objects/subfolder/obj/fTitle/root.json; ```. The result will be: `""title""`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md:15914,perform,perform,15914,documentation/HttpServer/HttpServer.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md,1,['perform'],['perform']
Performance,"ble to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of som",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10729,optimiz,optimize,10729,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,1,['optimiz'],['optimize']
Performance,"ble vector <vscale x N x eltty>, imm is a signed; integer constant in the range -X <= imm < X where X=vscale_range_min * N. '``llvm.experimental.stepvector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This is an overloaded intrinsic. You can use ``llvm.experimental.stepvector``; to generate a vector whose lane values comprise the linear sequence; <0, 1, 2, ...>. It is primarily intended for scalable vectors. ::. declare <vscale x 4 x i32> @llvm.experimental.stepvector.nxv4i32(); declare <vscale x 8 x i16> @llvm.experimental.stepvector.nxv8i16(). The '``llvm.experimental.stepvector``' intrinsics are used to create vectors; of integers whose elements contain a linear sequence of values starting from 0; with a step of 1. This experimental intrinsic can only be used for vectors; with integer elements that are at least 8 bits in size. If the sequence value; exceeds the allowed limit for the element type then the result for that lane is; undefined. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to generate a constant vector instead. Arguments:; """""""""""""""""""". None. '``llvm.experimental.get.vector.length``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.experimental.get.vector.length.i32(i32 %cnt, i32 immarg %vf, i1 immarg %scalable); declare i32 @llvm.experimental.get.vector.length.i64(i64 %cnt, i32 immarg %vf, i1 immarg %scalable). Overview:; """""""""""""""""". The '``llvm.experimental.get.vector.length.*``' intrinsics take a number of; elements to process and returns how many of the elements can be processed; with the requested vectorization factor. Arguments:; """""""""""""""""""". The first argument is an unsigned value of any scalar integer type and specifies; the total number of elements to be processed. The second argument is an i32; immediate f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:673225,scalab,scalable,673225,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"ble_t z, Double_t weight); ```. - `iel:` index of the element` [0,nel-1]`; - `a` and `z:` the atomic mass and charge; - `weight:` proportion by mass of the elements; - `natoms`: number of atoms of the element in the molecule making the; mixture. The radiation length is automatically computed when all elements are; defined. Since tracking MC provide several other ways to create; materials/mixtures, the materials classes are likely to evolve as the; interfaces to these engines are being developed. Generally in the; process of tracking material properties are not enough and more specific; media properties have to be defined. These highly depend on the MC; performing tracking and sometimes allow the definition of different; media properties (e.g. energy or range cuts) for the same material. ### Radionuclides. A new class **`TGeoElementRN`** was introduced in this version to; provide support for radioactive nuclides and their decays. A database of; 3162 radionuclides can be loaded on demand via the table of elements; (**`TGeoElementTable`** class). One can make then materials/mixtures; based on these radionuclides and use them in a geometry. ``` {.cpp}; root[] TGeoManager *geom = new TGeoManager(""geom"",""radionuclides"");; root[] TGeoElementTable *table = geom->GetElementTable();; root[] TGeoElementRN *c14 = table->GetElementRN(14,6); // A,Z; root[] c14->Print();; 6-C-014 ENDF=60140; A=14; Z=6; Iso=0; Level=0[MeV]; Dmass=3.0199[MeV];; Hlife=1.81e+11[s] J/P=0+; Abund=0; Htox=5.8e-10; Itox=5.8e-10; Stat=0; Decay modes:; BetaMinus Diso: 0 BR: 100.000% Qval: 0.1565; ```. One can make materials or mixtures from radionuclides:. ``` {.cpp}; root[] TGeoMaterial *mat = new TGeoMaterial(""C14"", c14, 2.0);; ```. The following properties of radionuclides can be currently accessed via; getters in the **`TGeoElementRN`** class:. Atomic number and charge (from the base class **`TGeoElement`**). - Isomeric number (`ISO`); - ENDF code - following the convention: `ENDF=10000*Z+100*A+ISO`; - I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:17663,load,loaded,17663,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,"blem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===----------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2816,load,load,2816,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"blem:; ; {; gStyle->SetOptFit(1);; TGraph *gr = new TGraph(2);; gr->SetPoint(0,1,1);; gr->SetPoint(1,2,2);; TLatex *l1 = new TLatex(gr->GetX()[0], gr->GetY()[0], ""#1"");; gr->GetListOfFunctions()->Add(l1);; gr->Draw(""APL"");; }; . Fixed the bug #45607 by creating a list of functions when using TGraph default constructor.; Fixed a bug when fitting TGraphErrors with zero error in y but non-zero error in x. In GetHistogram:; if fHistogram exists, and the log scale is on, and the computed range minimum is > 0, and; the fHistogram minimum is zero, then it means fHistogram limits have been computed; in linear scale therefore they might be too strict and cut some points. In that; case the fHistogram limits should be recomputed ie: the existing fHistogram; should not be returned. A example covering this case has been added in; stressGraphics. TH1. Speed up TH1::GetStats, TH2::GetStats, TH3::GetStats in case the sum of weights is null and the number of entries is also null; Optimize the way the function integral is computed in TH1::FillRandom; Add new functions TH1::IsBinUnderflow(bin) and TH1::IsBinOverflow(bin) which use the global bin number.; Add new functions Int_t TH1::FindFirstBinAbove(Double_t threshold, Int_t axis) and Int_t TH1::FindLastBinAbove(Double_t threshold, Int_t axis) which find first (and last) bin with the content above the given threshold. Same function have been added in TH2 and TH3.; Add a protection in TH1::Sumw2() to avoid calling GetBinContent when the histograms are empty.; In TH1::Copy reset temporarily the kCanRebin bit before calling SetBinContent.; Fix the bug #48649in TH1::Add.; Fix a bug when calling TH1::Sumw2() on a non-empty histogram with default sum2 (i.e when TH1::GetDefaultSumw2() is true).; Add a method, TH1::ResetStats() to reset the internal statistics and force then the re-calculation suing the bin center first time is needed; Fix some problem with the statistics (in particular the number of entries) after some of the histogram opera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v524/index.html:1980,Optimiz,Optimize,1980,hist/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v524/index.html,1,['Optimiz'],['Optimize']
Performance,"block:: llvm. @lock = global i1 true. define void @f(ptr %a) {; store ptr %a, ptr* @glb; store atomic i1 false, ptr @lock release ; %a is captured because another thread can safely read @glb; store ptr null, ptr @glb; ret void; }. 3. The call's behavior depends on any bit of the pointer carrying information. .. code-block:: llvm. @glb = global i8 0. define void @f(ptr %a) {; %c = icmp eq ptr %a, @glb; br i1 %c, label %BB_EXIT, label %BB_CONTINUE ; escapes %a; BB_EXIT:; call void @exit(); unreachable; BB_CONTINUE:; ret void; }. 4. The pointer is used in a volatile access as its address. .. _volatile:. Volatile Memory Accesses; ------------------------. Certain memory accesses, such as :ref:`load <i_load>`'s,; :ref:`store <i_store>`'s, and :ref:`llvm.memcpy <int_memcpy>`'s may be; marked ``volatile``. The optimizers must not change the number of; volatile operations or change their order of execution relative to other; volatile operations. The optimizers *may* change the order of volatile; operations relative to non-volatile operations. This is not Java's; ""volatile"" and has no cross-thread synchronization behavior. A volatile load or store may have additional target-specific semantics.; Any volatile operation can have side effects, and any volatile operation; can read and/or modify state which is not accessible via a regular load; or store in this module. Volatile operations may use addresses which do; not point to memory (like MMIO registers). This means the compiler may; not use a volatile operation to prove a non-volatile access to that; address has defined behavior. The allowed side-effects for volatile accesses are limited. If a; non-volatile store to a given address would be legal, a volatile; operation may modify the memory at that address. A volatile operation; may not modify any other memory accessible by the module being compiled.; A volatile operation may not call any code in the current module. In general (without target specific context), the address spac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:146126,optimiz,optimizers,146126,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"blocks, losing some global optimization opportunities. GlobalISel operates on the whole function. * **Modularity** --- SelectionDAG and FastISel are radically different and share; very little code. GlobalISel is built in a way that enables code reuse. For instance, both the; optimized and fast selectors share the :ref:`pipeline`, and targets can; configure that pipeline to better suit their needs. Design and Implementation Reference; ===================================. More information on the design and implementation of GlobalISel can be found in; the following sections. .. toctree::; :maxdepth: 1. GMIR; GenericOpcode; MIRPatterns; Pipeline; Porting; Resources. More information on specific passes can be found in the following sections:. .. toctree::; :maxdepth: 1. IRTranslator; Legalizer; RegBankSelect; InstructionSelect; KnownBits. .. _progress:. Progress and Future Work; ========================. The initial goal is to replace FastISel on AArch64. The next step will be to; replace SelectionDAG as the optimized ISel. ``NOTE``:; While we iterate on GlobalISel, we strive to avoid affecting the performance of; SelectionDAG, FastISel, or the other MIR passes. For instance, the types of; :ref:`gmir-gvregs` are stored in a separate table in ``MachineRegisterInfo``,; that is destroyed after :ref:`instructionselect`. .. _progress-fastisel:. FastISel Replacement; --------------------. For the initial FastISel replacement, we intend to fallback to SelectionDAG on; selection failures. Currently, compile-time of the fast pipeline is within 1.5x of FastISel.; We're optimistic we can get to within 1.1/1.2x, but beating FastISel will be; challenging given the multi-pass approach.; Still, supporting all IR (via a complete legalizer) and avoiding the fallback; to SelectionDAG in the worst case should enable better amortized performance; than SelectionDAG+FastISel. ``NOTE``:; We considered never having a fallback to SelectionDAG, instead deciding early; whether a given function is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst:2010,optimiz,optimized,2010,interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,1,['optimiz'],['optimized']
Performance,"bmit; the ""foo.bc"" file and the option that llc crashes with. LTO bugs; ---------------------------. If you encounter a bug that leads to crashes in the LLVM LTO phase when using; the ``-flto`` option, follow these steps to diagnose and report the issue:. Compile your source file to a ``.bc`` (Bitcode) file with the following options,; in addition to your existing compilation options:. .. code-block:: bash. export CFLAGS=""-flto -fuse-ld=lld"" CXXFLAGS=""-flto -fuse-ld=lld"" LDFLAGS=""-Wl,-plugin-opt=save-temps"". These options enable LTO and save temporary files generated during compilation; for later analysis. On Windows, you should be using lld-link as the linker. Adjust your compilation ; flags as follows:; * Add ``/lldsavetemps`` to the linker flags.; * When linking from the compiler driver, add ``/link /lldsavetemps`` in order to forward that flag to the linker. Using the specified flags will generate four intermediate bytecode files:. #. a.out.0.0.preopt.bc (Before any link-time optimizations (LTO) are applied); #. a.out.0.2.internalize.bc (After initial optimizations are applied); #. a.out.0.4.opt.bc (After an extensive set of optimizations); #. a.out.0.5.precodegen.bc (After LTO but before translating into machine code). Execute one of the following commands to identify the source of the problem:. #. ``opt ""-passes=lto<O3>"" a.out.0.2.internalize.bc``; #. ``llc a.out.0.5.precodegen.bc``. If one of these do crash, you should be able to reduce; this with :program:`llvm-reduce`; command line (use the bc file corresponding to the command above that failed):. .. code-block:: bash. llvm-reduce --test reduce.sh a.out.0.2.internalize.bc. Example of reduce.sh script. .. code-block:: bash. $ cat reduce.sh; #!/bin/bash -e. path/to/not --crash path/to/opt ""-passes=lto<O3>"" $1 -o temp.bc 2> err.log; grep -q ""It->second == &Insn"" err.log. Here we have grepped the failed assert message. Please run this, then file a bug with the instructions and reduced .bc file; that llvm-reduce ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:6911,optimiz,optimizations,6911,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimizations']
Performance,"bounds. This builtin always returns the value of ``expr``. Query for this feature with ``__has_builtin(__builtin_expect_with_probability)``. ``__builtin_prefetch``; ----------------------. ``__builtin_prefetch`` is used to communicate with the cache handler to bring; data into the cache before it gets used. **Syntax**:. .. code-block:: c++. void __builtin_prefetch(const void *addr, int rw=0, int locality=3). **Example of use**:. .. code-block:: c++. __builtin_prefetch(a + i);. **Description**:. The ``__builtin_prefetch(addr, rw, locality)`` builtin is expected to be used to; avoid cache misses when the developer has a good understanding of which data; are going to be used next. ``addr`` is the address that needs to be brought into; the cache. ``rw`` indicates the expected access mode: ``0`` for *read* and ``1``; for *write*. In case of *read write* access, ``1`` is to be used. ``locality``; indicates the expected persistence of data in cache, from ``0`` which means that; data can be discarded from cache after its next use to ``3`` which means that; data is going to be reused a lot once in cache. ``1`` and ``2`` provide; intermediate behavior between these two extremes. Query for this feature with ``__has_builtin(__builtin_prefetch)``. ``__sync_swap``; ---------------. ``__sync_swap`` is used to atomically swap integers or pointers in memory. **Syntax**:. .. code-block:: c++. type __sync_swap(type *ptr, type value, ...). **Example of Use**:. .. code-block:: c++. int old_value = __sync_swap(&value, new_value);. **Description**:. The ``__sync_swap()`` builtin extends the existing ``__sync_*()`` family of; atomic intrinsics to allow code to atomically swap the current value with the; new value. More importantly, it helps developers write more efficient and; correct code by avoiding expensive loops around; ``__sync_bool_compare_and_swap()`` or relying on the platform specific; implementation details of ``__sync_lock_test_and_set()``. The; ``__sync_swap()`` builtin is a fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:116269,cache,cache,116269,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,3,['cache'],['cache']
Performance,"bove are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:8902,perform,performing,8902,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,2,"['optimiz', 'perform']","['optimization', 'performing']"
Performance,"br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16314,load,load,16314,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['load'],['load']
Performance,"branches . PruneStrength No 0  Pruning strength. PruningValFraction No 0.5  Fraction of events to use for optimizing automatic pruning. nEventsMin No 0  deprecated: Use MinNodeSize (in % of training events) instead. GradBaggingFraction No 0.6  deprecated: Use *BaggedSampleFraction* instead: Defines the fraction of events to be used in each iteration, e.g. when UseBaggedGrad=kTRUE. . UseNTrainEvents No 0  deprecated: Use *BaggedSampleFraction* instead: Number of randomly picked training events used in randomised (and bagged) trees. NNodesMax No 0  deprecated: Use MaxDepth instead to limit the tree size. Configuration options for MVA method :. Configuration options reference for MVA method: Boost. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Boost_Num No 100  Number of times the classifier is boosted. Boost_MonitorMethod No True  Write monitoring histograms for each boosted classifier. Boost_DetailedMonitoring No False  Produce histograms for detailed boost-wise monitoring. Boost_Type No AdaBoost AdaBoost, Bagging, HighEdgeGauss, HighEdgeCoPara Boosting type for the classifiers. Boost_BaggedSampleFraction No 0.6  Relative size of bagged event sample ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:15994,perform,performed,15994,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"brary memory intrinsics except; that they perform memory transfer as a sequence of atomic memory accesses. .. _int_memcpy_element_unordered_atomic:. '``llvm.memcpy.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.memcpy.element.unordered.atomic`` on; any integer bit width and for different address spaces. Not all targets; support all bit widths however. ::. declare void @llvm.memcpy.element.unordered.atomic.p0.p0.i32(ptr <dest>,; ptr <src>,; i32 <len>,; i32 <element_size>); declare void @llvm.memcpy.element.unordered.atomic.p0.p0.i64(ptr <dest>,; ptr <src>,; i64 <len>,; i32 <element_size>). Overview:; """""""""""""""""". The '``llvm.memcpy.element.unordered.atomic.*``' intrinsic is a specialization of the; '``llvm.memcpy.*``' intrinsic. It differs in that the ``dest`` and ``src`` are treated; as arrays with elements that are exactly ``element_size`` bytes, and the copy between; buffers uses a sequence of :ref:`unordered atomic <ordering>` load/store operations; that are a positive integer multiple of the ``element_size`` in size. Arguments:; """""""""""""""""""". The first three arguments are the same as they are in the :ref:`@llvm.memcpy <int_memcpy>`; intrinsic, with the added constraint that ``len`` is required to be a positive integer; multiple of the ``element_size``. If ``len`` is not a positive integer multiple of; ``element_size``, then the behaviour of the intrinsic is undefined. ``element_size`` must be a compile-time constant positive power of two no greater than; target-specific atomic access size limit. For each of the input pointers ``align`` parameter attribute must be specified. It; must be a power of two no less than the ``element_size``. Caller guarantees that; both the source and destination pointers are aligned to that boundary. Semantics:; """""""""""""""""""". The '``llvm.memcpy.element.unordered.atomic.*``' intrinsic copies ``len`` bytes of; memory from the sou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:957947,load,load,957947,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"brary, you should use the; ``LLVM_BUILD_LLVM_DYLIB`` option. **LLVM_ABI_BREAKING_CHECKS**:STRING; Used to decide if LLVM should be built with ABI breaking checks or; not. Allowed values are `WITH_ASSERTS` (default), `FORCE_ON` and; `FORCE_OFF`. `WITH_ASSERTS` turns on ABI breaking checks in an; assertion enabled build. `FORCE_ON` (`FORCE_OFF`) turns them on; (off) irrespective of whether normal (`NDEBUG`-based) assertions are; enabled or not. A version of LLVM built with ABI breaking checks; is not ABI compatible with a version built without it. **LLVM_ADDITIONAL_BUILD_TYPES**:LIST; Adding a semicolon separated list of additional build types to this flag; allows for them to be specified as values in CMAKE_BUILD_TYPE without; encountering a fatal error during the configuration process. **LLVM_UNREACHABLE_OPTIMIZE**:BOOL; This flag controls the behavior of `llvm_unreachable()` in release build; (when assertions are disabled in general). When ON (default) then; `llvm_unreachable()` is considered ""undefined behavior"" and optimized as; such. When OFF it is instead replaced with a guaranteed ""trap"". **LLVM_APPEND_VC_REV**:BOOL; Embed version control revision info (Git revision id).; The version info is provided by the ``LLVM_REVISION`` macro in; ``llvm/include/llvm/Support/VCSRevision.h``. Developers using git who don't; need revision info can disable this option to avoid re-linking most binaries; after a branch switch. Defaults to ON. **LLVM_FORCE_VC_REVISION**:STRING; Force a specific Git revision id rather than calling to git to determine it.; This is useful in environments where git is not available or non-functional; but the VC revision is available through other means. **LLVM_FORCE_VC_REPOSITORY**:STRING; Set the git repository to include in version info rather than calling git to; determine it. **LLVM_BUILD_32_BITS**:BOOL; Build 32-bit executables and libraries on 64-bit systems. This option is; available only on some 64-bit Unix systems. Defaults to OFF. **LLVM_BU",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:12963,optimiz,optimized,12963,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['optimiz'],['optimized']
Performance,"buf_t *sbuf, sized_t nelems) {; // Materialize RHS values:; int *tmp_ptr = (int *)malloc(sizeof(int) * nelems);; int tmp_count = nelems;; // Inserted check:; // - checks to ensure that `lower <= tmp_ptr <= upper`; // - if (upper(tmp_ptr) - tmp_ptr < tmp_count) trap();; sbuf->buf = tmp_ptr;; sbuf->count = tmp_count;; }. Whether the compiler can optimize such run-time checks depends on how the upper; bound of the pointer is derived. If the source pointer has ``__sized_by``,; ``__counted_by``, or a variant of such, the compiler assumes that the upper; bound calculation doesn't overflow, e.g., ``ptr + size`` (where the type of; ``ptr`` is ``void *__sized_by(size)``), because when the ``__sized_by`` pointer; is initialized, ``-fbounds-safety`` inserts run-time checks to ensure that ``ptr; + size`` doesn't overflow and that ``size >= 0``. Assuming the upper bound calculation doesn't overflow, the compiler can simplify; the trap condition ``upper(tmp_ptr) - tmp_ptr < tmp_count`` to ``size <; tmp_count`` so if both ``size`` and ``tmp_count`` values are known at compile; time such that ``0 <= tmp_count <= size``, the optimizer can remove the check. ``ptr + size`` may still overflow if the ``__sized_by`` pointer is created from; code that doesn't enable ``-fbounds-safety``, which is undefined behavior. In the previous code example with the transformed ``alloc_buf()``, the upper; bound of ``tmp_ptr`` is derived from ``void *__sized_by_or_null(size)``, which; is the return type of ``malloc()``. Hence, the pointer arithmetic doesn't; overflow or ``tmp_ptr`` is null. Therefore, if ``nelems`` was given as a; compile-time constant, the compiler could remove the checks. Cast rules; ----------. ``-fbounds-safety`` does not enforce overall type safety and bounds invariants; can still be violated by incorrect casts in some cases. That said,; ``-fbounds-safety`` prevents type conversions that change bounds attributes in a; way to violate the bounds invariant of the destination's pointer ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst:38343,optimiz,optimizer,38343,interpreter/llvm-project/clang/docs/BoundsSafety.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst,1,['optimiz'],['optimizer']
Performance,"buf`` and ``sbuf->count`` are updated side by side; without any side effects in between the assignments. Furthermore, the compiler inserts additional run-time checks to ensure the new; ``buf`` has at least as many elements as the new ``count`` indicates as shown in; the transformed pseudo code of function ``alloc_buf()`` in the example below. .. code-block:: c. typedef struct {; int *__counted_by(count) buf;; size_t count;; } sized_buf_t;. void alloc_buf(sized_buf_t *sbuf, sized_t nelems) {; sbuf->buf = (int *)malloc(sizeof(int) * nelems);; sbuf->count = nelems;; }. // Transformed pseudo code:; void alloc_buf(sized_buf_t *sbuf, sized_t nelems) {; // Materialize RHS values:; int *tmp_ptr = (int *)malloc(sizeof(int) * nelems);; int tmp_count = nelems;; // Inserted check:; // - checks to ensure that `lower <= tmp_ptr <= upper`; // - if (upper(tmp_ptr) - tmp_ptr < tmp_count) trap();; sbuf->buf = tmp_ptr;; sbuf->count = tmp_count;; }. Whether the compiler can optimize such run-time checks depends on how the upper; bound of the pointer is derived. If the source pointer has ``__sized_by``,; ``__counted_by``, or a variant of such, the compiler assumes that the upper; bound calculation doesn't overflow, e.g., ``ptr + size`` (where the type of; ``ptr`` is ``void *__sized_by(size)``), because when the ``__sized_by`` pointer; is initialized, ``-fbounds-safety`` inserts run-time checks to ensure that ``ptr; + size`` doesn't overflow and that ``size >= 0``. Assuming the upper bound calculation doesn't overflow, the compiler can simplify; the trap condition ``upper(tmp_ptr) - tmp_ptr < tmp_count`` to ``size <; tmp_count`` so if both ``size`` and ``tmp_count`` values are known at compile; time such that ``0 <= tmp_count <= size``, the optimizer can remove the check. ``ptr + size`` may still overflow if the ``__sized_by`` pointer is created from; code that doesn't enable ``-fbounds-safety``, which is undefined behavior. In the previous code example with the transformed ``alloc_buf()",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst:37563,optimiz,optimize,37563,interpreter/llvm-project/clang/docs/BoundsSafety.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst,1,['optimiz'],['optimize']
Performance,"buffer.fLocalFrame) {; TransformPoints(buffer.fBBVertex[0], 8);; }; buffer.SetSectionsValid(TBuffer3D::kBoundingBox);; }; ```. #### Logical and Physical Objects. Some viewers can support two types of object placement:. - Add object as a single independent entity in the world reference; frame - e.g. a sphere, radius `r`, at `x`, `y`, `z`. - Repeated placement (copying) in world frame of this locally unique; piece of geometry (described in local reference frame) e.g. define a; sphere `S` (radius `r`), place copy at `x1`, `y1`, `z1`, another; copy at `x2`, `y2`, `z2` etc. The second case is very typical in geometry packages, e.g. ROOT's; **`TGeo`** package, GEANT4 etc, where we have very large number repeated; placements of relatively few unique ""shapes"". Some viewers (GL Viewer only at present) are able to take advantage of; this by identifying unique logical shapes from the `fID` logical ID; member of **`TBuffer3D`**. If repeated addition of the same `fID` is; found, the shape is cached already - and the costly tessellation does; not need to be sent again. The viewer can also perform internal GL; specific caching (display lists) with considerable performance gains in; these cases. For this to work correctly the logical object in must be; described in **`TBuffer3D`** in the local reference frame, complete with; the local`/`master translation. In some cases you will not have a real; object you can reasonably set **`TBuffer3D::fID` to, or the object is; recycled or temporary. To suppress internal caching in the GL Viewer in; these cases, set `TBuffer3D::fID` to 0 (null).**. The viewer indicates it can support local frame objects through the; **`TVirtualViewer3D`** interface method: `PreferLocalFrame()`. If this; returns `kTRUE` you can make repeated calls to `AddObject()`, with; **`TBuffer3D`** containing the same `fID`, and different `fLocalMaster`; placements. For viewers supporting logical/physical objects, the TBuffer3D content; refers to the properties of the logica",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:135054,cache,cached,135054,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['cache'],['cached']
Performance,"bug and; CodeViewDebug. DwarfDebug produces DWARF suitable for use with GDB, LLDB, and; other DWARF-based debuggers. :ref:`CodeViewDebug <codeview>` produces CodeView,; the Microsoft debug info format, which is usable with Microsoft debuggers such; as Visual Studio and WinDBG. LLVM's debug information format is mostly derived; from and inspired by DWARF, but it is feasible to translate into other target; debug info formats such as STABS. It would also be reasonable to use debug information to feed profiling tools; for analysis of generated code, or, tools for reconstructing the original; source from generated code. .. _intro_debugopt:. Debug information and optimizations; -----------------------------------. An extremely high priority of LLVM debugging information is to make it interact; well with optimizations and analysis. In particular, the LLVM debug; information provides the following guarantees:. * LLVM debug information **always provides information to accurately read; the source-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means that, with effort, the LLVM; optimizers could optimize debug code just as well as non-debug code. * LLVM debug information does not prevent optimizations from; happening (for example inlining, basic block reordering/merging/cleanup,; tail duplication, etc). * LLVM debug information is automatically optimized along with",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:4011,optimiz,optimizations,4011,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"bugpoint - automatic test case reduction tool; =============================================. .. program:: bugpoint. SYNOPSIS; --------. **bugpoint** [*options*] [*input LLVM ll/bc files*] [*LLVM passes*] **--args**; *program arguments*. DESCRIPTION; -----------. **bugpoint** narrows down the source of problems in LLVM tools and passes. It; can be used to debug three types of failures: optimizer crashes, miscompilations; by optimizers, or bad native code generation (including problems in the static; and JIT compilers). It aims to reduce large test cases to small, useful ones.; For more information on the design and inner workings of **bugpoint**, as well as; advice for using bugpoint, see :doc:`/Bugpoint` in the LLVM; distribution. OPTIONS; -------. **--additional-so** *library*. Load the dynamic shared object *library* into the test program whenever it is; run. This is useful if you are debugging programs which depend on non-LLVM; libraries (such as the X or curses libraries) to run. **--append-exit-code**\ =\ *{true,false}*. Append the test programs exit code to the output file so that a change in exit; code is considered a test failure. Defaults to false. **--args** *program args*. Pass all arguments specified after **--args** to the test program whenever it runs.; Note that if any of the *program args* start with a ""``-``"", you should use:. .. code-block:: bash. bugpoint [bugpoint args] --args -- [program args]. The ""``--``"" right after the **--args** option tells **bugpoint** to consider; any options starting with ""``-``"" to be part of the **--args** option, not as; options to **bugpoint** itself. **--tool-args** *tool args*. Pass all arguments specified after **--tool-args** to the LLVM tool under test; (**llc**, **lli**, etc.) whenever it runs. You should use this option in the; following way:. .. code-block:: bash. bugpoint [bugpoint args] --tool-args -- [tool args]. The ""``--``"" right after the **--tool-args** option tells **bugpoint** to; consider any optio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst:389,optimiz,optimizer,389,interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,2,['optimiz'],"['optimizer', 'optimizers']"
Performance,"build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compiler against sample source files. While the; instrumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a single profdata file that you; can feed into the LLVM_PROFDATA_FILE option. Our PGO.cmake cache automates that whole process. You can use it for; configuration with CMake with the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; <path to source>/llvm. There are several additional options that the cache file also accepts to modify; the build, particularly the PGO_INSTRUMENT_LTO option. Setting this option to; Thin or Full will enable ThinLTO or full LTO respectively, further enhancing; the performance gains from a PGO build by enabling interprocedural; optimizations. For example, to run a CMake configuration for a PGO build; that also enables ThinTLO, use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; -DPGO_INSTRUMENT_LTO=Thin \; <path to source>/llvm. By default, clang will generate profile data by compiling a simple; hello world program. You can also tell clang use an external; project for generating profile data that may be a better fit for your; use case. The project you specify must either be a lit test suite; (use the CLANG_PGO_TRAINING_DATA option) or a CMake project (use the; CLANG_PERF_TRAINING_DATA_SOURCE_DIR option). For example, If you wanted to use the; `LLVM Test Suite <https://github.com/llvm/llvm-test-suite/>`_ to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:5661,cache,cache,5661,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['cache']
Performance,"build, load, and execute the function with the same name as the; file you can use the `.x` command. This is the same as executing a; named script; you can also provide parameters. The only; difference is you need to append a + or a ++. ``` {.cpp}; root[] .x MyScript.C+(4000); Creating shared library /home/./MyScript_C.so; ```. You can select whether the script in compiled with debug symbol or; with optimization by appending the letter 'g' or 'O' after the '+' or; '++'. Without the specification, the script is compiled with the same; level of debugging symbol and optimization as the currently running; ROOT executable. For example:. ``` {.cpp}; root[] .L MyScript.C++g; ```. will compile `MyScript.C` with debug symbols; usually this means; giving the `-g` option to compiler. ``` {.cpp}; root[] .L MyScript.C++O; ```. will compile `MyScript.C` with optimizations; usually this means; giving the `-O` option to compiler. The syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. is using the default optimization level. The initial default is to; compile with the same level of optimization as the root executable; itself. The default can be changed by:. ``` {.cpp}; root[] gSystem->SetAclicMode(TSystem::kDebug);; root[] gSystem->SetAclicMode(TSystem::kOpt);; ```. Note that the commands:. ``` {.cpp}; root[] .L MyScript.C+g; root[] .L MyScript.C+O; ```. respectively compile `MyScript.C` with debug and optimization if the; library does not exist yet; they will not change the debug and the; optimization level if the library already exist and it is up to date.; To use ACLiC from compiled code or from inside another macro, we; recommend using `gROOT->ProcessLine()`. For; example, in one script you can use ACLiC to compile and load another; script. ``` {.cpp}; gROOT->ProcessLine("".L MyScript.C+""); gROOT->ProcessLine("".L MyScript.C++""); ```. ### Setting the Include Path. You can get the include path by typing:. ``` {.cpp}; root[] .include; ```. You can append to the include path by typing:.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:15533,optimiz,optimization,15533,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['optimiz'],['optimization']
Performance,"but if you have; information in your source language about the range of an integer value, it can; be profitable to use a zext rather than a sext. Alternatively, you can :ref:`specify the range of the value using metadata; <range-metadata>` and LLVM can do the sext to zext conversion for you. Zext GEP indices to machine register width; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Internally, LLVM often promotes the width of GEP indices to machine register; width. When it does so, it will default to using sign extension (sext); operations for safety. If your source language provides information about; the range of the index, you may wish to manually extend indices to machine; register width using a zext instruction. When to specify alignment; ^^^^^^^^^^^^^^^^^^^^^^^^^^; LLVM will always generate correct code if you dont specify alignment, but may; generate inefficient code. For example, if you are targeting MIPS (or older; ARM ISAs) then the hardware does not handle unaligned loads and stores, and; so you will enter a trap-and-emulate path if you do a load or store with; lower-than-natural alignment. To avoid this, LLVM will emit a slower; sequence of loads, shifts and masks (or load-right + load-left on MIPS) for; all cases where the load / store does not have a sufficiently high alignment; in the IR. The alignment is used to guarantee the alignment on allocas and globals,; though in most cases this is unnecessary (most targets have a sufficiently; high default alignment that theyll be fine). It is also used to provide a; contract to the back end saying either this load/store has this alignment, or; it is undefined behavior. This means that the back end is free to emit; instructions that rely on that alignment (and mid-level optimizers are free to; perform transforms that require that alignment). For x86, it doesnt make; much difference, as almost all instructions are alignment-independent. For; MIPS, it can make a big difference. Note that if your loads and stor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:4777,load,loads,4777,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,2,['load'],"['load', 'loads']"
Performance,"but not ``second_param``:. .. code-block:: objc. __attribute__((objc_externally_retained)); void f(NSArray *first_param, __strong NSArray *second_param) {; // ...; }. You can test if your compiler has support for ``objc_externally_retained`` with; ``__has_attribute``:. .. code-block:: objc. #if __has_attribute(objc_externally_retained); // Use externally retained...; #endif. .. _arc.misc.self:. ``self``; --------. The ``self`` parameter variable of an non-init Objective-C method is considered; :ref:`externally-retained <arc.misc.externally_retained>` by the implementation.; It is undefined behavior, or at least dangerous, to cause an object to be; deallocated during a message send to that object. In an init method, ``self``; follows the :ref:``init family rules <arc.family.semantics.init>``. .. admonition:: Rationale. The cost of retaining ``self`` in all methods was found to be prohibitive, as; it tends to be live across calls, preventing the optimizer from proving that; the retain and release are unnecessary --- for good reason, as it's quite; possible in theory to cause an object to be deallocated during its execution; without this retain and release. Since it's extremely uncommon to actually; do so, even unintentionally, and since there's no natural way for the; programmer to remove this retain/release pair otherwise (as there is for; other parameters by, say, making the variable ``objc_externally_retained`` or; qualifying it with ``__unsafe_unretained``), we chose to make this optimizing; assumption and shift some amount of risk to the user. .. _arc.misc.enumeration:. Fast enumeration iteration variables; ------------------------------------. If a variable is declared in the condition of an Objective-C fast enumeration; loop, and the variable has no explicit ownership qualifier, then it is; implicitly :ref:`externally-retained <arc.misc.externally_retained>` so that; objects encountered during the enumeration are not actually retained and; released. .. admonitio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:94730,optimiz,optimizer,94730,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizer']
Performance,"but the argument has type 'int' [-Wformat,1]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,Format String]. This category can be used by clients that want to group diagnostics; by category, so it should be a high level category. We want dozens; of these, not hundreds or thousands of them. .. _opt_fsave-optimization-record:. .. option:: -f[no-]save-optimization-record[=<format>]. Enable optimization remarks during compilation and write them to a separate; file. This option, which defaults to off, controls whether Clang writes; optimization reports to a separate file. By recording diagnostics in a file,; users can parse or sort the remarks in a convenient way. By default, the serialization format is YAML. The supported serialization formats are:. - .. _opt_fsave_optimization_record_yaml:. ``-fsave-optimization-record=yaml``: A structured YAML format. - .. _opt_fsave_optimization_record_bitstream:. ``-fsave-optimization-record=bitstream``: A binary format based on LLVM; Bitstream. The output file is controlled by :option:`-foptimization-record-file`. In the absence of an explicit output file, the file is chosen using the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when used with `-c` or `-S`.; For example:. * ``clang -fsave-optimization-record -c in.c -o out.o`` will generate; ``out.opt.yaml``. * ``clang -fsave-optimization-record -c in.c`` will generate; ``in.opt.yaml``. When targeting (Thin)LTO, the base is derived from the output filename, and; the extension is not dropped. When targeting ThinLTO, the following scheme is used:. ``<base>.opt.<format>.thin.<num>.<format>``. Darwin-only: when used for generating a linked binary from a source file; (through an intermediate object file), the driver will invoke `cc1` to; generate a temporary object file. The temporary remark file will be emitted; next t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:11399,optimiz,optimization-record,11399,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization-record']
Performance,"bute must be; specified. It must be a power of two no less than the ``element_size``. Caller; guarantees that both the source and destination pointers are aligned to that; boundary. Semantics:; """""""""""""""""""". The '``llvm.memmove.element.unordered.atomic.*``' intrinsic copies ``len`` bytes; of memory from the source location to the destination location. These locations; are allowed to overlap. The memory copy is performed as a sequence of load/store; operations where each access is guaranteed to be a multiple of ``element_size``; bytes wide and aligned at an ``element_size`` boundary. The order of the copy is unspecified. The same value may be read from the source; buffer many times, but only one write is issued to the destination buffer per; element. It is well defined to have concurrent reads and writes to both source; and destination provided those reads and writes are unordered atomic when; specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered loads from the source location and stores to the; destination. Lowering:; """""""""""""""""". In the most general case call to the; '``llvm.memmove.element.unordered.atomic.*``' is lowered to a call to the symbol; ``__llvm_memmove_element_unordered_atomic_*``. Where '*' is replaced with an; actual element size. See :ref:`RewriteStatepointsForGC intrinsic lowering; <RewriteStatepointsForGC_intrinsic_lowering>` for details on GC specific; lowering. The optimizer is allowed to inline the memory copy when it's profitable to do so. .. _int_memset_element_unordered_atomic:. '``llvm.memset.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.memset.element.unordered.atomic`` on; any integer bit width and for different address spaces. Not all targets; support all bit widths however. ::. declare void @llvm.memset.element.unordered.atomic.p0.i32(ptr <dest>,; i8 <value>,; i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:962707,load,loads,962707,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"bution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compiler against sample source files. While the; instrumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a single profdata file that you; can feed into the LLVM_PROFDATA_FILE option. Our PGO.cmake cache automates that whole process. You can use it for; configuration with CMake with the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; <path to source>/llvm. There are several additional options that the cache file also accepts to modify; the build, particularly the PGO_INSTRUMENT_LTO option. Setting this option to; Thin or Full will enable ThinLTO or full LTO respectively, further enhancing; the performance gains from a PGO build by enabling interprocedural; optimizations. For example, to run a CMake configuration for a PGO build; that also enables ThinTLO, use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:5171,perform,performance,5171,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['perform'],['performance']
Performance,"by GFX8 to allow conversion between private segment; and flat addresses. See :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`. ""hidden_shared_base""; The high 32 bits of the flat addressing shared aperture base.; Only used by GFX8 to allow conversion between shared segment; and flat addresses. See :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`. ""hidden_queue_ptr""; A global memory address space pointer to the ROCm runtime; ``struct amd_queue_t`` structure for the HSA queue of the; associated dispatch AQL packet. It is only required for pre-GFX9; devices for the trap handler ABI (see :ref:`amdgpu-amdhsa-trap-handler-abi`). ====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149053,queue,queues,149053,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"by parsing C++ definitions through ``cling``, generating tiny; wrapper codes to honor compile-time features and create standardized; interfaces, then compiling/linking those wrappers with the ``clang`` JIT.; It thus requires only those two ingredients: *C++ definitions* and; *linker symbols*.; All cppyy uses, the basic and the more advanced, are variations on the; theme of bringing these two together at the point of use. Definitions typically live in header files and symbols in libraries.; Headers can be loaded with ``cppyy.include`` and libraries with the; ``cppyy.load_library`` call.; Loading the header is sufficient to start exploring, with ``cppyy.gbl`` the; starting point of all things C++, while the linker symbols are only needed at ; the point of first use. Here is an example using the `zlib`_ library, which is likely available on; your system:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('zlib.h') # bring in C++ definitions; >>> cppyy.load_library('libz') # load linker symbols; >>> cppyy.gbl.zlibVersion() # use a zlib API; '1.2.11'; >>>. Since header files can include other header files, it is easy to aggregate; all relevant ones into a single header to include.; If there are project-specific include paths, you can add those paths through; ``cppyy.add_include_path``.; If a header is C-only and not set for use with C++, use ``cppyy.c_include``,; which adds ``extern ""C""`` around the header. Library files can be aggregated by linking all relevant ones to a single; library to load.; Using the linker for this purpose allows regular system features such as; ``rpath`` and envars such as ``LD_LIBRARY_PATH`` to be applied as usual.; Note that any mechanism that exposes the library symbols will work.; For example, you could also use the standard module ``ctypes`` through; ``ctypes.CDLL`` with the ``ctypes.RTLD_GLOBAL`` option. To explore, start from ``cppyy.gbl`` to access your namespaces, classes,; functions, etc., etc. directly; or use python's ``dir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst:1598,load,load,1598,bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,1,['load'],['load']
Performance,"by remark name using regular expressions. .. option:: --filter-arg-by[=<string>]; Filter count by argument value. .. option:: --rfilter-arg-by[=<string>]; Filter count by argument value using regular expressions. .. option:: --remark-type=<value>; Filter remarks by type with the following options.; * ``unknown``; * ``passed``; * ``missed``; * ``analysis``; * ``analysis-fp-commute``; * ``analysis-aliasing``; * ``failure``. .. _size-diff_subcommand:. size-diff; ~~~~~~~~~; .. program:: llvm-remarkutil size-diff. USAGE: :program:`llvm-remarkutil` size-diff [*options*] *file_a* *file_b* **--parser** *parser*. Summary; ^^^^^^^. :program:`llvm-remarkutil size-diff` diffs size `remarks <https://llvm.org/docs/Remarks.html>`_ in two remark files: ``file_a``; and ``file_b``. :program:`llvm-remarkutil size-diff` can be used to gain insight into which; functions were impacted the most by code generation changes. In most common use-cases ``file_a`` and ``file_b`` will be remarks output by; compiling a **fixed source** with **differing compilers** or; **differing optimization settings**. :program:`llvm-remarkutil size-diff` handles both; `YAML <https://llvm.org/docs/Remarks.html#yaml-remarks>`_ and; `bitstream <https://llvm.org/docs/Remarks.html#llvm-bitstream-remarks>`_; remarks. OPTIONS; -------. .. option:: --parser=<yaml|bitstream>. Select the type of input remark parser. Required.; * ``yaml``: The tool will parse YAML remarks.; * ``bitstream``: The tool will parse bitstream remarks. .. option:: --report-style=<human|json>. Output style.; * ``human``: Human-readable textual report. Default option.; * ``json``: JSON report. .. option:: --pretty. Pretty-print JSON output. Optional. If output is not set to JSON, this does nothing. .. option:: -o=<file>. Output file for the report. Outputs to stdout by default. HUMAN-READABLE OUTPUT; ---------------------. The human-readable format for :program:`llvm-remarkutil size-diff` is composed of; two sections:. * Per-function changes.; * A ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-remarkutil.rst:6074,optimiz,optimization,6074,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-remarkutil.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-remarkutil.rst,1,['optimiz'],['optimization']
Performance,"by the second operand. The; order of evaluation of multiplications is not defined. When a vector of; floating-point type is used, the second argument remains a scalar integer value. Arguments:; """""""""""""""""""". The first argument and the return value are floating-point numbers of the same; type. The second argument is a 32-bit signed integer specifying the power to; which the first argument should be raised. The third and fourth arguments specify the rounding mode and exception; behavior as described above. Semantics:; """""""""""""""""""". This function returns the first value raised to the second power with an; unspecified sequence of rounding operations. '``llvm.experimental.constrained.ldexp``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type0>; @llvm.experimental.constrained.ldexp(<type0> <op1>, <type1> <op2>,; metadata <rounding mode>,; metadata <exception behavior>). Overview:; """""""""""""""""". The '``llvm.experimental.constrained.ldexp``' performs the ldexp function. Arguments:; """""""""""""""""""". The first argument and the return value are :ref:`floating-point; <t_floating>` or :ref:`vector <t_vector>` of floating-point values of; the same type. The second argument is an integer with the same number; of elements. The third and fourth arguments specify the rounding mode and exception; behavior as described above. Semantics:; """""""""""""""""""". This function multiplies the first argument by 2 raised to the second; argument's power. If the first argument is NaN or infinite, the same; value is returned. If the result underflows a zero with the same sign; is returned. If the result overflows, the result is an infinity with; the same sign. '``llvm.experimental.constrained.sin``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.experimental.constrained.sin(<type> <op1>,; metadata <rounding mode>,; metadata <exception behavior>). Overview:; """""""""""""""""". The '``llvm.experimental.constrained.sin``'",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:893248,perform,performs,893248,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. The default is no byte size-based pruning. Note that ThinLTO will apply both size-based pruning policies simultaneously,; and changing one does not affect the other. For example, a policy of; ``cache_size_bytes=1g`` on its own will cause both the 1GB and default 75%; policies to be applied unless the default ``cache_size`` is overridden. - ``cache_size_files=X``:; Set the maximum number of files in the cache directory. Set to 0 to indicate; no limit. The default is 1000000 files. - ``prune_after=Xs``, ``prune_after=Xm``, ``prune_after=Xh``: Sets the; expiration time for cache files to ``X`` seconds (or minutes, hours; respectively). When a file hasn't been accessed for ``prune_after`` seconds,; it is removed from the cache. A value of 0 disables the expiration-based; pruning. The default is 1 week. - ``prune_interval=Xs``, ``prune_interval=Xm``, ``prune_interval=Xh``:; Sets the pruning interval to ``X`` seconds (or minutes, hours; respectively). This is intended to be used to avoid scanning the directory; too often. It does not impact the decision of which files to prune. A; value of 0 forces the scan to occur. The default is every 20 minutes. Clang Bootstrap; ---------------. To `bootstrap clang/LLVM <https://llvm.org/docs/AdvancedBuilds.html#bootstrap-builds>`_; with ThinLTO, follow these steps:. 1. The host compiler_ must be a version of clang that supports ThinLTO.; #. The host linker_ must support ThinLTO (and in the case of gold, must be; `configured with plugins enabled <https://llvm.org/docs/GoldPlugin.html>`_).; #. Use the following additional `CMake variables; <https://llvm.org/docs/CMake.html#options-and-variables>`_; when configuring the bootstrap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:7129,cache,cache,7129,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache']
Performance,"c License, and you are; welcome to change it and/or distribute copies of it under certain conditions.; Type ""show copying"" to see the conditions.; There is absolutely no warranty for GDB. Type ""show warranty"" for details.; This GDB was configured as ""sparc-sun-solaris2.6""...; (gdb). Note that :program:`opt` has a lot of debugging information in it, so it takes; time to load. Be patient. Since we cannot set a breakpoint in our pass yet; (the shared object isn't loaded until runtime), we must execute the process,; and have it stop before it invokes our pass, but after it has loaded the shared; object. The most foolproof way of doing this is to set a breakpoint in; ``PassManager::run`` and then run the process with the arguments you want:. .. code-block:: console. $ (gdb) break llvm::PassManager::run; Breakpoint 1 at 0x2413bc: file Pass.cpp, line 70.; (gdb) run test.bc -load $(LLVMTOP)/llvm/Debug+Asserts/lib/[libname].so -[passoption]; Starting program: opt test.bc -load $(LLVMTOP)/llvm/Debug+Asserts/lib/[libname].so -[passoption]; Breakpoint 1, PassManager::run (this=0xffbef174, M=@0x70b298) at Pass.cpp:70; 70 bool PassManager::run(Module &M) { return PM->run(M); }; (gdb). Once the :program:`opt` stops in the ``PassManager::run`` method you are now; free to set breakpoints in your pass so that you can trace through execution or; do other standard debugging stuff. Miscellaneous Problems; ^^^^^^^^^^^^^^^^^^^^^^. Once you have the basics down, there are a couple of problems that GDB has,; some with solutions, some without. * Inline functions have bogus stack information. In general, GDB does a pretty; good job getting stack traces and stepping through inline functions. When a; pass is dynamically loaded however, it somehow completely loses this; capability. The only solution I know of is to de-inline a function (move it; from the body of a class to a ``.cpp`` file). * Restarting the program breaks breakpoints. After following the information; above, you have succeeded in ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:53805,load,load,53805,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"c default (``always`` for all; non-arm64-platforms). ``no-compact-unwind`` is a performance optimization -- Clang will emit smaller; object files that are more quickly processed by the linker. This may cause; binary compatibility issues on older x86_64 targets, however, so use it with; caution. .. _configuration-files:. Configuration files; -------------------. Configuration files group command-line options and allow all of them to be; specified just by referencing the configuration file. They may be used, for; example, to collect options required to tune compilation for particular; target, such as ``-L``, ``-I``, ``-l``, ``--sysroot``, codegen options, etc. Configuration files can be either specified on the command line or loaded; from default locations. If both variants are present, the default configuration; files are loaded first. The command line option ``--config=`` can be used to specify explicit; configuration files in a Clang invocation. If the option is used multiple times,; all specified files are loaded, in order. For example:. ::. clang --config=/home/user/cfgs/testing.txt; clang --config=debug.cfg --config=runtimes.cfg. If the provided argument contains a directory separator, it is considered as; a file path, and options are read from that file. Otherwise the argument is; treated as a file name and is searched for sequentially in the directories:. - user directory,; - system directory,; - the directory where Clang executable resides. Both user and system directories for configuration files are specified during; clang build using CMake parameters, ``CLANG_CONFIG_FILE_USER_DIR`` and; ``CLANG_CONFIG_FILE_SYSTEM_DIR`` respectively. The first file found is used.; It is an error if the required file cannot be found. The default configuration files are searched for in the same directories; following the rules described in the next paragraphs. Loading default; configuration files can be disabled entirely via passing; the ``--no-default-config`` flag. First, the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:31213,load,loaded,31213,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['loaded']
Performance,"c function"". These functions; have well known names and semantics and are required to follow certain; restrictions. Overall, these intrinsics represent an extension mechanism; for the LLVM language that does not require changing all of the; transformations in LLVM when adding to the language (or the bitcode; reader/writer, the parser, etc...). Intrinsic function names must all start with an ""``llvm.``"" prefix. This; prefix is reserved in LLVM for intrinsic names; thus, function names may; not begin with this prefix. Intrinsic functions must always be external; functions: you cannot define the body of intrinsic functions. Intrinsic; functions may only be used in call or invoke instructions: it is illegal; to take the address of an intrinsic function. Additionally, because; intrinsic functions are part of the LLVM language, it is required if any; are added that they be documented here. Some intrinsic functions can be overloaded, i.e., the intrinsic; represents a family of functions that perform the same operation but on; different data types. Because LLVM can represent over 8 million; different integer types, overloading is used commonly to allow an; intrinsic function to operate on any integer type. One or more of the; argument types or the result type can be overloaded to accept any; integer type. Argument types may also be defined as exactly matching a; previous argument's type or the result type. This allows an intrinsic; function which accepts multiple arguments, but needs all of them to be; of the same type, to only be overloaded with respect to a single; argument or the result. Overloaded intrinsics will have the names of its overloaded argument; types encoded into its function name, each preceded by a period. Only; those types which are overloaded result in a name suffix. Arguments; whose type is matched against another type do not. For example, the; ``llvm.ctpop`` function can take an integer of any width and returns an; integer of exactly the same integer wid",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:488989,perform,perform,488989,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"c load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt vscnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must hap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:357173,load,load,357173,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"c ordering involved, concurrency does not matter, with; one exception: if a variable might be visible to another thread or signal; handler, a store cannot be inserted along a path where it might not execute; otherwise. Take the following example:. .. code-block:: c. /* C code, for readability; run through clang -O2 -S -emit-llvm to get; equivalent IR */; int x;; void f(int* a) {; for (int i = 0; i < 100; i++) {; if (a[i]); x += 1;; }; }. The following is equivalent in non-concurrent situations:. .. code-block:: c. int x;; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; if (a[i]); xtemp += 1;; }; x = xtemp;; }. However, LLVM is not allowed to transform the former to the latter: it could; indirectly introduce undefined behavior if another thread can access ``x`` at; the same time. That thread would read `undef` instead of the value it was; expecting, which can lead to undefined behavior down the line. (This example is; particularly of interest because before the concurrency model was implemented,; LLVM would perform this transformation.). Note that speculative loads are allowed; a load which is part of a race returns; ``undef``, but does not have undefined behavior. Atomic instructions; ===================. For cases where simple loads and stores are not sufficient, LLVM provides; various atomic instructions. The exact guarantees provided depend on the; ordering; see `Atomic orderings`_. ``load atomic`` and ``store atomic`` provide the same basic functionality as; non-atomic loads and stores, but provide additional guarantees in situations; where threads and signals are involved. ``cmpxchg`` and ``atomicrmw`` are essentially like an atomic load followed by an; atomic store (where the store is conditional for ``cmpxchg``), but no other; memory operation can happen on any thread between the load and store. A ``fence`` provides Acquire and/or Release ordering which is not part; of another operation; it is normally used along with Monotonic memory; oper",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:3937,concurren,concurrency,3937,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,"['concurren', 'perform']","['concurrency', 'perform']"
Performance,"c value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:247103,load,loads,247103,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25807,optimiz,optimized,25807,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"c**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load sc0=1; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:296437,load,load,296437,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"c, label %dyn.alloc, label %coro.begin; dyn.alloc:; %size = call i32 @llvm.coro.size.i32(); %alloc = call ptr @malloc(i32 %size); br label %coro.begin; coro.begin:; %phi = phi ptr [ null, %entry ], [ %alloc, %dyn.alloc ]; %hdl = call noalias ptr @llvm.coro.begin(token %id, ptr %phi); br label %loop; loop:; %n.val = phi i32 [ %n, %coro.begin ], [ %inc, %loop ]; %inc = add nsw i32 %n.val, 1; store i32 %n.val, ptr %promise; %0 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %0, label %suspend [i8 0, label %loop; i8 1, label %cleanup]; cleanup:; %mem = call ptr @llvm.coro.free(token %id, ptr %hdl); call void @free(ptr %mem); br label %suspend; suspend:; %unused = call i1 @llvm.coro.end(ptr %hdl, i1 false, token none); ret ptr %hdl; }. A coroutine consumer can rely on the `coro.promise`_ intrinsic to access the; coroutine promise. .. code-block:: llvm. define i32 @main() {; entry:; %hdl = call ptr @f(i32 4); %promise.addr = call ptr @llvm.coro.promise(ptr %hdl, i32 4, i1 false); %val0 = load i32, ptr %promise.addr; call void @print(i32 %val0); call void @llvm.coro.resume(ptr %hdl); %val1 = load i32, ptr %promise.addr; call void @print(i32 %val1); call void @llvm.coro.resume(ptr %hdl); %val2 = load i32, ptr %promise.addr; call void @print(i32 %val2); call void @llvm.coro.destroy(ptr %hdl); ret i32 0; }. After example in this section is compiled, result of the compilation will be:. .. code-block:: llvm. define i32 @main() {; entry:; tail call void @print(i32 4); tail call void @print(i32 5); tail call void @print(i32 6); ret i32 0; }. .. _final:; .. _final suspend:. Final Suspend; -------------. A coroutine author or a frontend may designate a particular suspend to be final,; by setting the second argument of the `coro.suspend`_ intrinsic to `true`.; Such a suspend point has two properties:. * it is possible to check whether a suspended coroutine is at the final suspend; point via `coro.done`_ intrinsic;. * a resumption of a coroutine stopped at the final susp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:23890,load,load,23890,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['load'],['load']
Performance,"c/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:224365,load,load,224365,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,c/include/llvm-libc-types/float_t.h; libc/include/llvm-libc-types/imaxdiv_t.h; libc/include/llvm-libc-types/jmp_buf.h; libc/include/llvm-libc-types/ldiv_t.h; libc/include/llvm-libc-types/lldiv_t.h; libc/include/llvm-libc-types/mode_t.h; libc/include/llvm-libc-types/mtx_t.h; libc/include/llvm-libc-types/off_t.h; libc/include/llvm-libc-types/once_flag.h; libc/include/llvm-libc-types/size_t.h; libc/include/llvm-libc-types/ssize_t.h; libc/include/llvm-libc-types/struct_sigaction.h; libc/include/llvm-libc-types/struct_tm.h; libc/include/llvm-libc-types/thrd_start_t.h; libc/include/llvm-libc-types/thrd_t.h; libc/include/llvm-libc-types/time_t.h; libc/include/llvm-libc-types/__atexithandler_t.h; libc/include/llvm-libc-types/__bsearchcompare_t.h; libc/include/llvm-libc-types/__call_once_func_t.h; libc/include/llvm-libc-types/__futex_word.h; libc/include/llvm-libc-types/__mutex_type.h; libc/include/llvm-libc-types/__qsortcompare_t.h; libc/include/llvm-libc-types/__sighandler_t.h; libc/loader/linux/aarch64/start.cpp; libc/loader/linux/x86_64/start.cpp; libc/src/assert/__assert_fail.h; libc/src/ctype/isalnum.cpp; libc/src/ctype/isalnum.h; libc/src/ctype/isalpha.cpp; libc/src/ctype/isalpha.h; libc/src/ctype/isascii.cpp; libc/src/ctype/isascii.h; libc/src/ctype/isblank.cpp; libc/src/ctype/isblank.h; libc/src/ctype/iscntrl.cpp; libc/src/ctype/iscntrl.h; libc/src/ctype/isdigit.cpp; libc/src/ctype/isdigit.h; libc/src/ctype/isgraph.cpp; libc/src/ctype/isgraph.h; libc/src/ctype/islower.cpp; libc/src/ctype/islower.h; libc/src/ctype/isprint.cpp; libc/src/ctype/isprint.h; libc/src/ctype/ispunct.cpp; libc/src/ctype/ispunct.h; libc/src/ctype/isspace.cpp; libc/src/ctype/isspace.h; libc/src/ctype/isupper.cpp; libc/src/ctype/isupper.h; libc/src/ctype/isxdigit.cpp; libc/src/ctype/isxdigit.h; libc/src/ctype/toascii.cpp; libc/src/ctype/toascii.h; libc/src/ctype/tolower.cpp; libc/src/ctype/tolower.h; libc/src/ctype/toupper.cpp; libc/src/ctype/toupper.h; libc/src/errno/dummy_errno.cpp; libc/src/er,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:132244,load,loader,132244,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['load'],['loader']
Performance,"c/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; inv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:224994,load,load,224994,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"c; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:274962,load,load,274962,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"c; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2 sc0=1 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/stor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:328613,load,loads,328613,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ca's and spill slots), the prolog and epilog code for the function; can be inserted and ""abstract stack location references"" can be eliminated.; This stage is responsible for implementing optimizations like frame-pointer; elimination and stack packing. 6. `Late Machine Code Optimizations`_ --- Optimizations that operate on ""final""; machine code can go here, such as spill code scheduling and peephole; optimizations. 7. `Code Emission`_ --- The final stage actually puts out the code for the; current function, either in the target assembler format or in machine; code. The code generator is based on the assumption that the instruction selector will; use an optimal pattern matching selector to create high-quality sequences of; native instructions. Alternative code generator designs based on pattern; expansion and aggressive iterative peephole optimization are much slower. This; design permits efficient compilation (important for JIT environments) and; aggressive optimization (used when generating code offline) by allowing; components of varying levels of sophistication to be used for any step of; compilation. In addition to these stages, target implementations can insert arbitrary; target-specific passes into the flow. For example, the X86 target uses a; special pass to handle the 80x87 floating point stack architecture. Other; targets with unusual requirements can be supported with custom passes as needed. Using TableGen for target description; -------------------------------------. The target description classes require a detailed description of the target; architecture. These target descriptions often have a large amount of common; information (e.g., an ``add`` instruction is almost identical to a ``sub``; instruction). In order to allow the maximum amount of commonality to be; factored out, the LLVM code generator uses the; :doc:`TableGen/index` tool to describe big chunks of the; target machine, which allows the use of domain-specific and target-specific; abstractio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:7678,optimiz,optimization,7678,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"cal 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:215313,load,load,215313,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"cal address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic glc=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:245188,load,load,245188,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"cal. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; local load; atomic/store",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:376943,load,load,376943,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"cal; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:224829,perform,performing,224829,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"calar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU over XGMI or PCIe the kernarg backing memory is allocated in host; memory accessed as MTYPE UC (uncached) to avoid needing to invalidate the L2; cache. This also causes it to be treated as non-volatile and so is not; invalidated by ``*_vol``.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX90A are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table`. .. table:: AMDHSA Memory Model Code Sequences GFX90A; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table. ============ ===",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:241217,cache,cache,241217,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"calar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU over XGMI or PCIe the kernarg backing memory is allocated in host; memory accessed as MTYPE UC (uncached) to avoid needing to invalidate the L2; cache. This also causes it to be treated as non-volatile and so is not; invalidated by ``*_vol``.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX940, GFX941, GFX942; are defined in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table`. .. table:: AMDHSA Memory Model Code Sequences GFX940, GFX941, GFX942; :name: amdgpu-amdhsa-memory-model",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:291235,cache,cache,291235,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"cale x 4 x float> <left_op>, <vscale x 4 x float> <middle_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fma.v256f64 (<256 x double> <left_op>, <256 x double> <middle_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point fused multiply-add of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first three operands and the result have the same vector of floating-point type. The; fourth operand is the vector mask and has the same number of elements as the; result vector type. The fifth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fma``' intrinsic performs floating-point fused multiply-add (:ref:`llvm.fma <int_fma>`); of the first, second, and third vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fma.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.fma(<4 x float> %a, <4 x float> %b, <4 x float> %c); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fmuladd:. '``llvm.vp.fmuladd.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fmuladd.v16f32 (<16 x float> <left_op>, <16 x float> <middle_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fmuladd.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <middle_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fmuladd.v256f64 (<256 x double> <left_op>,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:745779,perform,performed,745779,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"calhost:8080/jsrootsys/demo/demo.htm?addr=../../Files/job1.root/hpx/root.json.gz&layout=3x3>; 18. Also for online server process url options like 'item', 'items', 'layout'; 19. Possibility to generate URL, which reproduces opened page with layout and drawn items. ### August 2014; 1. All communication between server and browser done with JSON format.; 2. Fix small error in dtree.js - one should always set; last sibling (_ls) property while tree can be dynamically changed.; 3. In JSRootCore.js provide central function, which handles different kinds; of XMLHttpRequest. Use only async requests, also when getting file header.; 4. Fully reorganize data management in file/tree/directory/collection hierarchical; display. Now complete description collected in HPainter class and decoupled from; visualization, performed with dTree.js.; 5. Remove all global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify '2d', 'io' (default) or '3d'.; 13. Use new AssertPrerequisites functionality to load only required functionality.; 14. When displaying single element, one could specify draw options and monitor property like:; <http://local",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:74510,load,loaded,74510,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loaded']
Performance,"call i8 @llvm.fptoui.sat.i8.f32(float 377.0) ; yields i8: 255; %d = call i8 @llvm.fptoui.sat.i8.f32(float 0xFFF8000000000000) ; yields i8: 0. '``llvm.fptosi.sat.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.fptosi.sat`` on any; floating-point argument type and any integer result type, or vectors thereof.; Not all targets may support all types, however. ::. declare i32 @llvm.fptosi.sat.i32.f32(float %f); declare i19 @llvm.fptosi.sat.i19.f64(double %f); declare <4 x i100> @llvm.fptosi.sat.v4i100.v4f128(<4 x fp128> %f). Overview:; """""""""""""""""". This intrinsic converts the argument into a signed integer using saturating; semantics. Arguments:; """""""""""""""""""". The argument may be any floating-point or vector of floating-point type. The; return value may be any integer or vector of integer type. The number of vector; elements in argument and return must be the same. Semantics:; """""""""""""""""""". The conversion to integer is performed subject to the following rules:. - If the argument is any NaN, zero is returned.; - If the argument is smaller than the smallest representable signed integer of; the result type (this includes negative infinity), the smallest; representable signed integer is returned.; - If the argument is larger than the largest representable signed integer of; the result type (this includes positive infinity), the largest representable; signed integer is returned.; - Otherwise, the result of rounding the argument towards zero is returned. Example:; """""""""""""""". .. code-block:: text. %a = call i8 @llvm.fptosi.sat.i8.f32(float 23.9) ; yields i8: 23; %b = call i8 @llvm.fptosi.sat.i8.f32(float -130.8) ; yields i8: -128; %c = call i8 @llvm.fptosi.sat.i8.f32(float 999.0) ; yields i8: 127; %d = call i8 @llvm.fptosi.sat.i8.f32(float 0xFFF8000000000000) ; yields i8: 0. Convergence Intrinsics; ----------------------. The LLVM convergence intrinsics for controlling the semantics of ``convergent``; operations, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:686119,perform,performed,686119,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"call void @foo(ptr %ptr). %a = load i8, ptr %ptr, !invariant.group !0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.group !0 ; Can assume that %unknownValue == 42. call void @foo(ptr %ptr); %newPtr2 = call ptr @llvm.launder.invariant.group.p0(ptr %ptr); %d = load i8, ptr %newPtr2, !invariant.group !0 ; Can't step through launder.invariant.group to get value of %ptr. ...; declare void @foo(ptr); declare ptr @getPointer(ptr); declare ptr @llvm.launder.invariant.group.p0(ptr). !0 = !{}. The invariant.group metadata must be dropped when replacing one pointer by; another based on aliasing information. This is because invariant.group is tied; to the SSA value of the pointer operand. .. code-block:: llvm. %v = load i8, ptr %x, !invariant.group !0; ; if %x mustalias %y then we can replace the above instruction with; %v = load i8, ptr %y. Note that this is an experimental feature, which means that its semantics might; change in the future. '``type``' Metadata; ^^^^^^^^^^^^^^^^^^^. See :doc:`TypeMetadata`. '``associated``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^. The ``associated`` metadata may be attached to a global variable definition with; a single argument that references a global object (optionally through an alias). This metadata lowers to the ELF section flag ``SHF_LINK_ORDER`` which prevents; discarding of the global variable in linker GC unless the referenced object is; also discarded. The linker support for this feature is spotty. For best; compatibility, globals carrying this metadata should:. - Be in ``@llvm.compiler.used``.; - If the referenced global variable is in a comdat, be in the same comdat. ``!associated`` can not express many-to-one relationship. A global variable with; the metadata sho",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:317589,load,load,317589,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"can be applied using the syntax; ""dataset<<entrylist"", e.g.; ""dataset1<<el1|dataset2<<el2|"".; The datasets to be processed can also be specified on one or multiple lines in a text file.; Add; support for automatic download of a package when available on the; master but not locally. The downloaded packages are store under <sandbox>/packages/downloaded; and automatically checked for updates against the master repository. If; a local version of the same package is created (using the; UploadPackage) the entry indownloaded is; cleared, so that the behaviour is unchanged.; Add; the possibility to remap the server for the files in a dataset. This; allows, for example, to reuse the dataset information for the same; files stored in a different cluster.; Add a local cache for; TDataSetManagerFile. This is mainly used to improve the speed of; TDataSetManager::ShowDataSets, which is run very often by users and may; be very slow if the number of dataset is large. The cache is also used; to cache frequently received dataset objects.Add the possibility to audit the activity on the nodes via syslog. .; New packetizer TPacketizerFile generating packets which contain a single; file path to be used in processing single files. Used, for example, in; tasks generating files. The files are specified into a TMap - named; 'PROOF_FilesToProcess' - containing the list of files to be generated; per host (the key is the host name, the value the TList of TObjString; (or TFileInfo) with the files names - or a TFileCollection: the output; of TFileCollection::GetFilesPerServer() can be directly passed as files; map). Workers are first assigned files belonging to; the list with host name matching the worker name. The map is; distributed to the master via the input list.Add support for; automatic setting of pointer data members to the relevant object in the; output list. The use of fOutputList->FindObject(""name"") in; TSelector::Terminate is not needed anymore for pointer data members,; e.g. histogra",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html:1693,cache,cache,1693,proof/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html,4,['cache'],['cache']
Performance,"can be significantly faster than an ordinary division; but can also have significantly less precision. Defaults to; ``-fno-reciprocal-math``. .. option:: -f[no-]unsafe-math-optimizations. Allow unsafe floating-point optimizations.; ``-funsafe-math-optimizations`` also implies:. * ``-fapprox-func``; * ``-fassociative-math``; * ``-freciprocal-math``; * ``-fno-signed-zeros``; * ``-fno-trapping-math``; * ``-ffp-contract=fast``. ``-fno-unsafe-math-optimizations`` implies:. * ``-fno-approx-func``; * ``-fno-associative-math``; * ``-fno-reciprocal-math``; * ``-fsigned-zeros``; * ``-ftrapping-math``; * ``-ffp-contract=on``; * ``-fdenormal-fp-math=ieee``. There is ambiguity about how ``-ffp-contract``,; ``-funsafe-math-optimizations``, and ``-fno-unsafe-math-optimizations``; behave when combined. Explanation in :option:`-fno-fast-math` also applies; to these options. Defaults to ``-fno-unsafe-math-optimizations``. .. option:: -f[no-]finite-math-only. Allow floating-point optimizations that assume arguments and results are; not NaNs or +-Inf. ``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * ``-fno-honor-infinities``; * ``-fno-honor-nans``. ``-ffno-inite-math-only`` implies:. * ``-fhonor-infinities``; * ``-fhonor-nans``. Defaults to ``-fno-finite-math-only``. .. option:: -f[no-]rounding-math. Force floating-point operations to honor the dynamically-set rounding mode by default. The result of a floating-point operation often cannot be exactly represented in the result type and therefore must be rounded. IEEE 754 describes different rounding modes that control how to perform this rounding, not all of which are supported by all implementations. C provides interfaces (``fesetround`` and ``fesetenv``) for dynamically controlling the rounding mode, and while it also recommends certain conventions for changing the rounding mode, these conventions are not typically enforced in the ABI. Since the rounding mode changes th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:60032,optimiz,optimizations,60032,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"can have flags, for NaN/NoNaN variants. Integer/bitwise reductions; ^^^^^^^^^^^^^^^^^^^^^^^^^^. * G_VECREDUCE_ADD; * G_VECREDUCE_MUL; * G_VECREDUCE_AND; * G_VECREDUCE_OR; * G_VECREDUCE_XOR; * G_VECREDUCE_SMAX; * G_VECREDUCE_SMIN; * G_VECREDUCE_UMAX; * G_VECREDUCE_UMIN. Integer reductions may have a result type larger than the vector element type.; However, the reduction is performed using the vector element type and the value; in the top bits is unspecified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is sign-extending, as with G_SEXTLOAD. G_INDEXED_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is zero-extending, as with G_ZEXTLOAD. G_STORE; ^^^^^^^. Generic store. Expects a MachineMemOperand in addition to explicit; operands. If the stored value size is greater than the memory size,; the high bits are implicitly truncated. If this is a vector store, the; high elements are discarded (i.e. this does not function as a per-lane; vector, truncating store). G_INDEXED_STORE; ^^^^^^^^^^^^^^^. Comb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:15921,load,loads,15921,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,2,['load'],['loads']
Performance,"cans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop passes inherently require modifying the function the; loop is in, and that includes some function analyses the loop analyses depend; on. This discounts future concurrency over separate loops in a function, but; that's a tradeoff due to how tightly a loop and its function are coupled. To; make sure the function analyses that loop passes use are ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:10002,cache,cached,10002,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,2,"['cache', 'concurren']","['cached', 'concurrency']"
Performance,"case, we want to be able to elide copies into ``bar``'s argument; slots. That means we need to have more than one set of argument frames; active at the same time. First, we need to allocate the frame for the; outer call so we can pass it in as the hidden struct return pointer to; the middle call. Then we do the same for the middle call, allocating a; frame and passing its address to ``Foo``'s default constructor. By; wrapping the evaluation of the inner ``bar`` with stack save and; restore, we can have multiple overlapping active call frames. Callee-cleanup Calling Conventions; ----------------------------------. Another wrinkle is the existence of callee-cleanup conventions. On; Windows, all methods and many other functions adjust the stack to clear; the memory used to pass their arguments. In some sense, this means that; the allocas are automatically cleared by the call. However, LLVM; instead models this as a write of undef to all of the inalloca values; passed to the call instead of a stack adjustment. Frontends should; still restore the stack pointer to avoid a stack leak. Exceptions; ----------. There is also the possibility of an exception. If argument evaluation; or copy construction throws an exception, the landing pad must do; cleanup, which includes adjusting the stack pointer to avoid a stack; leak. This means the cleanup of the stack memory cannot be tied to the; call itself. There needs to be a separate IR-level instruction that can; perform independent cleanup of arguments. Efficiency; ----------. Eventually, it should be possible to generate efficient code for this; construct. In particular, using inalloca should not require a base; pointer. If the backend can prove that all points in the CFG only have; one possible stack level, then it can address the stack directly from; the stack pointer. While this is not yet implemented, the plan is that; the inalloca attribute should not change much, but the frontend IR; generation recommendations may change.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst:5776,perform,perform,5776,interpreter/llvm-project/llvm/docs/InAlloca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst,1,['perform'],['perform']
Performance,"cated memory.; Issues resulting from self-assignment in C++.; Incorrect usage of MPI APIs in C and C++. This check can be enabled by passing the following command to scan-build: ; -enable-checker optin.mpi.MPI-Checker. The scan-build tool now supports a --force-analyze-debug-code flag that forces projects to analyze in debug mode. This flag leaves in assertions and so typically results in fewer false positives.; Additional miscellaneous improvements.; Now requires macOS 10.8 or later. checker-278; built: February 5, 2016; download: checker-278.tar.bz2; highlights:. Greatly improves analysis of C++ lambdas, including interprocedural analysis of lambda applications and reduced 'dead store'; false positives for variables captured by reference.; The analyzer now checks for misuse of 'vfork()'. This check is enabled by default.; The analyzer can now detect excessively-padded structs. This check can be enabled by passing the following; command to scan-build:; -enable-checker optin.performance.Padding ; The checks to detect misuse of _Nonnull are now enabled by default.; The checks to detect misuse of Objective-C generics are now enabled by default.; Many miscellaneous improvements. checker-277; built: October 28, 2015; download: checker-277.tar.bz2; highlights:. Includes about 20 months of change to Clang itself.; New checker for C++ leaks is turned on by default.; Added various small checks and bug fixes.; Added experimental checkers for Objective-C:. New localizability checks:; ; Checker warning about uses of non-localized NSStrings passed to UI methods expecting localized strings.; Checker warning when the comment argument is missing from NSLocalizedString macros.; These can be enabled by passing the following command to scan-build:. -enable-checker alpha.osx.cocoa.NonLocalizedStringChecker,alpha.osx.cocoa.EmptyLocalizationContextChecker. New checks for _Nonnull type qualifiers. These can be enabled with:. -enable-checker nullability.NullPassedToNonnull,nullabi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html:1497,perform,performance,1497,interpreter/llvm-project/clang/www/analyzer/release_notes.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html,2,['perform'],['performance']
Performance,"cblog/2018/01/15/spectre-mitigations-in-msvc/).; Their technique is to use static analysis within the compiler to only insert; `lfence` instructions into conditional edges at risk of attack. However,; [initial](https://arstechnica.com/gadgets/2018/02/microsofts-compiler-level-spectre-fix-shows-how-hard-this-problem-will-be-to-solve/); [analysis](https://www.paulkocher.com/doc/MicrosoftCompilerSpectreMitigation.html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hard",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:46734,perform,performance,46734,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"ccordingly.; Bug triage: Clang's ; issue trackercurrently has over 20,000 open issues, many of which are not; appropriately tagged, are no longer reproducible, could use a reduced test case,; or otherwise needs some manual interaction. We can always use help with; bug triage and; issue tracker maintenance. Improve build times with Clang: the time it takes Clang to process a; translation unit is very important to our users; the lower the build time, the; better the overall user experience. It would be good to improve Clang's; performance as well as to find ways to proactively alert us when we've; introduced a change that has significant negative impact on build times.; Complete support for the experimental constant expression interpreter; : Clang's production constant expression interpreter computes a constant; expression result by walking over AST nodes, performing calculations as it; goes. This does not have good performance properties, and so we've begun work; on an ; experimental constant expression interpreter that works by converting the; AST into bytecode that is interpreted. This effort has a long tail of work left; to complete because it requires implementing byte code for every kind of; expression and type that can be used in a constant expression for C++ and C. Improve clang-doc: Clang's library-based design allows it to be used; by a variety of tools that reason about source code.; clang-doc is one; great application of this functionality, which generates code documentation; from source code. The tool is in early stages of development and could use more; dedicated effort to complete the implementation.; Self-testing using clang: There are several neat ways to; improve the quality of clang by self-testing. Some examples:. Improve the reliability of AST printing and serialization by; ensuring that the AST produced by clang on an input doesn't change; when it is reparsed or unserialized. Improve parser reliability and error generation by automatically; or ran",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/OpenProjects.html:3537,perform,performance,3537,interpreter/llvm-project/clang/www/OpenProjects.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/OpenProjects.html,2,['perform'],['performance']
Performance,"ce LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw-with-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; atomicrmw-no-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:352435,load,load,352435,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ce a widget has been set as DND Target, it has to handle Drag and Drop; events. `Atom_t HandleDNDenter(Atom_t *typelist)` - this method is used to; handle a drag operation entering the widget. The typelist parameter is; the list of types the data contains. If the list contains one type the; widget understands, it should return this type to notify the manager; that the drop would be accepted, i.e. :. ``` {.cpp}; for (int i = 0; typelist[i] != kNone; ++i) {; if (typelist[i] == gVirtualX->InternAtom(""application/root"")); // accept ""application/root"" DND type; return typelist[i];; }; // other type not accepted; return kNone;; ```. `Atom_t HandleDNDposition(Int_t x,Int_t y,Atom_t action,Int_t xroot,; Int_t yroot)` - this; method should be used to handle the drag position in widget coordinates; (`x,y`) or in root coordinates (`xroot,yroot`). ``` {.cpp}; // get the pad over which the cursor is; TPad *pad = fCanvas->Pick(x, y, 0);; if (pad) {; pad->cd();; gROOT->SetSelectedPad(pad);; }; return action;; ```. `Bool_t HandleDNDdrop(TDNDdata *data)` - this is the place where the; widget actually receives the data. First, check the data format (see; description of **`TDNDData`** - Drag and Drop data class) and then use; it accordingly. In the case of ROOT object, here is an example of how to; retrieve it:. ``` {.cpp}; if (data->fDataType == gVirtualX->InternAtom(""application/root"")) {; TBufferFile buf(TBuffer::kRead, data->fDataLength,; (void *)data->fData);; buf.SetReadMode();; TObject *obj = (TObject *)buf.ReadObjectAny(TObject::Class());; if (obj->IsA()->GetMethodAllAny(""Draw"")); obj->Draw();; }; ```. In the case of URI/list, the use is:. ``` {.cpp}; if (data->fDataType == gVirtualX->InternAtom(""text/uri-list"")) {; TString sfname((char *)data->fData);; TUrl uri(sfname.Data());; if (sfname.EndsWith("".jpg""); TImage *img = TImage::Open(uri.GetFile());; }; ```. `Bool_t HandleDNDleave()` is used if a specific action has to be; performed when the drag operation leaves the widget.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:112865,perform,performed,112865,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['perform'],['performed']
Performance,"ce between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 channel. Therefore, the vector; and scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different SAs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is; required to ensure synchronization between vector memory operations of; different SAs. It ensures a previous vector memory operation has completed; before executing a subsequent vector memory and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence.; * On GFX10.3 and GFX11 a memory attached last level (MALL) cache exists for GPU memory.; The MALL cache is fully coherent with GPU",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:338815,cache,cache,338815,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5294,optimiz,optimized,5294,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,7,"['cache', 'optimiz']","['cache', 'cache-line-aligned', 'caches', 'optimized']"
Performance,"ce interface allows developers to pipe any kind of columnar data format into TDataFrame. Two example data sources have been provided: the TRootDS and the TTrivialDS. The former allows to read via the novel data source mechanism ROOT data, while the latter is a simple generator, created for testing and didactic purposes. It is therefore now possible to interface *any* kind of dataset/data format to ROOT as long as an adaptor which implements the pure virtual methods of the TDataSource interface can be written in C++.; - TDF can now read CSV files through a specialized TDataSource. Just create the TDF with `MakeCsvDataFrame(""f.csv"")`. Just create the TDF with MakeCsvDataFrame(""f.csv""). The data types of the CSV columns are automatically inferred. You can also specify if you want to use a different delimiter or if your file does not have headers.; - Users can now configure Snapshot to use different file open modes (""RECREATE"" or ""UPDATE""), compression level, compression algorithm, TTree split-level and autoflush settings; - Users can now access multi-threading slot and entry number as pre-defined columns ""tdfslot_"" and ""tdfentry_"". Especially useful for pyROOT users.; - Users can now specify filters and definitions as strings containing multiple C++ expressions, e.g. ""static int a = 0; return ++a"". Especially useful for pyROOT users.; - Histograms can be initialised by *models*, which allow to create histograms with the same parameters of their constructors, for example; ```c++; auto myHisto = myTdf.Histo1D({""histName"", ""histTitle"", 64, 0, 128}, ""myColumn"");; ```; or; ```c++; auto myHistoCustomBinning = myTdf.Histo1D({""histName"", ""histTitle"", 64, binEdges}, ""myColumn"");; ```; Models can be created as stand alone objects:; ```c++; TDF::TH1DModel myModel {""histName"", ""histTitle"", 64, binEdges};; auto myHistoCustomBinning = myTdf.Histo1D(myModel, ""myColumn"");; ```; - pyROOT users can now easily specify parameters for the TDF histograms and profiles thanks to the newly intr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:13483,multi-thread,multi-threading,13483,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['multi-thread'],['multi-threading']
Performance,"ce languages through; `Clang <https://clang.llvm.org/>`_. Many other language frontends have; been written using LLVM, and an incomplete list is available at; `projects with LLVM <https://llvm.org/ProjectsWithLLVM/>`_. I'd like to write a self-hosting LLVM compiler. How should I interface with the LLVM middle-end optimizers and back-end code generators?; ----------------------------------------------------------------------------------------------------------------------------------------; Your compiler front-end will communicate with LLVM by creating a module in the; LLVM intermediate representation (IR) format. Assuming you want to write your; language's compiler in the language itself (rather than C++), there are 3; major ways to tackle generating LLVM IR from a front-end:. 1. **Call into the LLVM libraries code using your language's FFI (foreign; function interface).**. * *for:* best tracks changes to the LLVM IR, .ll syntax, and .bc format. * *for:* enables running LLVM optimization passes without a emit/parse; overhead. * *for:* adapts well to a JIT context. * *against:* lots of ugly glue code to write. 2. **Emit LLVM assembly from your compiler's native language.**. * *for:* very straightforward to get started. * *against:* the .ll parser is slower than the bitcode reader when; interfacing to the middle end. * *against:* it may be harder to track changes to the IR. 3. **Emit LLVM bitcode from your compiler's native language.**. * *for:* can use the more-efficient bitcode reader when interfacing to the; middle end. * *against:* you'll have to re-engineer the LLVM IR object model and bitcode; writer in your language. * *against:* it may be harder to track changes to the IR. If you go with the first option, the C bindings in include/llvm-c should help; a lot, since most languages have strong support for interfacing with C. The; most common hurdle with calling C from managed code is interfacing with the; garbage collector. The C interface was designed to require ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst:3818,optimiz,optimization,3818,interpreter/llvm-project/llvm/docs/FAQ.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst,1,['optimiz'],['optimization']
Performance,"ce's dependence graph, which may result in; these symbols (and possibly others) becoming *Ready* if all of their; dependencies have also been emitted. .. _passes:. Passes; ------. JITLink passes are ``std::function<Error(LinkGraph&)>`` instances. They are free; to inspect and modify the given ``LinkGraph`` subject to the constraints of; whatever phase they are running in (see :ref:`generic_link_algorithm`). If a; pass returns ``Error::success()`` then linking continues. If a pass returns; a failure value then linking is stopped and the ``JITLinkContext`` is notified; that the link failed. Passes may be used by both JITLink backends (e.g. MachO/x86-64 implements GOT; and PLT construction as a pass), and external clients like; ``ObjectLinkingLayer::Plugin``. In combination with the open ``LinkGraph`` API, JITLink passes enable the; implementation of powerful new features. For example:. * Relaxation optimizations -- A pre-fixup pass can inspect GOT accesses and PLT; calls and identify situations where the addresses of the entry target and the; access are close enough to be accessed directly. In this case the pass can; rewrite the instruction stream of the containing block and update the fixup; edges to make the access direct. Code for this looks like:. .. code-block:: c++. Error relaxGOTEdges(LinkGraph &G) {; for (auto *B : G.blocks()); for (auto &E : B->edges()); if (E.getKind() == x86_64::GOTLoad) {; auto &GOTTarget = getGOTEntryTarget(E.getTarget());; if (isInRange(B.getFixupAddress(E), GOTTarget)) {; // Rewrite B.getContent() at fixup address from; // MOVQ to LEAQ. // Update edge target and kind.; E.setTarget(GOTTarget);; E.setKind(x86_64::PCRel32);; }; }. return Error::success();; }. * Metadata registration -- Post allocation passes can be used to record the; address range of sections in the target. This can be used to register the; metadata (e.g exception handling frames, language metadata) in the target; once memory has been finalized. .. code-block:: c++. Error ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:23530,optimiz,optimizations,23530,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['optimiz'],['optimizations']
Performance,"ce-paired-atomic. 2. buffer_inv sc0=1 sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. GFX940, GFX941; - wavefront - generic buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc0=1; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must hap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:307302,load,load,307302,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ce_region:. Convergence Regions; -------------------. The *convergence region* of a convergence token T is the minimal region in; which T is live and used, i.e., the set of program points dominated by the; definition D of T from which a use of T can be reached. The following static rule about convergence regions must be satisfied by; valid programs:. If a convergence region R for a token T1 contains a use of a convergence; token T2, then R must also contain the definition of T2. (In other words,; convergence regions must be reasonably nested.). .. note::. For brevity, this document uses the term ""convergence region of a token; definition ``D``"" to actually refer to the convergence region of the token; ``T`` defined by ``D``. .. _inferring_noconvergent:. Inferring non-convergence; =========================. When the target or the environment guarantees that threads do not; communicate using convergent operations or that threads never diverge,; the dynamic instances in the program are irrelevant and an optimizer; may remove any occurrence of the ``convergent`` attribute on a; call-site or a function and any explicit ``convergencectrl`` operand; bundle at a call-site. An optimizer may remove the ``convergent`` attribute and any explicit; ``convergencectrl`` operand bundle from a call-site if it can prove; that the execution of this call-site always results in a call to a; non-convergent function. An optimizer may remove the ``convergent`` attribute on a function if it can; prove that the function does not contain a call to; :ref:`llvm.experimental.convergence.entry; <llvm.experimental.convergence.entry>`, or any uncontrolled convergent; operations. Memory Model Non-Interaction; ============================. The fact that an operation is convergent has no effect on how it is treated for; memory model purposes. In particular, an operation that is ``convergent`` and; ``readnone`` does not introduce additional ordering constraints as far as the; memory model is concerned. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst:33472,optimiz,optimizer,33472,interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ConvergentOperations.rst,1,['optimiz'],['optimizer']
Performance,"ce`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``container`` function is also in the region and will not be optimized, but; it causes the instantiation of ``twice`` and ``thrice`` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ign",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161558,optimiz,optimize,161558,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['optimiz'],"['optimize', 'optimized']"
Performance,"cement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used for; positioning volume daughters, then: `MASTER = T * LOCAL`. Therefore `T `is used to perform a local to master conversion, while; `T-1` for a master to local conversion. The second use case is the; computation of the global transformation of a given object in the; geometry. Since the geometry is built as 'volumes-inside-volumes', the; global transformation represents the pile-up of all local; transformations in the corresponding branch. Once a given object in the; hierarchy becomes the current one, the conversion from master to local; coordinates or the other way around can be done from the manager class. A general homogenous transformation is defined as a 4x4 matrix embedding; a rotation, a translation and a scale. The advantage of this description; is that each basic transformation can be represented as a homogenous; matrix, composition being performed as simple matrix multiplication. Rotation:. \f[; \left|\begin{array}{cccc}; r_{11} & r_{12} & r_{13} & 0 \\; r_{21} & r_{22} & r_{23} & 0 \\; r_{31} & r_{32} & r_{33} & 0 \\; 0 & 0 & 0 & 1; \end{array}; \right|; \f]. Translation:. \f[; \left|\begin{array}{cccc}; 1 & 0 & 0 & 0 \\; 0 & 1 & 0 & 0 \\; 0 & 0 & 1 & 0 \\; t_x & t_y & t_z & 1; \end{array}; \right|; \f]. Scale:. \f[; \left|\begin{array}{cccc}; s_x & 0 & 0 & 0 \\; 0 & s_y & 0 & 0 \\; 0 & 0 & s_z & 0 \\; 0 & 0 & 0 & 1; \end{array}; \right|; \f]. Inverse rotation:. \f[; \left|\begin{array}{cccc}; r_{11} & r_{21} & r_{31} & 0 \\; r_{12} & r_{22} & r_{32} & 0 \\; r_{13} & r_{23} & r_{33} & 0 \\; 0 & 0 & 0 & 1; \end{array}; \right|; \f]. Inverse translation:. \f[; \left|\begin{array}{cccc}; 1 & 0 & 0 & 0 \\; 0 & 1 & 0 & 0 \\; 0 & 0 & 1 & 0 \\; -t_x & -t_y & -t_z & 1; \end{array}; \right|; \f]. Inverse scale:. \f[; \left|\begin{array}{cccc}; \frac{1}{s_x} & 0 & 0 & 0 \\; 0 & \frac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:52743,perform,performed,52743,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"cement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used for; positioning volume daughters, then: `MASTER = T * LOCAL`. Therefore `T `is used to perform a local to master conversion, while; `T-1` for a master to local conversion. The second use case is the; computation of the global transformation of a given object in the; geometry. Since the geometry is built as 'volumes-inside-volumes', the; global transformation represents the pile-up of all local; transformations in the corresponding branch. Once a given object in the; hierarchy becomes the current one, the conversion from master to local; coordinates or the other way around can be done from the manager class. A general homogenous transformation is defined as a 4x4 matrix embedding; a rotation, a translation and a scale. The advantage of this description; is that each basic transformation can be represented as a homogenous; matrix, composition being performed as simple matrix multiplication. Rotation:; $\left|\begin{array}{cccc}; r_{11} & r_{12} & r_{13} & 0 \\; r_{21} & r_{22} & r_{23} & 0 \\; r_{31} & r_{32} & r_{33} & 0 \\; 0 & 0 & 0 & 1; \end{array}; \right|$; Translation:; $\left|\begin{array}{cccc}; 1 & 0 & 0 & 0 \\; 0 & 1 & 0 & 0 \\; 0 & 0 & 1 & 0 \\; t_x & t_y & t_z & 1; \end{array}; \right|$; Scale:; $\left|\begin{array}{cccc}; s_x & 0 & 0 & 0 \\; 0 & s_y & 0 & 0 \\; 0 & 0 & s_z & 0 \\; 0 & 0 & 0 & 1; \end{array}; \right|$. Inverse rotation:; $\left|\begin{array}{cccc}; r_{11} & r_{21} & r_{31} & 0 \\; r_{12} & r_{22} & r_{32} & 0 \\; r_{13} & r_{23} & r_{33} & 0 \\; 0 & 0 & 0 & 1; \end{array}; \right|$; Inverse translation:; $\left|\begin{array}{cccc}; 1 & 0 & 0 & 0 \\; 0 & 1 & 0 & 0 \\; 0 & 0 & 1 & 0 \\; -t_x & -t_y & -t_z & 1; \end{array}; \right|$; Inverse scale:; $\left|\begin{array}{cccc}; \frac{1}{s_x} & 0 & 0 & 0 \\; 0 & \frac{1}{s_y} & 0 & 0 \\; 0 & 0 & \frac{1}{s_z} &",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:92416,perform,performed,92416,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"ces it with Python's ``__len__``:. .. code-block:: python. >>> import cppyy; >>>; >>> def replace_getlength(klass, name):; ... try:; ... klass.__len__ = klass.__dict__['GetLength']; ... del klass.GetLength; ... except KeyError:; ... pass; ...; >>> cppyy.py.add_pythonization(replace_getlength, 'MyNamespace'); >>>; >>> cppyy.cppdef(""""""; ... namespace MyNamespace {; ... class MyClass {; ... public:; ... MyClass(int i) : fInt(i) {}; ... int GetLength() { return fInt; }; ... ; ... private:; ... int fInt;; ... };; ... }""""""); True; >>> m = cppyy.gbl.MyNamespace.MyClass(42); >>> len(m); 42; >>> m.GetLength(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; AttributeError: 'MyClass' object has no attribute 'GetLength'; >>>. The deletion of ``GetLength`` method with ``del`` can be omitted; if both ``MyClass.GetLength`` and ``MyClass.__len__`` should be valid. C++ callbacks; -------------. If you are familiar with the Python C-API, it may sometimes be beneficial to; add unique optimizations to your C++ classes to be picked up by the; pythonization layer.; There are two conventional function that cppyy will look for (no registration; of callbacks needed):. .. code-block:: C++. static void __cppyy_explicit_pythonize__(PyObject* klass, const std::string&);. which is called *only* for the class that declares it.; And:. .. code-block:: C++. static void __cppyy_pythonize__(PyObject* klass, const std::string&);. which is also called for all derived classes. Just as with the Python callbacks, the first argument will be the Python; class proxy, the second the C++ name, for easy filtering.; When called, cppyy will be completely finished with the class proxy, so any; and all changes are fair game, including the low-level ones such as the replacement of; iteration or buffer protocols. An example pythonization replacing ``MyClass.GetLength`` method with Python's ``__len__``; done with the C++ callbacks:. .. code-block:: python. >>> import cppyy; >>> ; >>> cppyy.cpp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/pythonizations.rst:3215,optimiz,optimizations,3215,bindings/pyroot/cppyy/cppyy/doc/source/pythonizations.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/pythonizations.rst,1,['optimiz'],['optimizations']
Performance,"cessary code generation through recursive calls; to `translate()`. - **assembleCode()**: combines the generated code statements into the final; code body of the squashed function. These functions will appear again in this document with more contextual; examples. For detailed in-line documentation (code comments), please see:. > [roofit/roofitcore/src/RooFit/Detail/CodeSquashContext.cxx](https://github.com/root-project/root/blob/master/roofit/roofitcore/src/RooFit/Detail/CodeSquashContext.cxx). ### b. RooFuncWrapper. > [roofit/roofitcore/inc/RooFuncWrapper.h](https://github.com/root-project/root/blob/master/roofit/roofitcore/inc/RooFuncWrapper.h). This class wraps the generated C++ code in a RooFit object, so that it can be; used like other RooFit objects. It takes a function body as input and creates a callable function from it.; This allows users to evaluate the function and its derivatives efficiently. #### Helper Functions. - **loadParamsAndData()** extracts parameters and observables from the; provided data and prepares them for evaluation. - **declareAndDiffFunction()**: declare the function and create its; derivative. - **gradient()**: calculates the gradient of the function with respect to its; parameters. - **buildCode()**: generates the optimized code for evaluating the function; and its derivatives. - **dumpCode()**: prints the squashed code body to console (useful for; debugging). - **dumpGradient()**: prints the derivative code body to console (useful for; debugging). These functions will appear again in this document with more contextual; examples. For detailed in-line documentation (code comments), please see:. > [roofit/roofitcore/src/RooFuncWrapp9er.cxx](https://github.com/root-project/root/blob/master/roofit/roofitcore/src/RooFuncWrapper.cxx). ## Appendix C - Helper functions discussed in this document. - **RooFit::Detail::CodeSquashContext::addResult()**: For a specific class, it; will add whatever is represented on the right-hand side (a function ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:33577,load,loadParamsAndData,33577,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,1,['load'],['loadParamsAndData']
Performance,"cessors). Among other issues, the register allocator is known to; perform badly with confronted with such structures. The only exception to; this guidance is that a unified return block with high in-degree is fine. Use of allocas; ^^^^^^^^^^^^^^. An alloca instruction can be used to represent a function scoped stack slot,; but can also represent dynamic frame expansion. When representing function; scoped variables or locations, placing alloca instructions at the beginning of; the entry block should be preferred. In particular, place them before any; call instructions. Call instructions might get inlined and replaced with; multiple basic blocks. The end result is that a following alloca instruction; would no longer be in the entry basic block afterward. The SROA (Scalar Replacement Of Aggregates) and Mem2Reg passes only attempt; to eliminate alloca instructions that are in the entry basic block. Given; SSA is the canonical form expected by much of the optimizer; if allocas can; not be eliminated by Mem2Reg or SROA, the optimizer is likely to be less; effective than it could be. Avoid loads and stores of large aggregate type; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. LLVM currently does not optimize well loads and stores of large :ref:`aggregate; types <t_aggregate>` (i.e. structs and arrays). As an alternative, consider; loading individual fields from memory. Aggregates that are smaller than the largest (performant) load or store; instruction supported by the targeted hardware are well supported. These can; be an effective way to represent collections of small packed fields. Prefer zext over sext when legal; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. On some architectures (X86_64 is one), sign extension can involve an extra; instruction whereas zero extension can be folded into a load. LLVM will try to; replace a sext with a zext when it can be proven safe, but if you have; information in your source language about the range of an integer value, it can; be profitabl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:2862,optimiz,optimizer,2862,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,2,['optimiz'],['optimizer']
Performance,"cfg>` pass will clean up blocks; which are split out but end up being unnecessary, so usage of this pass should; not pessimize generated code. This pass obviously modifies the CFG, but updates loop information and; dominator information. ``loop-unroll``: Unroll loops; -----------------------------. This pass implements a simple loop unroller. It works best when loops have; been canonicalized by the :ref:`indvars <passes-indvars>` pass, allowing it to; determine the trip counts of loops easily. ``loop-unroll-and-jam``: Unroll and Jam loops; ---------------------------------------------. This pass implements a simple unroll and jam classical loop optimisation pass.; It transforms loop from:. .. code-block:: c++. for i.. i+= 1 for i.. i+= 4; for j.. for j..; code(i, j) code(i, j); code(i+1, j); code(i+2, j); code(i+3, j); remainder loop. Which can be seen as unrolling the outer loop and ""jamming"" (fusing) the inner; loops into one. When variables or loads can be shared in the new inner loop, this; can lead to significant performance improvements. It uses; :ref:`Dependence Analysis <passes-da>` for proving the transformations are safe. ``lower-global-dtors``: Lower global destructors; ------------------------------------------------. This pass lowers global module destructors (``llvm.global_dtors``) by creating; wrapper functions that are registered as global constructors in; ``llvm.global_ctors`` and which contain a call to ``__cxa_atexit`` to register; their destructor functions. ``loweratomic``: Lower atomic intrinsics to non-atomic form; -----------------------------------------------------------. This pass lowers atomic intrinsics to non-atomic form for use in a known; non-preemptible environment. The pass does not verify that the environment is non-preemptible (in general; this would require knowledge of the entire call graph of the program including; any libraries which may not be available in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:28610,load,loads,28610,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,2,"['load', 'perform']","['loads', 'performance']"
Performance,"ch 15, 2012; highlights:. Enables experimental interprocedural analysis (within a file), which greatly amplifies the analyzer's ability to find issues.; Many bug fixes to the malloc/free checker.; Support for new Objective-C NSArray/NSDictionary/NSNumber literals syntax, and Objective-C container subscripting. NOTE: This build contains new interprocedural analysis that allows the analyzer to find more complicated bugs that span function boundaries. It may have problems, performance issues, etc. We'd like to hear about them. checker-261; built: February 22, 2012; highlights:. Contains a new experimental malloc/free checker.; Better support for projects using ARC.; Warns about null pointers passed as arguments to C string functions.; Warns about common anti-patterns in 'strncat' size argument, which can lead to buffer overflows.; set-xcode-analyzer now supports self-contained Xcode.app (Xcode 4.3 and later).; Contains a newer version of the analyzer than Xcode 4.3.; Misc. bug fixes and performance work. checker-260; built: January 25, 2012; highlights:; This is essentially the same as checker-259, but enables the following experimental checkers (please provide feedback):. Warns about unsafe uses of CFArrayCreate, CFSetCreate, and CFDictionaryCreate; Warns about unsafe uses of getpw, gets, which are sources of buffer overflows; Warns about unsafe uses of mktemp and mktemps, which can lead to insecure temporary files; Warns about unsafe uses of vfork, which is insecure to use; Warns about not checking the return values of setuid, setgid, seteuid, setegid, setreuid, setregid (another security issue). checker-259; built: January 25, 2012; highlights:. Contains a newer version of the analyzer than the one shipped in Xcode 4.2.; Significant performance optimizations to reduce memory usage of the analyzer.; Tweaks to scan-build to have it work more easily with Xcode projects using Clang.; Numerous bug fixes to better support code using ARC. checker-258; built: October 13, 20",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html:9245,perform,performance,9245,interpreter/llvm-project/clang/www/analyzer/release_notes.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html,2,['perform'],['performance']
Performance,"ch ROOT to use; C++ Modules by default. However, since it is essentially the same technology,; optimizations of C++ Modules also affect the PCH. We have a few tricks up in; the sleeves to but they come with given trade-offs. #### Preloading of C++ Modules. The main focus for the technology preview was not in performance until recently.; We have invested some resources in optimizations and we would like to show you; (probably outdated) performance results:. * Memory footprint -- mostly due to importing all C++ Modules at startup; we see overhead which depends on the number of preloaded modules. For; ROOT it is between 40-60 MB depending on the concrete configuration.; When the workload increases we notice that the overall memory performance; decreases in number of cases.; * Execution times -- likewise we have an execution overhead. For ; workflows which take ms the slowdown can be 2x. Increasing of the work; to seconds shows 50-60% slowdowns. The performance is dependent on many factors such as configuration of ROOT and; workflow. You can read more at our Intel IPCC-ROOT Showcase presentation; here (pp 25-33)[[8]]. #### Loading C++ Modules on Demand. In long term, we should optimize the preloading of modules to be a no-op and; avoid recursive behavior based on identifier lookup callbacks. Unfortunately,; at the moment the loading of C++ modules on demand shows significantly better; performance results. You can visit our continuous performance monitoring tool where we compare; the performance of ROOT against ROOT with a PCH [[9]].; *Note: if you get error 400, clean your cache or open a private browser session.*. ## How to use; C++ Modules in ROOT are default since v6.20 (Unix) and v6.22 (OSX). Enjoy. To disable C++ Modules in ROOT use `-Druntime_cxxmodules=Off`. ## Citing ROOT's C++ Modules; ```latex; % Peer-Reviewed Publication; %; % 22nd International Conference on Computing in High Energy and Nuclear Physics (CHEP); % 8-14 October, 2016, San Francisco, USA; %; @in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:18675,perform,performance,18675,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['perform'],['performance']
Performance,"ch callees to attach information about where in the source language such a; call came from. A string value can be provided as a note.; ``fn_ret_thunk_extern``; This attribute tells the code generator that returns from functions should; be replaced with jumps to externally-defined architecture-specific symbols.; For X86, this symbol's identifier is ``__x86_return_thunk``.; ``""frame-pointer""``; This attribute tells the code generator whether the function; should keep the frame pointer. The code generator may emit the frame pointer; even if this attribute says the frame pointer can be eliminated.; The allowed string values are:. * ``""none""`` (default) - the frame pointer can be eliminated.; * ``""non-leaf""`` - the frame pointer should be kept if the function calls; other functions.; * ``""all""`` - the frame pointer should be kept.; ``hot``; This attribute indicates that this function is a hot spot of the program; execution. The function will be optimized more aggressively and will be; placed into special subsection of the text section to improving locality. When profile feedback is enabled, this attribute has the precedence over; the profile information. By marking a function ``hot``, users can work; around the cases where the training input does not have good coverage; on all the hot functions.; ``inlinehint``; This attribute indicates that the source code contained a hint that; inlining this function is desirable (such as the ""inline"" keyword in; C/C++). It is just a hint; it imposes no requirements on the; inliner.; ``jumptable``; This attribute indicates that the function should be added to a; jump-instruction table at code-generation time, and that all address-taken; references to this function should be replaced with a reference to the; appropriate jump-instruction-table function pointer. Note that this creates; a new pointer for the original function, which means that code that depends; on function-pointer identity can break. So, any function annotated with; ``jum",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:83580,optimiz,optimized,83580,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"ch instructions. This performs signed dot product; with two i32 operands (holding a vector of 8 4bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output.; When applicable (i.e. no clamping / operand modifiers), this is lowered; into v_dot8c_i32_i4 for targets which support it.; RDNA3 does not offer v_dot8_i32_i4, and rather offers; v_dot4_i32_iu4 which has operands to hold the signedness of the; vector operands. Thus, this intrinsic lowers to the signed version; of this instruction for gfx11 targets. llvm.amdgcn.sudot4 Provides direct access to v_dot4_i32_iu8 on gfx11 targets. This performs; dot product with two i32 operands (holding a vector of 4 8bit values), summed; with the fifth i32 operand. The i1 sixth operand is used to clamp; the output. The i1s preceding the vector operands decide the signedness. llvm.amdgcn.sudot8 Provides direct access to v_dot8_i32_iu4 on gfx11 targets. This performs; dot product with two i32 operands (holding a vector of 8 4bit values), summed; with the fifth i32 operand. The i1 sixth operand is used to clamp; the output. The i1s preceding the vector operands decide the signedness. llvm.amdgcn.sched_barrier Controls the types of instructions that may be allowed to cross the intrinsic; during instruction scheduling. The parameter is a mask for the instruction types; that can cross the intrinsic. - 0x0000: No instructions may be scheduled across sched_barrier.; - 0x0001: All, non-memory, non-side-effect producing instructions may be; scheduled across sched_barrier, *i.e.* allow ALU instructions to pass.; - 0x0002: VALU instructions may be scheduled across sched_barrier.; - 0x0004: SALU instructions may be scheduled across sched_barrier.; - 0x0008: MFMA/WMMA instructions may be scheduled across sched_barrier.; - 0x0010: All VMEM instructions may be scheduled across sched_barrier.; - 0x0020: VMEM read instructions may be scheduled across sched_barrier.; - 0x0040: VMEM write instructions may be sc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:42006,perform,performs,42006,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performs']
Performance,"ch is more appropriate in each case. **Using ``target=...``**. Checking the target triple can be tricky; it's easy to mis-specify. For; example, ``target=mips{{.*}}`` will match not only mips, but also mipsel,; mips64, and mips64el. ``target={{.*}}-linux-gnu`` will match; x86_64-unknown-linux-gnu, but not armv8l-unknown-linux-gnueabihf.; Prefer to use hyphens to delimit triple components (``target=mips-{{.*}}``); and it's generally a good idea to use a trailing wildcard to allow for; unexpected suffixes. Also, it's generally better to write regular expressions that use entire; triple components, than to do something clever to shorten them. For; example, to match both freebsd and netbsd in an expression, you could write; ``target={{.*(free|net)bsd.*}}`` and that would work. However, it would; prevent a ``grep freebsd`` from finding this test. Better to use:; ``target={{.+-freebsd.*}} || target={{.+-netbsd.*}}``. Substitutions; -------------. Besides replacing LLVM tool names the following substitutions are performed in; RUN lines:. ``%%``; Replaced by a single ``%``. This allows escaping other substitutions. ``%s``; File path to the test case's source. This is suitable for passing on the; command line as the input to an LLVM tool. Example: ``/home/user/llvm/test/MC/ELF/foo_test.s``. ``%S``; Directory path to the test case's source. Example: ``/home/user/llvm/test/MC/ELF``. ``%t``; File path to a temporary file name that could be used for this test case.; The file name won't conflict with other test cases. You can append to it; if you need multiple temporaries. This is useful as the destination of; some redirected output. Example: ``/home/user/llvm.build/test/MC/ELF/Output/foo_test.s.tmp``. ``%T``; Directory of ``%t``. Deprecated. Shouldn't be used, because it can be easily; misused and cause race conditions between tests. Use ``rm -rf %t && mkdir %t`` instead if a temporary directory is necessary. Example: ``/home/user/llvm.build/test/MC/ELF/Output``. ``%{pathsep}``.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:23788,perform,performed,23788,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,1,['perform'],['performed']
Performance,"ch wishes to relocate objects directly reachable from; running code, a higher standard is required. One additional challenge is that the compiler may compute intermediate; results (""derived pointers"") which point outside of the allocation or; even into the middle of another allocation. The eventual use of this; intermediate value must yield an address within the bounds of the; allocation, but such ""exterior derived pointers"" may be visible to the; collector. Given this, a garbage collector can not safely rely on the; runtime value of an address to indicate the object it is associated; with. If the garbage collector wishes to move any object, the; compiler must provide a mapping, for each pointer, to an indication of; its allocation. To simplify the interaction between a collector and the compiled code,; most garbage collectors are organized in terms of three abstractions:; load barriers, store barriers, and safepoints. #. A load barrier is a bit of code executed immediately after the; machine load instruction, but before any use of the value loaded.; Depending on the collector, such a barrier may be needed for all; loads, merely loads of a particular type (in the original source; language), or none at all. #. Analogously, a store barrier is a code fragment that runs; immediately before the machine store instruction, but after the; computation of the value stored. The most common use of a store; barrier is to update a 'card table' in a generational garbage; collector. #. A safepoint is a location at which pointers visible to the compiled; code (i.e. currently in registers or on the stack) are allowed to; change. After the safepoint completes, the actual pointer value; may differ, but the 'object' (as seen by the source language); pointed to will not. Note that the term 'safepoint' is somewhat overloaded. It refers to; both the location at which the machine state is parsable and the; coordination protocol involved in bring application threads to a; point at which the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:2579,load,load,2579,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,3,['load'],"['load', 'loaded']"
Performance,"ch2.patch""); #set(_clad_patch_command; # ${CMAKE_COMMAND} -E copy_directory; # ${CMAKE_SOURCE_DIR}/interpreter/cling/tools/plugins/clad/patches <SOURCE_DIR>; # && git checkout <SOURCE_DIR>; # && git apply --ignore-space-change --ignore-whitespace ${_clad_patches_list}; # ). ExternalProject_Add(; clad; GIT_REPOSITORY https://github.com/vgvassilev/clad.git; GIT_TAG v1.7; UPDATE_COMMAND """"; PATCH_COMMAND ${_clad_patch_command}; CMAKE_ARGS -G ${CMAKE_GENERATOR}; -DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}; -DCMAKE_C_COMPILER=${CMAKE_C_COMPILER}; -DCMAKE_C_FLAGS=${CMAKE_C_FLAGS}; -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}; -DCMAKE_CXX_FLAGS=${CLAD_CXX_FLAGS}; -DCMAKE_INSTALL_PREFIX=${clad_install_dir}/plugins; -DLLVM_DIR=${LLVM_BINARY_DIR}; -DCLANG_INCLUDE_DIRS=${CLANG_INCLUDE_DIRS}; ${_clad_extra_cmake_args}; # FIXME; # Building with 1 core is a temporary workaround for #16654 and has to be ; # there until the behaviour of the clad build on ubuntu 24.10 is understood.; # The performance penalty in the build is negligible.; BUILD_COMMAND ${CMAKE_COMMAND} --build . ${EXTRA_BUILD_ARGS} -j 1; INSTALL_COMMAND ${CMAKE_COMMAND} --build . ${EXTRA_BUILD_ARGS} -j 1 --target install; BUILD_BYPRODUCTS ${CLAD_BYPRODUCTS}; ${_clad_extra_settings}; # We need the target clangBasic to be built before building clad. However, we; # support building prebuilt clang and adding clangBasic breaks this case.; # Delegate the dependency resolution to the clingInterpreter target (which; # will always depend on clangBasic).; DEPENDS clingInterpreter; ). # Register cladPlugin, cladDifferentiator; foreach (lib cladPlugin cladDifferentiator); add_library(${lib} IMPORTED STATIC GLOBAL); add_dependencies(${lib} clad); endforeach(). set_property(TARGET cladPlugin PROPERTY IMPORTED_LOCATION ${_CLAD_LIBRARY_PATH}/${CMAKE_STATIC_LIBRARY_PREFIX}cladPlugin${CMAKE_STATIC_LIBRARY_SUFFIX}); set_property(TARGET cladDifferentiator PROPERTY IMPORTED_LOCATION ${_CLAD_LIBRARY_PATH}/${CMAKE_STATIC_LIBRARY_PREFIX}cladDiffere",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/plugins/clad/CMakeLists.txt:3668,perform,performance,3668,interpreter/cling/tools/plugins/clad/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/plugins/clad/CMakeLists.txt,1,['perform'],['performance']
Performance,"change the Loop method. ``` {.cpp}; ...; if (fChain == 0) return;; Int_t nentries = Int_t(fChain->GetEntries());; TH1F *myHisto = new TH1F(""myHisto"",""fPx"", 100, -5,5);; TH1F *smallHisto = new TH1F(""small"",""fPx"", 100, -5,5);; ...; ```. In the for-loop, we need to add another for-loop to go over all the; tracks. In the outer for-loop, we get the entry and the number of; tracks. In the inner for-loop, we fill the large histogram (`myHisto`); with all tracks and the small histogram (`smallHisto`) with the track if; it is in the first 100. ``` {.cpp}; ...; for (Int_t jentry=0; jentry<nentries;jentry++) {; GetEntry(jentry);; for (Int_t j = 0; j < 100; j++) {; myHisto->Fill(fTracks_fPx[j]);; if (j < 100) {; smallHisto->Fill(fTracks_fPx[j]);; }; }; }; ...; ```. Outside of the for-loop, we draw both histograms on the same canvas. ``` {.cpp}; ...; myHisto->Draw();; smallHisto->Draw(""Same"");; ...; ```. Save these changes to `MyClass.C` and start a fresh root session. We; will now load `MyClass` and experiment with its methods. ### Loading MyClass. The first step is to load the library and the class file. Then we can; instantiate a `MyClass` object. ``` {.cpp}; root[] .L libEvent.so; root[] .L MyClass.C; root[] MyClass m; ```. Now we can get a specific entry and populate the event leaf. In the code; snipped below, we get entry 0, and print the number of tracks (594).; Then we get entry 1 and print the number of tracks (597). ``` {.cpp}; root[] m.GetEntry(0); (int)57503; root[] m.fNtrack(); (Int_t)594; root[] m.GetEntry(1); (int)48045; root[] m.fNtrack(); (Int_t)597; ```. Now we can call the `Loop` method, which will build and display the two; histograms. ``` {.cpp}; root[] m.Loop(); ```. You should now see a canvas that looks like this. ![](pictures/03000106.png). To conclude the discussion on `MakeClass` let us lists the steps that; got us here. - Call `TTree::MakeClass`, which automatically creates a class to loop; over the tree. - Modify the `MyClass::Loop()` method in `MyCla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:129918,load,load,129918,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"char *name, Double_t *point[3],; Double_t *norm[3]);; ```. ### Composite Shapes. Composite shapes are Boolean combinations of two or more shape; components. The supported Boolean operations are union (+), intersection; (\*) and subtraction(-). Composite shapes derive from the base; **`TGeoShape`** class, therefore providing all shape features:; computation of bounding box, finding if a given point is inside or; outside the combination, as well as computing the distance to; entering/exiting. They can be directly used for creating volumes or used; in the definition of other composite shapes. Composite shapes are provided in order to complement and extend the set; of basic shape primitives. They have a binary tree internal structure,; therefore all shape-related geometry queries are signals propagated from; top level down to the final leaves, while the provided answers are; assembled and interpreted back at top. This `CSG`; `(composite solid geometry)` hierarchy is effective for small number of; components, while performance drops dramatically for large structures.; Building a complete geometry in this style is virtually possible but; highly not recommended. #### The Structure of Composite Shapes. A composite shape can always be looked as the result of a Boolean; operation between only two shape components. All information identifying; these two components as well as their positions with respect to the; frame of the composite is represented by an object called Boolean node.; A composite shape has a pointer to such a Boolean node. Since the shape; components may also be composites, they will also contain binary Boolean; nodes branching out other two shapes in the hierarchy. Any such branch; ends-up when the final leaves are no longer composite shapes, but basic; primitives. The figure shows the composite shapes structure. ![The composite shapes structure](pictures/080001CA.png). Suppose that A, B, C and D represent basic shapes, we will illustrate; how the internal repr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:43255,perform,performance,43255,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performance']
Performance,"che lines.; * To ensure coherence of local and remote memory reads of work-groups in; different agents a ``buffer_invl2`` is required. It will invalidate L2; cache lines with MTYPE NC (used for remote coarse grain memory). Note that; MTYPE CC (used for local fine grain memory) and MTYPE RW (used for local; coarse memory) cause local reads to be invalidated by remote writes with; with the PTE C-bit so these cache lines are not invalidated. Note that; MTYPE UC (used for remote fine grain memory) bypasses the L2, so will; never result in L2 cache lines that need to be invalidated. * PCIe access from the GPU to the CPU memory is kept coherent by using the; MTYPE UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:240080,cache,cache,240080,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"che. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:298219,load,load,298219,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"che; this plot; is present only if some I/O is done, i.e. not for pure CPU tasks.; The number of active workers; The number of total and effecive sessions running; concurrently on the cluster (started by the same daemon); this plot is; present only is the number is at least onec different from 1. If enabled, send monitoring information from the master; at each GetNextPacket (at each call of TPerfStat::PacketEvent) to allow; extrnal real-time progress monitoring.; Save the status of a 'proofserv' session into a new file; in the 'activesessions' area. The full path of the new file is; ; <admin_path>/.xproofd.<port>/activesessions/<user>.<group>.<pid>.status. The status indicates whether the session is idle, running or queued.; The status is updated every 'checkfq' secs (see xpd.proofservmgr;; default 30 s). The status is dumped by the reader thread of TXProofServ; and therefore its r/w access is protected. Enable the use of the tree cache also for local files,; adapting the default settings for the cache to the recent changes; In the XrdProofd plug-in. Improve synchronization between parent and child during; fork; Optimize loops over directory entries; Improve error and notification messages. Improved handling of Ctrl-C; this follows from a fix in; TMonitor and an improved handling of non-finished query state in the; workers (results are not send to master if the query was aborted) . Fixes. TFileMerger. Fix a problem preventing correct transmission of all; non-mergeable objects (fixes bug #52886); Remove the argument isdir from the function; MergeRecursive; Do not remove the first file in the list when returning; from MergeRecursive (fixes bug #54591); Fix a major leak when merging files with collections; written using kSingleKey option. The merger was reading each; key in memory and deleted the object at the end, but the container is; not owner by default, so all objects inside leaked. PROOF-Lite. Fix a couple of memory leaks showing up when running; repeate",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:6014,cache,cache,6014,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,4,['cache'],['cache']
Performance,"checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't pan out due to failure to; uphold the security model. One in particular is worth discussing as many others; will reduce to it. We wondered whether only the *first* load in a basic block could be checked. If; the check works as intended, it forms an invalid pointer that doesn't even; virtual-address translate in the hardware. It should fault very early on in its; processing. Maybe that would stop things in time for the misspeculated path to; fail to leak any secrets. This doesn't end up working because the processor is; fundamentally out-of-order, even in its speculative domain. As a consequence,; the attacker could cause the initial address computation itself to stall and; allow an arbitrary number of unrelated loads (including attacked loads of; secret data) to pass through. #### Interprocedural Checking. Modern x86 processors may speculate into called functions and out of functions; to their return address. As a consequence, we need a way to check loads that; occur after a misspeculated predicate but where the load and the misspeculated; predicate are in different functions. In essence, we need some interprocedural; generalization of the predica",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:37149,load,load,37149,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANAL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34574,load,loaded,34574,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['loaded']
Performance,"ched the end; of a serialized expression or statement; other expression or statement records; may follow, but they are part of a different expression. .. _pchinternals-ident-table:. Identifier Table Block; ^^^^^^^^^^^^^^^^^^^^^^. The identifier table block contains an on-disk hash table that maps each; identifier mentioned within the AST file to the serialized representation of; the identifier's information (e.g, the ``IdentifierInfo`` structure). The; serialized representation contains:. * The actual identifier string.; * Flags that describe whether this identifier is the name of a built-in, a; poisoned identifier, an extension token, or a macro.; * If the identifier names a macro, the offset of the macro definition within; the :ref:`pchinternals-preprocessor`.; * If the identifier names one or more declarations visible from translation; unit scope, the :ref:`declaration IDs <pchinternals-decls>` of these; declarations. When an AST file is loaded, the AST file reader mechanism introduces itself; into the identifier table as an external lookup source. Thus, when the user; program refers to an identifier that has not yet been seen, Clang will perform; a lookup into the identifier table. If an identifier is found, its contents; (macro definitions, flags, top-level declarations, etc.) will be deserialized,; at which point the corresponding ``IdentifierInfo`` structure will have the; same contents it would have after parsing the headers in the AST file. Within the AST file, the identifiers used to name declarations are represented; with an integral value. A separate table provides a mapping from this integral; value (the identifier ID) to the location within the on-disk hash table where; that identifier is stored. This mapping is used when deserializing the name of; a declaration, the identifier of a token, or any other construct in the AST; file that refers to a name. .. _pchinternals-method-pool:. Method Pool Block; ^^^^^^^^^^^^^^^^^. The method pool block is represent",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:19224,load,loaded,19224,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance,"chedResult()``, but getting direct access to an outer level IR analysis; manager to compute an outer level IR analysis is not allowed. This is for a; couple reasons. The first reason is that running analyses across outer level IR in inner level; IR passes can result in quadratic compile time behavior. For example, a module; analysis often scans every function and allowing function passes to run a module; analysis may cause us to scan functions a quadratic number of times. If passes; could keep outer level analyses up to date rather than computing them on demand; this wouldn't be an issue, but that would be a lot of work to ensure every pass; updates all outer level analyses, and so far this hasn't been necessary and; there isn't infrastructure for this (aside from function analyses in loop passes; as described below). Self-updating analyses that gracefully degrade also handle; this problem (e.g. GlobalsAA), but they run into the issue of having to be; manually recomputed somewhere in the optimization pipeline if we want precision,; and they block potential future concurrency. The second reason is to keep in mind potential future pass concurrency, for; example parallelizing function passes over different functions in a CGSCC or; module. Since passes can ask for a cached analysis result, allowing passes to; trigger outer level analysis computation could result in non-determinism if; concurrency was supported. A related limitation is that outer level IR analyses; that are used must be immutable, or else they could be invalidated by changes to; inner level IR. Outer analyses unused by inner passes can and often will be; invalidated by changes to inner level IR. These invalidations happen after the; inner pass manager finishes, so accessing mutable analyses would give invalid; results. The exception to not being able to access outer level analyses is accessing; function analyses in loop passes. Loop passes often use function analyses such; as the dominator tree. Loop pas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:9722,optimiz,optimization,9722,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,2,"['concurren', 'optimiz']","['concurrency', 'optimization']"
Performance,"cheme:. ``<base>-<arch>.opt.<format>``. Note that this is incompatible with passing the; :option:`-foptimization-record-file` option. .. option:: -foptimization-record-file. Control the file to which optimization reports are written. This implies; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`. On Darwin platforms, this is incompatible with passing multiple; ``-arch <arch>`` options. .. option:: -foptimization-record-passes. Only include passes which match a specified regular expression. When optimization reports are being output (see; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`), this; option controls the passes that will be included in the final report. If this option is not used, all the passes are included in the optimization; record. .. _opt_fdiagnostics-show-hotness:. .. option:: -f[no-]diagnostics-show-hotness. Enable profile hotness information in diagnostic line. This option controls whether Clang prints the profile hotness associated; with diagnostics in the presence of profile-guided optimization information.; This is currently supported with optimization remarks (see; :ref:`Options to Emit Optimization Reports <rpass>`). The hotness information; allows users to focus on the hot optimization remarks that are likely to be; more relevant for run-time performance. For example, in this output, the block containing the callsite of `foo` was; executed 3000 times according to the profile data:. ::. s.c:7:10: remark: foo inlined into bar (hotness: 3000) [-Rpass-analysis=inline]; sum += foo(x, x - 2);; ^. This option is implied when; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>` is used.; Otherwise, it defaults to off. .. option:: -fdiagnostics-hotness-threshold. Prevent optimization remarks from being output if they do not have at least; this hotness value. This option, which defaults to zero, controls the minimum hotness an; optimization remark would need in order to be output by Clang. This is; current",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:13992,optimiz,optimization,13992,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"chronous exceptions. Exception handling schemes; that are recognized by LLVM to handle asynchronous exceptions, such; as SEH, will still provide their implementation defined semantics.; ``nosanitize_bounds``; This attribute indicates that bounds checking sanitizer instrumentation; is disabled for this function.; ``nosanitize_coverage``; This attribute indicates that SanitizerCoverage instrumentation is disabled; for this function.; ``null_pointer_is_valid``; If ``null_pointer_is_valid`` is set, then the ``null`` address; in address-space 0 is considered to be a valid address for memory loads and; stores. Any analysis or optimization should not treat dereferencing a; pointer to ``null`` as undefined behavior in this function.; Note: Comparing address of a global variable to ``null`` may still; evaluate to false because of a limitation in querying this attribute inside; constant expressions.; ``optdebug``; This attribute suggests that optimization passes and code generator passes; should make choices that try to preserve debug info without significantly; degrading runtime performance.; This attribute is incompatible with the ``minsize``, ``optsize``, and; ``optnone`` attributes.; ``optforfuzzing``; This attribute indicates that this function should be optimized; for maximum fuzzing signal.; ``optnone``; This function attribute indicates that most optimization passes will skip; this function, with the exception of interprocedural optimization passes.; Code generation defaults to the ""fast"" instruction selector.; This attribute cannot be used together with the ``alwaysinline``; attribute; this attribute is also incompatible; with the ``minsize``, ``optsize``, and ``optdebug`` attributes. This attribute requires the ``noinline`` attribute to be specified on; the function as well, so the function is never inlined into any caller.; Only functions with the ``alwaysinline`` attribute are valid; candidates for inlining into the body of this function.; ``optsize``; This attribu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:95375,optimiz,optimization,95375,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"ciated with it. The first; defined global in a translation unit has a pseudorandom tag associated; with it, based on the hash of the file path. Subsequent global tags are; incremental from the previously-assigned tag. * The global's tag is added to its symbol address in the object file's symbol; table. This causes the global's address to be tagged when its address is; taken. * When the address of a global is taken directly (i.e. not via the GOT), a special; instruction sequence needs to be used to add the tag to the address,; because the tag would otherwise take the address outside of the small code; model (4GB on AArch64). No changes are required when the address is taken; via the GOT because the address stored in the GOT will contain the tag. * An associated ``hwasan_globals`` section is emitted for each tagged global,; which indicates the address of the global, its size and its tag. These; sections are concatenated by the linker into a single ``hwasan_globals``; section that is enumerated by the runtime (via an ELF note) when a binary; is loaded and the memory is tagged accordingly. A complete example is given below:. .. code-block:: none. // int x = 1; int *f() { return &x; }; // clang -O2 --target=aarch64-linux-android30 -fsanitize=hwaddress -S -o - global.c. [...]; f:; adrp x0, :pg_hi21_nc:x // set bits 12-63 to upper bits of untagged address; movk x0, #:prel_g3:x+0x100000000 // set bits 48-63 to tag; add x0, x0, :lo12:x // set bits 0-11 to lower bits of address; ret. [...]; .data; .Lx.hwasan:; .word 1. .globl x; .set x, .Lx.hwasan+0x2d00000000000000. [...]; .section .note.hwasan.globals,""aG"",@note,hwasan.module_ctor,comdat; .Lhwasan.note:; .word 8 // namesz; .word 8 // descsz; .word 3 // NT_LLVM_HWASAN_GLOBALS; .asciz ""LLVM\000\000\000""; .word __start_hwasan_globals-.Lhwasan.note; .word __stop_hwasan_globals-.Lhwasan.note. [...]; .section hwasan_globals,""ao"",@progbits,.Lx.hwasan,unique,2; .Lx.hwasan.descriptor:; .word .Lx.hwasan-.Lx.hwasan.descriptor; .word 0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:7428,load,loaded,7428,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,1,['load'],['loaded']
Performance,"ciates commutative expressions in an order that is designed to; promote better constant propagation, GCSE, :ref:`LICM <passes-licm>`, PRE, etc. For example: 4 + (x + 5)  x + (4 + 5). In the implementation of this algorithm, constants are assigned rank = 0,; function arguments are rank = 1, and other values are assigned ranks; corresponding to the reverse post order traversal of current function (starting; at 2), which effectively gives values in deep loops higher rank than values not; in loops. ``rel-lookup-table-converter``: Relative lookup table converter; ---------------------------------------------------------------. This pass converts lookup tables to PIC-friendly relative lookup tables. ``reg2mem``: Demote all values to stack slots; ---------------------------------------------. This file demotes all registers to memory references. It is intended to be the; inverse of :ref:`mem2reg <passes-mem2reg>`. By converting to ``load``; instructions, the only values live across basic blocks are ``alloca``; instructions and ``load`` instructions before ``phi`` nodes. It is intended; that this should make CFG hacking much easier. To make later hacking easier,; the entry block is split into two, such that all introduced ``alloca``; instructions (and nothing else) are in the entry block. ``sroa``: Scalar Replacement of Aggregates; ------------------------------------------. The well-known scalar replacement of aggregates transformation. This transform; breaks up ``alloca`` instructions of aggregate type (structure or array) into; individual ``alloca`` instructions for each member if possible. Then, if; possible, it transforms the individual ``alloca`` instructions into nice clean; scalar SSA form. .. _passes-sccp:. ``sccp``: Sparse Conditional Constant Propagation; -------------------------------------------------. Sparse conditional constant propagation and merging, which can be summarized; as:. * Assumes values are constant unless proven otherwise; * Assumes BasicBlocks ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:33341,load,load,33341,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,2,['load'],['load']
Performance,"cific technique we propose is to cause loads to be checked using; branchless code to ensure that they are executing along a valid control flow; path. Consider the following C-pseudo-code representing the core idea of a; predicate guarding potentially invalid loads:; ```; void leak(int data);; void example(int* pointer1, int* pointer2) {; if (condition) {; // ... lots of code ...; leak(*pointer1);; } else {; // ... more code ...; leak(*pointer2);; }; }; ```. This would get transformed into something resembling the following:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; if (condition) {; // Assuming ?: is implemented using branchless logic...; predicate_state = !condition ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; } else {; predicate_state = condition ? all_zeros_mask : predicate_state;; // ... more code ...; //; // Alternative: Harden the loaded value; int value2 = *pointer2 & predicate_state;; leak(value2);; }; }; ```. The result should be that if the `if (condition) {` branch is mis-predicted,; there is a *data* dependency on the condition used to zero out any pointers; prior to loading through them or to zero out all of the loaded bits. Even; though this code pattern may still execute speculatively, *invalid* speculative; executions are prevented from leaking secret data from memory (but note that; this data might still be loaded in safe ways, and some regions of memory are; required to not hold secrets, see below for detailed limitations). This; approach only requires the underlying hardware have a way to implement a; branchless and unpredicted conditional update of a register's value. All modern; architectures have support for this, and in fact such support is necessary to; corre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:3979,load,loaded,3979,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"cified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is sign-extending, as with G_SEXTLOAD. G_INDEXED_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is zero-extending, as with G_ZEXTLOAD. G_STORE; ^^^^^^^. Generic store. Expects a MachineMemOperand in addition to explicit; operands. If the stored value size is greater than the memory size,; the high bits are implicitly truncated. If this is a vector store, the; high elements are discarded (i.e. this does not function as a per-lane; vector, truncating store). G_INDEXED_STORE; ^^^^^^^^^^^^^^^. Combines a store with a GEP. See description of G_INDEXED_LOAD for indexing behaviour. G_ATOMIC_CMPXCHG_WITH_SUCCESS; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic atomic cmpxchg with internal success check. Expects a; MachineMemOperand in addition to explicit operands. G_ATOMIC_CMPXCHG; ^^^^^^^^^^^^^^^^. Generic atomic cmpxchg. Expects a MachineMemOperand in addition to explicit; operands. G_ATOMICRMW_XCHG, G_ATOMICRMW_ADD, G_ATOMICRMW_SUB, G_ATOMICRMW_AND,; G_A",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:16384,load,load,16384,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,2,"['load', 'perform']","['load', 'performed']"
Performance,"cifies the amount of feedback printed on the; root command line after performed fits. *Verbose'* - prints fit results after each iteration. *Quiet'* - no fit information is printed. *Default'* - between Verbose and Quiet. ### Command Buttons. *Fit button* - performs a fit taking different option settings via the; Fit Panel interface. *Reset* - sets the GUI elements and related fit settings to the; default ones. *Close* - closes the Fit panel window. ### Minimization Options. With this tab one can select specific options for minimization. These include. * The minimizer library ( *Minuit*, *Minuit2*, *Fumili*, *GSL*, *Genetics* ); * The method (algorithm) for minimization. For example for Minuit one can choose between (*Migrad*, *Simplex* or *Scan*); * Error definition; * Minimization tolerance; * Number of iterations/function calls; * Print Level: (*Default*, *Verbose* or *Quiet*). ## New ROOT::Fit classes. The fitting of the data objects in ROOT, histograms, graphs and tree is performed via some common classes,; which are defined in the `ROOT::Fit` namespace.; These classes can be classified in the following groups:. * User classes driving the fit: `ROOT::Fit::Fitter` for executing the fit, `ROOT::Fit::FitConfig` for configuring the fit,; 	`ROOT::Fit::ParameterSettings` to define the properties of the fit parameters (initial; 	values, bounds, etc..), `ROOT::Fit::FitResult` for storing the result of the fit.; * Data classes containing the data sets used in the fitting. These classes are the`ROOT::Fit::BinData`for describing bin data sets,; 	 thus data points containing both coordinates and a corresponding value/weight; 	 with optionally an error on the value or the coordinate and the `ROOT::Fit::UnBinData` for un-binned data sets,; 	 which consists only of a vector of coordinate values. The coordinate values can be; 	 one-dimensional (i.e. one entry per event) or multi-dimensional (N entries per event).; * Function classes defining the type of fit (the objective f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:26401,perform,performed,26401,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['performed']
Performance,"cify a non-power-of-two alignment; (including a zero alignment). If this is the case, then the pointer value; must be a null pointer, otherwise the behavior is undefined. In addition to allowing operand bundles encoding function and parameter; attributes, an assume operand bundle my also encode a ``separate_storage``; operand bundle. This has the form:. .. code-block:: llvm. separate_storage(<val1>, <val2>)``. This indicates that no pointer :ref:`based <pointeraliasing>` on one of its; arguments can alias any pointer based on the other. Even if the assumed property can be encoded as a boolean value, like; ``nonnull``, using operand bundles to express the property can still have; benefits:. * Attributes that can be expressed via operand bundles are directly the; property that the optimizer uses and cares about. Encoding attributes as; operand bundles removes the need for an instruction sequence that represents; the property (e.g., `icmp ne ptr %p, null` for `nonnull`) and for the; optimizer to deduce the property from that instruction sequence.; * Expressing the property using operand bundles makes it easy to identify the; use of the value as a use in an :ref:`llvm.assume <int_assume>`. This then; simplifies and improves heuristics, e.g., for use ""use-sensitive""; optimizations. .. _ob_preallocated:. Preallocated Operand Bundles; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Preallocated operand bundles are characterized by the ``""preallocated""``; operand bundle tag. These operand bundles allow separation of the allocation; of the call argument memory from the call site. This is necessary to pass; non-trivially copyable objects by value in a way that is compatible with MSVC; on some targets. There can be at most one ``""preallocated""`` operand bundle; attached to a call site and it must have exactly one bundle operand, which is; a token generated by ``@llvm.call.preallocated.setup``. A call with this; operand bundle should not adjust the stack before entering the function, as; that wil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:126134,optimiz,optimizer,126134,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"cision to unroll the loop depends on the register pressure and the generated code size. Epilogue Vectorization; ^^^^^^^^^^^^^^^^^^^^^^. When vectorizing a loop, often a scalar remainder (epilogue) loop is necessary; to execute tail iterations of the loop if the loop trip count is unknown or it; does not evenly divide the vectorization and unroll factors. When the; vectorization and unroll factors are large, it's possible for loops with smaller; trip counts to end up spending most of their time in the scalar (rather than; the vector) code. In order to address this issue, the inner loop vectorizer is; enhanced with a feature that allows it to vectorize epilogue loops with a; vectorization and unroll factor combination that makes it more likely for small; trip count loops to still execute in vectorized code. The diagram below shows; the CFG for a typical epilogue vectorized loop with runtime checks. As; illustrated the control flow is structured in a way that avoids duplicating the; runtime pointer checks and optimizes the path length for loops that have very; small trip counts. .. image:: epilogue-vectorization-cfg.png. Performance; -----------. This section shows the execution time of Clang on a simple benchmark:; `gcc-loops <https://github.com/llvm/llvm-test-suite/tree/main/SingleSource/UnitTests/Vectorizer>`_.; This benchmarks is a collection of loops from the GCC autovectorization; `page <http://gcc.gnu.org/projects/tree-ssa/vectorization.html>`_ by Dorit Nuzman. The chart below compares GCC-4.7, ICC-13, and Clang-SVN with and without loop vectorization at -O3, tuned for ""corei7-avx"", running on a Sandybridge iMac.; The Y-axis shows the time in msec. Lower is better. The last column shows the geomean of all the kernels. .. image:: gcc-loops.png. And Linpack-pc with the same configuration. Result is Mflops, higher is better. .. image:: linpack-pc.png. Ongoing Development Directions; ------------------------------. .. toctree::; :hidden:. VectorizationPlan. :doc:`Vec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:12144,optimiz,optimizes,12144,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimizes']
Performance,"citly specify the generator with the command line option ``-G ""Name of the; generator""``. To see a list of the available generators on your system, execute. .. code-block:: console. $ cmake --help. This will list the generator names at the end of the help text. Generators' names are case-sensitive, and may contain spaces. For this reason,; you should enter them exactly as they are listed in the ``cmake --help``; output, in quotes. For example, to generate project files specifically for; Visual Studio 12, you can execute:. .. code-block:: console. $ cmake -G ""Visual Studio 12"" path/to/llvm/source/root. For a given development platform there can be more than one adequate; generator. If you use Visual Studio, ""NMake Makefiles"" is a generator you can use; for building with NMake. By default, CMake chooses the most specific generator; supported by your development environment. If you want an alternative generator,; you must tell this to CMake with the ``-G`` option. .. todo::. Explain variables and cache. Move explanation here from #options section. .. _Options and variables:. Options and variables; =====================. Variables customize how the build will be generated. Options are boolean; variables, with possible values ON/OFF. Options and variables are defined on the; CMake command line like this:. .. code-block:: console. $ cmake -DVARIABLE=value path/to/llvm/source. You can set a variable after the initial CMake invocation to change its; value. You can also undefine a variable:. .. code-block:: console. $ cmake -UVARIABLE path/to/llvm/source. Variables are stored in the CMake cache. This is a file named ``CMakeCache.txt``; stored at the root of your build directory that is generated by ``cmake``.; Editing it yourself is not recommended. Variables are listed in the CMake cache and later in this document with; the variable name and type separated by a colon. You can also specify the; variable and type on the CMake command line:. .. code-block:: console. $ cmake -DV",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:5495,cache,cache,5495,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,"ck map location that lies between; the current call site and the load (typically, the runtime would; simply patch all stack map locations to invalidate the function). If; the compiler had hoisted the load above the stack map, then the; program could crash before the runtime could take back control. To enforce these semantics, stackmap and patchpoint intrinsics are; considered to potentially read and write all memory. This may limit; optimization more than some clients desire. This limitation may be; avoided by marking the call site as ""readonly"". In the future we may; also allow meta-data to be added to the intrinsic call to express; aliasing, thereby allowing optimizations to hoist certain loads above; stack maps. Direct Stack Map Entries; ^^^^^^^^^^^^^^^^^^^^^^^^. As shown in :ref:`stackmap-section`, a Direct stack map location; records the address of frame index. This address is itself the value; that the runtime requested. This differs from Indirect locations,; which refer to a stack locations from which the requested values must; be loaded. Direct locations can communicate the address if an alloca,; while Indirect locations handle register spills. For example:. .. code-block:: none. entry:; %a = alloca i64...; llvm.experimental.stackmap(i64 <ID>, i32 <shadowBytes>, ptr %a). The runtime can determine this alloca's relative location on the; stack immediately after compilation, or at any time thereafter. This; differs from Register and Indirect locations, because the runtime can; only read the values in those locations when execution reaches the; instruction address of the stack map. This functionality requires LLVM to treat entry-block allocas; specially when they are directly consumed by an intrinsics. (This is; the same requirement imposed by the llvm.gcroot intrinsic.) LLVM; transformations must not substitute the alloca with any intervening; value. This can be verified by the runtime simply by checking that the; stack map's location is a Direct location type. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:19504,load,loaded,19504,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['load'],['loaded']
Performance,ck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerCont,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66021,perform,performance,66021,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"ck:: c. #if defined(__has_feature); # if __has_feature(address_sanitizer); // code that builds only under AddressSanitizer; # endif; #endif. Disabling Instrumentation with ``__attribute__((no_sanitize(""address"")))``; --------------------------------------------------------------------------. Some code should not be instrumented by AddressSanitizer. One may use; the attribute ``__attribute__((no_sanitize(""address"")))`` (which has; deprecated synonyms `no_sanitize_address` and; `no_address_safety_analysis`) to disable instrumentation of a; particular function. This attribute may not be supported by other; compilers, so we suggest to use it together with; ``__has_feature(address_sanitizer)``. The same attribute used on a global variable prevents AddressSanitizer; from adding redzones around it and detecting out of bounds accesses. AddressSanitizer also supports; ``__attribute__((disable_sanitizer_instrumentation))``. This attribute; works similar to ``__attribute__((no_sanitize(""address"")))``, but it also; prevents instrumentation performed by other sanitizers. Suppressing Errors in Recompiled Code (Ignorelist); --------------------------------------------------. AddressSanitizer supports ``src`` and ``fun`` entity types in; :doc:`SanitizerSpecialCaseList`, that can be used to suppress error reports; in the specified source files or functions. Additionally, AddressSanitizer; introduces ``global`` and ``type`` entity types that can be used to; suppress error reports for out-of-bound access to globals with certain; names and types (you may only specify class or struct types). You may use an ``init`` category to suppress reports about initialization-order; problems happening in certain source files or with certain global variables. .. code-block:: bash. # Suppress error reports for code in a file or in a function:; src:bad_file.cpp; # Ignore all functions with names containing MyFooBar:; fun:*MyFooBar*; # Disable out-of-bound checks for global:; global:bad_array; # Disable",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst:9815,perform,performed,9815,interpreter/llvm-project/clang/docs/AddressSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst,1,['perform'],['performed']
Performance,"ck:: text. DBG_VALUE_LIST !123, !DIExpression(DW_OP_LLVM_arg, 0, DW_OP_LLVM_arg, 1, DW_OP_plus), %1, %2. And has the following operands:; * The first operand is the Variable field of the original debug intrinsic.; * The second operand is the Expression field of the original debug intrinsic.; * Any number of operands, from the 3rd onwards, record a sequence of variable; location operands, which may take any of the same values as the first; operand of the ``DBG_VALUE`` instruction above. These variable location; operands are inserted into the final DWARF Expression in positions indicated; by the DW_OP_LLVM_arg operator in the `DIExpression; <LangRef.html#diexpression>`_. The position at which the DBG_VALUEs are inserted should correspond to the; positions of their matching ``llvm.dbg.value`` intrinsics in the IR block. As; with optimization, LLVM aims to preserve the order in which variable; assignments occurred in the source program. However SelectionDAG performs some; instruction scheduling, which can reorder assignments (discussed below).; Function parameter locations are moved to the beginning of the function if; they're not already, to ensure they're immediately available on function entry. To demonstrate variable locations during instruction selection, consider; the following example:. .. code-block:: llvm. define i32 @foo(i32* %addr) {; entry:; call void @llvm.dbg.value(metadata i32 0, metadata !3, metadata !DIExpression()), !dbg !5; br label %bb1, !dbg !5. bb1: ; preds = %bb1, %entry; %bar.0 = phi i32 [ 0, %entry ], [ %add, %bb1 ]; call void @llvm.dbg.value(metadata i32 %bar.0, metadata !3, metadata !DIExpression()), !dbg !5; %addr1 = getelementptr i32, i32 *%addr, i32 1, !dbg !5; call void @llvm.dbg.value(metadata i32 *%addr1, metadata !3, metadata !DIExpression()), !dbg !5; %loaded1 = load i32, i32* %addr1, !dbg !5; %addr2 = getelementptr i32, i32 *%addr, i32 %bar.0, !dbg !5; call void @llvm.dbg.value(metadata i32 *%addr2, metadata !3, metadata !DIExpression(",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:29034,perform,performs,29034,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['perform'],['performs']
Performance,"clang - the Clang C, C++, and Objective-C compiler; ==================================================. SYNOPSIS; --------. :program:`clang` [*options*] *filename ...*. DESCRIPTION; -----------. :program:`clang` is a C, C++, and Objective-C compiler which encompasses; preprocessing, parsing, optimization, code generation, assembly, and linking.; Depending on which high-level mode setting is passed, Clang will stop before; doing a full link. While Clang is highly integrated, it is important to; understand the stages of compilation, to understand how to invoke it. These; stages are:. Driver; The clang executable is actually a small driver which controls the overall; execution of other tools such as the compiler, assembler and linker.; Typically you do not need to interact with the driver, but you; transparently use it to run the other tools. Preprocessing; This stage handles tokenization of the input source file, macro expansion,; #include expansion and handling of other preprocessor directives. The; output of this stage is typically called a "".i"" (for C), "".ii"" (for C++),; "".mi"" (for Objective-C), or "".mii"" (for Objective-C++) file. Parsing and Semantic Analysis; This stage parses the input file, translating preprocessor tokens into a; parse tree. Once in the form of a parse tree, it applies semantic; analysis to compute types for expressions as well and determine whether; the code is well formed. This stage is responsible for generating most of; the compiler warnings as well as parse errors. The output of this stage is; an ""Abstract Syntax Tree"" (AST). Code Generation and Optimization; This stage translates an AST into low-level intermediate code (known as; ""LLVM IR"") and ultimately to machine code. This phase is responsible for; optimizing the generated code and handling target-specific code generation.; The output of this stage is typically called a "".s"" file or ""assembly"" file. Clang also supports the use of an integrated assembler, in which the code; generator pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:293,optimiz,optimization,293,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimization']
Performance,"clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24248,optimiz,optimized,24248,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cp,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65614,perform,performance,65614,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"class can scan the CLs+b values (default) or alternatively CLs (if the; method HypoTestInverter::UseCLs has been called).; The estimated error due to the MC toys statistics from the HybridCalculator is propagated into the limits obtained from the HypoTestResult; A new tutorial rs801_HypoTestInverter.C has been added in the tutorials/roostats directory to show the usage of this class. New class BayesianCalculator. New class for calculating Bayesian interval using numerical integration. It implements the IntervalCalculator interface and returns as result a SimpleInterval. . The BayesianCalculator::GetInterval() method returns a SimpleInterval which contains the lower and upper value of the bayesian interval obtained from the posterior probability for the given confidence level.; The class return also the posterior pdf (BayesianCalculator::GetPosteriorPdf()) obtained from integrating (marginalizing) on the nuisance parameters.; It works currently only for one-dimensional problems by relying on RooFit for performing analytical or numerical integration.; A plot of the posterior and the desired interval can be obtained using BayesianCalculator::GetPosteriorPlot().; A new tutorial rs701_BayesianCalculator.C has been added in the tutorials/roostats directory to show the usage of this class. MCMCCalculator. Add possibility to specify the prior function in the constructor of the class to have a signature similar to the BayesianCalculator class. When no prior is specified it is assumed is part of the global model (pdf) passed to the class. Improvements and Bug fixes. Various improvements and fixes have been applied also to all the calculator classes. Internally now the RooArgSet objects are used by value instead of a pointer.; All the calculator have a consistent way for being constructed, either by passing pdf pointers and the set defining the parameters or by passing a reference to a ModelConfig class.; The result classes are now more consistent and have similar constructors.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:16549,perform,performing,16549,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,2,['perform'],['performing']
Performance,"cling-config: for compile time flags; * rootcling and genreflex: for dictionary generation; * cppyy-generator: part of the :doc:`CMake interface <cmake_interface>`. Compiler/linker flags; ---------------------. ``cling-config`` is a small utility to provide access to the as-installed; configuration, such as compiler/linker flags and installation directories, of; other components.; Usage examples::. $ cling-config --help; Usage: cling-config [--cflags] [--cppflags] [--cmake]; $ cling-config --cmake; /usr/local/lib/python2.7/dist-packages/cppyy_backend/cmake. .. _dictionaries:. Dictionaries; ------------. Loading header files or code directly into ``cling`` is fine for interactive; work and smaller packages, but large scale applications benefit from; pre-compiling code, using the automatic class loader, and packaging; dependencies in so-called ""dictionaries."". A `dictionary` is a generated C++ source file containing references to the; header locations used when building (and any additional locations provided),; a set of forward declarations to reduce the need of loading header files, and; a few I/O helper functions.; The name ""dictionary"" is historic: before ``cling`` was used, it contained; the complete generated C++ reflection information, whereas now that is; derived at run-time from the header files.; It is still possible to fully embed header files rather than only storing; their names and search locations, to make the dictionary more self-contained. After generating the dictionary, it should be compiled into a shared library.; This provides additional dependency control: by linking it directly with any; further libraries needed, you can use standard mechanisms such as ``rpath``; to locate those library dependencies.; Alternatively, you can add the additional libraries to load to the mapping; files of the class loader (see below). .. note::. The JIT needs to resolve linker symbols in order to call them through; generated wrappers.; Thus, any classes, functions, an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:1228,load,loading,1228,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loading']
Performance,"cm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A -DOTHER_OPTIONS. This way, a single directory containing multiple variants of modules can be prepared and reused. The options configuring the module cache are independent of other options. Module Semantics; ================. Modules are modeled as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visible when building later submodules in that module. This can lead to fragile modules that depend on the build order used for the submodules of the module, and should not be relied upon. This behavior is subject to change. As an example, in C, this implies that if two structs are defined in different submodules with the same n",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:23864,cache,cache-path,23864,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"cnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen aft",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:366201,load,load,366201,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"cnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:274018,load,loads,274018,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"cnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:253579,load,load,253579,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"cnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address spa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:368746,load,load,368746,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"cnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:370636,perform,performing,370636,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"cnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:299470,load,load,299470,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"cnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:273352,load,load,273352,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"code (using the binary representation expected by the; runtime library). This can be accomplished in about 100 lines of code. This is not the appropriate place to implement a garbage collected heap or a; garbage collector itself. That code should exist in the language's runtime; library. The compiler plugin is responsible for generating code which conforms; to the binary interface defined by library, most essentially the :ref:`stack map; <stack-map>`. To subclass ``llvm::GCStrategy`` and register it with the compiler:. .. code-block:: c++. // lib/MyGC/MyGC.cpp - Example LLVM GC plugin. #include ""llvm/CodeGen/GCStrategy.h""; #include ""llvm/CodeGen/GCMetadata.h""; #include ""llvm/Support/Compiler.h"". using namespace llvm;. namespace {; class LLVM_LIBRARY_VISIBILITY MyGC : public GCStrategy {; public:; MyGC() {}; };. GCRegistry::Add<MyGC>; X(""mygc"", ""My bespoke garbage collector."");; }. This boilerplate collector does nothing. More specifically:. * ``llvm.gcread`` calls are replaced with the corresponding ``load``; instruction. * ``llvm.gcwrite`` calls are replaced with the corresponding ``store``; instruction. * No safe points are added to the code. * The stack map is not compiled into the executable. Using the LLVM makefiles, this code; can be compiled as a plugin using a simple makefile:. .. code-block:: make. # lib/MyGC/Makefile. LEVEL := ../..; LIBRARYNAME = MyGC; LOADABLE_MODULE = 1. include $(LEVEL)/Makefile.common. Once the plugin is compiled, code using it may be compiled using ``llc; -load=MyGC.so`` (though MyGC.so may have some other platform-specific; extension):. ::. $ cat sample.ll; define void @f() gc ""mygc"" {; entry:; ret void; }; $ llvm-as < sample.ll | llc -load=MyGC.so. It is also possible to statically link the collector plugin into tools, such as; a language-specific compiler front-end. .. _collector-algos:. Overview of available features; ------------------------------. ``GCStrategy`` provides a range of features through which a plugin may do useful; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:24910,load,load,24910,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['load']
Performance,"code file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super tool or a; separate tool per platform is required. This increases maintenance cost for; link time optimizer significantly, which is not necessary. This approach; also requires staying synchronized with linker developments on various; platforms, which is not the main focus of the link time optimizer. Finally,; this approach increases end user's build time due to the duplication of work; done by this separate tool and the linker itself. Multi-phase c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3606,optimiz,optimizer,3606,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"code format (``.bc``). If an output filename is not specified with the :option:`-o` option,; :program:`opt` writes its output to the standard output. OPTIONS; -------. .. option:: -f. Enable binary output on terminals. Normally, :program:`opt` will refuse to; write raw bitcode output if the output stream is a terminal. With this option,; :program:`opt` will write raw bitcode regardless of the output device. .. option:: -help. Print a summary of command line options. .. option:: -o <filename>. Specify the output filename. .. option:: -S. Write output in LLVM intermediate language (instead of bitcode). .. option:: -{passname}. :program:`opt` provides the ability to run any of LLVM's optimization or; analysis passes in any order. The :option:`-help` option lists all the passes; available. The order in which the options occur on the command line are the; order in which they are executed (within pass constraints). .. option:: -strip-debug. This option causes opt to strip debug information from the module before; applying other optimizations. It is essentially the same as `-strip`; but it ensures that stripping of debug information is done first. .. option:: -verify-each. This option causes opt to add a verify pass after every pass otherwise; specified on the command line (including `-verify`). This is useful; for cases where it is suspected that a pass is creating an invalid module but; it is not clear which pass is doing it. .. option:: -stats. Print statistics. .. option:: -time-passes. Record the amount of time needed for each pass and print it to standard; error. .. option:: -debug. If this is a debug build, this option will enable debug printouts from passes; which use the ``LLVM_DEBUG()`` macro. See the `LLVM Programmer's Manual; <../ProgrammersManual.html>`_, section ``#DEBUG`` for more information. .. option:: -load=<plugin>. Load the dynamic object ``plugin``. This object should register new; optimization or analysis passes. Once loaded, the object will add new ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst:1851,optimiz,optimizations,1851,interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,1,['optimiz'],['optimizations']
Performance,"code-block:: c`, but; it looks like garbage; the line numbers don't even line up with the; lines. Is this a Sphinx bug, or is it a CSS problem?. .. code-block:: c. 1 int compute_factorial(int n); 2 {; 3 if (n <= 1); 4 return 1;; 5; 6 int f = n;; 7 while (--n > 1); 8 f *= n;; 9 return f;; 10 }; 11; 12; 13 int main(int argc, char** argv); 14 {; 15 if (argc < 2); 16 return -1;; 17 char firstletter = argv[1][0];; 18 int result = compute_factorial(firstletter - '0');; 19; 20 // Returned result is clipped at 255...; 21 return result;; 22 }. Here is a sample command line session that shows how to build and run this; code via ``lli`` inside LLDB:. .. code-block:: bash. > export BINPATH=/workspaces/llvm-project/build/bin; > $BINPATH/clang -g -S -emit-llvm --target=x86_64-unknown-unknown-elf showdebug.c; > lldb $BINPATH/lli; (lldb) target create ""/workspaces/llvm-project/build/bin/lli""; Current executable set to '/workspaces/llvm-project/build/bin/lli' (x86_64).; (lldb) settings set plugin.jit-loader.gdb.enable on; (lldb) b compute_factorial; Breakpoint 1: no locations (pending).; WARNING: Unable to resolve breakpoint to any actual locations.; (lldb) run --jit-kind=mcjit showdebug.ll 5; 1 location added to breakpoint 1; Process 21340 stopped; * thread #1, name = 'lli', stop reason = breakpoint 1.1; frame #0: 0x00007ffff7fd0007 JIT(0x45c2cb0)`compute_factorial(n=5) at showdebug.c:3:11; 1 int compute_factorial(int n); 2 {; -> 3 if (n <= 1); 4 return 1;; 5 int f = n;; 6 while (--n > 1); 7 f *= n;; (lldb) p n; (int) $0 = 5; (lldb) b showdebug.c:9; Breakpoint 2: where = JIT(0x45c2cb0)`compute_factorial + 60 at showdebug.c:9:1, address = 0x00007ffff7fd003c; (lldb) c; Process 21340 resuming; Process 21340 stopped; * thread #1, name = 'lli', stop reason = breakpoint 2.1; frame #0: 0x00007ffff7fd003c JIT(0x45c2cb0)`compute_factorial(n=1) at showdebug.c:9:1; 6 while (--n > 1); 7 f *= n;; 8 return f;; -> 9 }; 10; 11 int main(int argc, char** argv); 12 {; (lldb) p f; (int) $1 = 120; (lldb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DebuggingJITedCode.rst:3535,load,loader,3535,interpreter/llvm-project/llvm/docs/DebuggingJITedCode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DebuggingJITedCode.rst,1,['load'],['loader']
Performance,"code-block:: llvm. define void @test() gc ""statepoint-example"" {; call void @foo(); ret void; }. declare void @do_safepoint(); define void @gc.safepoint_poll() {; call void @do_safepoint(); ret void; }. This pass would produce the following IR:. .. code-block:: llvm. define void @test() gc ""statepoint-example"" {; call void @do_safepoint(); call void @foo(); ret void; }. In this case, we've added an (unconditional) entry safepoint poll. Note that; despite appearances, the entry poll is not necessarily redundant. We'd have to; know that ``foo`` and ``test`` were not mutually recursive for the poll to be; redundant. In practice, you'd probably want to your poll definition to contain; a conditional branch of some form. At the moment, PlaceSafepoints can insert safepoint polls at method entry and; loop backedges locations. Extending this to work with return polls would be; straight forward if desired. PlaceSafepoints includes a number of optimizations to avoid placing safepoint; polls at particular sites unless needed to ensure timely execution of a poll; under normal conditions. PlaceSafepoints does not attempt to ensure timely; execution of a poll under worst case conditions such as heavy system paging. The implementation of a safepoint poll action is specified by looking up a; function of the name ``gc.safepoint_poll`` in the containing Module. The body; of this function is inserted at each poll site desired. While calls or invokes; inside this method are transformed to a ``gc.statepoints``, recursive poll; insertion is not performed. This pass is useful for any language frontend which only has to support; garbage collection semantics at safepoints. If you need other abstract; frame information at safepoints (e.g. for deoptimization or introspection),; you can insert safepoint polls in the frontend. If you have the later case,; please ask on llvm-dev for suggestions. There's been a good amount of work; done on making such a scheme work well in practice which is not ye",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:32733,optimiz,optimizations,32733,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['optimiz'],['optimizations']
Performance,"col string),; the plugin class of which an object will be created; (e.g. **`TRFIOFile`**), the library to be loaded (in short; `libRFIO.so` to RFIO), and the constructor to be called (e.g.; ""`TRFIOFile()`""). This can be specified in the `.rootrc` which already; contains many plugin definitions, or by calls to; `gROOT->GetPluginManager()->AddHandler()`. #### Library AutoLoading. When using a class in Cling, e.g. in an interpreted source file, ROOT; will automatically load the library that defines this class. On; start-up, ROOT parses all files ending on `.rootmap` rootmap that are; in one of the `$LD_LIBRARY_PATH` (or `$DYLD_LIBRARY_PATH` for `MacOS`,; or `$PATH` for `Windows`). They contain class names and the library; names that the class depends on. After reading them, ROOT knows which; classes are available, and which libraries to load for them. When `TSystem::Load(""ALib"")` is called, ROOT uses this information to; determine which libraries `libALib.so` depends on. It will load these; libraries first. Otherwise, loading the requested library could cause; a system (dynamic loader) error due to unresolved symbols. ### \$ROOTSYS/tutorials. tutorials The tutorials directory contains many example example; scripts. They assume some basic knowledge of ROOT, and for the new; user we recommend reading the chapters: ""Histograms"" and; ""Input/Output"" before trying the examples. The more experienced user; can jump to chapter ""The Tutorials and Tests"" to find more explicit; and specific information about how to build and run the examples. The `$ROOTSYS/tutorials/` directory include the following; sub-directories:. `fft`: Fast Fourier Transform with the fftw package `fit`: Several; examples illustrating minimization/fitting `foam`: Random generator in; multidimensional space `geom`: Examples of use of the geometry package; (**`TGeo`** classes) `gl`: Visualisation with OpenGL `graphics`: Basic; graphics `graphs`: Use of **`TGraph`**, **`TGraphErrors`**, etc.; `gui`: Scripts to cr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:20363,load,load,20363,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,1,['load'],['load']
Performance,"come dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionality into a new internal function and leave two; overridable thunks to it. If there is no equivalent, then we add this function to tree. Lookup routine has O(log(n)) complexity, while whole merging process has; complexity of O(n*log(n)). Read; :doc:`this <MergeFunctions>`; article for more details. ``mergereturn``: Unify function exit nodes; ------------------------------------------. Ensure that functions have at most one ``ret`` instr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30953,perform,performs,30953,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performs']
Performance,"compatibility guarantees; ===============================. * There are no backwards or forwards compatibility guarantees for the raw; profile format. Raw profiles may be dependent on the specific compiler; revision used to generate them. It's inadvisable to store raw profiles for; long periods of time. * Tools must retain **backwards** compatibility with indexed profile formats.; These formats are not forwards-compatible: i.e, a tool which uses format; version X will not be able to understand format version (X+k). * Tools must also retain **backwards** compatibility with the format of the; coverage mappings emitted into instrumented binaries. These formats are not; forwards-compatible. * The JSON coverage export format has a (major, minor, patch) version triple.; Only a major version increment indicates a backwards-incompatible change. A; minor version increment is for added functionality, and patch version; increments are for bugfixes. Impact of llvm optimizations on coverage reports; ================================================. llvm optimizations (such as inlining or CFG simplification) should have no; impact on coverage report quality. This is due to the fact that the mapping; from source regions to profile counters is immutable, and is generated before; the llvm optimizer kicks in. The optimizer can't prove that profile counter; instrumentation is safe to delete (because it's not: it affects the profile the; program emits), and so leaves it alone. Note that this coverage feature does not rely on information that can degrade; during the course of optimization, such as debug info line tables. Using the profiling runtime without static initializers; =======================================================. By default the compiler runtime uses a static initializer to determine the; profile output path and to register a writer function. To collect profiles; without using static initializers, do this manually:. * Export a ``int __llvm_profile_runtime`` symbol from",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst:14627,optimiz,optimizations,14627,interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,1,['optimiz'],['optimizations']
Performance,"compile-time constant positive power of two no; greater than a target-specific atomic access size limit. For each of the input pointers the ``align`` parameter attribute must be; specified. It must be a power of two no less than the ``element_size``. Caller; guarantees that both the source and destination pointers are aligned to that; boundary. Semantics:; """""""""""""""""""". The '``llvm.memmove.element.unordered.atomic.*``' intrinsic copies ``len`` bytes; of memory from the source location to the destination location. These locations; are allowed to overlap. The memory copy is performed as a sequence of load/store; operations where each access is guaranteed to be a multiple of ``element_size``; bytes wide and aligned at an ``element_size`` boundary. The order of the copy is unspecified. The same value may be read from the source; buffer many times, but only one write is issued to the destination buffer per; element. It is well defined to have concurrent reads and writes to both source; and destination provided those reads and writes are unordered atomic when; specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered loads from the source location and stores to the; destination. Lowering:; """""""""""""""""". In the most general case call to the; '``llvm.memmove.element.unordered.atomic.*``' is lowered to a call to the symbol; ``__llvm_memmove_element_unordered_atomic_*``. Where '*' is replaced with an; actual element size. See :ref:`RewriteStatepointsForGC intrinsic lowering; <RewriteStatepointsForGC_intrinsic_lowering>` for details on GC specific; lowering. The optimizer is allowed to inline the memory copy when it's profitable to do so. .. _int_memset_element_unordered_atomic:. '``llvm.memset.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.memset.element.unordered.atomic`` on; any integer bit width and for",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:962467,concurren,concurrent,962467,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['concurren'],['concurrent']
Performance,"compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2136,optimiz,optimized,2136,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,1,['optimiz'],['optimized']
Performance,"completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/re",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:330905,load,load,330905,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"completely fleshed out.; Currently, ROOT objects and built-in types can cross the boundary; between the two interpreters, but other objects are much more; restricted. For example, for a Python object to cross, it has to be a; class instance, and its class has to be known to Cling first (i.e. the; class has to cross first, before the instance can). All other; cross-coding is based on strings that are run on the Python interpreter; and vise-versa. With the ROOT v4.00/06 and later, the **`TPython`** class will be loaded; automatically on use, for older editions, the `libPyROOT.so` needs to be; loaded first before use. It is possible to switch between interpreters; by calling **`TPython::Prompt()`** on the ROOT side, while returning with; `^D` (`EOF`). State is preserved between successive switches, and string; based cross calls can nest as long as shared resources are properly; handled. ``` {.cpp}; // Example: accessing the Python interpreter from ROOT; // either load PyROOT explicitly or rely on auto-loading; root[] gSystem->Load( ""libPyROOT"" );; root[] TPython::Exec(""print1+1"");; 2. // create a TBrowser on the Python side, and transfer it back and forth; root[] TBrowser* b = (void*)TPython::Eval(""ROOT.TBrowser()"");; (class TObject*)0x8d1daa0; root[] TPython::Bind(b,""b"");. // builtin variables can cross-over (after the call i==2); root[] int i = TPython::Eval( ""1+1"" );; root[] i; (int)2; ```. ### Installation. There are several ways of obtaining `PyROOT`, and which is best depends; on your specific situation. If you work at CERN, you can use the; installation available on `afs`. Otherwise, you will want to build from; source, as `PyROOT` is not build by default in the binaries distributed; from the ROOT project site. If you download the ROOT binaries, take care; to download and install the Python distribution from; <http://www.python.org/> against which they were built. #### Environment Settings. ROOT installations with the build of `PyROOT` enabled are available from;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:4976,load,load,4976,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,2,['load'],"['load', 'loading']"
Performance,"completion of a send of ``release`` that is not; preceded by a send of ``retain`` to the same object. The behavior of ``autorelease`` must be equivalent to sending ``release`` when; one of the autorelease pools currently in scope is popped. It may not throw an; exception. When the semantics call for performing one of these operations on a retainable; object pointer, if that pointer is ``null`` then the effect is a no-op. All of the semantics described in this document are subject to additional; :ref:`optimization rules <arc.optimization>` which permit the removal or; optimization of operations based on local knowledge of data flow. The; semantics describe the high-level behaviors that the compiler implements, not; an exact sequence of operations that a program will be compiled into. .. _arc.objects.operands:. Retainable object pointers as operands and arguments; ----------------------------------------------------. In general, ARC does not perform retain or release operations when simply using; a retainable object pointer as an operand within an expression. This includes:. * loading a retainable pointer from an object with non-weak :ref:`ownership; <arc.ownership>`,; * passing a retainable pointer as an argument to a function or method, and; * receiving a retainable pointer as the result of a function or method call. .. admonition:: Rationale. While this might seem uncontroversial, it is actually unsafe when multiple; expressions are evaluated in ""parallel"", as with binary operators and calls,; because (for example) one expression might load from an object while another; writes to it. However, C and C++ already call this undefined behavior; because the evaluations are unsequenced, and ARC simply exploits that here to; avoid needing to retain arguments across a large number of calls. The remainder of this section describes exceptions to these rules, how those; exceptions are detected, and what those exceptions imply semantically. .. _arc.objects.operands.consumed:. Co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:15514,perform,perform,15514,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['perform']
Performance,"complexity of inserting bitcasts everywhere that they might be; required. The community consensus is that the costs of pointee types; outweight the benefits, and that they should be removed. Many operations do not actually care about the underlying type. These; operations, typically intrinsics, usually end up taking an arbitrary pointer; type ``i8*`` and sometimes a size. This causes lots of redundant no-op bitcasts; in the IR to and from a pointer with a different pointee type. No-op bitcasts take up memory/disk space and also take up compile time to look; through. However, perhaps the biggest issue is the code complexity required to; deal with bitcasts. When looking up through def-use chains for pointers it's; easy to forget to call `Value::stripPointerCasts()` to find the true underlying; pointer obfuscated by bitcasts. And when looking down through def-use chains; passes need to iterate through bitcasts to handle uses. Removing no-op pointer; bitcasts prevents a category of missed optimizations and makes writing LLVM; passes a little bit easier. Fewer no-op pointer bitcasts also reduces the chances of incorrect bitcasts in; regards to address spaces. People maintaining backends that care a lot about; address spaces have complained that frontends like Clang often incorrectly; bitcast pointers, losing address space information. An analogous transition that happened earlier in LLVM is integer signedness.; Currently there is no distinction between signed and unsigned integer types, but; rather each integer operation (e.g. add) contains flags to signal how to treat; the integer. Previously LLVM IR distinguished between unsigned and signed; integer types and ran into similar issues of no-op casts. The transition from; manifesting signedness in types to instructions happened early on in LLVM's; timeline to make LLVM easier to work with. Opaque Pointers Mode; ====================. During the transition phase, LLVM can be used in two modes: In typed pointer; mode all poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:4280,optimiz,optimizations,4280,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['optimiz'],['optimizations']
Performance,"config: ''; cpu_name: haswell; llvm_triple: x86_64-unknown-linux-gnu; num_repetitions: 10000; measurements:; - { key: latency, value: 1.0058, debug_string: '' }; error: ''; info: 'explicit self cycles, selecting one aliasing configuration.; Snippet:; ADD64rr R8, R8, R10; '; ... To measure the latency of all instructions for the host architecture, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-index=-1. EXAMPLE 2: benchmarking a custom code snippet; ---------------------------------------------. To measure the latency/uops of a custom piece of code, you can specify the; `snippets-file` option (`-` reads from standard input). .. code-block:: bash. $ echo ""vzeroupper"" | llvm-exegesis --mode=uops --snippets-file=-. Real-life code snippets typically depend on registers or memory.; :program:`llvm-exegesis` checks the liveliness of registers (i.e. any register; use has a corresponding def or is a ""live in""). If your code depends on the; value of some registers, you need to use snippet annotations to ensure setup; is performed properly. For example, the following code snippet depends on the values of XMM1 (which; will be set by the tool) and the memory buffer passed in RDI (live in). .. code-block:: none. # LLVM-EXEGESIS-LIVEIN RDI; # LLVM-EXEGESIS-DEFREG XMM1 42; vmulps	(%rdi), %xmm1, %xmm2; vhaddps	%xmm2, %xmm2, %xmm3; addq $0x10, %rdi. Example 3: benchmarking with memory annotations; -----------------------------------------------. Some snippets require memory setup in specific places to execute without; crashing. Setting up memory can be accomplished with the `LLVM-EXEGESIS-MEM-DEF`; and `LLVM-EXEGESIS-MEM-MAP` annotations. To execute the following snippet:. .. code-block:: none. movq $8192, %rax; movq (%rax), %rdi. We need to have at least eight bytes of memory allocated starting `0x2000`.; We can create the necessary execution environment with the following; annotations added to the snippet:. .. code-block:: none. # LLVM-EXEGESIS-MEM-DEF test1 409",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:6439,perform,performed,6439,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['perform'],['performed']
Performance,"configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized binary, build the stage2-clang-bolt target:. .. code-block:: console. $ ninja stage2-clang-bolt. 3-Stage Non-Determinism; =======================. In the ancient lore of compilers non-determinism is like the multi-headed hydra.; Whenever its head pops up, terror and chaos ensue. Historically one of the tests to verify that a compiler was deterministic would; be a three stage build. The idea of a three stage build is you take your sources; and build a compiler (stage1), then use that compiler to rebuild the sources; (stage2), then you use that compiler to rebuild the sources a third time; (stage3) with an identical configuration to the stage2 build. At the end of; this, you have a stage2 and stage3 compiler that should be bit-for-bit; identical. You can perform one of these 3-stage builds with LLVM and clang using the; following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/3-stage.cmake <path to source>/llvm; $ ninja stage3. After the build you can compare the stage2 and stage3 compilers.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:12039,perform,perform,12039,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,2,"['cache', 'perform']","['caches', 'perform']"
Performance,"const_use_iterator`` and; ``Value::const_op_iterator``. They automatically arise when calling; ``use/op_begin()`` on ``const Value*``\ s or ``const User*``\ s respectively.; Upon dereferencing, they return ``const Use*``\ s. Otherwise the above patterns; remain unchanged. .. _iterate_preds:. Iterating over predecessors & successors of blocks; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Iterating over the predecessors and successors of a block is quite easy with the; routines defined in ``""llvm/IR/CFG.h""``. Just use code like this to; iterate over all predecessors of BB:. .. code-block:: c++. #include ""llvm/IR/CFG.h""; BasicBlock *BB = ...;. for (BasicBlock *Pred : predecessors(BB)) {; // ...; }. Similarly, to iterate over successors use ``successors``. .. _simplechanges:. Making simple changes; ---------------------. There are some primitive transformation operations present in the LLVM; infrastructure that are worth knowing about. When performing transformations,; it's fairly common to manipulate the contents of basic blocks. This section; describes some of the common methods for doing so and gives example code. .. _schanges_creating:. Creating and inserting new ``Instruction``\ s; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. *Instantiating Instructions*. Creation of ``Instruction``\ s is straight-forward: simply call the constructor; for the kind of instruction to instantiate and provide the necessary parameters.; For example, an ``AllocaInst`` only *requires* a (const-ptr-to) ``Type``. Thus:. .. code-block:: c++. auto *ai = new AllocaInst(Type::Int32Ty);. will create an ``AllocaInst`` instance that represents the allocation of one; integer in the current stack frame, at run time. Each ``Instruction`` subclass; is likely to have varying default parameters which change the semantics of the; instruction, so refer to the `doxygen documentation for the subclass of; Instruction <https://llvm.org/doxygen/classllvm_1_1Instruction.html>`_ that; you're interested i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:113355,perform,performing,113355,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performing']
Performance,"constant contiguous sequence of char-like objects with the first element of the sequence at position zero. This type is used throughout the ROOT code to avoid copying strings when a; sub-string is needed and to extent interfaces that uses to take a const char*; to take a std::string_view as thus be able to be directly directly passed a; TString, a std::string or a std::string_view. Usage example:. ``` {.cpp}; // With SetName(std::string_view); std::string str; ; obj.SetName( str );; obj.SetName( {str.data()+pos, len} );; ```. ### Meta library. #### Backward Incompatibilities. TIsAProxy's constructor no longer take the optional and unused 2nd argument which was reserved for a 'context'. This context was unused in TIsAProxy itself and was not accessible from derived classes. #### Interpreter. The new interface `TInterpreter::Declare(const char* code)` will declare the; code to the interpreter with all interpreter extensions disabled, i.e. as; ""proper"" C++ code. No autoloading or synamic lookup will be performed. A new R__LOAD_LIBRARY(libWhatever) will load libWhatever at parse time. This allows ROOT to resolve symbols from this library very early on. It is a work-around for the following code from ROOT 5:. ``` {.cpp}; // ROOT 5:; void func() {; gSystem->Load(""libEvent"");; Event* e = new Event;; }; ```. Instead, write:. ``` {.cpp}; // ROOT 6:; R__LOAD_LIBRARY(libEvent); #include ""Event.h"". void func() {; Event* e = new Event;; }; ```. #### TClass. Introduced new overload for calculating the TClass CheckSum:. ``` {.cpp}; UInt_t TClass::GetCheckSum(ECheckSum code, Bool_t &isvalid) const;; ```. which indicates via the 'isvalid' boolean whether the checksum could be; calculated correctly or not. ### TROOT. Implemented new gROOT->GetTutorialsDir() static method to return the actual location of the tutorials directory.; This is $ROOTSYS/tutorials when not configuring with --prefix or -Dgnuinstall for CMake. ### TColor. Add an enum to access the palette by name. Add new pale",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:2929,perform,performed,2929,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['perform'],['performed']
Performance,"contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how C++ exception handling could be implemented in; llvm. Java would be very similar, except it only uses destructors to unlock; synchronized blocks, not to destroy data. Also, it uses two stack walks: a; nondestructive walk that builds a stack trace, then a destructive walk that; unwinds the stack as shown here. . It would be trivial to get exception inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:6507,optimiz,optimization,6507,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,1,['optimiz'],['optimization']
Performance,"contains the description of all; versions of all classes in the file. When a file is opened the; `StreamerInfo `is read into memory and it provides enough information to; make the file browsable. The `TStreamerInfo `enables us to recreate a; header file for the class in case the compiled class is not available.; This is done with the `TFile::MakeProject` method. It creates a; directory with the header files for the named classes and a `makefile`; to compile a shared library with the class definitions. ### Example: MakeProject. To explain the details, we use the example of the `ATLFast` project that; is a fast simulation for the ATLAS experiment. The complete source for; `ATLFast` can be down loaded at; <ftp://root.cern.ch/root/atlfast.tar.gz>. Once we compile and run; `ATLFast` we get a ROOT file called `atlfast.root`, containing the; `ATLFast` objects. When we open the file, we get a warning that the file; contains classes that are not in the dictionary. This is correct; since we did not load the class definitions. ``` {.cpp}; root[] TFile f(""atlfast.root""); Warning in <TClass::TClass>: no dictionary for class TMCParticle is available; Warning in <TClass::TClass>: no dictionary for class ATLFMuon available; ```. We can see the `StreamerInfo `for the classes:. ``` {.cpp}; root[] f.ShowStreamerInfo(); ...; StreamerInfo for class: ATLFMuon, version=1; BASE TObject offset= 0 type=66 Basic ROOT object; BASE TAtt3D offset= 0 type= 0 3D attributes; Int_t m_KFcode offset= 0 type= 3 Muon KF-code; Int_t m_MCParticle offset= 0 type= 3 Muon position in MCParticles list; Int_t m_KFmother offset= 0 type= 3 Muon mother KF-code; Int_t m_UseFlag offset= 0 type= 3 Muon energy usage flag; Int_t m_Isolated offset= 0 type= 3 Muon isolation (1 for isolated); Float_t m_Eta offset= 0 type= 5 Eta coordinate; Float_t m_Phi offset= 0 type= 5 Phi coordinate; Float_t m_PT offset= 0 type= 5 Transverse energy; Int_t m_Trigger offset= 0 type= 3 Result of trigger...; ```. However, when we try to u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:85360,load,load,85360,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['load'],['load']
Performance,"contents::; :local:; :depth: 2. .. toctree::; :maxdepth: 1. Introduction; ============. FatLTO objects are a special type of `fat object file; <https://en.wikipedia.org/wiki/Fat_binary>`_ that contain LTO compatible IR in; addition to generated object code, instead of containing object code for; multiple target architectures. This allows users to defer the choice of whether; to use LTO or not to link-time, and has been a feature available in other; compilers, like `GCC; <https://gcc.gnu.org/onlinedocs/gccint/LTO-Overview.html>`_, for some time. Under FatLTO the compiler can emit standard object files which contain both the; machine code in the ``.text`` section and LLVM bitcode in the ``.llvm.lto``; section. Overview; ========. Within LLVM, FatLTO is supported by choosing the ``FatLTODefaultPipeline``.; This pipeline will:. #) Run the pre-link (Thin)LTO pipeline on the current module.; #) Embed the pre-link bitcode in a special ``.llvm.lto`` section.; #) Finish optimizing the module using the ModuleOptimization pipeline.; #) Emit the object file, including the new ``.llvm.lto`` section. .. NOTE. Previously, we conservatively ran independent pipelines on separate copies; of the LLVM module to generate the bitcode section and the object code,; which happen to be identical to those used outside of FatLTO. While that; resulted in compiled artifacts that were identical to those produced by the; default and (Thin)LTO pipelines, module cloning led to some cases of; miscompilation, and we have moved away from trying to keep bitcode; generation and optimization completely disjoint. Bit-for-bit compatibility is not (and never was) a guarantee, and we reserve; the right to change this at any time. Explicitly, users should not rely on; the produced bitcode or object code to match their non-LTO counterparts; precisely. They will exhibit similar performance characteristics, but may; not be bit-for-bit the same. Internally, the ``.llvm.lto`` section is created by running the; ``Em",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FatLTO.rst:1029,optimiz,optimizing,1029,interpreter/llvm-project/llvm/docs/FatLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FatLTO.rst,1,['optimiz'],['optimizing']
Performance,"conversions are extended as follows. Note that these conversions; are intentionally not listed as satisfying the constraints for assignment,; which is to say, they are only permitted as explicit casts, not as implicit; conversions. A value of matrix type can be converted to another matrix type if the number of; rows and columns are the same and the value's elements can be converted to the; element type of the result type. The result is a matrix where each element is; the converted corresponding element. A value of any real type (as in C23 6.2.5p14) can be converted to a matrix type; if it can be converted to the element type of the matrix. The result is a; matrix where all elements are the converted original value. If the number of rows or columns differ between the original and resulting; type, the program is ill-formed. Arithmetic Conversions; ----------------------. The usual arithmetic conversions are extended as follows. Insert at the start:. * If both operands are of matrix type, no arithmetic conversion is performed.; * If one operand is of matrix type and the other operand is of a real type,; convert the real type operand to the matrix type; according to the standard conversion rules. Matrix Type Element Access Operator; -----------------------------------. An expression of the form ``E1 [E2] [E3]``, where ``E1`` has matrix type ``cv; M``, is a matrix element access expression. Let ``T`` be the element type; of ``M``, and let ``R`` and ``C`` be the number of rows and columns in ``M``; respectively. The index expressions shall have integral or unscoped; enumeration type and shall not be uses of the comma operator unless; parenthesized. The first index expression shall evaluate to a; non-negative value less than ``R``, and the second index expression shall; evaluate to a non-negative value less than ``C``, or else the expression has; undefined behavior. If ``E1`` is a prvalue, the result is a prvalue with type; ``T`` and is the value of the element at the give",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst:3250,perform,performed,3250,interpreter/llvm-project/clang/docs/MatrixTypes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst,1,['perform'],['performed']
Performance,"cope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:281633,load,load,281633,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"coping rules. In order to handle this, the LLVM debug format uses the metadata attached to; llvm instructions to encode line number and scoping information. Consider the; following C fragment, for example:. .. code-block:: c. 1. void foo() {; 2. int X = 21;; 3. int Y = 22;; 4. {; 5. int Z = 23;; 6. Z = X;; 7. }; 8. X = Y;; 9. }. Compiled to LLVM, this function would be represented like this:. .. code-block:: text. ; Function Attrs: nounwind ssp uwtable; define void @foo() #0 !dbg !4 {; entry:; %X = alloca i32, align 4; %Y = alloca i32, align 4; %Z = alloca i32, align 4; call void @llvm.dbg.declare(metadata i32* %X, metadata !11, metadata !13), !dbg !14; store i32 21, i32* %X, align 4, !dbg !14; call void @llvm.dbg.declare(metadata i32* %Y, metadata !15, metadata !13), !dbg !16; store i32 22, i32* %Y, align 4, !dbg !16; call void @llvm.dbg.declare(metadata i32* %Z, metadata !17, metadata !13), !dbg !19; store i32 23, i32* %Z, align 4, !dbg !19; %0 = load i32, i32* %X, align 4, !dbg !20; store i32 %0, i32* %Z, align 4, !dbg !21; %1 = load i32, i32* %Y, align 4, !dbg !22; store i32 %1, i32* %X, align 4, !dbg !23; ret void, !dbg !24; }. ; Function Attrs: nounwind readnone; declare void @llvm.dbg.declare(metadata, metadata, metadata) #1. attributes #0 = { nounwind ssp uwtable ""less-precise-fpmad""=""false"" ""frame-pointer""=""all"" ""no-infs-fp-math""=""false"" ""no-nans-fp-math""=""false"" ""stack-protector-buffer-size""=""8"" ""unsafe-fp-math""=""false"" ""use-soft-float""=""false"" }; attributes #1 = { nounwind readnone }. !llvm.dbg.cu = !{!0}; !llvm.module.flags = !{!7, !8, !9}; !llvm.ident = !{!10}. !0 = !DICompileUnit(language: DW_LANG_C99, file: !1, producer: ""clang version 3.7.0 (trunk 231150) (llvm/trunk 231154)"", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2, retainedTypes: !2, subprograms: !3, globals: !2, imports: !2); !1 = !DIFile(filename: ""/dev/stdin"", directory: ""/Users/dexonsmith/data/llvm/debug-info""); !2 = !{}; !3 = !{!4}; !4 = distinct !DISubprogram(",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:13389,load,load,13389,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,2,['load'],['load']
Performance,"corresponding to the predecessor basic block that; executed just prior to the current block. Example:; """""""""""""""". .. code-block:: llvm. Loop: ; Infinite loop that counts from 0 on up...; %indvar = phi i32 [ 0, %LoopHeader ], [ %nextindvar, %Loop ]; %nextindvar = add i32 %indvar, 1; br label %Loop. .. _i_select:. '``select``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = select [fast-math flags] selty <cond>, <ty> <val1>, <ty> <val2> ; yields ty. selty is either i1 or {<N x i1>}. Overview:; """""""""""""""""". The '``select``' instruction is used to choose one value based on a; condition, without IR-level branching. Arguments:; """""""""""""""""""". The '``select``' instruction requires an 'i1' value or a vector of 'i1'; values indicating the condition, and two values of the same :ref:`first; class <t_firstclass>` type. #. The optional ``fast-math flags`` marker indicates that the select has one or more; :ref:`fast-math flags <fastmath>`. These are optimization hints to enable; otherwise unsafe floating-point optimizations. Fast-math flags are only valid; for selects that return a floating-point scalar or vector type, or an array; (nested to any depth) of floating-point scalar or vector types. Semantics:; """""""""""""""""""". If the condition is an i1 and it evaluates to 1, the instruction returns; the first value argument; otherwise, it returns the second value; argument. If the condition is a vector of i1, then the value arguments must be; vectors of the same size, and the selection is done element by element. If the condition is an i1 and the value arguments are vectors of the; same size, then an entire vector is selected. Example:; """""""""""""""". .. code-block:: llvm. %X = select i1 true, i8 17, i8 42 ; yields i8:17. .. _i_freeze:. '``freeze``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = freeze ty <val> ; yields ty:result. Overview:; """""""""""""""""". The '``freeze``' instruction is used to stop propagation of; :ref:`undef <undefvalues>` and :ref:`poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:469419,optimiz,optimization,469419,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"cos etc.. are overloaded, take care passing it. ~~~{.cxx}; #include<TRInterface.h>. Double_t myfun(Double_t x); {; return 2*cos(x);; }. Int_t myfun(Int_t x); {; return x;; }. void fun(); {; ROOT::R::TRInterface &r=ROOT::R::TRInterface::Instance();; r[""myfund""]<<(Double_t (*)(Double_t))myfun;; r[""myfuni""]<<(Int_t (*)(Int_t))myfun;. r<<""print(myfund(0.0))"";; r<<""print(myfuni(1))"";; }; ~~~. ## Wrapping a class; You can wrap a class and expose it in R environment using only a pair of macrodefinitions and the template class; `ROOT::R::class_<>`; The `ROOTR_EXPOSED_CLASS(Class)` macro allows you to expose the class as a new datatype of R, but it has to be alongside; the `ROOTR_MODULE(Module)` macro which allows you to create an internal R module and make the class wrapping; To do this you must use inside the `ROOTR_MODULE` braces the class `ROOT::R::class_<>` and specify; each constructor, attribute or method that the class to export has.; Then the macrodefinition `LOAD_ROOTR_MODULE(Module)` can load the module and the class in R's environment.; You can find a more clear instruction by looking at a example below in Functor section. ##DataFrames; DataFrame? is a very important datatype in R and in ROOTR we have a class to manipulate; dataframes called TRDataFrame, with a lot of very useful operators overloaded to work with TRDataFrame's objects; in a similar way that in the R environment but from c++ in ROOT.; Example:. Lets to create need data to play with dataframe features. ~~~{.cxx}; ////////////////////////; //creating variables//; ////////////////////////; TVectorD v1(3);; std::vector<Double_t> v2(3);; std::array<Int_t,3> v3{ {1,2,3} };; std::list<std::string> names;. //////////////////////; //assigning values//; //////////////////////; v1[0]=1;; v1[1]=2;; v1[2]=3;. v2[0]=0.101;; v2[1]=0.202;; v2[2]=0.303;. names.push_back(""v1"");; names.push_back(""v2"");; names.push_back(""v3"");. ROOT::R::TRInterface &r=ROOT::R::TRInterface::Instance();; ~~~; In R the dataframe have as",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/r/doc/users-guide/ROOTR_Users_Guide.md:7336,load,load,7336,bindings/r/doc/users-guide/ROOTR_Users_Guide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/r/doc/users-guide/ROOTR_Users_Guide.md,1,['load'],['load']
Performance,"counted_by(T) // defined as nothing; // ... other bounds annotations; #endif. // expands to `void foo(int * ptr, size_t count);`; // when extension is not enabled or not available; void foo(int *__counted_by(count) ptr, size_t count);. Other potential applications of bounds annotations; ==================================================. The bounds annotations provided by the ``-fbounds-safety`` programming model; have potential use cases beyond the language extension itself. For example,; static and dynamic analysis tools could use the bounds information to improve; diagnostics for out-of-bounds accesses, even if ``-fbounds-safety`` is not used.; The bounds annotations could be used to improve C interoperability with; bounds-safe languages, providing a better mapping to bounds-safe types in the; safe language interface. The bounds annotations can also serve as documentation; specifying the relationship between declarations. Limitations; ===========. ``-fbounds-safety`` aims to bring the bounds safety guarantee to the C language,; and it does not guarantee other types of memory safety properties. Consequently,; it may not prevent some of the secondary bounds safety violations caused by; other types of safety violations such as type confusion. For instance,; ``-fbounds-safety`` does not perform type-safety checks on conversions between; `__single`` pointers of different pointee types (e.g., ``char *__single`` ; ``void *__single``  ``int *__single``) beyond what the foundation languages; (C/C++) already offer. ``-fbounds-safety`` heavily relies on run-time checks to keep the bounds safety; and the soundness of the type system. This may incur significant code size; overhead in unoptimized builds and leaving some of the adoption mistakes to be; caught only at run time. This is not a fundamental limitation, however, because; incrementally adding necessary static analysis will allow us to catch issues; early on and remove unnecessary bounds checks in unoptimized builds.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst:47511,perform,perform,47511,interpreter/llvm-project/clang/docs/BoundsSafety.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst,1,['perform'],['perform']
Performance,cpp; Caching.cpp; circular_raw_ostream.cpp; Chrono.cpp; COM.cpp; CodeGenCoverage.cpp; CommandLine.cpp; Compression.cpp; CRC.cpp; ConvertUTF.cpp; ConvertEBCDIC.cpp; ConvertUTFWrapper.cpp; CrashRecoveryContext.cpp; CSKYAttributes.cpp; CSKYAttributeParser.cpp; DataExtractor.cpp; Debug.cpp; DebugCounter.cpp; DeltaAlgorithm.cpp; DivisionByConstantInfo.cpp; DAGDeltaAlgorithm.cpp; DJB.cpp; ELFAttributeParser.cpp; ELFAttributes.cpp; Error.cpp; ErrorHandling.cpp; ExtensibleRTTI.cpp; FileCollector.cpp; FileUtilities.cpp; FileOutputBuffer.cpp; FloatingPointMode.cpp; FoldingSet.cpp; FormattedStream.cpp; FormatVariadic.cpp; GlobPattern.cpp; GraphWriter.cpp; Hashing.cpp; InitLLVM.cpp; InstructionCost.cpp; IntEqClasses.cpp; IntervalMap.cpp; JSON.cpp; KnownBits.cpp; LEB128.cpp; LineIterator.cpp; Locale.cpp; LockFileManager.cpp; ManagedStatic.cpp; MathExtras.cpp; MemAlloc.cpp; MemoryBuffer.cpp; MemoryBufferRef.cpp; MD5.cpp; MSP430Attributes.cpp; MSP430AttributeParser.cpp; NativeFormatting.cpp; OptimizedStructLayout.cpp; Optional.cpp; PGOOptions.cpp; Parallel.cpp; PluginLoader.cpp; PrettyStackTrace.cpp; RandomNumberGenerator.cpp; Regex.cpp; RISCVAttributes.cpp; RISCVAttributeParser.cpp; RISCVISAInfo.cpp; ScaledNumber.cpp; ScopedPrinter.cpp; SHA1.cpp; SHA256.cpp; Signposts.cpp; SmallPtrSet.cpp; SmallVector.cpp; SourceMgr.cpp; SpecialCaseList.cpp; Statistic.cpp; StringExtras.cpp; StringMap.cpp; StringSaver.cpp; StringRef.cpp; SuffixTreeNode.cpp; SuffixTree.cpp; SystemUtils.cpp; TarWriter.cpp; ThreadPool.cpp; TimeProfiler.cpp; Timer.cpp; ToolOutputFile.cpp; Twine.cpp; TypeSize.cpp; Unicode.cpp; UnicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; UnicodeNameToCodepointGenerated.cpp; VersionTuple.cpp; VirtualFileSystem.cpp; WithColor.cpp; YAMLParser.cpp; YAMLTraits.cpp; raw_os_ostream.cpp; raw_ostream.cpp; raw_socket_stream.cpp; regcomp.c; regerror.c; regexec.c; regfree.c; regstrlcpy.c; xxhash.cpp; Z3Solver.cpp. ${ALLOCATOR_FILES}; $<TARGET_OBJECTS:LLVMSupportBlake3>. # System; Atomic.cpp; Dy,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:6955,Optimiz,OptimizedStructLayout,6955,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,1,['Optimiz'],['OptimizedStructLayout']
Performance,"cppyy-cling; ===========. A repackaging of Cling, the LLVM-based interactive C++ interpreter, as a; library for use as the backend to cppyy. This version of Cling is patched for; improved performance and better use with Python. Wheels are available for the major platforms, but if you have to build from; source, building of LLVM will take a long time. By default, all cores will be; used, but it is also recommended to add the verbose flag to see progress:. $ python -m pip install --verbose cppyy-cling. For further details, see cppyy's installation instructions:; https://cppyy.readthedocs.io/en/latest/installation.html. Cling documentation is here:; https://root.cern.ch/cling. ----. Full cppyy documentation is here:; http://cppyy.readthedocs.io/. Change log:; https://cppyy.readthedocs.io/en/latest/changelog.html. Bug reports/feedback:; https://bitbucket.org/wlav/cppyy/issues?status=new&status=open; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/README.rst:188,perform,performance,188,bindings/pyroot/cppyy/cppyy-backend/cling/README.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/README.rst,1,['perform'],['performance']
Performance,"cquire the given; capability, and returns a boolean value indicating success or failure.; The first argument must be ``true`` or ``false``, to specify which return value; indicates success, and the remaining arguments are interpreted in the same way; as ``ACQUIRE``. See :ref:`mutexheader`, below, for example uses. Because the analysis doesn't support conditional locking, a capability is; treated as acquired after the first branch on the return value of a try-acquire; function. .. code-block:: c++. Mutex mu;; int a GUARDED_BY(mu);. void foo() {; bool success = mu.TryLock();; a = 0; // Warning, mu is not locked.; if (success) {; a = 0; // Ok.; mu.Unlock();; } else {; a = 0; // Warning, mu is not locked.; }; }. ASSERT_CAPABILITY(...) and ASSERT_SHARED_CAPABILITY(...); --------------------------------------------------------. *Previously:* ``ASSERT_EXCLUSIVE_LOCK``, ``ASSERT_SHARED_LOCK``. These are attributes on a function or method which asserts the calling thread; already holds the given capability, for example by performing a run-time test; and terminating if the capability is not held. Presence of this annotation; causes the analysis to assume the capability is held after calls to the; annotated function. See :ref:`mutexheader`, below, for example uses. GUARDED_VAR and PT_GUARDED_VAR; ------------------------------. Use of these attributes has been deprecated. Warning flags; -------------. * ``-Wthread-safety``: Umbrella flag which turns on the following:. + ``-Wthread-safety-attributes``: Semantic checks for thread safety attributes.; + ``-Wthread-safety-analysis``: The core analysis.; + ``-Wthread-safety-precise``: Requires that mutex expressions match precisely.; This warning can be disabled for code which has a lot of aliases.; + ``-Wthread-safety-reference``: Checks when guarded members are passed by reference. :ref:`negative` are an experimental feature, which are enabled with:. * ``-Wthread-safety-negative``: Negative capabilities. Off by default. When new fe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst:15386,perform,performing,15386,interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,1,['perform'],['performing']
Performance,"cquire-fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the caches. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequentia",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:372562,load,load,372562,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"crash:. Front-end bugs; --------------. On a ``clang`` crash, the compiler will dump a preprocessed file and a script; to replay the ``clang`` command. For example, you should see something like. .. code-block:: text. PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:; Preprocessed source(s) and associated run script(s) are located at:; clang: note: diagnostic msg: /tmp/foo-xxxxxx.c; clang: note: diagnostic msg: /tmp/foo-xxxxxx.sh. The `creduce <https://github.com/csmith-project/creduce>`_ tool helps to; reduce the preprocessed file down to the smallest amount of code that still; replicates the problem. You're encouraged to use creduce to reduce the code; to make the developers' lives easier. The; ``clang/utils/creduce-clang-crash.py`` script can be used on the files; that clang dumps to help with automating creating a test to check for the; compiler crash. `cvise <https://github.com/marxin/cvise>`_ is an alternative to ``creduce``. .. _middleend-crash:. Middle-end optimization bugs; ----------------------------. If you find that a bug crashes in the optimizer, compile your test-case to a; ``.bc`` file by passing ""``-emit-llvm -O1 -Xclang -disable-llvm-passes -c -o; foo.bc``"". The ``-O1`` is important because ``-O0`` adds the ``optnone``; function attribute to all functions and many passes don't run on ``optnone``; functions. Then run:. .. code-block:: bash. opt -O3 foo.bc -disable-output. If this doesn't crash, please follow the instructions for a :ref:`front-end; bug <frontend-crash>`. If this does crash, then you should be able to debug this with the following; :doc:`bugpoint <Bugpoint>` command:. .. code-block:: bash. bugpoint foo.bc -O3. Run this, then file a bug with the instructions and reduced .bc; files that bugpoint emits. If bugpoint doesn't reproduce the crash, ``llvm-reduce`` is an alternative; way to reduce LLVM IR. Create a script that repros the crash and run:. .. code-block:: bash. llvm-reduce --test=path/to/script foo.bc. which should produce red",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:3512,optimiz,optimization,3512,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimization']
Performance,"cremental compiles. In ThinLTO mode, as with regular LTO, clang emits LLVM bitcode after the; compile phase. The ThinLTO bitcode is augmented with a compact summary; of the module. During the link step, only the summaries are read and; merged into a combined summary index, which includes an index of function; locations for later cross-module function importing. Fast and efficient; whole-program analysis is then performed on the combined summary index. However, all transformations, including function importing, occur; later when the modules are optimized in fully parallel backends.; By default, linkers_ that support ThinLTO are set up to launch; the ThinLTO backends in threads. So the usage model is not affected; as the distinction between the fast serial thin link step and the backends; is transparent to the user. For more information on the ThinLTO design and current performance,; see the LLVM blog post `ThinLTO: Scalable and Incremental LTO; <http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html>`_.; While tuning is still in progress, results in the blog post show that; ThinLTO already performs well compared to LTO, in many cases matching; the performance improvement. Current Status; ==============. Clang/LLVM; ----------; .. _compiler:. The 3.9 release of clang includes ThinLTO support. However, ThinLTO; is under active development, and new features, improvements and bugfixes; are being added for the next release. For the latest ThinLTO support,; `build a recent version of clang and LLVM; <https://llvm.org/docs/CMake.html>`_. Linkers; -------; .. _linkers:; .. _linker:. ThinLTO is currently supported for the following linkers:. - **gold (via the gold-plugin)**:; Similar to monolithic LTO, this requires using; a `gold linker configured with plugins enabled; <https://llvm.org/docs/GoldPlugin.html>`_.; - **ld64**:; Starting with `Xcode 8 <https://developer.apple.com/xcode/>`_.; - **lld**:; Starting with r284050 for ELF, r298942 for COFF. Usage; =====",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:1440,scalab,scalable-and-incremental-lto,1440,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['scalab'],['scalable-and-incremental-lto']
Performance,"cribed in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12185,load,load,12185,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,2,['load'],['load']
Performance,"crmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:273229,load,load,273229,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"crmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:317363,load,load,317363,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"crmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:266840,load,load,266840,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"crmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 3. buffer/global/flat_atomic sc1=1; atomicrmw release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:311465,load,load,311465,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"cros include a comment for context. .. code-block:: objc. - (void)test {; NSString *string = NSLocalizedString(@""LocalizedString"", nil); // warn; NSString *string2 = NSLocalizedString(@""LocalizedString"", @"" ""); // warn; NSString *string3 = NSLocalizedStringWithDefaultValue(; @""LocalizedString"", nil, [[NSBundle alloc] init], nil,@""""); // warn; }. .. _optin-osx-cocoa-localizability-NonLocalizedStringChecker:. optin.osx.cocoa.localizability.NonLocalizedStringChecker (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns about uses of non-localized NSStrings passed to UI methods expecting localized NSStrings. .. code-block:: objc. NSString *alarmText =; NSLocalizedString(@""Enabled"", @""Indicates alarm is turned on"");; if (!isEnabled) {; alarmText = @""Disabled"";; }; UILabel *alarmStateLabel = [[UILabel alloc] init];. // Warning: User-facing text should use localized string macro; [alarmStateLabel setText:alarmText];. .. _optin-performance-GCDAntipattern:. optin.performance.GCDAntipattern; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Check for performance anti-patterns when using Grand Central Dispatch. .. _optin-performance-Padding:. optin.performance.Padding; """"""""""""""""""""""""""""""""""""""""""""""""""; Check for excessively padded structs. .. _optin-portability-UnixAPI:. optin.portability.UnixAPI; """"""""""""""""""""""""""""""""""""""""""""""""""; Finds implementation-defined behavior in UNIX/Posix functions. .. _security-checkers:. security; ^^^^^^^^. Security related checkers. .. _security-cert-env-InvalidPtr:. security.cert.env.InvalidPtr; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". Corresponds to SEI CERT Rules `ENV31-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV31-C.+Do+not+rely+on+an+environment+pointer+following+an+operation+that+may+invalidate+it>`_ and `ENV34-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV34-C.+Do+not+store+pointers+returned+by+certain+functions>`_. * **ENV31-C**:; Rule is about the possible problem with ``main`` function's third argument, environment pointer,; ""envp"".",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:19341,perform,performance,19341,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performance']
Performance,"cross LLVM versions. **LLVM_TOOLS_INSTALL_DIR**:STRING; The path to install the main LLVM tools, relative to the *CMAKE_INSTALL_PREFIX*.; Defaults to *CMAKE_INSTALL_BINDIR*. **LLVM_UTILS_INSTALL_DIR**:STRING; The path to install auxiliary LLVM utilities, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_INSTALL_UTILS* is enabled.; Defaults to *LLVM_TOOLS_INSTALL_DIR*. **LLVM_EXAMPLES_INSTALL_DIR**:STRING; The path for examples of using LLVM, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_BUILD_EXAMPLES* is enabled.; Defaults to ""examples"". CMake Caches; ============. Recently LLVM and Clang have been adding some more complicated build system; features. Utilizing these new features often involves a complicated chain of; CMake variables passed on the command line. Clang provides a collection of CMake; cache scripts to make these features more approachable. CMake cache files are utilized using CMake's -C flag:. .. code-block:: console. $ cmake -C <path to cache file> <path to sources>. CMake cache scripts are processed in an isolated scope, only cached variables; remain set when the main configuration runs. CMake cached variables do not reset; variables that are already set unless the FORCE option is specified. A few notes about CMake Caches:. - Order of command line arguments is important. - -D arguments specified before -C are set before the cache is processed and; can be read inside the cache file; - -D arguments specified after -C are set after the cache is processed and; are unset inside the cache file. - All -D arguments will override cache file settings; - CMAKE_TOOLCHAIN_FILE is evaluated after both the cache file and the command; line arguments; - It is recommended that all -D options should be specified *before* -C. For more information about some of the advanced build configurations supported; via Cache files see :doc:`AdvancedBuilds`. Executing the Tests; ===================. Testing is performed when the *check-all* target is bu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:38950,cache,cache,38950,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,cross-dso; Disable control flow integrity (CFI) checks for cross-DSO calls.; -fno-sanitize-coverage=<value>; Disable specified features of coverage instrumentation for Sanitizers; -fno-sanitize-memory-track-origins; Disable origins tracking in MemorySanitizer; -fno-sanitize-memory-use-after-dtor; Disable use-after-destroy detection in MemorySanitizer; -fno-sanitize-recover=<value>; Disable recovery for specified sanitizers; -fno-sanitize-stats Disable sanitizer statistics gathering.; -fno-sanitize-thread-atomics; Disable atomic operations instrumentation in ThreadSanitizer; -fno-sanitize-thread-func-entry-exit; Disable function entry/exit instrumentation in ThreadSanitizer; -fno-sanitize-thread-memory-access; Disable memory access instrumentation in ThreadSanitizer; -fno-sanitize-trap=<value>; Disable trapping for specified sanitizers; -fno-standalone-debug Limit debug information produced to reduce size of debug binary; -fno-strict-aliasing Disable optimizations based on strict aliasing rules (default); -fobjc-runtime=<value> Specify the target Objective-C runtime kind and version; -fprofile-exclude-files=<value>; Instrument only functions from files where names don't match all the regexes separated by a semi-colon; -fprofile-filter-files=<value>; Instrument only functions from files where names match any regex separated by a semi-colon; -fprofile-generate=<dirname>; Generate instrumented code to collect execution counts into a raw profile file in the directory specified by the argument. The filename uses default_%m.profraw pattern; (overridden by LLVM_PROFILE_FILE env var); -fprofile-generate; Generate instrumented code to collect execution counts into default_%m.profraw file; (overridden by '=' form of option or LLVM_PROFILE_FILE env var); -fprofile-instr-generate=<file_name_pattern>; Generate instrumented code to collect execution counts into the file whose name pattern is specified as the argument; (overridden by LLVM_PROFILE_FILE env var); -fprofile-instr-gene,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:181301,optimiz,optimizations,181301,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"cs:; """""""""""""""""""". The '``llvm.vp.store``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.store``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way. The; alignment of the operation (corresponding to the '``alignment``' operand of; '``llvm.masked.store``') is specified by the ``align`` parameter attribute (see; above). If it is not provided then the ABI alignment of the type of the; '``value``' operand as specified by the :ref:`datalayout; string<langref_datalayout>` is used instead. Examples:; """""""""""""""""". .. code-block:: text. call void @llvm.vp.store.v8i8.p0(<8 x i8> %val, ptr align 4 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, the call above is lane-wise equivalent to the call below. call void @llvm.masked.store.v8i8.p0(<8 x i8> %val, ptr %ptr, i32 4, <8 x i1> %mask). .. _int_experimental_vp_strided_load:. '``llvm.experimental.vp.strided.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.experimental.vp.strided.load.v4f32.i64(ptr %ptr, i64 %stride, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.experimental.vp.strided.load.nxv2i16.i64(ptr %ptr, i64 %stride, <vscale x 2 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, scalar values from; memory locations evenly spaced apart by '``stride``' number of bytes, starting from '``ptr``'. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the stride; value expressed in bytes. The third operand is a vector of boolean values; with the same number of elements as the return type. The fourth is the explicit; vector length of the operation. The base pointer underlying type matches the type of the scalar; elements of the return operand. The :ref:`align <attr_align>` parameter attribute can",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:787068,load,load,787068,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ct and position the components inside as; for any volume:. ~~~{.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ~~~. Note that components cannot be declared as ""overlapping"" and that a; component can be an assembly volume. For existing flat volume; structures, one can define assemblies to force a hierarchical structure; therefore optimizing the performance. Usage of assemblies does NOT imply; penalties in performance, but in some cases, it can be observed that it; is not as performing as bounding the structure in a container volume; with a simple shape. Choosing a normal container is therefore; recommended whenever possible. \image html geometry006.png ""Assemblies of volumes"" width=600px. \anchor GP01c; ### Geometrical Transformations. All geometrical transformations handled by the modeller are provided as; a built-in package. This was designed to minimize memory requirements; and optimize performance of point/vector master-to-local and; local-to-master computation. We need to have in mind that a; transformation in **`TGeo`** has two major use-cases. The first one is; for defining the placement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used for; positioning volume daughters, then: `MASTER = T * LOCAL`. Therefore `T `is used to perform a local to master conversion, while; `T-1` for a master to local conversion. The second use case is the; computation of the global transformation of a given object in the; geometry. Since the geometry is built as 'volumes-inside-volumes', the; global transformation represents the pile-up of all local; transformations in the corresponding branch. Once a given object in the; hierarchy becomes the current one, the conversion from master to local; coordinates or the other way around can be done from the manager class. A g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:51487,optimiz,optimize,51487,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"ct control flow edge (or it cannot be mispredicted in; the case of a filled RSB) and so it is also protected from variant #1 style; attacks. However, some architectures, micro-architectures, or vendors do not; employ the retpoline mitigation, and on future x86 hardware (both Intel and; AMD) it is expected to become unnecessary due to hardware-based mitigation. When not using a retpoline, these edges will need independent protection from; variant #1 style attacks. The analogous approach to that used for conditional; control flow should work:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; switch (condition) {; case 0:; // Assuming ?: is implemented using branchless logic...; predicate_state = (condition != 0) ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; break;. case 1:; predicate_state = (condition != 1) ? all_zeros_mask : predicate_state;; // ... more code ...; //; // Alternative: Harden the loaded value; int value2 = *pointer2 & predicate_state;; leak(value2);; break;. // ...; }; }; ```. The core idea remains the same: validate the control flow using data-flow and; use that validation to check that loads cannot leak information along; misspeculated paths. Typically this involves passing the desired target of such; control flow across the edge and checking that it is correct afterwards. Note; that while it is tempting to think that this mitigates variant #2 attacks, it; does not. Those attacks go to arbitrary gadgets that don't include the checks. ### Variant #1.1 and #1.2 attacks: ""Bounds Check Bypass Store"". Beyond the core variant #1 attack, there are techniques to extend this attack.; The primary technique is known as ""Bounds Check Bypass Store"" and is discussed; in this research pape",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:10181,load,loaded,10181,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"ct::Paint()`**. #### Physical IDs. TVirtualViewer3D provides for two methods of object addition:. ``` {.cpp}; virtual Int_t AddObject(const TBuffer3D &buffer,; Bool_t * addChildren = 0); virtual Int_t AddObject(UInt_t physicalID,; const TBuffer3D & buffer,; Bool_t *addChildren = 0); ```. If you use the first (simple) case a viewer using logical/physical pairs; will generate sequential IDs for each physical object internally. Scene; rebuilds will require destruction and recreation of all physical; objects. For the second you can specify an identifier from the client; side, which must be unique and stable - i.e. the IDs of a published; object is consistent, regardless of changes in termination of contained; child geometry branches. In this case the viewer can safely cache the; physical objects across scene rebuilds, discarding those no longer of; interest. #### Child Objects. In many geometries there is a rigid containment hierarchy, and so if the; viewer is not interested in a certain object due to limits/size then it; will also not be interest in any of the contained branch of siblings.; Both `TBuffer3D::AddObject()` methods have an `addChildren` return; parameter. The viewer will complete this (if passed) indicating if; children of the object just sent are worth sending. #### Recycling TBuffer3D. Once add `TBuffer3D::AddObject()` has been called, the contents are; copied to the viewer's internal data structures. You are free to destroy; this **`TBuffer3D`**, or recycle it for the next object if suitable. #### Examples. For an example of a simple geometry, working in master reference frame; examine the code under `$ROOTSYS/g3d`. For a more complex example, which; works in both master and local frames, and uses logical`/`physical; division of shape geometry and placement, examine the code under; `$ROOTSYS/geom` - in particular **`TGeoShape`** hierarchy, and the; painter object **`TGeoPainter`** (under geopainter) where the; negotiation with the viewer is performed.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:139508,perform,performed,139508,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['performed']
Performance,"cted by the ""Advanced Drawing Tool"" panel that pops up when clicking the ""Advanced"" button, are:. * *Contour*: to plot the confidence contour of two chosen parameters. One can select the number of points to draw the contour; (more points might require more time to compute it), the parameters and the desired confidence level . * *Scan* : to plot a scan of the minimization function (likelihood or chi-squared) around the minimum as function of the chosen parameter. * *Conf Interval* : to plot the confidence interval of the fitted function as a filled coloured band around its central value.; One can select the desired confidence level for the band to be plotted. ### Print Options. This set of options specifies the amount of feedback printed on the; root command line after performed fits. *Verbose'* - prints fit results after each iteration. *Quiet'* - no fit information is printed. *Default'* - between Verbose and Quiet. ### Command Buttons. *Fit button* - performs a fit taking different option settings via the; Fit Panel interface. *Reset* - sets the GUI elements and related fit settings to the; default ones. *Close* - closes the Fit panel window. ### Minimization Options. With this tab one can select specific options for minimization. These include. * The minimizer library ( *Minuit*, *Minuit2*, *Fumili*, *GSL*, *Genetics* ); * The method (algorithm) for minimization. For example for Minuit one can choose between (*Migrad*, *Simplex* or *Scan*); * Error definition; * Minimization tolerance; * Number of iterations/function calls; * Print Level: (*Default*, *Verbose* or *Quiet*). ## New ROOT::Fit classes. The fitting of the data objects in ROOT, histograms, graphs and tree is performed via some common classes,; which are defined in the `ROOT::Fit` namespace.; These classes can be classified in the following groups:. * User classes driving the fit: `ROOT::Fit::Fitter` for executing the fit, `ROOT::Fit::FitConfig` for configuring the fit,; 	`ROOT::Fit::ParameterSettings",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:25667,perform,performs,25667,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['performs']
Performance,"cted, we walk up the path to find the first node; before inlining was started and enqueue it on the WorkList with a special; ReplayWithoutInlining bit added to it (ExprEngine::replayWithoutInlining). The; path is then re-analyzed from that point without inlining that particular call. Deciding When to Inline; ^^^^^^^^^^^^^^^^^^^^^^^. In general, the analyzer attempts to inline as much as possible, since it; provides a better summary of what actually happens in the program. There are; some cases, however, where the analyzer chooses not to inline:. - If there is no definition available for the called function or method. In; this case, there is no opportunity to inline. - If the CFG cannot be constructed for a called function, or the liveness; cannot be computed. These are prerequisites for analyzing a function body,; with or without inlining. - If the LocationContext chain for a given ExplodedNode reaches a maximum cutoff; depth. This prevents unbounded analysis due to infinite recursion, but also; serves as a useful cutoff for performance reasons. - If the function is variadic. This is not a hard limitation, but an engineering; limitation. Tracked by: <rdar://problem/12147064> Support inlining of variadic functions. - In C++, constructors are not inlined unless the destructor call will be; processed by the ExprEngine. Thus, if the CFG was built without nodes for; implicit destructors, or if the destructors for the given object are not; represented in the CFG, the constructor will not be inlined. (As an exception,; constructors for objects with trivial constructors can still be inlined.); See ""C++ Caveats"" below. - In C++, ExprEngine does not inline custom implementations of operator 'new'; or operator 'delete', nor does it inline the constructors and destructors; associated with these. See ""C++ Caveats"" below. - Calls resulting in ""dynamic dispatch"" are specially handled. See more below. - The FunctionSummaries map stores additional information about declarations,; som",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/IPA.rst:7703,perform,performance,7703,interpreter/llvm-project/clang/docs/analyzer/developer-docs/IPA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/IPA.rst,1,['perform'],['performance']
Performance,"ctelement <2 x i32> %v, i32 1 ; poison; %add = add i32 %a, %a ; undef. %v.fr = freeze <2 x i32> %v ; element-wise freeze; %d = extractelement <2 x i32> %v.fr, i32 0 ; not undef; %add.f = add i32 %d, %d ; even number. ; branching on frozen value; %poison = add nsw i1 %k, undef ; poison; %c = freeze i1 %poison; br i1 %c, label %foo, label %bar ; non-deterministic branch to %foo or %bar. .. _i_call:. '``call``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = [tail | musttail | notail ] call [fast-math flags] [cconv] [ret attrs] [addrspace(<num>)]; <ty>|<fnty> <fnptrval>(<function args>) [fn attrs] [ operand bundles ]. Overview:; """""""""""""""""". The '``call``' instruction represents a simple function call. Arguments:; """""""""""""""""""". This instruction requires several arguments:. #. The optional ``tail`` and ``musttail`` markers indicate that the optimizers; should perform tail call optimization. The ``tail`` marker is a hint that; `can be ignored <CodeGenerator.html#tail-call-optimization>`_. The; ``musttail`` marker means that the call must be tail call optimized in order; for the program to be correct. This is true even in the presence of; attributes like ""disable-tail-calls"". The ``musttail`` marker provides these; guarantees:. #. The call will not cause unbounded stack growth if it is part of a; recursive cycle in the call graph.; #. Arguments with the :ref:`inalloca <attr_inalloca>` or; :ref:`preallocated <attr_preallocated>` attribute are forwarded in place.; #. If the musttail call appears in a function with the ``""thunk""`` attribute; and the caller and callee both have varargs, then any unprototyped; arguments in register or memory are forwarded to the callee. Similarly,; the return value of the callee is returned to the caller's caller, even; if a void return type is in use. Both markers imply that the callee does not access allocas from the caller.; The ``tail`` marker additionally implies that the callee does not access; varargs from the caller. C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:472751,optimiz,optimization,472751,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"ctices and general; information about packaging LLVM. If you are new to CMake you may find the :doc:`CMake` or :doc:`CMakePrimer`; documentation useful. Some of the things covered in this document are the inner; workings of the builds described in the :doc:`AdvancedBuilds` document. General Distribution Guidance; =============================. When building a distribution of a compiler it is generally advised to perform a; bootstrap build of the compiler. That means building a ""stage 1"" compiler with; your host toolchain, then building the ""stage 2"" compiler using the ""stage 1""; compiler. This is done so that the compiler you distribute benefits from all the; bug fixes, performance optimizations and general improvements provided by the; new compiler. In deciding how to build your distribution there are a few trade-offs that you; will need to evaluate. The big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving symbols; at process launch time, which can be very slow for C++ code. .. _shared_libs:. .. warning::; One very important note: Distributions should never be built using the; *BUILD_SHARED_LIBS* CMake option. That option exists for optimizing developer; workflow only. Due to design and implementation decisions, LLVM relies on; global data which can end up being duplicate",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:1375,perform,performance,1375,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['perform'],['performance']
Performance,"ction (0) cache. The ``rw``, ``locality`` and ``cache type``; arguments must be constant integers. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. In; particular, prefetches cannot trap and do not produce a value. On; targets that support this intrinsic, the prefetch can provide hints to; the processor cache for better performance. '``llvm.pcmarker``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.pcmarker(i32 <id>). Overview:; """""""""""""""""". The '``llvm.pcmarker``' intrinsic is a method to export a Program; Counter (PC) in a region of code to simulators and other tools. The; method is target specific, but it is expected that the marker will use; exported symbols to transmit the PC of the marker. The marker makes no; guarantees that it will remain with any specific instruction after; optimizations. It is possible that the presence of a marker will inhibit; optimizations. The intended use is to be inserted after optimizations to; allow correlations of simulation runs. Arguments:; """""""""""""""""""". ``id`` is a numerical id identifying the marker. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. Backends; that do not support this intrinsic may ignore it. '``llvm.readcyclecounter``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i64 @llvm.readcyclecounter(). Overview:; """""""""""""""""". The '``llvm.readcyclecounter``' intrinsic provides access to the cycle; counter register (or similar low latency, high accuracy clocks) on those; targets that support it. On X86, it should map to RDTSC. On Alpha, it; should map to RPCC. As the backing counters overflow quickly (on the; order of 9 seconds on alpha), this should only be used for small; timings. Semantics:; """""""""""""""""""". When directly supported, reading the cycle counter should not modify any; memory. Implementations are allowed to either return an application; specific value or a system wide value. On ba",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:524665,optimiz,optimizations,524665,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"ction Addresses; --------------------------------------. The addresses of :ref:`global variables <globalvars>` and; :ref:`functions <functionstructure>` are always implicitly valid; (link-time) constants. These constants are explicitly referenced when; the :ref:`identifier for the global <identifiers>` is used and always have; :ref:`pointer <t_pointer>` type. For example, the following is a legal LLVM; file:. .. code-block:: llvm. @X = global i32 17; @Y = global i32 42; @Z = global [2 x ptr] [ ptr @X, ptr @Y ]. .. _undefvalues:. Undefined Values; ----------------. The string '``undef``' can be used anywhere a constant is expected, and; indicates that the user of the value may receive an unspecified; bit-pattern. Undefined values may be of any type (other than '``label``'; or '``void``') and be used anywhere a constant is permitted. .. note::. A '``poison``' value (described in the next section) should be used instead of; '``undef``' whenever possible. Poison values are stronger than undef, and; enable more optimizations. Just the existence of '``undef``' blocks certain; optimizations (see the examples below). Undefined values are useful because they indicate to the compiler that; the program is well defined no matter what value is used. This gives the; compiler more freedom to optimize. Here are some examples of; (potentially surprising) transformations that are valid (in pseudo IR):. .. code-block:: llvm. %A = add %X, undef; %B = sub %X, undef; %C = xor %X, undef; Safe:; %A = undef; %B = undef; %C = undef. This is safe because all of the output bits are affected by the undef; bits. Any output bit can have a zero or one depending on the input bits. .. code-block:: llvm. %A = or %X, undef; %B = and %X, undef; Safe:; %A = -1; %B = 0; Safe:; %A = %X ;; By choosing undef as 0; %B = %X ;; By choosing undef as -1; Unsafe:; %A = undef; %B = undef. These logical operations have bits that are not always affected by the; input. For example, if ``%X`` has a zero bit, then the o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:191200,optimiz,optimizations,191200,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"ction and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis information to reason about loads and stores. .. _the clients:. Clients for debugging and evaluation of implementations; -------------------------------------------------------. These passes are useful for evaluating the various alias analysis; implementations. You can use them with commands like:. .. code-block:: bash. % opt -ds-aa -aa-eval foo.bc -disable-output -stats. The ``-print-alias-sets`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-print-alias-sets`` pass is exposed as part of the ``opt`` tool to print; out the Alias Sets formed by the `AliasSetTracker`_ class. This is useful if; you're using the ``AliasSetTracker`` class. To use it, use something like:. .. code-block:: bash. % opt -ds-aa -print-alias-sets -disable-output. The ``-aa-eval`` pass; ^^^^^^^^^^^^^^^^^^^^^. The ``-aa-eval`` pass simply iterates through all pairs of pointers in a; function and asks an alias analysis whether or not the pointers alias. This; gives an indication of the precision of the alias analysis. Statistics are; printed indicating the percent of no/may/must aliases found (a more precise; algorithm will have a lower number of may aliases). Memory Dependence Analysis; ==========================. .. note::. We are currently in the process of migrating things from; ``MemoryDependenceAnalysis`` to :doc:`MemorySSA`. Please try to use; that instead. If you're just looking to be a client of alias analysis information, consider; using the Memory Dependence Analysis interface instead. MemDep is a lazy,; caching layer on top of alias analysis that is able to answer the question of; what preceding memory operations a given instruction depends on, either at an; intra- or inter-block level. Because of its laziness and caching policy, using; MemDep can be a significant performance win over accessing alias analysis; directly.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:30565,perform,performance,30565,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['perform'],['performance']
Performance,"ction attributes are simple keywords that follow the type specified.; If multiple attributes are needed, they are space separated. For; example:. .. code-block:: llvm. define void @f() noinline { ... }; define void @f() alwaysinline { ... }; define void @f() alwaysinline optsize { ... }; define void @f() optsize { ... }. ``alignstack(<n>)``; This attribute indicates that, when emitting the prologue and; epilogue, the backend should forcibly align the stack pointer.; Specify the desired alignment, which must be a power of two, in; parentheses.; ``""alloc-family""=""FAMILY""``; This indicates which ""family"" an allocator function is part of. To avoid; collisions, the family name should match the mangled name of the primary; allocator function, that is ""malloc"" for malloc/calloc/realloc/free,; ""_Znwm"" for ``::operator::new`` and ``::operator::delete``, and; ""_ZnwmSt11align_val_t"" for aligned ``::operator::new`` and; ``::operator::delete``. Matching malloc/realloc/free calls within a family; can be optimized, but mismatched ones will be left alone.; ``allockind(""KIND"")``; Describes the behavior of an allocation function. The KIND string contains comma; separated entries from the following options:. * ""alloc"": the function returns a new block of memory or null.; * ""realloc"": the function returns a new block of memory or null. If the; result is non-null the memory contents from the start of the block up to; the smaller of the original allocation size and the new allocation size; will match that of the ``allocptr`` argument and the ``allocptr``; argument is invalidated, even if the function returns the same address.; * ""free"": the function frees the block of memory specified by ``allocptr``.; Functions marked as ""free"" ``allockind`` must return void.; * ""uninitialized"": Any newly-allocated memory (either a new block from; a ""alloc"" function or the enlarged capacity from a ""realloc"" function); will be uninitialized.; * ""zeroed"": Any newly-allocated memory (either a new block fro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:78126,optimiz,optimized,78126,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"ction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.udot2 Provides direct access to v_dot2_u32_u16 across targets which; support such instructions. This performs unsigned dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output. llvm.amdgcn.udot4 Provides direct access to v_dot4_u32_u8 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.udot8 Provides direct access to v_dot8_u32_u4 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 8 4bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.sdot2 Provides direct access to v_dot2_i32_i16 across targets which; support such instructions. This performs signed dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output.; When applicable (e.g. no clamping), this is lowered into; v_dot2c_i32_i16 for targets which support it. llvm.amdgcn.sdot4 Provides direct access to v_dot4_i32_i8 across targets which; support such instructions. This performs signed dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output.; When applicable (i.e. no clamping / operand modifiers), this is lowered; into v_dot4c_i32_i8 for targets which support it.; RDNA3 does not offer v_dot4_i32_i8, and rather offers; v_dot4_i32_iu8 which has operands to hold the signednes",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:39794,perform,performs,39794,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performs']
Performance,"ction*,Option_t*). #### TTreeCache. The TTreeCache is now enabled by default. The default size of the TTreeCache; is the estimated size of a cluster size for the TTree. The TTreeCache; prefilling is also enabled by default; when in learning phase rather than; reading each requested branch individually, the TTreeCache will read all the; branches thus trading off the latencies inherent to multiple small reads for; the potential of requesting more data than needed by read from the disk or; server the baskets for too many branches. The default behavior can be changed by either updating one of the rootrc files; or by setting environment variables. The rootrc files, both the global and the; local ones, now support the following the resource variable TTreeCache.Size; which set the default size factor for auto sizing TTreeCache for TTrees. The; estimated cluster size for the TTree and this factor is used to give the cache; size. If option is set to zero auto cache creation is disabled and the default; cache size is the historical one (equivalent to factor 1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally sever",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:13478,cache,cache,13478,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,2,['cache'],['cache']
Performance,"ctionDAG based; instruction selector. Portions of the DAG instruction selector are generated from the target; description (``*.td``) files. Our goal is for the entire instruction selector; to be generated from these ``.td`` files, though currently there are still; things that require custom C++ code. `GlobalISel <https://llvm.org/docs/GlobalISel/index.html>`_ is another; instruction selection framework. .. _SelectionDAG:. Introduction to SelectionDAGs; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The SelectionDAG provides an abstraction for code representation in a way that; is amenable to instruction selection using automatic techniques; (e.g. dynamic-programming based optimal pattern matching selectors). It is also; well-suited to other phases of code generation; in particular, instruction; scheduling (SelectionDAG's are very close to scheduling DAGs post-selection).; Additionally, the SelectionDAG provides a host representation where a large; variety of very-low-level (but target-independent) `optimizations`_ may be; performed; ones which require extensive information about the instructions; efficiently supported by the target. The SelectionDAG is a Directed-Acyclic-Graph whose nodes are instances of the; ``SDNode`` class. The primary payload of the ``SDNode`` is its operation code; (Opcode) that indicates what operation the node performs and the operands to the; operation. The various operation node types are described at the top of the; ``include/llvm/CodeGen/ISDOpcodes.h`` file. Although most operations define a single value, each node in the graph may; define multiple values. For example, a combined div/rem operation will define; both the dividend and the remainder. Many other situations require multiple; values as well. Each node also has some number of operands, which are edges to; the node defining the used value. Because nodes may define multiple values,; edges are represented by instances of the ``SDValue`` class, which is a; ``<SDNode, unsigned>`` pair, indicating the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:34052,optimiz,optimizations,34052,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['optimiz', 'perform']","['optimizations', 'performed']"
Performance,"ctor L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0)`` is required to; ensure synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:209137,cache,cache,209137,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"ctor length, so this operation; may map to multiple SelectionDAG nodes including ``shuffle_vector``,; ``concat_vectors``, ``insert_subvector``, and ``extract_subvector``. Prior to the existence of the Legalize passes, we required that every target; `selector`_ supported and handled every operator and type even if they are not; natively supported. The introduction of the Legalize phases allows all of the; canonicalization patterns to be shared across targets, and makes it very easy to; optimize the canonicalized code because it is still in the form of a DAG. .. _optimizations:; .. _Optimize SelectionDAG:; .. _selector:. SelectionDAG Optimization Phase: the DAG Combiner; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The SelectionDAG optimization phase is run multiple times for code generation,; immediately after the DAG is built and once after each legalization. The first; run of the pass allows the initial code to be cleaned up (e.g. performing; optimizations that depend on knowing that the operators have restricted type; inputs). Subsequent runs of the pass clean up the messy code generated by the; Legalize passes, which allows Legalize to be very simple (it can focus on making; code legal instead of focusing on generating *good* and legal code). One important class of optimizations performed is optimizing inserted sign and; zero extension instructions. We currently use ad-hoc techniques, but could move; to more rigorous techniques in the future. Here are some good papers on the; subject:. ""`Widening integer arithmetic <http://www.eecs.harvard.edu/~nr/pubs/widen-abstract.html>`_"" :raw-html:`<br>`; Kevin Redwine and Norman Ramsey :raw-html:`<br>`; International Conference on Compiler Construction (CC) 2004. ""`Effective sign extension elimination <http://portal.acm.org/citation.cfm?doid=512529.512552>`_"" :raw-html:`<br>`; Motohiro Kawahito, Hideaki Komatsu, and Toshio Nakatani :raw-html:`<br>`; Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:45451,perform,performing,45451,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['optimiz', 'perform']","['optimizations', 'performing']"
Performance,"ctor of RooFit objects using a loop, you don't; have to think about indexing them properly because the `beginLoop()` function; takes care of that. Simply call this function, place your function call in a; scope and after the scope ends, the loop will also end. - `addToCodeBody()` helps add things to the body of the C++ function that; you're creating. It takes whatever string is computed in its arguments and; adds it to the overall function string (which will later be just-in-time; compiled). The `addToCodeBody()` function is important since not everything; can be added in-line and this function helps split the code into multiple; lines. ### Step 3. analyticalIntegral() Use Case. > Besides the `evaluate()` function, this tutorial illustrates how the; `analyticalIntegral()` can be updated. This highly dependent on the class that; is being transformed for AD support, but will be necessary in those specific; instances. Let's consider a fictional class RooFoo, that performs some arbitrary; mathematical operations called 'Foo' (as seen in doFoo() function below). > Note that doFoo is a simplified example, in many cases the mathematical; operations are not limited to a single function, so they need to be spotted; within the `evaluate()` function. ``` {.cpp}; class RooFoo : public RooAbsReal {; int a;; int b;; int doFoo() { return a* b + a + b; }; int integralFoo() { return /* whatever */;}; public:; // Other functions...; double evaluate() override {; // Do some bookkeeping; return doFoo();; };; double analyticalIntegral(Int_t code, const char* rangeName) override {; // Select the right paths for integration using codes or whatever.; return integralFoo();; }; };; ```. \note All RooFit classes are deriving from the RooAbsReal object, but; its details are not relevant to the current example. Note how the `evaluate()` function overrides the `RooAbsReal` for the RooFoo; class. Similarly, the `analyticalIntegral()` function has also been overridden; from the `RooAbsReal` class. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:19034,perform,performs,19034,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,1,['perform'],['performs']
Performance,"ctor/assignment operator.; Source: Scott Meyers ""Effective C++"", item 11: Prevent exceptions from; leaving destructors. class C {; int *p; // warn; public:; C() { p = new int; }; ~C() { delete p; }; };. WinAPI. Name, DescriptionExampleProgress. WinAPI.CreateProcess; (C); CreateProcess(): if the first parameter ; lpApplicationName is NULL then the executable name must be in the; white space-delimited string pointed to by lpCommandLine.; If the executable or path name has a space in it, there is a risk that a; different executable could be run because of the way the function parses; spaces.; Source: ; MSDN: CreateProcess function, Security Remarks. #include <windows.h>. void test() {; STARTUPINFO si;; PROCESS_INFORMATION pi;; CreateProcess(NULL, TEXT(""C:\\Program Files\\App -L -S""),; NULL, NULL, TRUE, 0, NULL, NULL, &si, );; // warn; }. WinAPI.LoadLibrary; (C); The SearchPath() function is used to retrieve a path to a DLL for; a subsequent LoadLibrary() call.; Source: ; MSDN: LoadLibrary function, Security Remarks. #include <windows.h>. HINSTANCE test() {; char filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:26586,Load,LoadLibrary,26586,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,1,['Load'],['LoadLibrary']
Performance,"ctorization [2]_. 4. Support multiple candidates efficiently. In particular, similar candidates; related to a range of possible VF's and UF's must be represented efficiently.; Potential versioning needs to be supported efficiently. 5. Support vectorizing idioms, such as interleaved groups of strided loads or; stores. This is achieved by modeling a sequence of output instructions using; a ""Recipe"", which is responsible for computing its cost and generating its; code. 6. Encapsulate Single-Entry Single-Exit regions (SESE). During vectorization; such regions may need to be, for example, predicated and linearized, or; replicated VF*UF times to handle scalarized and predicated instructions.; Innerloops are also modelled as SESE regions. 7. Support instruction-level analysis and transformation, as part of Planning; Step 2.b: During vectorization instructions may need to be traversed, moved,; replaced by other instructions or be created. For example, vector idiom; detection and formation involves searching for and optimizing instruction; patterns. Definitions; ===========; The low-level design of VPlan comprises of the following classes. :LoopVectorizationPlanner:; A LoopVectorizationPlanner is designed to handle the vectorization of a loop; or a loop nest. It can construct, optimize and discard one or more VPlans,; each VPlan modelling a distinct way to vectorize the loop or the loop nest.; Once the best VPlan is determined, including the best VF and UF, this VPlan; drives the generation of output IR. :VPlan:; A model of a vectorized candidate for a given input IR loop or loop nest. This; candidate is represented using a Hierarchical CFG. VPlan supports estimating; the cost and driving the generation of the output IR code it represents. :Hierarchical CFG:; A control-flow graph whose nodes are basic-blocks or Hierarchical CFG's. The; Hierarchical CFG data structure is similar to the Tile Tree [5]_, where; cross-Tile edges are lifted to connect Tiles instead of the original",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst:3919,optimiz,optimizing,3919,interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,1,['optimiz'],['optimizing']
Performance,"ctorize.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.enable"", i1 1}. '``llvm.loop.vectorize.predicate.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables creating predicated instructions; for the loop, which can enable folding of the scalar epilogue loop into the; main loop. The first operand is the string; ``llvm.loop.vectorize.predicate.enable`` and the second operand is a bit. If; the bit operand value is 1 vectorization is enabled. A value of 0 disables; vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.predicate.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.predicate.enable"", i1 1}. '``llvm.loop.vectorize.scalable.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables scalable vectorization for the; loop, and only has any effect if vectorization for the loop is already enabled.; The first operand is the string ``llvm.loop.vectorize.scalable.enable``; and the second operand is a bit. If the bit operand value is 1 scalable; vectorization is enabled, whereas a value of 0 reverts to the default fixed; width vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.scalable.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.scalable.enable"", i1 1}. '``llvm.loop.vectorize.width``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata sets the target width of the vectorizer. The first; operand is the string ``llvm.loop.vectorize.width`` and the second; operand is an integer specifying the width. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.width"", i32 4}. Note that setting ``llvm.loop.vectorize.width`` to 1 disables; vectorization of the loop. If ``llvm.loop.vectorize.width`` is set to; 0 or if the loop does not have this metadata the width will be; determined automatically. '``llvm.loop.vectorize.followup_vectorized``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:298762,scalab,scalable,298762,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"ctually; does the event reaction. Its prototype is where `px` and `py` are the; coordinates at which the event occurred, except if the event is a key; press, in which case `px` contains the key code. ``` {.cpp}; void ExecuteEvent(Int_t event, Int_t px, Int_t py);; ```. Where `event` is the event that occurs and is one of the following; (defined in `Buttons.h`):. ``` {.cpp}; kNoEvent, kButton1Down, kButton2Down,; kButton3Down, kKeyDown, kButton1Up,; kButton2Up, kButton3Up, kButton1Motion,; kButton2Motion, kButton3Motion, kKeyPress,; kButton1Locate, kButton2Locate, kButton3Locate,; kKeyUp, kButton1Double, kButton2Double,; kButton3Double, kMouseMotion, kMouseEnter,; kMouseLeave; ```. We hope the names are self-explanatory. Designing an `ExecuteEvent` method is not very easy, except if one wants; very basic treatment. We will not go into that and let the reader refer; to the sources of classes like **`TLine`** or **`TBox`**. Go and look at; their `ExecuteEvent` method! We can nevertheless give some reference to; the various actions that may be performed. For example, one often wants; to change the shape of the cursor when passing on top of an object. This; is done with the `SetCursor` method:. ``` {.cpp}; gPad->SetCursor(cursor); ```. The argument `cursor` is the type of cursor. It may be:. ``` {.cpp}; kBottomLeft, kBottomRight, kTopLeft,; kTopRight, kBottomSide, kLeftSide,; kTopSide, kRightSide, kMove,; kCross, kArrowHor, kArrowVer,; kHand, kRotate, kPointer,; kArrowRight, kCaret, kWatch; ```. They are defined in `TVirtualX.h` and again we hope the names are; self-explanatory. If not, try them by designing a small class. It may; derive from something already known like **`TLine`**. Note that the `ExecuteEvent()` functions may in turn; invoke such; functions for other objects, in case an object is drawn using other; objects. You can also exploit at best the virtues of inheritance. See; for example how the class **`TArrow`** (derived from **`TLine`**) use or; redefine th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:11715,perform,performed,11715,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['performed']
Performance,"cture-related so this list; is an addition to the potential checkers; list. If you are interested in tackling one of these, please send an email; to the cfe-dev; mailing list to notify other members of the community. Release checkers from ""alpha""; New checkers which were contributed to the analyzer,; but have not passed a rigorous evaluation process,; are committed as ""alpha checkers"" (from ""alpha version""),; and are not enabled by default.; Ideally, only the checkers which are actively being worked on should be in; ""alpha"",; but over the years the development of many of those has stalled.; Such checkers should either be improved; up to a point where they can be enabled by default,; or removed from the analyzer entirely. ; alpha.security.ArrayBound and; alpha.security.ArrayBoundV2; Array bounds checking is a desired feature,; but having an acceptable rate of false positives might not be possible; without a proper; loop widening support.; Additionally, it might be more promising to perform index checking based on; tainted index values.; (Difficulty: Medium). alpha.unix.StreamChecker; A SimpleStreamChecker has been presented in the Building a Checker in 24; Hours talk; (slides; video).; This alpha checker is an attempt to write a production grade stream checker.; However, it was found to have an unacceptably high false positive rate.; One of the found problems was that eagerly splitting the state; based on whether the system call may fail leads to too many reports.; A delayed split where the implication is stored in the state; (similarly to nullability implications in TrustNonnullChecker); may produce much better results.; (Difficulty: Medium). Improve C++ support; ; Handle construction as part of aggregate initialization.; Aggregates; are objects that can be brace-initialized without calling a; constructor (that is, ; CXXConstructExpr does not occur in the AST),; but potentially calling; constructors for their fields and base classes; These; constructors of sub-objec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/open_projects.html:1157,perform,perform,1157,interpreter/llvm-project/clang/www/analyzer/open_projects.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/open_projects.html,2,['perform'],['perform']
Performance,"cuted is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directly accesses the; kernel argument memory in the same way constant memory is accessed. (Note; that the HSA specification allows an implementation to copy the kernel; argument contents to another location that is accessed by the kernel.); 5. An AQL kernel dispatch packet is created on the AQL queue. The HSA compatible; runtime api uses 64-bit atomic operations to reserve space in the AQL queue; for the packet. The packet must be set up, and the final write must use an; atomic store release to set the packet kind to ensure the packet contents are; visible to the kernel agent. AQL defines a doorbell signal mechanism to; notify the kernel agent that the AQL queue has been updated. These rules, and; the layout of the AQL queue and kernel dispatch packet is defined in the *HSA; System Architecture Specification* [HSA]_.; 6. A kernel dispatch packet includes information about the actual dispatch,; such as grid and work-group size, together with information from the code; object about the kernel, such as segment sizes. The HSA compatible runtime; queries on the kernel symbol can be used to obtain the code object values; which are recorded in the :ref:`amdgpu-amdhsa-code-object-metadata`.; 7. CP executes micro-code and is responsible for detecting and setting",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:151024,queue,queue,151024,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-Cpu COMMAND testBackpropagationCpu). # DNN - BackpropagationDL CPU; ROOT_EXECUTABLE(testBackpropagationDLCpu TestBackpropagationDLCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-DL-Cpu COMMAND testBackpropagationDLCpu). # DNN - Batch normalization; ROOT_EXECUTABLE(testBatchNormalizationCpu TestBatchNormalizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cpu COMMAND testBatchNormalizationCpu). # DNN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regressio,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:5810,Optimiz,Optimization,5810,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization']
Performance,"d '::' and any; of the STL container names was inadvertently omitted (in case of classes; that are part of the TTree but had only a base and no member or in some; cases where it had only object data members.; Prevent storing a 2nd time an object non derived from TObject in the case; where the object is both the top level object of branch and has; some of it sub-object containing a pointer back to the object. (This was; actually activated in v5.18).; ; void TBranch::DeleteBaskets(Option_t* option); new function which loops on all branch baskets. If the file where branch buffers reside is writable, free the disk space associated to the baskets of the branch, then call Reset(). If the option contains ""all"", delete also the baskets for the subbranches. The branch is reset.; NOTE that this function must be used with extreme care. Deleting branch baskets; fragments the file and may introduce inefficiencies when adding new entries; in the Tree or later on when reading the Tree. Protect TTree::GetCurrentFile in case the current directory is gROOT.; This case may happen when a TChain calls TChain::Process and no files have been; connected to the chain yet, but a TFile has been opened meanwhile.; Remove the calls to MapObject introduce in revision 21384 when; are unnecessary hence restoring lost performance in case where; the TTree contains many simple type (double, int, etc.); In TBranchElement::Streamer when writing, call ForceWriteInfo; not only for the TStreamerInfo directly concerning this branch; but also (in the case of the top level branch of a split TClonesArray; or a split STL container) call ForceWriteInfo for the class of; the value. This omission meant that slow CloneTree was (fataly) missing in; some cases the copy of the TStreamerInfo for class that are part; part of the TTree but had only a base and no member or in; some cases where it had only object data members.; Fix the return value of the lookup in TChainIndex; when the value searched for does not exist. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html:9169,perform,performance,9169,tree/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html,2,['perform'],['performance']
Performance,"d *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. GFX940, GFX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:294062,load,load,294062,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - workgroup - generic; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:213466,load,load,213466,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:346967,load,load,346967,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind. ; Compute pointers into A, B, and C; %ptrA = getelementptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25236,perform,performed,25236,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['perform'],['performed']
Performance,"d AST file (one per module) and use those AST; modules. From an implementation standpoint, modules are a generalization of; precompiled headers, lifting a number of restrictions placed on precompiled; headers. In particular, there can only be one precompiled header and it must; be included at the beginning of the translation unit. The extensions to the; AST file format required for modules are discussed in the section on; :ref:`modules <pchinternals-modules>`. Clang's AST files are designed with a compact on-disk representation, which; minimizes both creation time and the time required to initially load the AST; file. The AST file itself contains a serialized representation of Clang's; abstract syntax trees and supporting data structures, stored using the same; compressed bitstream as `LLVM's bitcode file format; <https://llvm.org/docs/BitCodeFormat.html>`_. Clang's AST files are loaded ""lazily"" from disk. When an AST file is initially; loaded, Clang reads only a small amount of data from the AST file to establish; where certain important data structures are stored. The amount of data read in; this initial load is independent of the size of the AST file, such that a; larger AST file does not lead to longer AST load times. The actual header data; in the AST file --- macros, functions, variables, types, etc. --- is loaded; only when it is referenced from the user's code, at which point only that; entity (and those entities it depends on) are deserialized from the AST file.; With this approach, the cost of using an AST file for a translation unit is; proportional to the amount of code actually used from the AST file, rather than; being proportional to the size of the AST file itself. When given the `-print-stats` option, Clang produces statistics; describing how much of the AST file was actually loaded from disk. For a; simple ""Hello, World!"" program that includes the Apple ``Cocoa.h`` header; (which is built as a precompiled header), this option illustrates how little ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:3469,load,loaded,3469,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance,"d Argument Hacking (BUGPOINT USE ONLY; DO NOT USE); -----------------------------------------------------------------------. Same as dead argument elimination, but deletes arguments to functions which are; external. This is only for use by :doc:`bugpoint <Bugpoint>`. ``extract-blocks``: Extract Basic Blocks From Module (for bugpoint use); -----------------------------------------------------------------------. This pass is used by bugpoint to extract all blocks from the module into their; own functions. ``instnamer``: Assign names to anonymous instructions; -----------------------------------------------------. This is a little utility pass that gives instructions names, this is mostly; useful when diffing the effect of an optimization because deleting an unnamed; instruction can change all other instruction numbering, making the diff very; noisy. .. _passes-verify:. ``verify``: Module Verifier; ---------------------------. Verifies an LLVM IR code. This is useful to run after an optimization which is; undergoing testing. Note that llvm-as verifies its input before emitting; bitcode, and also that malformed bitcode is likely to make LLVM crash. All; language front-ends are therefore encouraged to verify their output before; performing optimizing transformations. #. Both of a binary operator's parameters are of the same type.; #. Verify that the indices of mem access instructions match other operands.; #. Verify that arithmetic and other things are only performed on first-class; types. Verify that shifts and logicals only happen on integrals f.e.; #. All of the constants in a switch statement are of the correct type.; #. The code is in valid SSA form.; #. It is illegal to put a label into any other type (like a structure) or to; return one.; #. Only phi nodes can be self referential: ``%x = add i32 %x``, ``%x`` is; invalid.; #. PHI nodes must have an entry for each predecessor, with no extras.; #. PHI nodes must be the first thing in a basic block, all grouped togeth",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:41025,optimiz,optimization,41025,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimization']
Performance,"d Surface at Crossing Point. Supposing we have found out that a particle will cross a boundary during; the next step, it is sometimes useful to compute the normal to the; crossed surface. The modeller uses the following convention: we define; as `normal` ($\vec{n}$) the unit vector perpendicular; to a surface in the `next crossing point`, having the orientation such; that: $\vec{n}.\vec{d}>0$. Here $\vec{d}$; represents the current direction. The next crossing point represents the; point where a ray shot from the current point along the current; direction crosses the surface. ``` {.cpp}; Double_t *TGeoManager::FindNormal(Bool_t forward=kTRUE);; ```. The method above computes the normal to the next crossed surface in; forward or backward direction (i.e. the current one), assuming the state; corresponding to a current arbitrary point is initialized. An example of; usage of normal computation is ray tracing. The two most important features of the geometrical modeller concerning; tracking are scalability and performance as function of the total number; of physical nodes. The first refers to the possibility to make use of; the available memory resources and at the same time be able to resolve; any geometrical query, while the second defines the capability of the; modeller to respond quickly even for huge geometries. These parameters; can become critical when simulating big experiments like those at LHC. ### Creating and Visualizing Tracks. In case the modeller is interfaced with a tracking engine, one might; consider quite useful being able to store and visualize at least a part; of the tracks in the context of the geometry. The base class; **`TVirtualGeoTrack`** provides this functionality. It currently has one; implementation inside the drawing package (**`TGeoTrack`** class). A; track can be defined like:. ``` {.cpp}; TVirtualGeoTrack(Int_t id,Int_t pdg,TVirtualGeoTrack *parent=0,; TObject *particle=0);; ```. Where: `id` is user-defined id of the track, `pdg` - `pdg` c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:123821,scalab,scalability,123821,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,"['perform', 'scalab']","['performance', 'scalability']"
Performance,"d a zero value.; * The dbg.value of the PHI instruction leads to a DBG_VALUE of virtual register; ``%0``.; * The first GEP has its effect folded into the first load instruction; (as a 4-byte offset), but the variable location is salvaged by folding; the GEPs effect into the DIExpression.; * The second GEP is also folded into the corresponding load. However, it is; insufficiently simple to be salvaged, and is emitted as a ``$noreg``; DBG_VALUE, indicating that the variable takes on an undefined location.; * The final dbg.value has its Value placed in virtual register ``%1``. Instruction Scheduling; ----------------------. A number of passes can reschedule instructions, notably instruction selection; and the pre-and-post RA machine schedulers. Instruction scheduling can; significantly change the nature of the program -- in the (very unlikely) worst; case the instruction sequence could be completely reversed. In such; circumstances LLVM follows the principle applied to optimizations, that it is; better for the debugger not to display any state than a misleading state.; Thus, whenever instructions are advanced in order of execution, any; corresponding DBG_VALUE is kept in its original position, and if an instruction; is delayed then the variable is given an undefined location for the duration; of the delay. To illustrate, consider this pseudo-MIR:. .. code-block:: text. %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); DBG_VALUE %1, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DBG_VALUE %4, $noreg, !3, !4; %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; DBG_VALUE %7, $noreg, !5, !6. Imagine that the SUB32rr were moved forward to give us the following MIR:. .. code-block:: text. %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); DBG_VALUE %1, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DB",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:33044,optimiz,optimizations,33044,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"d and the; rest of the code generation passes are run. One of the most common ways to debug these steps is using ``-debug-only=isel``,; which prints out the DAG, along with other information like debug info,; after each of these steps. Alternatively, ``-debug-only=isel-dump`` shows only; the DAG dumps, but the results can be filtered by function names using; ``-filter-print-funcs=<function names>``. One great way to visualize what is going on here is to take advantage of a few; LLC command line options. The following options pop up a window displaying the; SelectionDAG at specific times (if you only get errors printed to the console; while using this, you probably `need to configure your; system <ProgrammersManual.html#viewing-graphs-while-debugging-code>`_ to add support for it). * ``-view-dag-combine1-dags`` displays the DAG after being built, before the; first optimization pass. * ``-view-legalize-dags`` displays the DAG before Legalization. * ``-view-dag-combine2-dags`` displays the DAG before the second optimization; pass. * ``-view-isel-dags`` displays the DAG before the Select phase. * ``-view-sched-dags`` displays the DAG before Scheduling. The ``-view-sunit-dags`` displays the Scheduler's dependency graph. This graph; is based on the final SelectionDAG, with nodes that must be scheduled together; bundled into a single scheduling-unit node, and with immediate operands and; other nodes that aren't relevant for scheduling omitted. The option ``-filter-view-dags`` allows to select the name of the basic block; that you are interested to visualize and filters all the previous; ``view-*-dags`` options. .. _Build initial DAG:. Initial SelectionDAG Construction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The initial SelectionDAG is na\ :raw-html:`&iuml;`\ vely peephole expanded from; the LLVM input by the ``SelectionDAGBuilder`` class. The intent of this pass; is to expose as much low-level, target-specific details to the SelectionDAG as; possible. This pass is mostly hard-c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:39511,optimiz,optimization,39511,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"d architectures. By default this is inferred from the target triple or; autodetected to the current architecture. .. option:: -mcpu=cpuname. Specify a specific chip in the current architecture to generate code for.; By default this is inferred from the target triple and autodetected to; the current architecture. For a list of available CPUs, use:; **llvm-as < /dev/null | llc -march=xyz -mcpu=help**. .. option:: -mattr=a1,+a2,-a3,... Override or control specific attributes of the target, such as whether SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:; **llvm-as < /dev/null | llc -march=xyz -mattr=help**. FLOATING POINT OPTIONS; ----------------------. .. option:: -disable-excess-fp-precision. Disable optimizations that may increase floating point precision. .. option:: -enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: -enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: -enable-unsafe-fp-math. Causes :program:`lli` to enable optimizations that may decrease floating point; precision. .. option:: -soft-float. Causes :program:`lli` to generate software floating point library calls instead of; equivalent hardware instructions. CODE GENERATION OPTIONS; -----------------------. .. option:: -code-model=model. Choose the code model from:. .. code-block:: text. default: Target default code model; tiny: Tiny code model; small: Small code model; kernel: Kernel code model; medium: Medium code model; large: Large code model. .. option:: -disable-post-RA-scheduler. Disable scheduling after register allocation. .. option:: -disable-spill-fusing. Disable fusing of spill code into instructions. .. option:: -jit-enable-eh. Exception handling should be enabled in the just-in-time compiler. .. option:: -join-liveintervals. Coalesce copies (default=true). .. option:: -nozero-initialized-in-bss. Don't place zero-initialized symbols into",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst:3196,optimiz,optimizations,3196,interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,1,['optimiz'],['optimizations']
Performance,"d as using all live out values in the; function. ``PHI`` nodes need to be handled specially, because the calculation of the live; variable information from a depth first traversal of the CFG of the function; won't guarantee that a virtual register used by the ``PHI`` node is defined; before it's used. When a ``PHI`` node is encountered, only the definition is; handled, because the uses will be handled in other basic blocks. For each ``PHI`` node of the current basic block, we simulate an assignment at; the end of the current basic block and traverse the successor basic blocks. If a; successor basic block has a ``PHI`` node and one of the ``PHI`` node's operands; is coming from the current basic block, then the variable is marked as *alive*; within the current basic block and all of its predecessor basic blocks, until; the basic block with the defining instruction is encountered. Live Intervals Analysis; ^^^^^^^^^^^^^^^^^^^^^^^. We now have the information available to perform the live intervals analysis and; build the live intervals themselves. We start off by numbering the basic blocks; and machine instructions. We then handle the ""live-in"" values. These are in; physical registers, so the physical register is assumed to be killed by the end; of the basic block. Live intervals for virtual registers are computed for some; ordering of the machine instructions ``[1, N]``. A live interval is an interval; ``[i, j)``, where ``1 >= i >= j > N``, for which a variable is live. .. note::; More to come... .. _Register Allocation:; .. _register allocator:. Register Allocation; -------------------. The *Register Allocation problem* consists in mapping a program; :raw-html:`<b><tt>` P\ :sub:`v`\ :raw-html:`</tt></b>`, that can use an unbounded; number of virtual registers, to a program :raw-html:`<b><tt>` P\ :sub:`p`\; :raw-html:`</tt></b>` that contains a finite (possibly small) number of physical; registers. Each target architecture has a different number of physical; registers.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:57225,perform,perform,57225,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['perform']
Performance,"d backtraces with inlining information, but does not; include any information about variables, their locations or types. :option:`-gmodules` Generate debug information that contains external; references to types defined in Clang modules or precompiled headers instead; of emitting redundant debug type information into every object file. This; option transparently switches the Clang module format to object file; containers that hold the Clang module together with the debug information.; When compiling a program that uses Clang modules or precompiled headers,; this option produces complete debug information with faster compile; times and much smaller object files. This option should not be used when building static libraries for; distribution to other machines because the debug info will contain; references to the module cache on the machine the object files in the; library were built on. .. option:: -fstandalone-debug -fno-standalone-debug. Clang supports a number of optimizations to reduce the size of debug; information in the binary. They work based on the assumption that the; debug type information can be spread out over multiple compilation units.; For instance, Clang will not emit type definitions for types that are not; needed by a module and could be replaced with a forward declaration.; Further, Clang will only emit type info for a dynamic C++ class in the; module that contains the vtable for the class. The :option:`-fstandalone-debug` option turns off these optimizations.; This is useful when working with 3rd-party libraries that don't come with; debug information. This is the default on Darwin. Note that Clang will; never emit type information for types that are not referenced at all by the; program. .. option:: -feliminate-unused-debug-types. By default, Clang does not emit type information for types that are defined; but not used in a program. To retain the debug info for these unused types,; the negation **-fno-eliminate-unused-debug-types** can be used. .",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:12884,optimiz,optimizations,12884,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimizations']
Performance,"d be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the followin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:305085,load,loads,305085,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"d by using `psyco`, see the next link:; <http://psyco.sourceforge.net>, a Python just in time compiler (JIT).; Note, however, that `psyco` is limited to Intel i386 CPUs. Since `psyco`; optimizes Python, not `PyROOT` calls; it generally does not improve; performance that much if most of your code consists of ROOT API calls.; Mathematical computations in Python, on the other hand, benefit a lot. Every call to a Python member function results in a lookup of that; member function and an association of this method with `'self'`.; Furthermore, a temporary object is created during this process that is; discarded after the method call. In inner loops, it may be worth your; while (up to 30%), to short-cut this process by looking up and binding; the method before the loop, and discarding it afterwards. Here is an; example:. ``` {.cpp}; hpx = TH1F('hpx','px',100,-4,4); hpxFill = hpx.Fill # cache bound method; for i in xrange(25000):; px = gRandom.Gaus(); hpxFill(px) # use bound method: no lookup needed; del hpxFill # done with cached method; ```. Note that if you do not discard the bound method, a reference to the; histogram will remain outstanding, and it will not be deleted when it; should be. It is therefore important to delete the method when you're; done with it. ### Use of Python Functions. It is possible to mix Python functions with ROOT and perform such; operations as plotting and fitting of histograms with them. In all; cases, the procedure consists of instantiating a ROOT **`TF1`**,; **`TF2`**, or **`TF3`** with the Python function and working with that; ROOT object. There are some memory issues, so it is for example not yet; possible to delete a **`TF1`** instance and then create another one with; the same name. In addition, the Python function, once used for; instantiating the **`TF1`**, is never deleted. Instead of a Python function, you can also use callable instances (e.g.,; an instance of a class that has implemented the `__call__` member; function). The signatu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:24655,cache,cached,24655,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['cache'],['cached']
Performance,"d check the type in C++, or duplicate the; pattern. GICombineRule; -------------. MIR patterns can appear in the ``match`` or ``apply`` patterns of a; ``GICombineRule``. The ``root`` of the rule can either be a def of an instruction, or a; named pattern. The latter is helpful when the instruction you want; to match has no defs. The former is generally preferred because; it's less verbose. .. code-block:: text; :caption: Combine Rule root is a def. // Fold x op 1 -> x; def right_identity_one: GICombineRule<; (defs root:$dst),; (match (G_MUL $dst, $x, 1)),; // Note: Patterns always need to create something, we can't just replace $dst with $x, so we need a COPY.; (apply (COPY $dst, $x)); >;. .. code-block:: text; :caption: Combine Rule root is a named pattern. def Foo : GICombineRule<; (defs root:$root),; (match (G_ZEXT $tmp, (i32 0)),; (G_STORE $tmp, $ptr):$root),; (apply (G_STORE (i32 0), $ptr):$root)>;. Combine Rules also allow mixing C++ code with MIR patterns, so that you; may perform additional checks when matching, or run additional code after; rewriting a pattern. The following expansions are available for MIR patterns:. * operand names (``MachineOperand &``); * pattern names (``MachineInstr *`` for ``match``,; ``MachineInstrBuilder &`` for apply). .. code-block:: text; :caption: Example C++ Expansions. def Foo : GICombineRule<; (defs root:$root),; (match (G_ZEXT $root, $src):$mi),; (apply ""foobar(${root}.getReg(), ${src}.getReg(), ${mi}->hasImplicitDef())"")>;. Common Pattern #1: Replace a Register with Another; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. The 'apply' pattern must always redefine all operands defined by the match root.; Sometimes, we do not need to create instructions, simply replace a def with; another matched register. The ``GIReplaceReg`` builtin can do just that. .. code-block:: text. def Foo : GICombineRule<; (defs root:$dst),; (match (G_FNEG $tmp, $src), (G_FNEG $dst, $tmp)),; (apply (GIReplaceReg $dst, $src))>;. This also works if ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/MIRPatterns.rst:7628,perform,perform,7628,interpreter/llvm-project/llvm/docs/GlobalISel/MIRPatterns.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/MIRPatterns.rst,1,['perform'],['perform']
Performance,"d combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26986,optimiz,optimized,26986,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"d compiled C++ into the interpreter.; This function takes the name of a shared library and loads it into current; process, exposing all external symbols to Cling.; Libraries are located through load paths given to Cling, either through the; ""-L"" compiler flag or the dynamic search path environment variable (system; dependent).; Any method that brings symbols into the process (including normal linking,; e.g. when embedding Python in a C++ application) is suitable to expose; symbols.; An alternative for ``load_library`` is for example ``ctypes.CDLL``, but; that function does not respect dynamic load paths on all platforms. If a compilation error occurs during JITing of C++ code in any of the above; helpers, a Python ``SyntaxError`` exception is raised.; If a compilation warning occurs, a Python warning is issued. `Configuring Cling`; -------------------. It is often convenient to add additional search paths for Cling to find; headers and libraries when loading a module (Python does not have standard; locations to place headers and libraries, but their locations can usually; be inferred from the location of the module, i.e. it's ``__file__``; attribute).; cppyy provides the following two helpers:. * ``add_include_path``: add additional paths for Cling to look for headers. * ``add_library_path``: add additional paths for Cling to look for libraries. Both functions accept either a string (a single path) or a list (for adding; multiple paths).; Paths are allowed to be relative, but absolute paths are recommended. `C++ language`; --------------. Some C++ compilation-time features have no Python equivalent.; Instead, convenience functions are provided:. * ``sizeof``: takes a proxied C++ type or its name as a string and returns; the storage size (in units of ``char``). * ``typeid``: takes a proxied C++ type or its name as a string and returns; the the C++ runtime type information (RTTI). * ``nullptr``: C++ ``NULL``. `Preprocessor`; --------------. Preprocessor macro's (``#de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:3556,load,loading,3556,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['loading']
Performance,"d cut range (set per variable). VarProp Yes NotEnforced NotEnforced, FMax, FMin, FSmart Categorisation of cuts. Configuration options for MVA method :. Configuration options reference for MVA method: PDEFoam. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). SigBgSeparate No False  Separate foams for signal and background. TailCut No 0.001  Fraction of outlier events that are excluded from the foam in each dimension. VolFrac No 0.0666667  Size of sampling box, used for density calculation during foam build-up (maximum value: 1.0 is equivalent to volume of entire foam). nActiveCells No 500  Maximum number of active cells to be created by the foam. nSampl No 2000  Number of generated MC events per cell. nBin No 5  Number of bins in edge histograms. Compress No True  Compress foam output file. MultiTargetRegression No False  Do regression with multiple targets. Nmin No 100  Number of events in cell required to split cell. MaxDepth No 0  Maximum depth of cell tree (0=unlimited). FillFoamWithOrigWeights No False  Fill foam with original or boost weights. UseYesNoCell No False  Return -1 or 1 for bkg or signal like events. DTLogic No None None, GiniIndex, M",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:26571,perform,performance,26571,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"d directory, all tests within it will be; reported as unsupported. Used by: *ShTest*. **parent** The parent configuration, this is the config object for the directory; containing the test suite, or None. **root** The root configuration. This is the top-most :program:`lit` configuration in; the project. **pipefail** Normally a test using a shell pipe fails if any of the commands; on the pipe fail. If this is not desired, setting this variable to false; makes the test fail only if the last command in the pipe fails. **available_features** A set of features that can be used in `XFAIL`,; `REQUIRES`, and `UNSUPPORTED` directives. TEST DISCOVERY; ~~~~~~~~~~~~~~. Once test suites are located, :program:`lit` recursively traverses the source; directory (following *test_source_root*) looking for tests. When :program:`lit`; enters a sub-directory, it first checks to see if a nested test suite is; defined in that directory. If so, it loads that test suite recursively,; otherwise it instantiates a local test config for the directory (see; :ref:`local-configuration-files`). Tests are identified by the test suite they are contained within, and the; relative path inside that suite. Note that the relative path may not refer to; an actual file on disk; some test formats (such as *GoogleTest*) define; ""virtual tests"" which have a path that contains both the path to the actual; test file and a subpath to identify the virtual test. .. _local-configuration-files:. LOCAL CONFIGURATION FILES; ~~~~~~~~~~~~~~~~~~~~~~~~~. When :program:`lit` loads a subdirectory in a test suite, it instantiates a; local test configuration by cloning the configuration for the parent directory; --- the root of this configuration chain will always be a test suite. Once the; test configuration is cloned :program:`lit` checks for a *lit.local.cfg* file; in the subdirectory. If present, this file will be loaded and can be used to; specialize the configuration for each individual directory. This facility can; be used",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst:17690,load,loads,17690,interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,1,['load'],['loads']
Performance,"d disk usage, see :ref:`CMAKE_BUILD_TYPE <cmake_build_type>`; for more information. * -DLLVM_ENABLE_ASSERTIONS; This option defaults to ON for Debug builds and defaults to OFF for Release; builds. As mentioned in the previous option, using the Release build type and; enabling assertions may be a good alternative to using the Debug build type. * -DLLVM_PARALLEL_LINK_JOBS; Set this equal to number of jobs you wish to run simultaneously. This is; similar to the -j option used with make, but only for link jobs. This option; can only be used with ninja. You may wish to use a very low number of jobs,; as this will greatly reduce the amount of memory used during the build; process. If you have limited memory, you may wish to set this to 1. * -DLLVM_TARGETS_TO_BUILD; Set this equal to the target you wish to build. You may wish to set this to; X86; however, you will find a full list of targets within the; llvm-project/llvm/lib/Target directory. * -DLLVM_OPTIMIZED_TABLEGEN; Set this to ON to generate a fully optimized tablegen during your build. This; will significantly improve your build time. This is only useful if you are; using the Debug build type. * -DLLVM_ENABLE_PROJECTS; Set this equal to the projects you wish to compile (e.g. clang, lld, etc.) If; compiling more than one project, separate the items with a semicolon. Should; you run into issues with the semicolon, try surrounding it with single quotes. * -DLLVM_ENABLE_RUNTIMES; Set this equal to the runtimes you wish to compile (e.g. libcxx, libcxxabi, etc.); If compiling more than one runtime, separate the items with a semicolon. Should; you run into issues with the semicolon, try surrounding it with single quotes. * -DCLANG_ENABLE_STATIC_ANALYZER; Set this option to OFF if you do not require the clang static analyzer. This; should improve your build time slightly. * -DLLVM_USE_SPLIT_DWARF; Consider setting this to ON if you require a debug build, as this will ease; memory pressure on the linker. This will make linki",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:45864,optimiz,optimized,45864,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimized']
Performance,"d drive). It is therefore expected that this; limitation cannot be eliminated with the usage of any parallel analysis; toolkit. ### Optimisation Regarding N-tuples ###. ROOT automatically applies compression algorithms on n-tuples to reduce; the memory consumption. A value that is in most cases the same will; consume only small space on your disk (but it has to be decompressed on; reading). Nevertheless, you should think about the design of your; n-tuples and your analyses as soon as the processing time exceeds some; minutes. - Try to keep your n-tuples simple and use appropriate variable types.; If your measurement has only a limited precision, it is needless to; store it with double precision. - Experimental conditions that do not change with every single; measurement should be stored in a separate tree. Although the; compression can handle redundant values, the processing time; increase with every variable that has to be filled. - The function `SetCacheSize(long)` specifies the size of the cache; for reading a `TTree` object from a file. The default value is 30MB.; A manual increase may help in certain situations. Please note that; the caching mechanism can cover only one `TTree` object per `TFile`; object. - You can select the branches to be covered by the caching algorithm; with `AddBranchToCache` and deactivate unneeded branches with; `SetBranchStatus`. This mechanism can result in a significant; speed-up for simple operations on trees with many branches. - You can measure the performance easily with `TTreePerfStats`. The; ROOT documentation on this class also includes an introductory; example. For example, `TTreePerfStats` can show you that it is; beneficial to store meta data and payload data separately, i.e.; write the meta data tree in a bulk to a file at the end of your job; instead of writing both trees interleaved. [^6]: The usage of `fOutput` is not really needed for this simple example, but it allows re-usage of the exact code in parallel processing wi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md:14112,cache,cache,14112,documentation/primer/filio.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md,1,['cache'],['cache']
Performance,"d for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 chan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:337986,cache,caches,337986,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"d from this software without; specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS; ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED; TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR; PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS; BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE; GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION); HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT; LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY; OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF; SUCH DAMAGE. You are under no obligation whatsoever to provide any bug fixes,; patches, or upgrades to the features, functionality or performance of; the source code (""Enhancements"") to anyone; however, if you choose to; make your Enhancements available either publicly, or directly to; Lawrence Berkeley National Laboratory, without imposing a separate; written license agreement for such Enhancements, then you hereby grant; the following license: a non-exclusive, royalty-free perpetual license; to install, use, modify, prepare derivative works, incorporate into; other computer software, distribute, and sublicense such Enhancements; or derivative works thereof, in binary and source code form. Additional copyright holders; ----------------------------. In addition to LBNL/UC Berkeley, this package contains files copyrighted by ; one or more of the following people and organizations, and licensed under; the same conditions (except for some compatible licenses as retained in the; source code):. Lucio Asnaghi; Simone Bacchio; Aditi Dutta; Shaheed Haque; Aaron Jomy; Toby StClere-Smithe; Stefan Wunsch. Conda-forge recipes were provided by Julian Rueth and Isuru Fernando.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/LICENSE.txt:1855,perform,performance,1855,bindings/pyroot/cppyy/cppyy/LICENSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/LICENSE.txt,1,['perform'],['performance']
Performance,"d intrinsic. You can use ``llvm.umul.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.umul.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.umul.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.umul.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.umul.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.umul.fix``' family of intrinsic functions perform unsigned; fixed point multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs unsigned fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.umul.fix.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.umul.fix.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5). ; The result in the following could be rounded down to 3.5 or up to 4; %res = call i4 @llvm.umul.fix.i4(i4 15, i4 1, i32 1) ; %res = 7 (or 8) (7.5 x 0.5 = 3.75). '``llvm.smul.fix.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.fix.sat``; on any integer bit width or vectors of integers. ::. declar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:624140,perform,performs,624140,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"d legalize or optimize this; using the rounding specified by this hook if it is provided. Operations like; constant folding, instruction combining, KnownBits, and ValueTracking should; also use this hook, if provided, and not assume the direction of rounding. A; rounded result must always be within one unit of precision from the true; result. That is, the error between the returned result and the true result must; be less than 1/2^(scale). '``llvm.smul.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.smul.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.smul.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.smul.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.smul.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.smul.fix``' family of intrinsic functions perform signed; fixed point multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type. Examples; """""""""""""""""". .. code-block:: llvm. %re",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:621645,perform,perform,621645,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"d library extension; for the current platform. For example on most platforms, `hsimple.cxx`; will generate `hsimple_cxx.so`. The + command rebuild the library only if the script or any of the; files it includes are newer than the library. When checking the; timestamp, ACLiC generates a dependency file which name is the same as; the library name, just replacing the 'so' extension by the extension; 'd'. For example on most platforms, `hsimple.cxx` will generate; `hsimple_cxx.d`. To ensure that the shared library is rebuilt you can use the ++; syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. To build, load, and execute the function with the same name as the; file you can use the `.x` command. This is the same as executing a; named script; you can also provide parameters. The only; difference is you need to append a + or a ++. ``` {.cpp}; root[] .x MyScript.C+(4000); Creating shared library /home/./MyScript_C.so; ```. You can select whether the script in compiled with debug symbol or; with optimization by appending the letter 'g' or 'O' after the '+' or; '++'. Without the specification, the script is compiled with the same; level of debugging symbol and optimization as the currently running; ROOT executable. For example:. ``` {.cpp}; root[] .L MyScript.C++g; ```. will compile `MyScript.C` with debug symbols; usually this means; giving the `-g` option to compiler. ``` {.cpp}; root[] .L MyScript.C++O; ```. will compile `MyScript.C` with optimizations; usually this means; giving the `-O` option to compiler. The syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. is using the default optimization level. The initial default is to; compile with the same level of optimization as the root executable; itself. The default can be changed by:. ``` {.cpp}; root[] gSystem->SetAclicMode(TSystem::kDebug);; root[] gSystem->SetAclicMode(TSystem::kOpt);; ```. Note that the commands:. ``` {.cpp}; root[] .L MyScript.C+g; root[] .L MyScript.C+O; ```. respectively compile `MyScript.C` with debu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:14933,optimiz,optimization,14933,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['optimiz'],['optimization']
Performance,"d may be removed in future releases.; - Added ```TReadLockGuard```,```TWriteLockGuard```, ```R__READ_LOCKGUARD``` and```R__WRITE_LOCKGUARD``` to take advantage of the new lock. The legacy ```TLockGuard``` and ```R__LOCKGUARD``` use the write lock.; - Improved scaling of TROOT::RecursiveRemove in the case of large collection.; - Added a thread safe mode for the following ROOT collections: THashList, THashTable, TList and TObjArray. When ROOT's thread safe mode is enabled and the collection is set to use internal locks by calling:; ```; collection->UseRWLock();; ```; all operations on the collection will take the read or write lock when needed, currently they shared the global lock (ROOT::gCoreMutex). ### Interpreter. - cling's LLVM is upgraded to version 5.0; - All of cling's patches to llvm have been upstreamed.; - The interpreter-related lock is now locking only the compilation step, not the execution step. This reduces the scope for lock contention. Most significantly, it enables the use of concurrency on the prompt!. ## I/O Libraries. - Introduce TKey::ReadObject<typeName>. This is a user friendly wrapper around ReadObjectAny. For example; ```; auto h1 = key->ReadObject<TH1>; ```; after which h1 will either be null if the key contains something that is not a TH1 (or derived class); or will be set to the address of the histogram read from the file.; - Add the ability to store the 'same' object several time (assumingly with different data) in a single buffer. Instead of. ```; while(...) {; TObjArray arr;; ... update the content of ""arr""; buffer << arr;; }; ```; which would only really stream the array at the first iteration because it will be detected has having the same address and thus assumed to be the same object. We can now do:; ```; while(...) {; TObjArray arr;; ... update the content of ""arr""; buffer.WriteObject(&arr, kFALSE);; }; ```; where the last argument of WriteObject tells the buffer do *not* remember this object's address and to always stream it. Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:6333,concurren,concurrency,6333,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['concurren'],['concurrency']
Performance,"d of a :ref:`call <i_call>` or :ref:`invoke <i_invoke>`; instruction.; - The parameter operand of a :ref:`call <i_call>` or :ref:`invoke <i_invoke>`; instruction, when the function or invoking call site has a ``noundef``; attribute in the corresponding position.; - The operand of a :ref:`ret <i_ret>` instruction if the function or invoking; call site has a `noundef` attribute in the return value position. Here are some examples:. .. code-block:: llvm. entry:; %poison = sub nuw i32 0, 1 ; Results in a poison value.; %poison2 = sub i32 poison, 1 ; Also results in a poison value.; %still_poison = and i32 %poison, 0 ; 0, but also poison.; %poison_yet_again = getelementptr i32, ptr @h, i32 %still_poison; store i32 0, ptr %poison_yet_again ; Undefined behavior due to; ; store to poison. store i32 %poison, ptr @g ; Poison value stored to memory.; %poison3 = load i32, ptr @g ; Poison value loaded back from memory. %poison4 = load i16, ptr @g ; Returns a poison value.; %poison5 = load i64, ptr @g ; Returns a poison value. %cmp = icmp slt i32 %poison, 0 ; Returns a poison value.; br i1 %cmp, label %end, label %end ; undefined behavior. end:. .. _welldefinedvalues:. Well-Defined Values; -------------------. Given a program execution, a value is *well defined* if the value does not; have an undef bit and is not poison in the execution.; An aggregate value or vector is well defined if its elements are well defined.; The padding of an aggregate isn't considered, since it isn't visible; without storing it into memory and loading it with a different type. A constant of a :ref:`single value <t_single_value>`, non-vector type is well; defined if it is neither '``undef``' constant nor '``poison``' constant.; The result of :ref:`freeze instruction <i_freeze>` is well defined regardless; of its operand. .. _blockaddress:. Addresses of Basic Blocks; -------------------------. ``blockaddress(@function, %block)``. The '``blockaddress``' constant computes the address of the specified; basic ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:198990,load,load,198990,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"d of movsd to implement (scalar_to_vector (loadf64)); when code size is critical. movlps is slower than movsd on core2 but it's one; byte shorter. //===---------------------------------------------------------------------===//. We should use a dynamic programming based approach to tell when using FPStack; operations is cheaper than SSE. SciMark montecarlo contains code like this; for example:. double MonteCarlo_num_flops(int Num_samples) {; return ((double) Num_samples)* 4.0;; }. In fpstack mode, this compiles into:. LCPI1_0:					; 	.long	1082130432	## float 4.000000e+00; _MonteCarlo_num_flops:; 	subl	$4, %esp; 	movl	8(%esp), %eax; 	movl	%eax, (%esp); 	fildl	(%esp); 	fmuls	LCPI1_0; 	addl	$4, %esp; 	ret; ; in SSE mode, it compiles into significantly slower code:. _MonteCarlo_num_flops:; 	subl	$12, %esp; 	cvtsi2sd	16(%esp), %xmm0; 	mulsd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp); 	fldl	(%esp); 	addl	$12, %esp; 	ret. There are also other cases in scimark where using fpstack is better, it is; cheaper to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define voi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:20172,load,load,20172,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,4,['load'],['load']
Performance,"d of running http server, one could use fast cgi interface; to normal web server like Apache or lighttpd or any other.; When creating server, one could specify:. root [0] serv = new THttpServer(""fastcgi:9000"");. This opens port 9000, which should be specified in web server configuration.; For instance, lighttpd.conf file could contain path like this:. fastcgi.server = (; ""/remote_scripts/"" =>; (( ""host"" => ""192.168.1.10"",; ""port"" => 9000,; ""check-local"" => ""disable"",; ""docroot"" => ""/""; )); ). In this case one should be able to access root application via address. http://your_lighttpd_host/remote_scripts/root.cgi/. AUTHOR:. Sergey Linev, S.Linev@gsi.de. CHANGES:. January 2015; - Provide exe.json request to execute arbitrary object method and return; result in JSON format. Server should run in non-readonly mode. Fall 2014; - Implement gzip for result of any submitted requests, automatically done ; when .gz extension is provided; - Provide access to arbitrary data member of objects, registered to the server; - Prevent data caching in the browser by setting no-cache header. April 2014; - In TCivetweb class support digest authentication method. User; can specify auth_file and auth_domain parameters to protect; access to the server; - Fix error in FastCgi, now correctly works with Apache; - Avoid direct usage of TASImage. March 2014; - Replace mongoose by civetweb due to more liberal MIT license.; Works out of the box while civetweb version fully corresponds to; previously used version of mongoose.; - Introduce TBufferJSON class to store arbitrary ROOT object; into JSON format. It is not one-to-one storage (like XML), but; rather JS-like structures. For instance, all TCollections converted; into JavaScript Array. Produced JS object is similar to JSRootIO.; - Process get.json request, which returns object in JSON form.; It can be used directly is script without special I/O of Bertrand.; - Use get.json on browser side to simplify logic. No need for extra; requests for stream",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/http/README.txt:1695,cache,cache,1695,net/http/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/http/README.txt,2,['cache'],['cache']
Performance,"d on; other options. For example if the user specifies ``-fno-exceptions`` then a; toolchain could select libraries built without exception support, thereby; reducing the size of the resulting binary. Design; ======. Clang supports GCC's ``-print-multi-lib`` and ``-print-multi-directory``; options. These are described in; `GCC Developer Options <https://gcc.gnu.org/onlinedocs/gcc-12.2.0/gcc/Developer-Options.html>`_. There are two ways to configure multilib in Clang: hard-coded or via a; configuration file. Hard-coded Multilib; ===================. The available libraries can be hard-coded in Clang. Typically this is done; using the ``MultilibBuilder`` interface in; ``clang/include/clang/Driver/MultilibBuilder.h``.; There are many examples of this in ``lib/Driver/ToolChains/Gnu.cpp``.; The remainder of this document will not focus on this type of multilib. EXPERIMENTAL Multilib via configuration file; ============================================. Some Clang toolchains support loading multilib configuration from a; ``multilib.yaml`` configuration file. A ``multilib.yaml`` configuration file specifies which multilib variants are; available, their relative location, what compilation options were used to build; them, and the criteria by which they are selected. Multilib processing; ===================. Clang goes through the following steps to use multilib from a configuration; file:. #. Normalize command line options. Clang can accept the same; information via different options - for example,; ``--target=arm-none-eabi -march=armv7-m`` and; ``--target=armv7m-none-eabi`` are equivalent.; Clang normalizes the command line before passing them to the multilib system.; To see what flags are emitted for a given set of command line options, use; the ``-print-multi-flags-experimental`` command line option; along with the rest of the options you want to use.; #. Load ``multilib.yaml`` from sysroot.; #. Generate additional flags. ``multilib.yaml`` contains a ``Mappings`` section,;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Multilib.rst:2016,load,loading,2016,interpreter/llvm-project/clang/docs/Multilib.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Multilib.rst,1,['load'],['loading']
Performance,"d operators are:. * ``+`` - Returns the sum of its two operands.; * ``-`` - Returns the difference of its two operands. The syntax of a function call is ``<name>(<arguments>)`` where:. * ``name`` is a predefined string literal. Accepted values are:. * add - Returns the sum of its two operands.; * div - Returns the quotient of its two operands.; * max - Returns the largest of its two operands.; * min - Returns the smallest of its two operands.; * mul - Returns the product of its two operands.; * sub - Returns the difference of its two operands. * ``<arguments>`` is a comma separated list of expressions. For example:. .. code-block:: llvm. ; CHECK: load r[[#REG:]], [r0]; ; CHECK: load r[[#REG+1]], [r1]; ; CHECK: Loading from 0x[[#%x,ADDR:]]; ; CHECK-SAME: to 0x[[#ADDR + 7]]. The above example would match the text:. .. code-block:: gas. load r5, [r0]; load r6, [r1]; Loading from 0xa0463440 to 0xa0463447. but would not match the text:. .. code-block:: gas. load r5, [r0]; load r7, [r1]; Loading from 0xa0463440 to 0xa0463443. Due to ``7`` being unequal to ``5 + 1`` and ``a0463443`` being unequal to; ``a0463440 + 7``. A numeric variable can also be defined to the result of a numeric expression,; in which case the numeric expression constraint is checked and if verified the; variable is assigned to the value. The unified syntax for both checking a; numeric expression and capturing its value into a numeric variable is thus; ``[[#%<fmtspec>,<NUMVAR>: <constraint> <expr>]]`` with each element as; described previously. One can use this syntax to make a testcase more; self-describing by using variables instead of values:. .. code-block:: gas. ; CHECK: mov r[[#REG_OFFSET:]], 0x[[#%X,FIELD_OFFSET:12]]; ; CHECK-NEXT: load r[[#]], [r[[#REG_BASE:]], r[[#REG_OFFSET]]]. which would match:. .. code-block:: gas. mov r4, 0xC; load r6, [r5, r4]. The ``--enable-var-scope`` option has the same effect on numeric variables as; on string variables. Important note: In its current implementation, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:32437,load,load,32437,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,2,['load'],['load']
Performance,"d range from; roughly 20% (for problems that were already implemented in a close-to optimal form) to more than 2000%; for certain type of problems. Below is a summary of the changes made. All of these changes are; transparent to end-use cases ; ; ; New implementation of RooFit data types. The implementation of data stored in RooDataSet and RooDataHist; was historically handled by ROOT TTrees (though class RooTreeDataStore). The default storage type; has now been changed to class RooVectorDataStore which stores the information in STL arrays. Existing; datasets based on trees can be read in transparently, and are converted to vector form in the ; persistent-to-transient conversion (the datafile is not modified in this operation); ; The vector store has two important advantages: 1) faster data access (raw data access times are 70 times ; faster than for TTrees), 2) ability to rewrite columns on the fly. The first advantage is important; for the existing constant-term precalculation optimization in roofit likelihoods as these are now; also stored in vectors rather than trees. The faster access speed of vectors make that the constant; term optimization inside likelihoods results in a larger speed increase. This is particulatly noticeable in pdfs with; many constant expressions from pdfs that were moderately fast to begin with (e.g. RooHistPdf).; The second advantages allows new types of algorithmic likelihood optimization in RooFit detailed below. New algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of; objects are identified: those that change with every event (i.e. the pdf) and those that change; only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated; for every event, whereas integrals are only evaluated when needed. The exception to the first type are pdfs; that only depend on constant parameters (or no parameters) - these are identified at the beginning, and precalculated once ; to avoid ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:1206,optimiz,optimization,1206,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"d set of training variables in that zone. The division; into categories in presence of distinct sub-populations reduces; the correlations between the training variables, improves the; modelling, and hence increases the classification and regression; performance. Presently, the Category method works for; classification only, but regression will follow soon. Please; contact us if urgently needed. An example scripts and data files illustrating how the new; Category method is configured and used. Please check the macros; test/TMVAClassificationCategory.C and; test/TMVAClassificationCategoryApplication.C or the; corresponding executables.; Regression functionality for gradient boosted trees using a Huber loss function. Comments. On Input Data: . New TMVA event vector building. The code for splitting the input; data into training and test samples for all classes and the; mixing of those samples to one training and one test sample has; been rewritten completely. The new code is more performant and; has a clearer structure. This fixes several bugs which have been; reported by the TMVA users. On Minimization: . Variables, targets and spectators are now checked if they are; constant. (The execution of TMVA is stopped for variables and; targets, a warning is given for spectators.). On Regression:; ; The analysis type is no longer defined by calling a dedicated; TestAllMethods-member-function of the Factory, but with the; option ""AnalysisType"" in the Factory. The default value is; ""Auto"" where TMVA tries to determine the most suitable analysis; type from the targets and classes the user has defined. Other; values are ""regression"", ""classification"" and ""multiclass"" for; the forthcoming multiclass classification.; Missing regression evaluation plots for training sample were; added. On Cut method:. Removed obsolete option ""FVerySmart"" from Cuts method. On MLP method:; ; Display of convergence information in the progress bar for MLP during training. Creation of animated gifs for ML",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html:1551,perform,performant,1551,tmva/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html,2,['perform'],['performant']
Performance,"d system; features. Utilizing these new features often involves a complicated chain of; CMake variables passed on the command line. Clang provides a collection of CMake; cache scripts to make these features more approachable. CMake cache files are utilized using CMake's -C flag:. .. code-block:: console. $ cmake -C <path to cache file> <path to sources>. CMake cache scripts are processed in an isolated scope, only cached variables; remain set when the main configuration runs. CMake cached variables do not reset; variables that are already set unless the FORCE option is specified. A few notes about CMake Caches:. - Order of command line arguments is important. - -D arguments specified before -C are set before the cache is processed and; can be read inside the cache file; - -D arguments specified after -C are set after the cache is processed and; are unset inside the cache file. - All -D arguments will override cache file settings; - CMAKE_TOOLCHAIN_FILE is evaluated after both the cache file and the command; line arguments; - It is recommended that all -D options should be specified *before* -C. For more information about some of the advanced build configurations supported; via Cache files see :doc:`AdvancedBuilds`. Executing the Tests; ===================. Testing is performed when the *check-all* target is built. For instance, if you are; using Makefiles, execute this command in the root of your build directory:. .. code-block:: console. $ make check-all. On Visual Studio, you may run tests by building the project ""check-all"".; For more information about testing, see the :doc:`TestingGuide`. Cross compiling; ===============. See `this wiki page <https://gitlab.kitware.com/cmake/community/wikis/doc/cmake/CrossCompiling>`_ for; generic instructions on how to cross-compile with CMake. It goes into detailed; explanations and may seem daunting, but it is not. On the wiki page there are; several examples including toolchain files. Go directly to the; ``Information how to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:39547,cache,cache,39547,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,2,['cache'],['cache']
Performance,"d that this; limitation cannot be eliminated with the usage of any parallel analysis; toolkit. ### Optimisation Regarding N-tuples ###. ROOT automatically applies compression algorithms on n-tuples to reduce; the memory consumption. A value that is in most cases the same will; consume only small space on your disk (but it has to be decompressed on; reading). Nevertheless, you should think about the design of your; n-tuples and your analyses as soon as the processing time exceeds some; minutes. - Try to keep your n-tuples simple and use appropriate variable types.; If your measurement has only a limited precision, it is needless to; store it with double precision. - Experimental conditions that do not change with every single; measurement should be stored in a separate tree. Although the; compression can handle redundant values, the processing time; increase with every variable that has to be filled. - The function `SetCacheSize(long)` specifies the size of the cache; for reading a `TTree` object from a file. The default value is 30MB.; A manual increase may help in certain situations. Please note that; the caching mechanism can cover only one `TTree` object per `TFile`; object. - You can select the branches to be covered by the caching algorithm; with `AddBranchToCache` and deactivate unneeded branches with; `SetBranchStatus`. This mechanism can result in a significant; speed-up for simple operations on trees with many branches. - You can measure the performance easily with `TTreePerfStats`. The; ROOT documentation on this class also includes an introductory; example. For example, `TTreePerfStats` can show you that it is; beneficial to store meta data and payload data separately, i.e.; write the meta data tree in a bulk to a file at the end of your job; instead of writing both trees interleaved. [^6]: The usage of `fOutput` is not really needed for this simple example, but it allows re-usage of the exact code in parallel processing with `PROOF` (see next section).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md:14612,perform,performance,14612,documentation/primer/filio.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/filio.md,1,['perform'],['performance']
Performance,"d the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object that it received; from EngineBuilder but it does not immediately generate code for this; module. Code generation is deferred until either the; MCJIT::finalizeObject method is called explicitly or a function such as; MCJIT::getPointerToFunction is called which requires the code to have been; generated. Code Generation; ===============. When code generation is triggered, as described above, MCJIT will first; attempt to retrieve an object image from its ObjectCache member, if one; has been set. If a cached object image cannot be retrieved, MCJIT will; call its emitObject method. MCJIT::emitObject uses a local PassManager; instance and creates a new ObjectBufferStream instance, both of which it; passes to TargetMachine::addPassesToEmitMC before calling PassManager::run; on the Module with which it was created. .. image:: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:2834,load,load,2834,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['load']
Performance,"d to implement an indirect call or indirect jump. Because this is; speculative, this will often be possible even when these are stored in; read-only pages. For example:; ```; class FancyObject : public BaseObject {; public:; void DoSomething() override;; };; void f(unsigned long attacker_offset, unsigned long attacker_data) {; FancyObject object = getMyObject();; unsigned long *arr[4] = getFourDataPointers();; if (attacker_offset < 4) {; // We have bypassed the bounds check speculatively.; unsigned long *data = arr[attacker_offset];; // Now we have computed a pointer inside of `object`, the vptr.; *data = attacker_data;; // The vptr points to the virtual table and we speculatively clobber that.; g(object); // Hand the object to some other routine.; }; }; // In another file, we call a method on the object.; void g(BaseObject &object) {; object.DoSomething();; // This speculatively calls the address stored over the vtable.; }; ```. Mitigating this requires hardening loads from these locations, or mitigating; the indirect call or indirect jump. Any of these are sufficient to block the; call or jump from using a speculatively stored value that has been read back. For both of these, using retpolines would be equally sufficient. One possible; hybrid approach is to use retpolines for indirect call and jump, while relying; on SLH to mitigate returns. Another approach that is sufficient for both of these is to harden all of the; speculative stores. However, as most stores aren't interesting and don't; inherently leak data, this is expected to be prohibitively expensive given the; attack it is defending against. ## Implementation Details. There are a number of complex details impacting the implementation of this; technique, both on a particular architecture and within a particular compiler.; We discuss proposed implementation techniques for the x86 architecture and the; LLVM compiler. These are primarily to serve as an example, as other; implementation techniques are very pos",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:13791,load,loads,13791,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"d to not hold secrets, see below for detailed limitations). This; approach only requires the underlying hardware have a way to implement a; branchless and unpredicted conditional update of a register's value. All modern; architectures have support for this, and in fact such support is necessary to; correctly implement constant time cryptographic primitives. Crucial properties of this approach:; * It is not preventing any particular side-channel from working. This is; important as there are an unknown number of potential side channels and we; expect to continue discovering more. Instead, it prevents the observation of; secret data in the first place.; * It accumulates the predicate state, protecting even in the face of nested; *correctly* predicted control flows.; * It passes this predicate state across function boundaries to provide; [interprocedural protection](#interprocedural-checking).; * When hardening the address of a load, it uses a *destructive* or; *non-reversible* modification of the address to prevent an attacker from; reversing the check using attacker-controlled inputs.; * It does not completely block speculative execution, and merely prevents; *mis*-speculated paths from leaking secrets from memory (and stalls; speculation until this can be determined).; * It is completely general and makes no fundamental assumptions about the; underlying architecture other than the ability to do branchless conditional; data updates and a lack of value prediction.; * It does not require programmers to identify all possible secret data using; static source code annotations or code vulnerable to a variant #1 style; attack. Limitations of this approach:; * It requires re-compiling source code to insert hardening instruction; sequences. Only software compiled in this mode is protected.; * The performance is heavily dependent on a particular architecture's; implementation strategy. We outline a potential x86 implementation below and; characterize its performance.; * It does ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:5658,load,load,5658,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"d to; a zero of the same sign by this operation. Note that per IEEE-754-2008 6.2, systems that support signaling NaNs with; default exception handling must signal an invalid exception, and produce a; quiet NaN result. This function should always be implementable as multiplication by 1.0, provided; that the compiler does not constant fold the operation. Likewise, division by; 1.0 and ``llvm.minnum(x, x)`` are possible implementations. Addition with; -0.0 is also sufficient provided that the rounding mode is not -Infinity. ``@llvm.canonicalize`` must preserve the equality relation. That is:. - ``(@llvm.canonicalize(x) == x)`` is equivalent to ``(x == x)``; - ``(@llvm.canonicalize(x) == @llvm.canonicalize(y))`` is equivalent; to ``(x == y)``. Additionally, the sign of zero must be conserved:; ``@llvm.canonicalize(-0.0) = -0.0`` and ``@llvm.canonicalize(+0.0) = +0.0``. The payload bits of a NaN must be conserved, with two exceptions.; First, environments which use only a single canonical representation of NaN; must perform said canonicalization. Second, SNaNs must be quieted per the; usual methods. The canonicalization operation may be optimized away if:. - The input is known to be canonical. For example, it was produced by a; floating-point operation that is required by the standard to be canonical.; - The result is consumed only by (or fused with) other floating-point; operations. That is, the bits of the floating-point value are not examined. .. _int_fmuladd:. '``llvm.fmuladd.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare float @llvm.fmuladd.f32(float %a, float %b, float %c); declare double @llvm.fmuladd.f64(double %a, double %b, double %c). Overview:; """""""""""""""""". The '``llvm.fmuladd.*``' intrinsic functions represent multiply-add; expressions that can be fused if the code generator determines that (a) the; target instruction set has support for a fused operation, and (b) that the; fused operation is more efficient than the equivalent, s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:640748,perform,perform,640748,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"d to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instru",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:232349,load,load,232349,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d vectors of these types. Some arithmetic; expression transformations that are mathematically correct and permissible; according to the C and C++ language standards may be incorrect when dealing; with floating-point types, such as reassociation and distribution. Further,; the optimizer may ignore parentheses when computing arithmetic expressions; in circumstances where the parenthesized and unparenthesized expression; express the same mathematical value. For example (a+b)+c is the same; mathematical value as a+(b+c), but the optimizer is free to evaluate the; additions in any order regardless of the parentheses. When enabled, this; option forces the optimizer to honor the order of operations with respect; to parentheses in all circumstances.; Defaults to ``-fno-protect-parens``. Note that floating-point contraction (option `-ffp-contract=`) is disabled; when `-fprotect-parens` is enabled. Also note that in safe floating-point; modes, such as `-ffp-model=precise` or `-ffp-model=strict`, this option; has no effect because the optimizer is prohibited from making unsafe; transformations. .. option:: -fexcess-precision:. The C and C++ standards allow floating-point expressions to be computed as if; intermediate results had more precision (and/or a wider range) than the type; of the expression strictly allows. This is called excess precision; arithmetic.; Excess precision arithmetic can improve the accuracy of results (although not; always), and it can make computation significantly faster if the target lacks; direct hardware support for arithmetic in a particular type. However, it can; also undermine strict floating-point reproducibility. Under the standards, assignments and explicit casts force the operand to be; converted to its formal type, discarding any excess precision. Because data; can only flow between statements via an assignment, this means that the use; of excess precision arithmetic is a reliable local property of a single; statement, and results do not chang",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:66191,optimiz,optimizer,66191,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizer']
Performance,"d when evaluating a block; literal expression have the same ownership semantics as the local variables; they capture. The capture is performed by reading from the captured variable; and initializing the capture variable with that value; the capture variable is; destroyed when the block literal is, i.e. at the end of the enclosing scope. The :ref:`inference <arc.ownership.inference>` rules apply equally to; ``__block`` variables, which is a shift in semantics from non-ARC, where; ``__block`` variables did not implicitly retain during capture. ``__block`` variables of retainable object owner type are moved off the stack; by initializing the heap copy with the result of moving from the stack copy. With the exception of retains done as part of initializing a ``__strong``; parameter variable or reading a ``__weak`` variable, whenever these semantics; call for retaining a value of block-pointer type, it has the effect of a; ``Block_copy``. The optimizer may remove such copies when it sees that the; result is used only as an argument to a call. When a block pointer type is converted to a non-block pointer type (such as; ``id``), ``Block_copy`` is called. This is necessary because a block allocated; on the stack won't get copied to the heap when the non-block pointer escapes.; A block pointer is implicitly converted to ``id`` when it is passed to a; function as a variadic argument. .. _arc.misc.exceptions:. Exceptions; ----------. By default in Objective C, ARC is not exception-safe for normal releases:. * It does not end the lifetime of ``__strong`` variables when their scopes are; abnormally terminated by an exception.; * It does not perform releases which would occur at the end of a; full-expression if that full-expression throws an exception. A program may be compiled with the option ``-fobjc-arc-exceptions`` in order to; enable these, or with the option ``-fno-objc-arc-exceptions`` to explicitly; disable them, with the last such argument ""winning"". .. admonition:: Rati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:97192,optimiz,optimizer,97192,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizer']
Performance,"d with a SIGUSR2 (i.e. on Linux and MacOS X) it will now print a backtrace.; - Move RStringView.h to ROOT/RStringView.hxx and always include ROOT/RStringView.hxx instead of RStringView.h for backward compatibility; - In `TClingCallFunc`, support r-value reference parameters. This paves the way for the corresponding support in PyROOT (implemented now in the latest Cppyy).; - Included the new TSequentialExecutor in ROOT, sharing the interfaces of TExecutor.This should improve code economy when providing a fallback for TThreadExecutor/TProcessExecutor. ### Thread safety; - Resolved several race conditions, dead-locks, performance and order of initialization/destruction issues still lingering because of or despite the new read-write lock mechanism. ## Interpreter. - Enabled use of multi-threaded code from the interpreter.; - Previouslyl multi-threaded code could be run from the interpreter as long as the call starting the threada was the same code that initialized the ROOT global lock, any other uses, including attempting to run the same code a second time in the same session would lead to a dead lock (if any other thread attempted to take on the ROOT lock).; - The interpreter now suspend the ROOT lock (which is taken to protect the interpreter global state) during user code execution. ## I/O Libraries; - LZ4 (with compression level 4) is now the default compression algorithm for new ROOT files (LZ4 is lossless data compression algorithm that is focused on compression and decompression speed, while in ROOT case providing benefit in faster decompression at the price of a bit worse compression ratio comparing to ZLIB); - If two or more files have an identical streamer info record, this is only treated once therewith avoiding to take the global lock.; - Allow writing temporary objects (with same address) in the same TBuffer(s). A new flag to TBuffer*::WriteObject allows to skip the mechanism that prevent the 2nd streaming of an object. This allows the (re)use of temporary o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:2811,multi-thread,multi-threaded,2811,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['multi-thread'],['multi-threaded']
Performance,"d()`) or create a new ROOT file with an RNTuple (`RNTupleWriter::Recreate()`).; Once created, entries are added to an RNTuple either serially (`RNTupleWriter::Fill()`) or in concurrently in multiple threads with the `RNTupleParallelWriter`.; Once committed (e.g. by releasing the RNTupleWriter), the RNTuple is immutable and cannot be amended.; An RNTuple that is currently being written cannot be read. ### RNTupleReader; The RNTupleReader is the primary interface to read and inspect an RNTuple.; An RNTupleReader owns a model: either a model created from the on-disk information or an imposed, user-provided model.; The user-provided model can be limited to a subset of fields.; Data is populated to an explicit `REntry` or the model's default entry through `RNTupleReader::LoadEntry()`. The reader can create `RNTupleView` objects for the independent reading of individual fields.; The reader can create `RBulk` objects for bulk reading of individual fields. Additionally, the reader provides access to a cached copy of the descriptor.; It can display individual entries (`RNTupleReader::Show()`) and summary information (`RNTupleReader::PrintInfo()`). ### RNTupleView<T>; RNTuple views provide read access to individual fields.; Views are created from an RNTupleReader.; Views are templated; for simple types (e.g., `float`, `int`), views provide read-only access directly to an RNTuple page in memory.; Complex types and void views require additional memory copies to populate an object in memory from the column data. A view can iterate over the entry range, over the field range, and over the range of a collection within an entry.; For instance, for a field `std::vector<float> pt`, a view can iterate over all `pt` values of all entries, or over the `pt` values of a particular entry. A view can safely outlive its originating reader.; Once the reader is deconstructed, any attempt to read data will throw an exception, but the view is still properly destructed. Views that originate from th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:13323,cache,cached,13323,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['cache'],['cached']
Performance,"d, a more consistent way of identifying; the instruction's value is to refer to the `MachineOperand` where the value is; defined: independently of which register is defined by that `MachineOperand`. In; the code above, the `DBG_INSTR_REF` instruction refers to instruction number; one, operand zero, while the `ADD32rr` has a `debug-instr-number` attribute; attached indicating that it is instruction number one. De-coupling variable locations from registers avoids difficulties involving; register allocation and optimisation, but requires additional instrumentation; when the instructions are optimised instead. Optimisations that replace; instructions with optimised versions that compute the same value must either; preserve the instruction number, or record a substitution from the old; instruction / operand number pair to the new instruction / operand pair -- see; `MachineFunction::substituteDebugValuesForInst`. If debug info maintenance is; not performed, or an instruction is eliminated as dead code, the variable; location is safely dropped and marked ""optimised out"". The exception is; instructions that are mutated rather than replaced, which always need debug info; maintenance. # Register allocator considerations. When the register allocator runs, debugging instructions do not directly refer; to any virtual registers, and thus there is no need for expensive location; maintenance during regalloc (i.e. `LiveDebugVariables`). Debug instructions are; unlinked from the function, then linked back in after register allocation; completes. The exception is `PHI` instructions: these become implicit definitions at; control flow merges once regalloc finishes, and any debug numbers attached to; `PHI` instructions are lost. To circumvent this, debug numbers of `PHI`s are; recorded at the start of register allocation (`phi-node-elimination`), then; `DBG_PHI` instructions are inserted after regalloc finishes. This requires some; maintenance of which register a variable is located in d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InstrRefDebugInfo.md:2949,perform,performed,2949,interpreter/llvm-project/llvm/docs/InstrRefDebugInfo.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InstrRefDebugInfo.md,1,['perform'],['performed']
Performance,"d-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:276700,load,load,276700,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"d-y"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workitem.id.y intrinsic. ""amdgpu-no-workitem-id-z"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workitem.id.z intrinsic. ""amdgpu-no-workgroup-id-x"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.x intrinsic. ""amdgpu-no-workgroup-id-y"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.y intrinsic. ""amdgpu-no-workgroup-id-z"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.z intrinsic. ""amdgpu-no-dispatch-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.ptr intrinsic. ""amdgpu-no-implicitarg-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.implicitarg.ptr intrinsic. ""amdgpu-no-dispatch-id"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.id intrinsic. ""amdgpu-no-queue-ptr"" Similar to amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.queue.ptr intrinsic. Note that unlike the other ABI hint; attributes, the queue pointer may be required in situations where the; intrinsic call does not directly appear in the program. Some subtargets; require the queue pointer for to handle some addrspacecasts, as well; as the llvm.amdgcn.is.shared, llvm.amdgcn.is.private, llvm.trap, and; llvm.debug intrinsics. ""amdgpu-no-hostcall-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to the hostcall buffer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-heap-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to an initialized memory buffer; that conforms to the requirements of the malloc/free device library V1; version implementation. If this attribute is absent, then the; amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-multigrid-sync-arg"" Similar to amdgpu-no-imp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:49673,queue,queue,49673,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"d. 3. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc1=1; store atomic release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; sc0=1 sc1=1; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:309692,perform,performing,309692,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"d. The speed gain; of this optimization depends much on the structure of the pdf: in models with many free parameters most of the; likelihood evaluations are executed when MINUIT calculates numerical likelihood derivatives which vary ; one parameter at a time and the speedup is potentially larger. In models with few free parameters the; effect will be smaller. The new per-component caching strategy is enabled by default for all pdfs that are a component of; a RooAddPdf or a RooRealSumPdf, unless that component is a RooProdPdf or a RooProduct, in that; case the components of the product are cached instead of the product itself. You can disable this; new optimization by adding Optimize(1) to the RooAbsPdf::fitTo() command line (0 = no caching,; 1 = cache constant terms only, 2 = cache also variable terms according to above mentioned strategy (DEFAULT)). It is also possible to tune this 'cache-and-track' optimization to perform a more fine-grained caching; of components than Optimize(2) implements: to do so, call arg->setAttribute(""CacheAndTrack"") on each; pdf component that you'd like to be cache-and-tracked individually. New pdf/data attach mechanism in likelihood objects (RooAbsOptTestStatistic). The new mechanism only; reattaches the dataset branch buffers and not the RooRealVars representing the data. This new designs; allows for a much faster RooAbsTestStatistic::setData() implementation, which changes the dataset in; an existing likelihood object. This will speed up RooStats tools based on 'simple' likelihood models; substantially. Automatic detections of 'binned' pdfs and automatic generation of binned data in generate(). RooFit will; now automatically generate binned pdf shapes. Binned pdfs shapes are fundamentally RooHistPdf and RooHistFuncs; (with interpolation order set to zero). Products and sums of exclusively binned shapes are also recognized; as binned shapes. For such binned shapes generate() will now by default follow the 'binned' strategy ; -- that i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:3709,tune,tune,3709,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,12,"['Cache', 'Optimiz', 'cache', 'optimiz', 'perform', 'tune']","['CacheAndTrack', 'Optimize', 'cache-and-track', 'cache-and-tracked', 'optimization', 'perform', 'tune']"
Performance,"d. There is no support for operator precedence, but parentheses; can be used to change the evaluation order. The supported operators are:. * ``+`` - Returns the sum of its two operands.; * ``-`` - Returns the difference of its two operands. The syntax of a function call is ``<name>(<arguments>)`` where:. * ``name`` is a predefined string literal. Accepted values are:. * add - Returns the sum of its two operands.; * div - Returns the quotient of its two operands.; * max - Returns the largest of its two operands.; * min - Returns the smallest of its two operands.; * mul - Returns the product of its two operands.; * sub - Returns the difference of its two operands. * ``<arguments>`` is a comma separated list of expressions. For example:. .. code-block:: llvm. ; CHECK: load r[[#REG:]], [r0]; ; CHECK: load r[[#REG+1]], [r1]; ; CHECK: Loading from 0x[[#%x,ADDR:]]; ; CHECK-SAME: to 0x[[#ADDR + 7]]. The above example would match the text:. .. code-block:: gas. load r5, [r0]; load r6, [r1]; Loading from 0xa0463440 to 0xa0463447. but would not match the text:. .. code-block:: gas. load r5, [r0]; load r7, [r1]; Loading from 0xa0463440 to 0xa0463443. Due to ``7`` being unequal to ``5 + 1`` and ``a0463443`` being unequal to; ``a0463440 + 7``. A numeric variable can also be defined to the result of a numeric expression,; in which case the numeric expression constraint is checked and if verified the; variable is assigned to the value. The unified syntax for both checking a; numeric expression and capturing its value into a numeric variable is thus; ``[[#%<fmtspec>,<NUMVAR>: <constraint> <expr>]]`` with each element as; described previously. One can use this syntax to make a testcase more; self-describing by using variables instead of values:. .. code-block:: gas. ; CHECK: mov r[[#REG_OFFSET:]], 0x[[#%X,FIELD_OFFSET:12]]; ; CHECK-NEXT: load r[[#]], [r[[#REG_BASE:]], r[[#REG_OFFSET]]]. which would match:. .. code-block:: gas. mov r4, 0xC; load r6, [r5, r4]. The ``--enable-var-scope``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:32316,load,load,32316,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,2,['load'],['load']
Performance,"d.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:231746,load,load,231746,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:300698,cache,cache,300698,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"d.; - `TTree::Branch()` now complains for wrong leaf list strings, e.g. ""value/F[4]"" (which should really be spelled as ""value[4]/F"").; - Allow reading of older version of TTreePerfStats (ROOT-8520); - In `TTree::OptimizeBaskets()` do not call GetBasket(0) to avoid disc reads; - It is now possible to define the precision of the default histogram created; by `TTree::Draw`. Three new parameters are available in `$ROOTSYS/etcsystem.rootrc`; ```{.cpp}; Hist.Precision.1D: float; Hist.Precision.2D: float; Hist.Precision.3D: float; ```; the default values are `float`. They can be set to `double`.; - Fix ROOT-8742: TTree::SetBranchAddress could not be invoked safely even when dealing with the same tree obtained from the same file opened in different threads.; - TTree::Branch() now complains if a ""name[size]/F"" branch specification is passed wrongly (e.g. as ""name/F[size]""). ### TDataFrame; - Creation of the TDataFrame class. The TDataFrame allows to interact with data; stored in columnar format in a functional and intuitive way in order to perform; data analysis. Parallelism is accessible simply by activating implicit; multi-threading with the ROOT::EnableImplicitMT() function.; In a nutshell, the functionality provided is:; - Create and fill histograms with one single method invocation; - Express filtering of entries with strings, lambdas or functions; - Easy creation of efficiencies of cut-flows; - Possibility to run on ranges of entries; - Creating columns not present in the original dataset; - Chain multiple actions to be executed on the same event loop; - Creation of events on-the-fly (e.g. via Pythia or user-define generator functors), with no need for an input TTree; - Snapshot on a rootfile the dataset after cuts and after augmentation with columns created by the user; - Run analyses expressed as chains of actions in parallel in a transparent way for the user; See [the online documentation](https://root.cern.ch/doc/master/classROOT_1_1Experimental_1_1TDF_1_1TDataFram",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:6260,perform,perform,6260,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,1,['perform'],['perform']
Performance,"d.; Fixes (`#67976 <https://github.com/llvm/llvm-project/issues/67976>`_). - Fix crash when using an immediate-escalated function at global scope.; (`#82258 <https://github.com/llvm/llvm-project/issues/82258>`_); - Correctly immediate-escalate lambda conversion functions.; (`#82258 <https://github.com/llvm/llvm-project/issues/82258>`_); - Fix a crash when an unresolved overload set is encountered on the RHS of a ``.*`` operator.; (`#53815 <https://github.com/llvm/llvm-project/issues/53815>`_). - Fixed a regression in CTAD that a friend declaration that befriends itself may cause; incorrect constraint substitution.; (`#86769 <https://github.com/llvm/llvm-project/issues/86769>`_). Bug Fixes to AST Handling; ^^^^^^^^^^^^^^^^^^^^^^^^^; - Fixed an import failure of recursive friend class template.; `Issue 64169 <https://github.com/llvm/llvm-project/issues/64169>`_; - Remove unnecessary RecordLayout computation when importing UnaryOperator. The; computed RecordLayout is incorrect if fields are not completely imported and; should not be cached.; `Issue 64170 <https://github.com/llvm/llvm-project/issues/64170>`_; - Fixed ``hasAnyBase`` not binding nodes in its submatcher.; (`#65421 <https://github.com/llvm/llvm-project/issues/65421>`_); - Fixed a bug where RecursiveASTVisitor fails to visit the; initializer of a bitfield.; `Issue 64916 <https://github.com/llvm/llvm-project/issues/64916>`_; - Fixed a bug where range-loop-analysis checks for trivial copyability,; rather than trivial copy-constructibility; `Issue 47355 <https://github.com/llvm/llvm-project/issues/47355>`_; - Fixed a bug where Template Instantiation failed to handle Lambda Expressions; with certain types of Attributes.; (`#76521 <https://github.com/llvm/llvm-project/issues/76521>`_). Miscellaneous Bug Fixes; ^^^^^^^^^^^^^^^^^^^^^^^. Miscellaneous Clang Crashes Fixed; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; - Fixed a crash when parsing top-level ObjC blocks that aren't properly; terminated. Clang should now also recov",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst:56599,cache,cached,56599,interpreter/llvm-project/clang/docs/ReleaseNotes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst,1,['cache'],['cached']
Performance,"d.; Painting it produced a dot with the X11 backend.; * New class `TRatioPlot` implemented by Paul Gessinger <hello@paulgessinger.com>.; Class for displaying ratios, differences and fit residuals. `TRatioPlot` has two constructors, one which accepts two histograms, and is responsible; for setting up the calculation of ratios and differences. This calculation is in part; delegated to `TEfficiency`. A single option can be given as a parameter, that is; used to determine which procedure is chosen. The remaining option string is then; passed through to the calculation, if applicable. Several examples illustrate how to use this class. See:; `$ROOTSYS/tutorials/hist/ratioplot?.C`. * New option ""I"" allowing to draw TGraph with invisible axis (used by `TRatioPlot`);. ## New histogram drawing options. ### COL2; COL2 is a new rendering technique providing potential performance improvements; compared to the standard COL option. The performance comparison of the COL2 to; the COL option depends on the histogram and the size of the rendering region in; the current pad. In general, a small (approx. less than 100 bins per axis),; sparsely populated TH2 will render faster with the COL option. However, for larger histograms (approx. more than 100 bins per axis) that are; not sparse, the COL2 option will provide up to 20 times performance improvements.; For example, a 1000x1000 bin TH2 that is not sparse will render an order of; magnitude faster with the COL2 option. The COL2 option will also scale its performance based on the size of the pixmap; the histogram image is being rendered into. It also is much better optimized for; sessions where the user is forwarding X11 windows through an `ssh` connection. For the most part, the COL2 and COLZ2 options are a drop in replacement to the COL; and COLZ options. There is one major difference and that concerns the treatment of; bins with zero content. The COL2 and COLZ2 options color these bins the color of zero. This has been implemented by Je",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:21919,perform,performance,21919,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['perform'],['performance']
Performance,"d.; The following calling conventions are supported by LLVM, and more may be; added in the future:. ""``ccc``"" - The C calling convention; This calling convention (the default if no other calling convention; is specified) matches the target C calling conventions. This calling; convention supports varargs function calls and tolerates some; mismatch in the declared prototype and implemented declaration of; the function (as does normal C).; ""``fastcc``"" - The fast calling convention; This calling convention attempts to make calls as fast as possible; (e.g. by passing things in registers). This calling convention; allows the target to use whatever tricks it wants to produce fast; code for the target, without having to conform to an externally; specified ABI (Application Binary Interface). `Tail calls can only; be optimized when this, the tailcc, the GHC or the HiPE convention is; used. <CodeGenerator.html#tail-call-optimization>`_ This calling; convention does not support varargs and requires the prototype of all; callees to exactly match the prototype of the function definition.; ""``coldcc``"" - The cold calling convention; This calling convention attempts to make code in the caller as; efficient as possible under the assumption that the call is not; commonly executed. As such, these calls often preserve all registers; so that the call does not break any live ranges in the caller side.; This calling convention does not support varargs and requires the; prototype of all callees to exactly match the prototype of the; function definition. Furthermore the inliner doesn't consider such function; calls for inlining.; ""``ghccc``"" - GHC convention; This calling convention has been implemented specifically for use by; the `Glasgow Haskell Compiler (GHC) <http://www.haskell.org/ghc>`_.; It passes everything in registers, going to extremes to achieve this; by disabling callee save registers. This calling convention should; not be used lightly but only for specific situations such a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:12907,optimiz,optimization,12907,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"d.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer to a; function from a virtual table pointer using metadata. Otherwise, its semantic is; identical to the ``llvm.type.checked.load`` intrinsic. A relative pointer is a pointer to an offset to the pointed to value. The; address of the underlying pointer of the relative pointer is obtained by adding; the offset to the address of the offset value. '``llvm.arithmetic.fence``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.arithmetic.fence(<type> <op>). Overview:; """""""""""""""""". The purpose of the ``llvm.arithmetic.fence`` intrinsic; is to prevent the optimizer from performing fast-math optimizations,; particularly reassociation,; between the argument and the expression that contains the argument.; It can be used to preserve the parentheses in the source language. Arguments:; """""""""""""""""""". The ``llvm.arithmetic.fence`` intrinsic takes only one argument.; The argument and the return value are floating-point numbers,; or vector floating-point numbers, of the same type. Semantics:; """""""""""""""""""". This intrinsic returns the value of its operand. The optimizer can optimize; the argument, but the optimizer cannot hoist any component of the operand; to the containing context, and the optimizer cannot move the calculation of; any expression in the containing context into the operand. '``llvm.donothing``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.donothing() nounwind memory(none). Overview:; """""""""""""""""". The ``llvm.donothing`` intrinsic doesn't perform any operation. It's one of only; three intrinsics (besides ``llvm.experimental.patchpoint`` and; ``llvm.experim",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:940716,optimiz,optimizer,940716,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['optimiz', 'perform']","['optimizations', 'optimizer', 'performing']"
Performance,"d/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:301223,load,loads,301223,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"d; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_load; glc=1 dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singleth",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:344848,load,load,344848,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d; - system - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acqu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214548,load,load,214548,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"d; TGeoManager::Safety () is invoked. A safety value less than; TGeoShape::Tolerance() will set the flag IsOnBoundary to true.; On the other hand, a safety value bigger than the proposed step will; stop the computation of the distance to next boundary, returning the; current geometry location with the meaning that the proposed step is; safe. The next stage is to check if computation of the distance to a give; physical object specified by a path was required. If this is the case,; the modeller changes the state to point to the required object, converts; the current point and direction coordinates to the local frame of this; object and computes the distance to its shape. The node returned is the; one pointed by the input path in case the shape is crossed; otherwise; the returned value is NULL. In case the distance to next crossed; boundary is required, the current point has to be physically INSIDE the; shape pointed by the current volume. This is only insured in case a call; to TGeoManager::FindNode() was performed for the current point.; Therefore, the first step is to convert the global current point and; direction in the local reference frame of the current volume and to; compute the distance to exit its shape from inside. The returned value; is again compared to the maximum allowed step (the proposed one) and in; case the distance is safe no other action is performed and the proposed; step is approved. In case the boundary is closer, the computed distance; is taken as maximum allowed step. For optimization purposed, for; particles starting very close to the current volume boundary (less than; 0.01 microns) and exiting the algorithm stops here. After computing the distance to exit the current node, the distance to; the daughter of the current volume which is crossed next is computed by; TGeoManager::FindNextDaughterBoundary(). This computes the; distance to all daughter candidates that can be possibly crossed by; using volume voxelization. The algorithm is efficient",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:122972,perform,performed,122972,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"d; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_store; store atomic release - agent - global 1. s_w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:355564,load,load,355564,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"d; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; atomicrmw release - agent - global 1.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:358201,load,load,358201,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"d; completes, pass -V to scan-build. Recommended Usage Guidelines; This section describes a few recommendations with running the analyzer.; ALWAYS analyze a project in its ""debug"" configuration; Most projects can be built in a ""debug"" mode that enables assertions.; Assertions are picked up by the static analyzer to prune infeasible paths, which; in some cases can greatly reduce the number of false positives (bogus error; reports) emitted by the tool.; Another option is to use --force-analyze-debug-code flag of; scan-build tool which would enable assertions automatically.; Use verbose output when debugging scan-build; scan-build takes a -v option to emit verbose output about; what it's doing; two -v options emit more information. Redirecting the; output of scan-build to a text file (make sure to redirect standard; error) is useful for filing bug reports against scan-build or the; analyzer, as we can see the exact options (and files) passed to the analyzer.; For more comprehensible logs, don't perform a parallel build.; Run './configure' through scan-build; If an analyzed project uses an autoconf generated configure script,; you will probably need to run configure script through; scan-build in order to analyze the project.; Example. $ scan-build ./configure; $ scan-build --keep-cc make. The reason configure also needs to be run through; scan-build is because scan-build scans your source files by; interposing on the compiler. This interposition is currently done by; scan-build temporarily setting the environment variable CC to; ccc-analyzer. The program ccc-analyzer acts like a fake; compiler, forwarding its command line arguments over to the compiler to perform; regular compilation and clang to perform static analysis.; Running configure typically generates makefiles that have hardwired; paths to the compiler, and by running configure through; scan-build that path is set to ccc-analyzer. Analyzing iPhone Projects; Conceptually Xcode projects for iPhone applications are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/scan-build.html:6989,perform,perform,6989,interpreter/llvm-project/clang/www/analyzer/scan-build.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/scan-build.html,2,['perform'],['perform']
Performance,"d; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above exam",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:10397,cache,cached,10397,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['cache'],['cached']
Performance,"d; rewrite the example at the beginning of `Concrete Bases and Deeper; Hierarchies`_ as:. .. code-block:: c++. enum ShapeKind {; SK_Square,; + SK_SpecialSquare,; + SK_OtherSpecialSquare,; + SK_LastSquare,; SK_Circle; }; ...; // Square::classof(); - static bool classof(const Shape *S) {; - return S->getKind() == SK_Square;; - }; + static bool classof(const Shape *S) {; + return S->getKind() >= SK_Square &&; + S->getKind() <= SK_LastSquare;; + }. Then, adding new subclasses is easy:. .. code-block:: c++. enum ShapeKind {; SK_Square,; SK_SpecialSquare,; SK_OtherSpecialSquare,; + SK_SomewhatSpecialSquare,; SK_LastSquare,; SK_Circle; }. Notice that ``Square::classof`` does not need to be changed. .. _classof-contract:. The Contract of ``classof``; ---------------------------. To be more precise, let ``classof`` be inside a class ``C``. Then the; contract for ``classof`` is ""return ``true`` if the dynamic type of the; argument is-a ``C``"". As long as your implementation fulfills this; contract, you can tweak and optimize it as much as you want. For example, LLVM-style RTTI can work fine in the presence of; multiple-inheritance by defining an appropriate ``classof``.; An example of this in practice is; `Decl <https://clang.llvm.org/doxygen/classclang_1_1Decl.html>`_ vs.; `DeclContext <https://clang.llvm.org/doxygen/classclang_1_1DeclContext.html>`_; inside Clang.; The ``Decl`` hierarchy is done very similarly to the example setup; demonstrated in this tutorial.; The key part is how to then incorporate ``DeclContext``: all that is needed; is in ``bool DeclContext::classof(const Decl *)``, which asks the question; ""Given a ``Decl``, how can I determine if it is-a ``DeclContext``?"".; It answers this with a simple switch over the set of ``Decl`` ""kinds"", and; returning true for ones that are known to be ``DeclContext``'s. .. TODO::. Touch on some of the more advanced features, like ``isa_impl`` and; ``simplify_type``. However, those two need reference documentation in; the form",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst:11014,optimiz,optimize,11014,interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,1,['optimiz'],['optimize']
Performance,"d; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23760,optimiz,optimized,23760,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,dDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(testMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Minimization-Cpu COMMAND testMinimizationCpu). # tests using TReference architecture; if ( reference-tests). # DNN - Activation Functions; ROOT_EXECUTABLE(testActivationFunctions TestActivationFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Activation-Functions COMMAND testActivationFunctions). # DNN - Loss Functions; ROOT_EXECUTABLE(testLossFunctions TestLossFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Loss-Functions COMMAND testLossFunctions). # DNN - Derivatives; ROOT_EXECUTABLE(testDerivatives TestDerivatives.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Derivatives COMMAND testDerivatives). # DNN - Backpropagation; ROOT_EXECUTABLE(testBackpropagation TestBackpropagation.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation COMMAND testBackpropagation).,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:7159,Load,Loader-Cpu,7159,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Load'],['Loader-Cpu']
Performance,dDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(testMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Minimization-Cpu COMMAND testMinimizationCpu). # tests using TReference architecture; if ( reference-tests). # DNN - Activation Functions; ROOT_EXECUTABLE(testActivationFunctions TestActivationFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Activation-Functions COMMAND testActivationFunctions). # DNN - Loss Functions; ROOT_EXECUTABLE(testLossFunctions TestLossFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMV,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:6768,Optimiz,Optimization-Cpu,6768,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cpu']
Performance,"dary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23855,load,loads,23855,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loads']
Performance,"data already loaded from memory and; residing in registers or leaked through other side-channels in; non-speculative execution. Code dealing with this, e.g cryptographic; routines, already uses constant-time algorithms and code to prevent; side-channels. Such code should also scrub registers of secret data following; [these; guidelines](https://github.com/HACS-workshop/spectre-mitigations/blob/master/crypto_guidelines.md).; * To achieve reasonable performance, many loads may not be checked, such as; those with compile-time fixed addresses. This primarily consists of accesses; at compile-time constant offsets of global and local variables. Code which; needs this protection and intentionally stores secret data must ensure the; memory regions used for secret data are necessarily dynamic mappings or heap; allocations. This is an area which can be tuned to provide more comprehensive; protection at the cost of performance.; * [Hardened loads](#hardening-the-address-of-the-load) may still load data from; _valid_ addresses if not _attacker-controlled_ addresses. To prevent these; from reading secret data, the low 2gb of the address space and 2gb above and; below any executable pages should be protected. Credit:; * The core idea of tracing misspeculation through data and marking pointers to; block misspeculated loads was developed as part of a HACS 2018 discussion; between Chandler Carruth, Paul Kocher, Thomas Pornin, and several other; individuals.; * Core idea of masking out loaded bits was part of the original mitigation; suggested by Jann Horn when these attacks were reported. ### Indirect Branches, Calls, and Returns. It is possible to attack control flow other than conditional branches with; variant #1 style mispredictions.; * A prediction towards a hot call target of a virtual method can lead to it; being speculatively executed when an expected type is used (often called; ""type confusion"").; * A hot case may be speculatively executed due to prediction instead of the; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:7691,load,loads,7691,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,3,['load'],"['load', 'loads']"
Performance,"data node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initialization; checkCudaErrors(cuInit(0));; checkCudaErrors(cuDeviceGetCount(&devCount));; checkCudaErrors(cuDeviceGet(&device, 0));. char name[128];; checkCudaErrors(cuDeviceGetName(name, 128, device));; std::cout << ""Using CUDA Device [0]: "" << name << ""\n"";. int devMajor, devMinor;;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:19212,perform,perform,19212,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['perform'],['perform']
Performance,"data races and memory access exceptions; %oldval = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %value, <16 x float> %oldval; store <16 x float> %res, ptr %ptr, align 4. Masked Vector Gather and Scatter Intrinsics; -------------------------------------------. LLVM provides intrinsics for vector gather and scatter operations. They are similar to :ref:`Masked Vector Load and Store <int_mload_mstore>`, except they are designed for arbitrary memory accesses, rather than sequential memory accesses. Gather and scatter also employ a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits are off, no memory is accessed. .. _int_mgather:. '``llvm.masked.gather.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data are multiple scalar values of any integer, floating-point or pointer data type gathered together into one vector. ::. declare <16 x float> @llvm.masked.gather.v16f32.v16p0(<16 x ptr> <ptrs>, i32 <alignment>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x double> @llvm.masked.gather.v2f64.v2p1(<2 x ptr addrspace(1)> <ptrs>, i32 <alignment>, <2 x i1> <mask>, <2 x double> <passthru>); declare <8 x ptr> @llvm.masked.gather.v8p0.v8p0(<8 x ptr> <ptrs>, i32 <alignment>, <8 x i1> <mask>, <8 x ptr> <passthru>). Overview:; """""""""""""""""". Reads scalar values from arbitrary memory locations and gathers them into one vector. The memory locations are provided in the vector of pointers '``ptrs``'. The memory is accessed according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is a vector of pointers which hold",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:848531,load,loaded,848531,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"data using their actual underlying; integer type. Previously they were treated as chars, which meant they could; alias with all other types.; - Clang now supports the C-only attribute ``counted_by``. When applied to a; struct's flexible array member, it points to the struct field that holds the; number of elements in the flexible array member. This information can improve; the results of the array bound sanitizer and the; ``__builtin_dynamic_object_size`` builtin. C23 Feature Support; ^^^^^^^^^^^^^^^^^^^; - Clang now accepts ``-std=c23`` and ``-std=gnu23`` as language standard modes,; and the ``__STDC_VERSION__`` macro now expands to ``202311L`` instead of its; previous placeholder value. Clang continues to accept ``-std=c2x`` and; ``-std=gnu2x`` as aliases for C23 and GNU C23, respectively.; - Clang now supports `requires c23` for module maps.; - Clang now supports ``N3007 Type inference for object definitions``. - Clang now supports ``<stdckdint.h>`` which defines several macros for performing; checked integer arithmetic. It is also exposed in pre-C23 modes. - Completed the implementation of; `N2508 <https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2508.pdf>`_. We; previously implemented allowing a label at the end of a compound statement,; and now we've implemented allowing a label to be followed by a declaration; instead of a statement.; - Implemented; `N2940 <https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2940.pdf>`_ which; removes support for trigraphs in C23 and later. In earlier language modes,; trigraphs remain enabled by default in conforming modes (e.g. ``-std=c17``); and disabled by default in GNU and Microsoft modes (e.g., ``-std=gnu17`` or; ``-fms-compatibility``). If needed, you can enable trigraphs by passing; ``-ftrigraphs``. Non-comprehensive list of changes in this release; -------------------------------------------------. * Clang now has a ``__builtin_vectorelements()`` function that determines the number of elements in a vector.; For fixed-s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst:13130,perform,performing,13130,interpreter/llvm-project/clang/docs/ReleaseNotes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst,1,['perform'],['performing']
Performance,"data; ^^^^^^^^^^^^^^^^^^^^^^^. ``irr_loop`` metadata may be attached to the terminator instruction of a basic; block that's an irreducible loop header (note that an irreducible loop has more; than once header basic blocks.) If ``irr_loop`` metadata is attached to the; terminator instruction of a basic block that is not really an irreducible loop; header, the behavior is undefined. The intent of this metadata is to improve the; accuracy of the block frequency propagation. For example, in the code below, the; block ``header0`` may have a loop header weight (relative to the other headers of; the irreducible loop) of 100:. .. code-block:: llvm. header0:; ...; br i1 %cmp, label %t1, label %t2, !irr_loop !0. ...; !0 = !{""loop_header_weight"", i64 100}. Irreducible loop header weights are typically based on profile data. .. _md_invariant.group:. '``invariant.group``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The experimental ``invariant.group`` metadata may be attached to; ``load``/``store`` instructions referencing a single metadata with no entries.; The existence of the ``invariant.group`` metadata on the instruction tells; the optimizer that every ``load`` and ``store`` to the same pointer operand; can be assumed to load or store the same; value (but see the ``llvm.launder.invariant.group`` intrinsic which affects; when two pointers are considered the same). Pointers returned by bitcast or; getelementptr with only zero indices are considered the same. Examples:. .. code-block:: llvm. @unknownPtr = external global i8; ...; %ptr = alloca i8; store i8 42, ptr %ptr, !invariant.group !0; call void @foo(ptr %ptr). %a = load i8, ptr %ptr, !invariant.group !0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.grou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:315977,load,load,315977,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"data; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables creating predicated instructions; for the loop, which can enable folding of the scalar epilogue loop into the; main loop. The first operand is the string; ``llvm.loop.vectorize.predicate.enable`` and the second operand is a bit. If; the bit operand value is 1 vectorization is enabled. A value of 0 disables; vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.predicate.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.predicate.enable"", i1 1}. '``llvm.loop.vectorize.scalable.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables scalable vectorization for the; loop, and only has any effect if vectorization for the loop is already enabled.; The first operand is the string ``llvm.loop.vectorize.scalable.enable``; and the second operand is a bit. If the bit operand value is 1 scalable; vectorization is enabled, whereas a value of 0 reverts to the default fixed; width vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.scalable.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.scalable.enable"", i1 1}. '``llvm.loop.vectorize.width``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata sets the target width of the vectorizer. The first; operand is the string ``llvm.loop.vectorize.width`` and the second; operand is an integer specifying the width. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.width"", i32 4}. Note that setting ``llvm.loop.vectorize.width`` to 1 disables; vectorization of the loop. If ``llvm.loop.vectorize.width`` is set to; 0 or if the loop does not have this metadata the width will be; determined automatically. '``llvm.loop.vectorize.followup_vectorized``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which loop attributes the vectorized loop will; have. See :ref:`transformation-metadata` for detail",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:298844,scalab,scalable,298844,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"date, time, or any other ""random"" data, ``bugpoint`` may; misinterpret differences in these data, when output, as the result of a; miscompilation. Programs should be temporarily modified to disable outputs; that are likely to vary from run to run. * In the `crash debugger`_, ``bugpoint`` does not distinguish different crashes; during reduction. Thus, if new crash or miscompilation happens, ``bugpoint``; will continue with the new crash instead. If you would like to stick to; particular crash, you should write check scripts to validate the error; message, see ``-compile-command`` in :doc:`CommandGuide/bugpoint`. * In the code generator and miscompilation debuggers, debugging will go faster; if you manually modify the program or its inputs to reduce the runtime, but; still exhibit the problem. * ``bugpoint`` is extremely useful when working on a new optimization: it helps; track down regressions quickly. To avoid having to relink ``bugpoint`` every; time you change your optimization however, have ``bugpoint`` dynamically load; your optimization with the ``-load`` option. * ``bugpoint`` can generate a lot of output and run for a long period of time.; It is often useful to capture the output of the program to file. For example,; in the C shell, you can run:. .. code-block:: console. $ bugpoint ... |& tee bugpoint.log. to get a copy of ``bugpoint``'s output in the file ``bugpoint.log``, as well; as on your terminal. * ``bugpoint`` cannot debug problems with the LLVM linker. If ``bugpoint``; crashes before you see its ""All input ok"" message, you might try ``llvm-link; -v`` on the same set of input files. If that also crashes, you may be; experiencing a linker bug. * ``bugpoint`` is useful for proactively finding bugs in LLVM. Invoking; ``bugpoint`` with the ``-find-bugs`` option will cause the list of specified; optimizations to be randomized and applied to the program. This process will; repeat until a bug is found or the user kills ``bugpoint``. * ``bugpoint`` can produ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:6901,optimiz,optimization,6901,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,4,"['load', 'optimiz']","['load', 'optimization']"
Performance,"dation if the outcome is less; predictable than the annotation suggests. Even when the annotation is correct; 90% of the time, it may be beneficial to either remove the annotation or to use; a different intrinsic that can communicate the probability more directly. Because this may be too strict, MisExpect diagnostics are not enabled by; default, and support an additional flag to tolerate some deviation from the; exact thresholds. The ``-fdiagnostic-misexpect-tolerance=N`` accepts; deviations when comparing branch weights within ``N%`` of the expected values.; So passing ``-fdiagnostic-misexpect-tolerance=5`` will not report diagnostic messages; if the branch weight from the profile is within 5% of the weight added by; the ``llvm.expect`` intrinsic. MisExpect diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -Rpass=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -Wmisexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. .. option:: -fdiagnostic-misexpect-tolerance=N. Relaxes misexpect checking to tolerate profiling values within N% of the; expected branch weight. e.g., a value of ``N=5`` allows misexpect to check against; ``0.95 * Threshold``. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+---------------------------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst:2322,optimiz,optimization,2322,interpreter/llvm-project/clang/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst,1,['optimiz'],['optimization']
Performance,"dcard to deal with anonymous namespace.; type:Namespace2::*::BadStructName; # Disable initialization-order checks for globals:; global:bad_init_global=init; type:*BadInitClassSubstring*=init; src:bad/init/files/*=init. Suppressing memory leaks; ------------------------. Memory leak reports produced by :doc:`LeakSanitizer` (if it is run as a part; of AddressSanitizer) can be suppressed by a separate file passed as. .. code-block:: bash. LSAN_OPTIONS=suppressions=MyLSan.supp. which contains lines of the form `leak:<pattern>`. Memory leak will be; suppressed if pattern matches any function name, source file name, or; library name in the symbolized stack trace of the leak report. See; `full documentation; <https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer#suppressions>`_; for more details. Code generation control; =======================. Instrumentation code outlining; ------------------------------. By default AddressSanitizer inlines the instrumentation code to improve the; run-time performance, which leads to increased binary size. Using the; (clang flag ``-fsanitize-address-outline-instrumentation` default: ``false``); flag forces all code instrumentation to be outlined, which reduces the size; of the generated code, but also reduces the run-time performance. Limitations; ===========. * AddressSanitizer uses more real memory than a native run. Exact overhead; depends on the allocations sizes. The smaller the allocations you make the; bigger the overhead is.; * AddressSanitizer uses more stack memory. We have seen up to 3x increase.; * On 64-bit platforms AddressSanitizer maps (but not reserves) 16+ Terabytes of; virtual address space. This means that tools like ``ulimit`` may not work as; usually expected.; * Static linking of executables is not supported. Supported Platforms; ===================. AddressSanitizer is supported on:. * Linux i386/x86\_64 (tested on Ubuntu 12.04); * macOS 10.7 - 10.11 (i386/x86\_64); * iOS Simulator; * Android ARM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst:11917,perform,performance,11917,interpreter/llvm-project/clang/docs/AddressSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst,1,['perform'],['performance']
Performance,"dcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36036,load,loads,36036,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['loads']
Performance,"dditional; constructs would be fairly low, but would give us lots of important; information... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to chec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5582,load,loading,5582,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,1,['load'],['loading']
Performance,"ddress space back to a small positive address. Small, negative addresses will; fault in user-mode for most operating systems, but targets which need the high; address space to be user accessible may need to adjust the exact sequence used; above. Additionally, the low addresses will need to be marked unreadable by the; OS to fully harden the load. ###### RIP-relative addressing is even easier to break. There is a common addressing mode idiom that is substantially harder to check:; addressing relative to the instruction pointer. We cannot change the value of; the instruction pointer register and so we have the harder problem of forcing; `%base + scale * %index + offset` to be an invalid address, by *only* changing; `%index`. The only advantage we have is that the attacker also cannot modify; `%base`. If we use the fast instruction sequence above, but only apply it to; the index, we will always access `%rip + (scale * -1) + offset`. If the; attacker can find a load which with this address happens to point to secret; data, then they can reach it. However, the loader and base libraries can also; simply refuse to map the heap, data segments, or stack within 2gb of any of the; text in the program, much like it can reserve the low 2gb of address space. ###### The flag registers again make everything hard. Unfortunately, the technique of using `orq`-instructions has a serious flaw on; x86. The very thing that makes it easy to accumulate state, the flag registers; containing predicates, causes serious problems here because they may be alive; and used by the loading instruction or subsequent instructions. On x86, the; `orq` instruction **sets** the flags and will override anything already there.; This makes inserting them into the instruction stream very hazardous.; Unfortunately, unlike when hardening the loaded value, we have no fallback here; and so we must have a fully general approach available. The first thing we must do when generating these sequences is try to analyze; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:30874,load,load,30874,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"ddress space from the; :ref:`datalayout string<langref_datalayout>` is meaningful only if the; target has assigned it a semantics. If the returned pointer is used by :ref:`llvm.lifetime.start <int_lifestart>`,; the returned object is initially dead.; See :ref:`llvm.lifetime.start <int_lifestart>` and; :ref:`llvm.lifetime.end <int_lifeend>` for the precise semantics of; lifetime-manipulating intrinsics. Example:; """""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; %ptr = alloca i32, i32 4 ; yields ptr; %ptr = alloca i32, i32 4, align 1024 ; yields ptr; %ptr = alloca i32, align 1024 ; yields ptr. .. _i_load:. '``load``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = load [volatile] <ty>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.load !<empty_node>][, !invariant.group !<empty_node>][, !nonnull !<empty_node>][, !dereferenceable !<deref_bytes_node>][, !dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argum",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:412236,load,load,412236,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ddress space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:370735,load,load,370735,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"de as well. These kind of cross-file issues are currently only support by Xcode, not the HTML output.; This build is built with LLVM's Link-Time Optimization (LTO), which should make it slightly faster.; LTO also reduces the download size (about 19% smaller than checker-274).; Many sundry fixes. checker-274; built: April 23, 2013; download: checker-274.tar.bz2; highlights:. Improved use-after-free and mismatched deallocator checking.; Diagnostic polish.; Fixes crashes found in checker-273. checker-273; built: April 8, 2013; download: checker-273.tar.bz2; highlights:. Additional checks for misuse of Foundation collection APIs.; New C++ checker for attempting to create a reference to null.; New use-after-free checker for C++ 'delete'.; New checker for simple cases of mismatched allocators and deallocators, e.g. ""delete malloc(4);""; Support for basic interprocedural analysis of C++ destructors.; Additional heuristics for suppressing null pointer false positives.; Misc. bug fixes and performance enhancements. checker-272; built: March 1, 2013; highlights:. Better modeling of C++ constructors:; ; Interprocedural analysis support for constructors of types with trivial destructors; Efficient model of trivial copy and move constructors. Better diagnostics for loops that execute 0 times; Fixes a linking issue that prevented the checker from running on OS X v10.6 and earlier; Fixes for misc. crashes and false positives. checker-271; built: February 8, 2013; highlights:. Faster analysis for scan-build xcodebuild when using Xcode 4.6 and higher:; ; scan-build now uses Xcode's built-in interposition mechanism for the static analyzer to provide faster builds while doing static analysis (PCH files are now built).; This change also allows scan-build to have better support for iOS project analysis without having to specifying weird SDK settings to scan-build. Better diagnostics for implicitly-defined member functions in C++.; New warning for malloc/free checker when passing malloc'e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html:5105,perform,performance,5105,interpreter/llvm-project/clang/www/analyzer/release_notes.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/release_notes.html,2,['perform'],['performance']
Performance,"de of; the loop will be used either only inside the loop or in a loop closing; PHI node. In this case, the only loop closing PHI node is X4.; This means that we can just copy the loop and change the X4; accordingly, like so:. .. code-block:: C. c = ...;; if (c) {; for (...) {; if (true); X1 = ...; else; X2 = ...; X3 = phi(X1, X2);; }; } else {; for (...) {; if (false); X1' = ...; else; X2' = ...; X3' = phi(X1', X2');; }; }; X4 = phi(X3, X3'). Now, all uses of X4 will get the updated value (in general,; if a loop is in LCSSA form, in any loop transformation,; we only need to update the loop closing PHI nodes for the changes; to take effect). If we did not have Loop Closed SSA form, it means that X3 could; possibly be used outside the loop. So, we would have to introduce the; X4 (which is the new X3) and replace all uses of X3 with that.; However, we should note that because LLVM keeps a def-use chain; [#def-use-chain]_ for each Value, we wouldn't need; to perform data-flow analysis to find and replace all the uses; (there is even a utility function, replaceAllUsesWith(),; that performs this transformation by iterating the def-use chain). Another important advantage is that the behavior of all uses; of an induction variable is the same. Without this, you need to; distinguish the case when the variable is used outside of; the loop it is defined in, for example:. .. code-block:: C. for (i = 0; i < 100; i++) {; for (j = 0; j < 100; j++) {; k = i + j;; use(k); // use 1; }; use(k); // use 2; }. Looking from the outer loop with the normal SSA form, the first use of k; is not well-behaved, while the second one is an induction variable with; base 100 and step 1. Although, in practice, and in the LLVM context,; such cases can be handled effectively by SCEV. Scalar Evolution; (:ref:`scalar-evolution <passes-scalar-evolution>`) or SCEV, is a; (analysis) pass that analyzes and categorizes the evolution of scalar; expressions in loops. In general, it's easier to use SCEV in loops t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:14358,perform,perform,14358,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,2,['perform'],"['perform', 'performs']"
Performance,"de portions of ``XXXInstrInfo``, which implements; the interface described in ``TargetInstrInfo.h`` (see :ref:`TargetInstrInfo`).; These functions return ``0`` or a Boolean or they assert, unless overridden.; Here's a list of functions that are overridden for the SPARC implementation in; ``SparcInstrInfo.cpp``:. * ``isLoadFromStackSlot`` --- If the specified machine instruction is a direct; load from a stack slot, return the register number of the destination and the; ``FrameIndex`` of the stack slot. * ``isStoreToStackSlot`` --- If the specified machine instruction is a direct; store to a stack slot, return the register number of the destination and the; ``FrameIndex`` of the stack slot. * ``copyPhysReg`` --- Copy values between a pair of physical registers. * ``storeRegToStackSlot`` --- Store a register value to a stack slot. * ``loadRegFromStackSlot`` --- Load a register value from a stack slot. * ``storeRegToAddr`` --- Store a register value to memory. * ``loadRegFromAddr`` --- Load a register value from memory. * ``foldMemoryOperand`` --- Attempt to combine instructions of any load or; store instruction for the specified operand(s). Branch Folding and If Conversion; --------------------------------. Performance can be improved by combining instructions or by eliminating; instructions that are never reached. The ``analyzeBranch`` method in; ``XXXInstrInfo`` may be implemented to examine conditional instructions and; remove unnecessary instructions. ``analyzeBranch`` looks at the end of a; machine basic block (MBB) for opportunities for improvement, such as branch; folding and if conversion. The ``BranchFolder`` and ``IfConverter`` machine; function passes (see the source files ``BranchFolding.cpp`` and; ``IfConversion.cpp`` in the ``lib/CodeGen`` directory) call ``analyzeBranch``; to improve the control flow graph that represents the instructions. Several implementations of ``analyzeBranch`` (for ARM, Alpha, and X86) can be; examined as models for your own ``anal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:46684,load,loadRegFromAddr,46684,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['loadRegFromAddr']
Performance,"de(""SLICEX"",1,N);; ```. Here `SLICEX` is the name of the new family representing all slices and; 1 is the slicing axis. The meaning of the axis index is the following:; for all volumes having shapes like `box`, `trd1`, `trd2`, `trap`,; `gtra `or` para - `1, 2, 3 mean X, Y, Z; for `tube`, `tubs`, `cone`,; `cons - `1 means `Rxy`, 2 means `phi` and 3 means Z; for `pcon` and; `pgon` - 2 means `phi` and 3 means Z; for spheres 1 means `R `and 2; means `phi.`. In fact, the division operation has the same effect as positioning; volumes in a given order inside the divided container - the advantage; being that the navigation in such a structure is much faster. When a; volume is divided, a volume family corresponding to the slices is; created. In case all slices can be represented by a single shape, only; one volume is added to the family and positioned N times inside the; divided volume, otherwise, each slice will be represented by a distinct; volume in the family. Divisions can be also performed in a given range of one axis. For that,; one has to specify also the starting coordinate value and the step:. ``` {.cpp}; TGeoVolume *slicex = box->Divide(""SLICEX"",1,N,start,step);; ```. A check is always done on the resulting division range: if not fitting; into the container limits, an error message is posted. If we will browse; the divided volume we will notice that it will contain N nodes starting; with index 1 up to N. The first one has the lower X limit at `START`; position, while the last one will have the upper X limit at; `START+N*STEP`. The resulting slices cannot be positioned inside another; volume (they are by default positioned inside the divided one) but can; be further divided and may contain other volumes:. ``` {.cpp}; TGeoVolume *slicey = slicex->Divide(""SLICEY"",2,N1);; slicey->AddNode(other_vol,index,some_matrix);; ```. When doing that, we have to remember that `SLICEY` represents a family,; therefore all members of the family will be divided on Y and the other; vo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:85182,perform,performed,85182,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"de(""SLICEX"",1,N);; ~~~. Here `SLICEX` is the name of the new family representing all slices and; 1 is the slicing axis. The meaning of the axis index is the following:; for all volumes having shapes like `box`, `trd1`, `trd2`, `trap`,; `gtra `or` para - `1, 2, 3 mean X, Y, Z; for `tube`, `tubs`, `cone`,; `cons - `1 means `Rxy`, 2 means `phi` and 3 means Z; for `pcon` and; `pgon` - 2 means `phi` and 3 means Z; for spheres 1 means `R `and 2; means `phi.`. In fact, the division operation has the same effect as positioning; volumes in a given order inside the divided container - the advantage; being that the navigation in such a structure is much faster. When a; volume is divided, a volume family corresponding to the slices is; created. In case all slices can be represented by a single shape, only; one volume is added to the family and positioned N times inside the; divided volume, otherwise, each slice will be represented by a distinct; volume in the family. Divisions can be also performed in a given range of one axis. For that,; one has to specify also the starting coordinate value and the step:. ~~~{.cpp}; TGeoVolume *slicex = box->Divide(""SLICEX"",1,N,start,step);; ~~~. A check is always done on the resulting division range: if not fitting; into the container limits, an error message is posted. If we will browse; the divided volume we will notice that it will contain N nodes starting; with index 1 up to N. The first one has the lower X limit at `START`; position, while the last one will have the upper X limit at; `START+N*STEP`. The resulting slices cannot be positioned inside another; volume (they are by default positioned inside the divided one) but can; be further divided and may contain other volumes:. ~~~{.cpp}; TGeoVolume *slicey = slicex->Divide(""SLICEY"",2,N1);; slicey->AddNode(other_vol,index,some_matrix);; ~~~. When doing that, we have to remember that `SLICEY` represents a family,; therefore all members of the family will be divided on Y and the other; volu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:45554,perform,performed,45554,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"de`` feature.; ``-mno-cumode``; Disable the ``cumode`` feature. .. table:: AMDGPU Target Features; :name: amdgpu-target-features-table. =============== ============================ ==================================================; Target Feature Clang Option to Control Description; Name; =============== ============================ ==================================================; cumode - ``-m[no-]cumode`` Control the wavefront execution mode used; when generating code for kernels. When disabled; native WGP wavefront execution mode is used,; when enabled CU wavefront execution mode is used; (see :ref:`amdgpu-amdhsa-memory-model`). sramecc - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for SRAMECC. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with SRAMECC enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of SRAMECC. tgsplit ``-m[no-]tgsplit`` Enable/disable generating code that assumes; work-groups are launched in threadgroup split mode.; When enabled the waves of a work-group may be; launched in different CUs. wavefrontsize64 - ``-m[no-]wavefrontsize64`` Control the wavefront size used when; generating code for kernels. When disabled; native wavefront size 32 is used, when enabled; wavefront size 64 is used. xnack - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for XNACK replay. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with XNACK replay enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of XNACK replay. XNACK replay can be used for demand paging and; page migration. If enabled in the device,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:17534,load,loaded,17534,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"debugify`` testing utility is just a pair of passes: ``debugify`` and; ``check-debugify``. The first applies synthetic debug information to every instruction of the; module, and the second checks that this DI is still available after an; optimization has occurred, reporting any errors/warnings while doing so. The instructions are assigned sequentially increasing line locations, and are; immediately used by debug value intrinsics everywhere possible. For example, here is a module before:. .. code-block:: llvm. define void @f(i32* %x) {; entry:; %x.addr = alloca i32*, align 8; store i32* %x, i32** %x.addr, align 8; %0 = load i32*, i32** %x.addr, align 8; store i32 10, i32* %0, align 4; ret void; }. and after running ``opt -debugify``:. .. code-block:: llvm. define void @f(i32* %x) !dbg !6 {; entry:; %x.addr = alloca i32*, align 8, !dbg !12; call void @llvm.dbg.value(metadata i32** %x.addr, metadata !9, metadata !DIExpression()), !dbg !12; store i32* %x, i32** %x.addr, align 8, !dbg !13; %0 = load i32*, i32** %x.addr, align 8, !dbg !14; call void @llvm.dbg.value(metadata i32* %0, metadata !11, metadata !DIExpression()), !dbg !14; store i32 10, i32* %0, align 4, !dbg !15; ret void, !dbg !16; }. !llvm.dbg.cu = !{!0}; !llvm.debugify = !{!3, !4}; !llvm.module.flags = !{!5}. !0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: ""debugify"", isOptimized: true, runtimeVersion: 0, emissionKind: FullDebug, enums: !2); !1 = !DIFile(filename: ""debugify-sample.ll"", directory: ""/""); !2 = !{}; !3 = !{i32 5}; !4 = !{i32 2}; !5 = !{i32 2, !""Debug Info Version"", i32 3}; !6 = distinct !DISubprogram(name: ""f"", linkageName: ""f"", scope: null, file: !1, line: 1, type: !7, isLocal: false, isDefinition: true, scopeLine: 1, isOptimized: true, unit: !0, retainedNodes: !8); !7 = !DISubroutineType(types: !2); !8 = !{!9, !11}; !9 = !DILocalVariable(name: ""1"", scope: !6, file: !1, line: 1, type: !10); !10 = !DIBasicType(name: ""ty64"", size: 64, encoding: DW_ATE_unsigned); !11 = !DILoc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:10654,load,load,10654,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['load'],['load']
Performance,"ded intrinsic. ::. declare <16 x float> @llvm.vp.sqrt.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.sqrt.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.sqrt.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point square root of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.sqrt``' intrinsic performs floating-point square root (:ref:`sqrt <int_sqrt>`) of; the first vector operand on each enabled lane. The result on disabled lanes is; a :ref:`poison value <poisonvalues>`. The operation is performed in the default; floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.sqrt.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.sqrt.v4f32(<4 x float> %a); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fma:. '``llvm.vp.fma.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fma.v16f32 (<16 x float> <left_op>, <16 x float> <middle_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fma.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <middle_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fma.v256f64 (<256 x double> <left_op>, <256 x double> <middle_op>, <256 x double> <right_op>, <256 x i1> <mask>, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:744067,perform,performed,744067,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ded. ``` {.cpp}; root[] T->Process(""MySelector.C+"","""",1000,100);; ```. When appending a ""+"", the class will also be compiled and dynamically; loaded. When it is called again, it recompiles only if the macro; (`MySelector.C`) has changed since it was compiled last. If not, it; loads the existing library. The next example shows how to create a; selector with a pointer:. ``` {.cpp}; MySelector *selector = (MySelector *)TSelector::GetSelector(""MySelector.C+"");; T->Process(selector);; ```. `Using this form, you can do things like:`. ``` {.cpp}; selector->public_attribute1 = init_value;; for (int i=0; i<limit; i++) {; T->Process(selector);; selector->public_attribute1 =; function(selector->public_attribute2);; }; ```. `TTree::Process()` is aware of PROOF, ROOT parallel processing facility.; If PROOF is setup, it divides the processing amongst the slave CPUs. ### Performance Benchmarks; \index{benchmarks}. The program `$ROOTSYS/test/bench.cxx` compares the I/O performance of; STL vectors to the ROOT native **`TClonesArray`**`s` collection class.; It creates trees with and without compression for the following cases:; `vector<THit>`, `vector<THit*>`, `TClonesArray(`**`TObjHit`**`)`; not split `TClonesArray(`**`TObjHit`**`)` split. The next graphs show the two columns on the right which represent the split and; non-split **`TClonesArray`**, are significantly lower than the vectors. The most; significant difference is in reading a file without compression. The file size with compression, write times with and without compression; and the read times with and without compression all favor the; **`TClonesArray`**. ## Impact of Compression on I/O. This benchmark illustrates the pros and cons of the compression option.; We recommend using compression when the time spent in I/O is small; compared to the total processing time. In this case, if the I/O; operation is increased by a factor of 5 it is still a small percentage; of the total time and it may very well save a factor of 10 on",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:135202,perform,performance,135202,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['perform'],['performance']
Performance,"default value is ``extended``, with SSE the default value is ``source``.; Details:. * ``source`` The compiler uses the floating-point type declared in the source program as the evaluation method.; * ``double`` The compiler uses ``double`` as the floating-point evaluation method for all float expressions of type that is narrower than ``double``.; * ``extended`` The compiler uses ``long double`` as the floating-point evaluation method for all float expressions of type that is narrower than ``long double``. .. option:: -f[no-]protect-parens. This option pertains to floating-point types, complex types with; floating-point components, and vectors of these types. Some arithmetic; expression transformations that are mathematically correct and permissible; according to the C and C++ language standards may be incorrect when dealing; with floating-point types, such as reassociation and distribution. Further,; the optimizer may ignore parentheses when computing arithmetic expressions; in circumstances where the parenthesized and unparenthesized expression; express the same mathematical value. For example (a+b)+c is the same; mathematical value as a+(b+c), but the optimizer is free to evaluate the; additions in any order regardless of the parentheses. When enabled, this; option forces the optimizer to honor the order of operations with respect; to parentheses in all circumstances.; Defaults to ``-fno-protect-parens``. Note that floating-point contraction (option `-ffp-contract=`) is disabled; when `-fprotect-parens` is enabled. Also note that in safe floating-point; modes, such as `-ffp-model=precise` or `-ffp-model=strict`, this option; has no effect because the optimizer is prohibited from making unsafe; transformations. .. option:: -fexcess-precision:. The C and C++ standards allow floating-point expressions to be computed as if; intermediate results had more precision (and/or a wider range) than the type; of the expression strictly allows. This is called excess precision; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:65428,optimiz,optimizer,65428,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizer']
Performance,"defaults is more likely to cause; things to go wrong. They are also unstable across LLVM versions. **LLVM_TOOLS_INSTALL_DIR**:STRING; The path to install the main LLVM tools, relative to the *CMAKE_INSTALL_PREFIX*.; Defaults to *CMAKE_INSTALL_BINDIR*. **LLVM_UTILS_INSTALL_DIR**:STRING; The path to install auxiliary LLVM utilities, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_INSTALL_UTILS* is enabled.; Defaults to *LLVM_TOOLS_INSTALL_DIR*. **LLVM_EXAMPLES_INSTALL_DIR**:STRING; The path for examples of using LLVM, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_BUILD_EXAMPLES* is enabled.; Defaults to ""examples"". CMake Caches; ============. Recently LLVM and Clang have been adding some more complicated build system; features. Utilizing these new features often involves a complicated chain of; CMake variables passed on the command line. Clang provides a collection of CMake; cache scripts to make these features more approachable. CMake cache files are utilized using CMake's -C flag:. .. code-block:: console. $ cmake -C <path to cache file> <path to sources>. CMake cache scripts are processed in an isolated scope, only cached variables; remain set when the main configuration runs. CMake cached variables do not reset; variables that are already set unless the FORCE option is specified. A few notes about CMake Caches:. - Order of command line arguments is important. - -D arguments specified before -C are set before the cache is processed and; can be read inside the cache file; - -D arguments specified after -C are set after the cache is processed and; are unset inside the cache file. - All -D arguments will override cache file settings; - CMAKE_TOOLCHAIN_FILE is evaluated after both the cache file and the command; line arguments; - It is recommended that all -D options should be specified *before* -C. For more information about some of the advanced build configurations supported; via Cache files see :doc:`AdvancedBuilds`. Executing the Test",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:38856,cache,cache,38856,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,"define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our interpreter. The; basic idea that we want for Kaleidoscope is to have the user enter; function bodies as they do now, but immediately evaluate the top-level; expressions they type in. For example, if they type in ""1 + 2;"", we; should evaluate and print out 3. If they define a function, they should; be able to call it from the command line. In order to do this, we first prepare the environment to create code for; the current native target and declare and initialize the JIT. This is; done by calling some ``InitializeNativeTarget\*`` functions and; adding a global variable ``TheJIT``, and initializing it in; ``main``:. .. code-block:: c++. static std::unique_ptr<KaleidoscopeJIT> ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:9711,optimiz,optimizations,9711,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance,"del with a more robust, more efficient semantic model. From the user's perspective, the code looks only slightly different, because one uses an ``import`` declaration rather than a ``#include`` preprocessor directive:. .. code-block:: c. import std.io; // pseudo-code; see below for syntax discussion. However, this module import behaves quite differently from the corresponding ``#include <stdio.h>``: when the compiler sees the module import above, it loads a binary representation of the ``std.io`` module and makes its API available to the application directly. Preprocessor definitions that precede the import declaration have no impact on the API provided by ``std.io``, because the module itself was compiled as a separate, standalone module. Additionally, any linker flags required to use the ``std.io`` module will automatically be provided when the module is imported [#]_; This semantic import model addresses many of the problems of the preprocessor inclusion model:. * **Compile-time scalability**: The ``std.io`` module is only compiled once, and importing the module into a translation unit is a constant-time operation (independent of module system). Thus, the API of each software library is only parsed once, reducing the *M x N* compilation problem to an *M + N* problem. * **Fragility**: Each module is parsed as a standalone entity, so it has a consistent preprocessor environment. This completely eliminates the need for ``__underscored`` names and similarly defensive tricks. Moreover, the current preprocessor definitions when an import declaration is encountered are ignored, so one software library can not affect how another software library is compiled, eliminating include-order dependencies. * **Tool confusion**: Modules describe the API of software libraries, and tools can reason about and present a module as a representation of that API. Because modules can only be built standalone, tools can rely on the module definition to ensure that they get the complete API f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:4972,scalab,scalability,4972,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['scalab'],['scalability']
Performance,"depends on its storage duration. Initialization proceeds in two stages:. #. First, a null pointer is stored into the lvalue using primitive semantics.; This step is skipped if the object is ``__unsafe_unretained``.; #. Second, if the object has an initializer, that expression is evaluated and; then assigned into the object using the usual assignment semantics. :arc-term:`Destruction` occurs when an object's lifetime ends. In all cases it; is semantically equivalent to assigning a null pointer to the object, with the; proviso that of course the object cannot be legally read after the object's; lifetime ends. :arc-term:`Moving` occurs in specific situations where an lvalue is ""moved; from"", meaning that its current pointee will be used but the object may be left; in a different (but still valid) state. This arises with ``__block`` variables; and rvalue references in C++. For ``__strong`` lvalues, moving is equivalent; to loading the lvalue with primitive semantics, writing a null pointer to it; with primitive semantics, and then releasing the result of the load at the end; of the current full-expression. For all other lvalues, moving is equivalent to; reading the object. .. _arc.ownership.restrictions:. Restrictions; ------------. .. _arc.ownership.restrictions.weak:. Weak-unavailable types; ^^^^^^^^^^^^^^^^^^^^^^. It is explicitly permitted for Objective-C classes to not support ``__weak``; references. It is undefined behavior to perform an operation with weak; assignment semantics with a pointer to an Objective-C object whose class does; not support ``__weak`` references. .. admonition:: Rationale. Historically, it has been possible for a class to provide its own; reference-count implementation by overriding ``retain``, ``release``, etc.; However, weak references to an object require coordination with its class's; reference-count implementation because, among other things, weak loads and; stores must be atomic with respect to the final release. Therefore, existing; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:40633,load,loading,40633,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,2,['load'],"['load', 'loading']"
Performance,"der basic block so; that the loop has a preheader and it introduced the %loop.exit; basic block so that the loop has dedicated exits; (otherwise, %exit would be jumped from both %latch and %entry,; but %entry is not contained in the loop).; Note that a loop has to be in Loop Simplify Form beforehand; too for LoopRotate to be applied successfully. The main advantage of this form is that it allows hoisting; invariant instructions, especially loads, into the preheader.; That could be done in non-rotated loops as well but with; some disadvantages. Let's illustrate them with an example:. .. code-block:: C. for (int i = 0; i < n; ++i) {; auto v = *p;; use(v);; }. We assume that loading from p is invariant and use(v) is some; statement that uses v.; If we wanted to execute the load only once we could move it; ""out"" of the loop body, resulting in this:. .. code-block:: C. auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }. However, now, in the case that n <= 0, in the initial form,; the loop body would never execute, and so, the load would; never execute. This is a problem mainly for semantic reasons.; Consider the case in which n <= 0 and loading from p is invalid.; In the initial program there would be no error. However, with this; transformation we would introduce one, effectively breaking; the initial semantics. To avoid both of these problems, we can insert a guard:. .. code-block:: C. if (n > 0) { // loop guard; auto v = *p;; for (int i = 0; i < n; ++i) {; use(v);; }; }. This is certainly better but it could be improved slightly. Notice; that the check for whether n is bigger than 0 is executed twice (and; n does not change in between). Once when we check the guard condition; and once in the first execution of the loop. To avoid that, we could; do an unconditional first execution and insert the loop condition; in the end. This effectively means transforming the loop into a do-while loop:. .. code-block:: C. if (0 < n) {; auto v = *p;; do {; use(v);; ++i;; } while ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:22855,load,load,22855,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['load'],['load']
Performance,"der the requested; streaming mode. The compiler needs to use the function attributes to ensure the; compiler doesn't do transformations under the assumption that certain operations; are available at runtime. We made a conscious choice not to model this with feature flags, because we; still want to support inline-asm in either mode (with the user placing; smstart/smstop manually), and this became rather complicated to implement at the; individual instruction level (see `D120261 <https://reviews.llvm.org/D120261>`_; and `D121208 <https://reviews.llvm.org/D121208>`_) because of limitations in; TableGen. As a first step, this means we'll disable vectorization (LoopVectorize/SLP); entirely when the a function has either of the ``aarch64_pstate_sm_enabled``,; ``aarch64_pstate_sm_body`` or ``aarch64_pstate_sm_compatible`` attributes,; in order to avoid the use of vector instructions. Later on we'll aim to relax these restrictions to enable scalable; auto-vectorization with a subset of streaming-compatible instructions, but that; requires changes to the CostModel, Legalization and SelectionDAG lowering. We will also emit diagnostics in Clang to prevent the use of; non-streaming(-compatible) operations, e.g. through ACLE intrinsics, when a; function is decorated with the streaming mode attributes. Other things to consider; ------------------------. * Inlining must be disabled when the call-site needs to toggle PSTATE.SM or; when the callee's function body is executed in a different streaming mode than; its caller. This is needed because function calls are the boundaries for; streaming mode changes. * Tail call optimization must be disabled when the call-site needs to toggle; PSTATE.SM, such that the caller can restore the original value of PSTATE.SM. 3. Handling PSTATE.ZA; =====================. In contrast to PSTATE.SM, enabling PSTATE.ZA does not affect the SVE vector; length and also doesn't clobber FP/AdvSIMD/SVE registers. This means it is safe; to toggle PSTATE.ZA using",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst:15962,scalab,scalable,15962,interpreter/llvm-project/llvm/docs/AArch64SME.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AArch64SME.rst,1,['scalab'],['scalable']
Performance,"der to find out this new state, one; has to propagate the point with a distance slightly bigger that the; computed step value (which is accurate within numerical precision). A; method that performs this task finding the next location is; `TGeoManager::Step()`, described in "" Making a Step "", but users may; implement more precise methods to insure post-step boundary crossing. ## Geometry Graphical User Interface. The geombuilder package allows you to create and edit geometries. The; package provides a library of all GUI classes related to geometry. Each; editable geometry class **`TGeoXXX`** have a correspondent editor; **`TGeoXXXEditor`** that provides a graphics user interface allowing to; edit some (or all) parameters of a geometry object. The editable objects; are geometry manager, volumes, nodes, shapes, media, materials and; matrices. The interfaces provide also access to specific functionality; of geometry objects. The editing mechanism is based on ROOT GED; (Graphics Editors) functionality and the library is loaded using the; plug-in mechanism. ### Editing a Geometry. There are two different use cases having different ways of invoking the; geometry editors. The first one applies when starting with geometry from; scratch and using the builder functionality to create new geometry; objects. In this case, one should use the sequence:. ``` {.cpp}; root[] TGeoManager *geom = new TGeoManager(""MyGeom"",; ""Test builder"");; root[] geom->Edit(Option_t *option="""");; ```. The lines above will create a new **`TGeoManager`** class, create an; empty canvas and start the editor in the left-sided editor frame; attached to the canvas. To open the editor in a separate frame one; should provide a non-empty string as option to the `Edit()` method. ![The geometry manager editor](pictures/030001E9.png). ### The Geometry Manager Editor. ![Accessing/creating different categories of editable; objects](pictures/020001EA.jpg) ![Accessing/creating different; categories of editable objects](",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:165850,load,loaded,165850,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,"dered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:264367,load,load,264367,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic loads. The optional constant ``align`` argument specifies the alignment of the; operation (that is, the alignment of the memory address). It is the; responsibility of the code emitter to ensure that the alignment information is; correct. Overestimating the alignmen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:413068,load,load,413068,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"des.; Complete the investigation into Clang's C and C++ Defect Report; conformance: Separate from (but related to) general conformance testing is; determining which C defect reports and; C++ defect reports Clang implements. These; lists currently have a number of entries marked as Unknown.; Completing the investigation involves adding test coverage for; C; and; C++; defect reports and updating the documentation accordingly.; Bug triage: Clang's ; issue trackercurrently has over 20,000 open issues, many of which are not; appropriately tagged, are no longer reproducible, could use a reduced test case,; or otherwise needs some manual interaction. We can always use help with; bug triage and; issue tracker maintenance. Improve build times with Clang: the time it takes Clang to process a; translation unit is very important to our users; the lower the build time, the; better the overall user experience. It would be good to improve Clang's; performance as well as to find ways to proactively alert us when we've; introduced a change that has significant negative impact on build times.; Complete support for the experimental constant expression interpreter; : Clang's production constant expression interpreter computes a constant; expression result by walking over AST nodes, performing calculations as it; goes. This does not have good performance properties, and so we've begun work; on an ; experimental constant expression interpreter that works by converting the; AST into bytecode that is interpreted. This effort has a long tail of work left; to complete because it requires implementing byte code for every kind of; expression and type that can be used in a constant expression for C++ and C. Improve clang-doc: Clang's library-based design allows it to be used; by a variety of tools that reason about source code.; clang-doc is one; great application of this functionality, which generates code documentation; from source code. The tool is in early stages of development and could us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/OpenProjects.html:3140,perform,performance,3140,interpreter/llvm-project/clang/www/OpenProjects.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/OpenProjects.html,2,['perform'],['performance']
Performance,"description is invalid (see; :ref:`amdgpu-dwarf-location-description`). *An initial stack*. This is a list of values or location descriptions that will be pushed on the; operation expression evaluation stack in the order provided before evaluation; of an operation expression starts. Some debugger information entries have attributes that evaluate their DWARF; expression value with initial stack entries. In all other cases the initial; stack is empty. The result is undefined if any location descriptions are invalid (see; :ref:`amdgpu-dwarf-location-description`). If the evaluation requires a context element that is not specified, then the; result of the evaluation is an error. *A DWARF expression for a location description may be able to be evaluated; without a thread, lane, call frame, program location, or architecture context.; For example, the location of a global variable may be able to be evaluated; without such context. If the expression evaluates with an error then it may; indicate the variable has been optimized and so requires more context.*. *The DWARF expression for call frame information (see*; :ref:`amdgpu-dwarf-call-frame-information`\ *) operations are restricted to; those that do not require the compilation unit context to be specified.*. The DWARF is ill-formed if all the ``address_size`` fields in the headers of all; the entries in the ``.debug_info``, ``.debug_addr``, ``.debug_line``,; ``.debug_rnglists``, ``.debug_rnglists.dwo``, ``.debug_loclists``, and; ``.debug_loclists.dwo`` sections corresponding to any given program location do; not match. .. _amdgpu-dwarf-expression-value:. A.2.5.2 DWARF Expression Value; ++++++++++++++++++++++++++++++. A value has a type and a literal value. It can represent a literal value of any; supported base type of the target architecture. The base type specifies the; size, encoding, and endianity of the literal value. .. note::. It may be desirable to add an implicit pointer base type encoding. It would be; used for ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:54511,optimiz,optimized,54511,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimized']
Performance,"deserialization of template specializations. This reduces the memory footprint for hsimple by around 30% while improving the runtime performance for various cases by around 15%.; - When ROOT is signaled with a SIGUSR2 (i.e. on Linux and MacOS X) it will now print a backtrace.; - Move RStringView.h to ROOT/RStringView.hxx and always include ROOT/RStringView.hxx instead of RStringView.h for backward compatibility; - In `TClingCallFunc`, support r-value reference parameters. This paves the way for the corresponding support in PyROOT (implemented now in the latest Cppyy).; - Included the new TSequentialExecutor in ROOT, sharing the interfaces of TExecutor.This should improve code economy when providing a fallback for TThreadExecutor/TProcessExecutor. ### Thread safety; - Resolved several race conditions, dead-locks, performance and order of initialization/destruction issues still lingering because of or despite the new read-write lock mechanism. ## Interpreter. - Enabled use of multi-threaded code from the interpreter.; - Previouslyl multi-threaded code could be run from the interpreter as long as the call starting the threada was the same code that initialized the ROOT global lock, any other uses, including attempting to run the same code a second time in the same session would lead to a dead lock (if any other thread attempted to take on the ROOT lock).; - The interpreter now suspend the ROOT lock (which is taken to protect the interpreter global state) during user code execution. ## I/O Libraries; - LZ4 (with compression level 4) is now the default compression algorithm for new ROOT files (LZ4 is lossless data compression algorithm that is focused on compression and decompression speed, while in ROOT case providing benefit in faster decompression at the price of a bit worse compression ratio comparing to ZLIB); - If two or more files have an identical streamer info record, this is only treated once therewith avoiding to take the global lock.; - Allow writing temporary",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:2754,multi-thread,multi-threaded,2754,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['multi-thread'],['multi-threaded']
Performance,"designed to be highly tunable and; configurable, and while we provide some default configurations, we encourage; consumers to come up with the parameters that will work best for their use; cases. The allocator combines several components that serve distinct purposes:. - the Primary allocator: fast and efficient, it services smaller allocation; sizes by carving reserved memory regions into blocks of identical size. There; are currently two Primary allocators implemented, specific to 32 and 64 bit; architectures. It is configurable via compile time options. - the Secondary allocator: slower, it services larger allocation sizes via the; memory mapping primitives of the underlying operating system. Secondary backed; allocations are surrounded by Guard Pages. It is also configurable via compile; time options. - the thread specific data Registry: defines how local caches operate for each; thread. There are currently two models implemented: the exclusive model where; each thread holds its own caches (using the ELF TLS); or the shared model; where threads share a fixed size pool of caches. - the Quarantine: offers a way to delay the deallocation operations, preventing; blocks to be immediately available for reuse. Blocks held will be recycled; once certain size criteria are reached. This is essentially a delayed freelist; which can help mitigate some use-after-free situations. This feature is fairly; costly in terms of performance and memory footprint, is mostly controlled by; runtime options and is disabled by default. Allocations Header; ------------------; Every chunk of heap memory returned to an application by the allocator will be; preceded by a header. This has two purposes:. - being to store various information about the chunk, that can be leveraged to; ensure consistency of the heap operations;. - being able to detect potential corruption. For this purpose, the header is; checksummed and corruption of the header will be detected when said header is; accessed (note t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:1945,cache,caches,1945,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,2,['cache'],['caches']
Performance,"dex 0. This type of shuffle maps directly to the ``insert_subvector``; SelectionDAG node with the ``index`` operand set to 0. * Extract subvector --- A vector is pulled from a longer vector type starting; at index 0. This type of shuffle maps directly to the ``extract_subvector``; SelectionDAG node with the ``index`` operand set to 0. * Splat --- All elements of the vector have identical scalar elements. This; operation may also be known as a ""broadcast"" or ""duplicate"" in target assembly.; The shufflevector IR instruction may change the vector length, so this operation; may map to multiple SelectionDAG nodes including ``shuffle_vector``,; ``concat_vectors``, ``insert_subvector``, and ``extract_subvector``. Prior to the existence of the Legalize passes, we required that every target; `selector`_ supported and handled every operator and type even if they are not; natively supported. The introduction of the Legalize phases allows all of the; canonicalization patterns to be shared across targets, and makes it very easy to; optimize the canonicalized code because it is still in the form of a DAG. .. _optimizations:; .. _Optimize SelectionDAG:; .. _selector:. SelectionDAG Optimization Phase: the DAG Combiner; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The SelectionDAG optimization phase is run multiple times for code generation,; immediately after the DAG is built and once after each legalization. The first; run of the pass allows the initial code to be cleaned up (e.g. performing; optimizations that depend on knowing that the operators have restricted type; inputs). Subsequent runs of the pass clean up the messy code generated by the; Legalize passes, which allows Legalize to be very simple (it can focus on making; code legal instead of focusing on generating *good* and legal code). One important class of optimizations performed is optimizing inserted sign and; zero extension instructions. We currently use ad-hoc techniques, but could move; to more rigorous techni",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:44989,optimiz,optimize,44989,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimize']
Performance,"dex of the test vector bitmap. Semantics:; """""""""""""""""""". This intrinsic represents the final operation of an MC/DC instrumentation; sequence and will cause the ``-instrprof`` pass to generate the code to; instrument an update of a function's global test vector bitmap to indicate that; a test vector has been executed. The global test vector bitmap can be consumed; by the ``llvm-profdata`` and ``llvm-cov`` tools. '``llvm.thread.pointer``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.thread.pointer(). Overview:; """""""""""""""""". The '``llvm.thread.pointer``' intrinsic returns the value of the thread; pointer. Semantics:; """""""""""""""""""". The '``llvm.thread.pointer``' intrinsic returns a pointer to the TLS area; for the current thread. The exact semantics of this value are target; specific: it may point to the start of TLS area, to the end, or somewhere; in the middle. Depending on the target, this intrinsic may read a register,; call a helper function, read from an alternate memory space, or perform; other operations necessary to locate the TLS area. Not all targets support; this intrinsic. '``llvm.call.preallocated.setup``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare token @llvm.call.preallocated.setup(i32 %num_args). Overview:; """""""""""""""""". The '``llvm.call.preallocated.setup``' intrinsic returns a token which can; be used with a call's ``""preallocated""`` operand bundle to indicate that; certain arguments are allocated and initialized before the call. Semantics:; """""""""""""""""""". The '``llvm.call.preallocated.setup``' intrinsic returns a token which is; associated with at most one call. The token can be passed to; '``@llvm.call.preallocated.arg``' to get a pointer to get that; corresponding argument. The token must be the parameter to a; ``""preallocated""`` operand bundle for the corresponding call. Nested calls to '``llvm.call.preallocated.setup``' are allowed, but must; be properly nested. e.g. :: cod",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:537313,perform,perform,537313,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"df>`_ is; also available, though the text has not yet been updated with the; errata corrected by C++20.). * Proper semantics for Java-style memory, for both ``volatile`` and regular; shared variables. (`Java Specification; <http://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html>`_). * gcc-compatible ``__sync_*`` builtins. (`Description; <https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html>`_). * Other scenarios with atomic semantics, including ``static`` variables with; non-trivial constructors in C++. Atomic and volatile in the IR are orthogonal; ""volatile"" is the C/C++ volatile,; which ensures that every volatile load and store happens and is performed in the; stated order. A couple examples: if a SequentiallyConsistent store is; immediately followed by another SequentiallyConsistent store to the same; address, the first store can be erased. This transformation is not allowed for a; pair of volatile stores. On the other hand, a non-volatile non-atomic load can; be moved across a volatile load freely, but not an Acquire load. This document is intended to provide a guide to anyone either writing a frontend; for LLVM or working on optimization passes for LLVM with a guide for how to deal; with instructions with special semantics in the presence of concurrency. This; is not intended to be a precise guide to the semantics; the details can get; extremely complicated and unreadable, and are not usually necessary. .. _Optimization outside atomic:. Optimization outside atomic; ===========================. The basic ``'load'`` and ``'store'`` allow a variety of optimizations, but can; lead to undefined results in a concurrent environment; see `NotAtomic`_. This; section specifically goes into the one optimizer restriction which applies in; concurrent environments, which gets a bit more of an extended description; because any optimization dealing with stores needs to be aware of it. From the optimizer's point of view, the rule is that if there are not any; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:1906,load,load,1906,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,3,['load'],['load']
Performance,"dfBuilder tool which had a similar functionality but; has a much cleaner interface, partly thanks to its use of the RooWorkspace class for both input; of prototype p.d.fs and output of built p.d.f.s. The simplest use case to to take a workspace p.d.f as prototype and 'split' a parameter of that p.d.f ; into two specialized parameters depending on a category in the dataset. ; For example, given a Gaussian p.d.f G(x,m,s) we want to construct a G_a(x,m_a,s) and a G_b(x,m_b,s); with different mean parameters to be fit to a dataset with observables; (x,c) where c is a category with states 'a' and 'b'.; Using RooSimWSTool one can create a simultaneous p.d.f from G_a and G_b; from G with the following command. RooSimWSTool wst(wspace) ;; wst.build(""G_sim"",""G"",SplitParam(""m"",""c"")) ;. From this simple example one can go to builds of arbitrary complexity; by specifying multiple SplitParam arguments on multiple parameters; involving multiple splitting categories. Splits can also be performed; in the product multiple categories, e.g. . SplitParam(""m"",""c,d"")) ;. splits parameter m in the product of states of c and d. Another possibility; is the 'constrained' split which clones the parameter for all but one state; and insert a formula specialization in a chosen state that evaluates; to 1 - sum_i(a_i) where a_i are all other specializations. For example,; given a category c with state ""A"",""B"",""C"",""D"" the specification. SplitParamConstrained(""m"",""c"",""D""). will result in parameters m_A,m_B,m_C and a formula expression m_D; that evaluates to (1-(m_A+m_B+m_C)). Constrained split can also be; specified in product of categories. In that case the name of the; remainder state follows the syntax {State1;State2} where State1; and State2 are the state names of the two spitting categories. Additional; functionality exists to work with multiple prototype p.d.f.s simultaneously. ; Improved infrastructure for caching p.d.f and functions. The infrastructure that exists for caching p.d.f.s, i.e. p.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:12555,perform,performed,12555,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,2,['perform'],['performed']
Performance,"dge to track it. Second, we trade register pressure for simpler `cmovCC` instructions by; allocating a register for the ""bad"" state. We could read that value from memory; as part of the conditional move instruction, however, this creates more; micro-ops and requires the load-store unit to be involved. Currently, we place; the value into a virtual register and allow the register allocator to decide; when the register pressure is sufficient to make it worth spilling to memory; and reloading. #### Hardening Loads. Once we have the predicate accumulated into a special value for correct vs.; misspeculated, we need to apply this to loads in a way that ensures they do not; leak secret data. There are two primary techniques for this: we can either; harden the loaded value to prevent observation, or we can harden the address; itself to prevent the load from occurring. These have significantly different; performance tradeoffs. ##### Hardening loaded values. The most appealing way to harden loads is to mask out all of the bits loaded.; The key requirement is that for each bit loaded, along the misspeculated path; that bit is always fixed at either 0 or 1 regardless of the value of the bit; loaded. The most obvious implementation uses either an `and` instruction with; an all-zero mask along misspeculated paths and an all-one mask along correct; paths, or an `or` instruction with an all-one mask along misspeculated paths; and an all-zero mask along correct paths. Other options become less appealing; such as multiplying by zero, or multiple shift instructions. For reasons we; elaborate on below, we end up suggesting you use `or` with an all-ones mask,; making the x86 instruction sequence look like the following:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; movl (%rsi), %edi # Load potentially secret data from %rsi.; orl %eax, %edi; ```. Other useful patterns may be to fold the load into the `or` instruction itself; at the cost of a regi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:22774,load,loads,22774,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],"['loaded', 'loads']"
Performance,dhsa-memory-model-code-sequences-gfx6-gfx9-table`. .. table:: AMDHSA Memory Model Code Sequences GFX6-GFX9; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX6-GFX9; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - workgroup - gene,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:212447,load,load,212447,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"di # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't pan out due to failure to; uphold the security model. One in particular is worth discussing as many others; will reduce to it. We wondered whether only the *first* load in a basic block could be checked. If; the check works as intended, it forms an invalid pointer that doesn't even; virtual-address translate in the hardware. It should fault very early on in its; processing. Maybe that would stop things in time for the misspeculated path to; fail to leak any secrets. This doesn't end up working because the processor is; fundamentally out-of-order, even in its speculative domain. As a consequence,; the attacker could cause the initial address computation itself to stall and; allow an arbitrary number of unrelated loads (including attacked loads of; secret data) to pass through. #### Interprocedural Checking. Modern x86 processors may speculate into called functions and out of functions; to their return address. As a consequence, we need a way to check loads that; occur after a misspeculat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:36942,optimiz,optimizations,36942,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['optimiz'],['optimizations']
Performance,"di # Shift out all 32 bits loaded.; ```. Because on x86 the zero-extend is free, this can efficiently harden the loaded; value. ##### Hardening the address of the load. When hardening the loaded value is inapplicable, most often because the; instruction directly leaks information (like `cmp` or `jmpq`), we switch to; hardening the _address_ of the load instead of the loaded value. This avoids; increasing register pressure by unfolding the load or paying some other high; cost. To understand how this works in practice, we need to examine the exact; semantics of the x86 addressing modes which, in its fully general form, looks; like `(%base,%index,scale)offset`. Here `%base` and `%index` are 64-bit; registers that can potentially be any value, and may be attacker controlled,; and `scale` and `offset` are fixed immediate values. `scale` must be `1`, `2`,; `4`, or `8`, and `offset` can be any 32-bit sign extended value. The exact; computation performed to find the address is then: `%base + (scale * %index) +; offset` under 64-bit 2's complement modular arithmetic. One issue with this approach is that, after hardening, the `%base + (scale *; %index)` subexpression will compute a value near zero (`-1 + (scale * -1)`) and; then a large, positive `offset` will index into memory within the first two; gigabytes of address space. While these offsets are not attacker controlled,; the attacker could chose to attack a load which happens to have the desired; offset and then successfully read memory in that region. This significantly; raises the burden on the attacker and limits the scope of attack but does not; eliminate it. To fully close the attack we must work with the operating system; to preclude mapping memory in the low two gigabytes of address space. ###### 64-bit load checking instructions. We can use the following instruction sequences to check loads. We set up `%r8`; in these examples to hold the special value of `-1` which will be `cmov`ed over; `%rax` in misspeculated pa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:28314,perform,performed,28314,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performed']
Performance,"diff; - auto output = _batchData.makeWritableBatchUnInit(begin, batchSize);; + auto output = evalData.makeBatch(this, nEvents);; ```; 6. **DO NOT use `RooSpan::isBatch()` and `RooSpan::empty()` methods!** Instead, distinguish between scalar (RooSpan of size 1) and vector (RooSpan of size>1) parameters as shown below.; ```diff; - const bool batchX = !xData.empty();; + const bool batchX = xData.size()>1;; ```; 7. Append `RooBatchCompute::` to the classes that have been moved to the RooBatchCompute Library: `RooSpan`,`BracketAdapterWithMask`, `BracketAdapter`, `RunContext`. Alternatively, you can write; ```c++; using namespace RooBatchCompute;; ```; 8. Replace `_rf_fast_<function>` with `RooBatchCompute::fast_<function>` and include `RooVDTHeaders.h` (if applicable).; ```diff; - output[i] = _rf_fast_exp(arg*arg * halfBySigmaSq);; + output[i] = RooBatchCompute::fast_exp(arg*arg * halfBySigmaSq);; ```. ### Unbiased binned fits. When RooFit performs binned fits, it takes the probability density at the bin centre as a proxy for the probability in the bin. This can lead to a bias.; To alleviate this, the new class [RooBinSamplingPdf](https://root.cern/doc/v624/classRooBinSamplingPdf.html) has been added to RooFit.; Also see [arxiv:2012.02746](https://arxiv.org/abs/2012.02746). ### More accurate residual and pull distributions. When making residual or pull distributions with `RooPlot::residHist` or `RooPlot::pullHist`, the histogram is now compared with the curve's average values within a given bin by default, ensuring that residual and pull distributions are valid for strongly curved distributions.; The old default behaviour was to interpolate the curve at the bin centres, which can still be enabled by setting the `useAverage` parameter of `RooPlot::residHist` or `RooPlot::pullHist` to `false`. ### Improved recovery from invalid parameters. When a function in RooFit is undefined (Poisson with negative mean, PDF with negative values, etc), RooFit can now pass information abo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:19071,perform,performs,19071,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['perform'],['performs']
Performance,"ding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx10-gfx11:. Memory Model GFX10-GFX11; ++++++++++++++++++++++++. For GFX10-GFX11:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple work-group processors (WGP).; * Each WGP has multiple compute units (CU)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:334307,load,load,334307,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx90a:. Memory Model GFX90A; +++++++++++++++++++. For GFX90A:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:233313,load,load,233313,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx942:. Memory Model GFX942; +++++++++++++++++++. For GFX942:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:283461,load,load,283461,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; n",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:231519,load,load,231519,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"dingADistribution`; A best-practices guide for using LLVM's CMake build system to package and; distribute LLVM-based tools. :doc:`CMake`; An addendum to the main Getting Started guide for those using the `CMake; build system <http://www.cmake.org>`_. :doc:`Docker`; A reference for using Dockerfiles provided with LLVM. :doc:`Support Library <SupportLibrary>`; This document describes the LLVM Support Library (``lib/Support``) and; how to keep LLVM source code portable. :doc:`AdvancedBuilds`; This document describes more advanced build configurations. Optimizations; -------------. :doc:`WritingAnLLVMPass`; Information on how to write LLVM transformations and analyses. :doc:`WritingAnLLVMNewPMPass`; Information on how to write LLVM transformations under the new pass; manager. :doc:`Passes`; A list of optimizations and analyses implemented in LLVM. :doc:`StackSafetyAnalysis`; This document describes the design of the stack safety analysis of local; variables. :doc:`MergeFunctions`; Describes functions merging optimization. :doc:`AliasAnalysis`; Information on how to write a new alias analysis implementation or how to; use existing analyses. :doc:`MemorySSA`; Information about the MemorySSA utility in LLVM, as well as how to use it. :doc:`LoopTerminology`; A document describing Loops and associated terms as used in LLVM. :doc:`CycleTerminology`; A document describing cycles as a generalization of loops. :doc:`Vectorizers`; This document describes the current status of vectorization in LLVM. :doc:`LinkTimeOptimization`; This document describes the interface between LLVM intermodular optimizer; and the linker and its design. :doc:`GoldPlugin`; How to build your programs with link-time optimization on Linux. :doc:`Remarks`; A reference on the implementation of remarks in LLVM. :doc:`Source Level Debugging with LLVM <SourceLevelDebugging>`; This document describes the design and philosophy behind the LLVM; source-level debugger. :doc:`How to Update Debug Info <HowToUpdateDebug",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst:3065,optimiz,optimization,3065,interpreter/llvm-project/llvm/docs/UserGuides.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst,1,['optimiz'],['optimization']
Performance,"dingly. Alternatively, it is possible to specify constraints to both RooAbsPdf::fitTo() and the RooMCStudy constructor; using the ExternalConstraint() named argument to supply constraint p.d.f.s that are not part of the 'master'; p.d.f but rather an ad-hoc supplied external constraint. The argument supplied to ExternalConstraint() should; be (a set of) constraint p.d.f(s), rather than (a set of) parameters for which internal constraint p.d.f.s should; be picked up. New operator class RooLinearMorph. A new numeric operator class RooLinearMorph has been added that provides a continuous; transformation between two p.d.f.s shapes in terms of a linear parameter alpha. The algorithm ; for histograms is described in the paper by Alex Read in NUM A 425 (1999) 357-369 ; 'Linear interpolation of histograms'. The implementation in RooLinearMorph is for; continuous functions. . // Observable and sampling binning to be used by RooLinearMorph (""cache""); RooRealVar x(""x"",""x"",-20,20) ;; x.setBins(1000,""cache"") ;. // End point shapes : a gaussian on one end, a polynomial on the other; RooGaussian f1(""f1"",""f1"",x,RooConst(-10),RooConst(2)) ;; RooPolynomial f2(""f2"",""f2"",x,RooArgSet(RooConst(-0.03),RooConst(-0.001))) ;. // Interpolation parameter: rlm=f1 at alpha=0, rlm=f2 at alpha=1; RooRealVar alpha(""alpha"",""alpha"",0,1.0) ;; RooLinearMorph rlm(""rlm"",""rlm"",g1,g2,x,alpha) ;. // Plot halfway shape; alpha=0.5; RooPlot* frame = x.frame() ;; rlm.plotOn(frame) ;. In short the algorithm works as follows: for both f1(x) and f2(x), the cumulative distribution; functions F1(x) and F2(x) are calculated. One finds takes a value 'y' of both c.d.fs and ; determines the corresponding x values x1,x2 at which F1(x1)=F2(x2)=y. The value of the interpolated ; p.d.f fbar(x) is then calculated as fbar(alpha*x1+(1-alpha)*x2) = f1(x1)*f2(x2) / ( alpha*f2(x2) + ; (1-alpha)*f1(x1) ). Given that it is not easily possible to calculate the value of RooLinearMorph; at a given value of x, the value for all values o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:10237,cache,cache,10237,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,2,['cache'],['cache']
Performance,"directly supported, reading the cycle counter should not modify any; memory. Implementations are allowed to either return an application; specific value or a system wide value. On backends without support, this; is lowered to a constant 0. Note that runtime support may be conditional on the privilege-level code is; running at and the host platform. '``llvm.clear_cache``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.clear_cache(ptr, ptr). Overview:; """""""""""""""""". The '``llvm.clear_cache``' intrinsic ensures visibility of modifications; in the specified range to the execution unit of the processor. On; targets with non-unified instruction and data cache, the implementation; flushes the instruction cache. Semantics:; """""""""""""""""""". On platforms with coherent instruction and data caches (e.g. x86), this; intrinsic is a nop. On platforms with non-coherent instruction and data; cache (e.g. ARM, MIPS), the intrinsic is lowered either to appropriate; instructions or a system call, if cache flushing requires special; privileges. The default behavior is to emit a call to ``__clear_cache`` from the run; time library. This intrinsic does *not* empty the instruction pipeline. Modifications; of the current function are outside the scope of the intrinsic. '``llvm.instrprof.increment``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.instrprof.increment(ptr <name>, i64 <hash>,; i32 <num-counters>, i32 <index>). Overview:; """""""""""""""""". The '``llvm.instrprof.increment``' intrinsic can be emitted by a; frontend for use with instrumentation based profiling. These will be; lowered by the ``-instrprof`` pass to generate execution counts of a; program at runtime. Arguments:; """""""""""""""""""". The first argument is a pointer to a global variable containing the; name of the entity being instrumented. This should generally be the; (mangled) function name for a set of counters. The second argument is a hash value that can",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:526516,cache,cache,526516,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['cache'],['cache']
Performance,"directory. If so, it loads that test suite recursively,; otherwise it instantiates a local test config for the directory (see; :ref:`local-configuration-files`). Tests are identified by the test suite they are contained within, and the; relative path inside that suite. Note that the relative path may not refer to; an actual file on disk; some test formats (such as *GoogleTest*) define; ""virtual tests"" which have a path that contains both the path to the actual; test file and a subpath to identify the virtual test. .. _local-configuration-files:. LOCAL CONFIGURATION FILES; ~~~~~~~~~~~~~~~~~~~~~~~~~. When :program:`lit` loads a subdirectory in a test suite, it instantiates a; local test configuration by cloning the configuration for the parent directory; --- the root of this configuration chain will always be a test suite. Once the; test configuration is cloned :program:`lit` checks for a *lit.local.cfg* file; in the subdirectory. If present, this file will be loaded and can be used to; specialize the configuration for each individual directory. This facility can; be used to define subdirectories of optional tests, or to change other; configuration parameters --- for example, to change the test format, or the; suffixes which identify test files. SUBSTITUTIONS; ~~~~~~~~~~~~~. :program:`lit` allows patterns to be substituted inside RUN commands. It also; provides the following base set of substitutions, which are defined in; TestRunner.py:. ======================= ==============; Macro Substitution; ======================= ==============; %s source path (path to the file currently being run); %S source dir (directory of the file currently being run); %p same as %S; %{pathsep} path separator; %{fs-src-root} root component of file system paths pointing to the LLVM checkout; %{fs-tmp-root} root component of file system paths pointing to the test's temporary directory; %{fs-sep} file system path separator; %t temporary file name unique to the test; %basename_t The last path ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst:18642,load,loaded,18642,interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,1,['load'],['loaded']
Performance,"displayed lines to 100; instead of 10. Improve diagnostic in case of worker death: clients will; now; receive a message containing the low level reason for the failure and a; hint for getting more informationIn; TProofOutputFile, support the ""<user>"" and ""<group>""; placeholders in the output file name to automatically re-direct the; output to an area specific to the logged user.; Addition of a new class TProofProgressStatus, which is used to keep; the query progress stauts in all the TProofPlayer objects and in the; TPacketizerAdaptive. It is also send in kPROOF_GETPACKET and; kPROOF_STOPPROCESS messages. ; The class TPacketizerProgressive is removed. . Fixes. Enable; the max number of sessions ('mxsess' parameter in the xpd.schedparam; directive); users are just refused to start a session if this limit is; reached.Make sure to collect consistently input messages when running in asynchronous modeFix; a few problems with TProof::SendFile (used by UploadPackage, Load); appearing when a rapid sequence of these commands was submitted Invalidate the TProofMgr when the physical connection is; closed; avoids; crashing when trying to get the logs after a failure. ; Fix a memory leak in log retrieval (the TProofLog object; was never; deleted); Add protections for the cases the manager cannot be; initialized; Fix a race condition possibly affecting the handling of; workers death; Avoid duplicating worker logs in the master log file; unless; when explicitly needed by the request (Exec(...), Print(...)) or when; an error occuredFix; problem with the determination and transmission of the name of the; object to be processed. The problem appeared when processing files; containing >1 trees in changing order.Fix problem with TProof::Load loading the macro to one worker only per machineFix wrong return code preventing the correct propagation of the full ClearPackage to workersFix a problem causing the whole query to stop even in the case a worker was terminated gently with SIGTERM.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v522/index.html:6245,Load,Load,6245,proof/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v522/index.html,1,['Load'],['Load']
Performance,"ditionally,; command line argument values are captured transparently into user defined; global variables, which can be accessed like any other variable (and with the; same performance). #. Type Safe: As a user of CommandLine, you don't have to worry about; remembering the type of arguments that you want (is it an int? a string? a; bool? an enum?) and keep casting it around. Not only does this help prevent; error prone constructs, it also leads to dramatically cleaner source code. #. No subclasses required: To use CommandLine, you instantiate variables that; correspond to the arguments that you would like to capture, you don't; subclass a parser. This means that you don't have to write **any**; boilerplate code. #. Globally accessible: Libraries can specify command line arguments that are; automatically enabled in any tool that links to the library. This is; possible because the application doesn't have to keep a list of arguments to; pass to the parser. This also makes supporting `dynamically loaded options`_; trivial. #. Cleaner: CommandLine supports enum and other types directly, meaning that; there is less error and more security built into the library. You don't have; to worry about whether your integral command line argument accidentally got; assigned a value that is not valid for your enum type. #. Powerful: The CommandLine library supports many different types of arguments,; from simple `boolean flags`_ to `scalars arguments`_ (`strings`_,; `integers`_, `enums`_, `doubles`_), to `lists of arguments`_. This is; possible because CommandLine is... #. Extensible: It is very simple to add a new argument type to CommandLine.; Simply specify the parser that you want to use with the command line option; when you declare it. `Custom parsers`_ are no problem. #. Labor Saving: The CommandLine library cuts down on the amount of grunt work; that you, the user, have to do. For example, it automatically provides a; ``-help`` option that shows the available command line optio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:2022,load,loaded,2022,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['load'],['loaded']
Performance,"dl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We generate really poor code for:. double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. For example, the entry BB is:. _testf:; subl $20, %esp; pxor %xmm0, %xmm0; movsd 24(%esp), %xmm1; ucomisd %xmm0, %xmm1; setnp %al; sete %cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:13018,load,load,13018,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"dle_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point multiply-add of two vectors of floating-point values; that can be fused if code generator determines that (a) the target instruction; set has support for a fused operation, and (b) that the fused operation is more; efficient than the equivalent, separate pair of mul and add instructions. Arguments:; """""""""""""""""""". The first three operands and the result have the same vector of floating-point; type. The fourth operand is the vector mask and has the same number of elements; as the result vector type. The fifth operand is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fmuladd``' intrinsic performs floating-point multiply-add (:ref:`llvm.fuladd <int_fmuladd>`); of the first, second, and third vector operand on each enabled lane. The result; on disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fmuladd.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.fmuladd(<4 x float> %a, <4 x float> %b, <4 x float> %c); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_reduce_add:. '``llvm.vp.reduce.add.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.add.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.add.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``ADD`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:747800,perform,performed,747800,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"dle`.; * Fix detection of errors that appears in nested TTreeFormula [ROOT-8218]; * Better basket size optimization by taking into account meta data and rounding up to next 512 bytes, ensuring a complete cluster fits into a single basket. ### Fast Cloning. We added a cache specifically for the fast option of the TTreeCloner to significantly reduce the run-time when fast-cloning remote files to address [ROOT-5078]. It can be controlled from the `TTreeCloner`, `TTree::CopyEntries` or `hadd` interfaces. The new cache is enabled by default, to update the size of the cache or disable it from `TTreeCloner` use: `TTreeCloner::SetCacheSize`. To do the same from `TTree::CopyEntries` add to the option string ""cachesize=SIZE"". To update the size of the cache or disable it from `hadd`, use the command line option `-cachesize SIZE`. `SIZE` shouyld be given in number bytes and can be expressed in 'human readable form' (number followed by size unit like MB, MiB, GB or GiB, etc. or SIZE can be set zero to disable the cache. ### Other Changes. * Update `TChain::LoadTree` so that the user call back routine is actually called for each input file even those containing `TTree` objects with no entries.; * Repair setting the branch address of a leaflist style branch taking directly the address of the struct. (Note that leaflist is nonetheless still deprecated and declaring the struct to the interpreter and passing the object directly to create the branch is much better).; * Provide an implicitly parallel implementation of `TTree::GetEntry`. The approach is based on creating a task per top-level branch in order to do the reading, unzipping and deserialisation in parallel. In addition, a getter and a setter methods are provided to check the status and enable/disable implicit multi-threading for that tree (see Parallelisation section for more information about implicit multi-threading).; * Properly support `std::cin` (and other stream that can not be rewound) in `TTree::ReadStream`. This fix",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:10242,cache,cache,10242,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['cache'],['cache']
Performance,"do_something_else();; }. **Description**:. The boolean argument to this function is defined to be true. The optimizer may; analyze the form of the expression provided as the argument and deduce from; that information used to optimize the program. If the condition is violated; during execution, the behavior is undefined. The argument itself is never; evaluated, so any side effects of the expression will be discarded. Query for this feature with ``__has_builtin(__builtin_assume)``. .. _langext-__builtin_assume_separate_storage:. ``__builtin_assume_separate_storage``; -------------------------------------. ``__builtin_assume_separate_storage`` is used to provide the optimizer with the; knowledge that its two arguments point to separately allocated objects. **Syntax**:. .. code-block:: c++. __builtin_assume_separate_storage(const volatile void *, const volatile void *). **Example of Use**:. .. code-block:: c++. int foo(int *x, int *y) {; __builtin_assume_separate_storage(x, y);; *x = 0;; *y = 1;; // The optimizer may optimize this to return 0 without reloading from *x.; return *x;; }. **Description**:. The arguments to this function are assumed to point into separately allocated; storage (either different variable definitions or different dynamic storage; allocations). The optimizer may use this fact to aid in alias analysis. If the; arguments point into the same storage, the behavior is undefined. Note that the; definition of ""storage"" here refers to the outermost enclosing allocation of any; particular object (so for example, it's never correct to call this function; passing the addresses of fields in the same struct, elements of the same array,; etc.). Query for this feature with ``__has_builtin(__builtin_assume_separate_storage)``. ``__builtin_offsetof``; ----------------------. ``__builtin_offsetof`` is used to implement the ``offsetof`` macro, which; calculates the offset (in bytes) to a given member of the given type. **Syntax**:. .. code-block:: c++. __builtin_of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:100049,optimiz,optimizer,100049,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"does not maintain any mapping between those values and; any higher-level entity. The runtime must be able to interpret the; stack map record given only the ID, offset, and the order of the; locations, records, and functions, which LLVM preserves. Note that this is quite different from the goal of debug information,; which is a best-effort attempt to track the location of named; variables at every instruction. An important motivation for this design is to allow a runtime to; commandeer a stack frame when execution reaches an instruction address; associated with a stack map. The runtime must be able to rebuild a; stack frame and resume program execution using the information; provided by the stack map. For example, execution may resume in an; interpreter or a recompiled version of the same function. This usage restricts LLVM optimization. Clearly, LLVM must not move; stores across a stack map. However, loads must also be handled; conservatively. If the load may trigger an exception, hoisting it; above a stack map could be invalid. For example, the runtime may; determine that a load is safe to execute without a type check given; the current state of the type system. If the type system changes while; some activation of the load's function exists on the stack, the load; becomes unsafe. The runtime can prevent subsequent execution of that; load by immediately patching any stack map location that lies between; the current call site and the load (typically, the runtime would; simply patch all stack map locations to invalidate the function). If; the compiler had hoisted the load above the stack map, then the; program could crash before the runtime could take back control. To enforce these semantics, stackmap and patchpoint intrinsics are; considered to potentially read and write all memory. This may limit; optimization more than some clients desire. This limitation may be; avoided by marking the call site as ""readonly"". In the future we may; also allow meta-data to be added t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:18023,load,load,18023,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['load'],['load']
Performance,"domRays(). This; shoots rays from a given point in the local reference frame with random; directions. The intersections with displayed nodes appear as segments; having the color of the touched node. \anchor GP04; ## The Drawing Package. \image html geometry012.png. The modeller provides a powerful drawing; package, supporting several different options of visualization. A; library separated from the main one provides all functionality being; linked with the underlying ROOT visualization system. This library is; dynamically loaded by the plug-in manager only when drawing features are; requested. The geometrical structures that can be visualized are volumes; and volume hierarchies. The main component of the visualization system is volume primitive; painting in a ROOT pad. Starting from this one, several specific options; or subsystems are available, like: X3D viewing using hidden line and; surface removal algorithms, OpenGL viewing\* or ray tracing. The method TGeoManager::GetGeomPainter() loads the painting library in; memory. This is generally not needed since it is called automatically by; TGeoVolume::Draw() as well as by few other methods setting; visualization attributes. \anchor GP04a; ### Drawing Volumes and Hierarchies of Volumes. The first thing one would like to do after building some geometry is to; visualize the volume tree. This provides the fastest validation check; for most common coding or design mistakes. As soon as the geometry is; successfully closed, one should draw it starting from the top-level; volume:. ~~~{.cpp}; //... code for geometry building; root[] gGeoManager->CloseGeometry();; root[] gGeoManager->GetMasterVolume()->Draw();; ~~~. Doing this ensures that the original top-level volume of the geometry is; drawn, even if another volume is currently the geometry `root`. OK, I; suppose you already did that with your simple geometry and immediately; noticed a new ROOT canvas popping-up and having some more or less; strange picture inside. Here ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:97669,load,loads,97669,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['loads']
Performance,"done; for functions for which the analysis ran, e.g. in the case of dominators you; should only ask for the ``DominatorTree`` for function definitions, not; declarations. To write a correct ``ModulePass`` subclass, derive from ``ModulePass`` and; override the ``runOnModule`` method with the following signature:. The ``runOnModule`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual bool runOnModule(Module &M) = 0;. The ``runOnModule`` method performs the interesting work of the pass. It; should return ``true`` if the module was modified by the transformation and; ``false`` otherwise. .. _writing-an-llvm-pass-CallGraphSCCPass:. The ``CallGraphSCCPass`` class; ------------------------------. The `CallGraphSCCPass; <https://llvm.org/doxygen/classllvm_1_1CallGraphSCCPass.html>`_ is used by; passes that need to traverse the program bottom-up on the call graph (callees; before callers). Deriving from ``CallGraphSCCPass`` provides some mechanics; for building and traversing the ``CallGraph``, but also allows the system to; optimize execution of ``CallGraphSCCPass``\ es. If your pass meets the; requirements outlined below, and doesn't meet the requirements of a; :ref:`FunctionPass <writing-an-llvm-pass-FunctionPass>`, you should derive from; ``CallGraphSCCPass``. ``TODO``: explain briefly what SCC, Tarjan's algo, and B-U mean. To be explicit, CallGraphSCCPass subclasses are:. #. ... *not allowed* to inspect or modify any ``Function``\ s other than those; in the current SCC and the direct callers and direct callees of the SCC.; #. ... *required* to preserve the current ``CallGraph`` object, updating it to; reflect any changes made to the program.; #. ... *not allowed* to add or remove SCC's from the current Module, though; they may change the contents of an SCC.; #. ... *allowed* to add or remove global variables from the current Module.; #. ... *allowed* to maintain state across invocations of :ref:`runOnSCC; <writing-an-llvm-pass-runOnSCC>` (including global ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:14503,optimiz,optimize,14503,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['optimiz'],['optimize']
Performance,"doper; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadata !2); ret i32 %toret; }. There are a few other dbg.value configurations that mean it terminates; dominating location definitions without adding a new location. The complete; list is:. * Any location operand is ``poison`` (or ``undef``).; * Any location operand is an empty metadata tuple (``!{}``) (which cannot; occur in a ``!DIArgList``).; * There are no location operands (empty ``DIArgList``) and the ``DIExpression``; is empty. This class of dbg.value that kills variable locations is called a ""kill; dbg.value"" or ""kill location"", and for legacy reasons the term ""undef; dbg.value"" may be used in existing code. The ``DbgVariableIntrinsic`` methods; ``isKillLocation`` and ``setKillLocation`` should be used where possible rather; than inspecting location operands directly to check or set whether a dbg.value; is a kill location. In general, if any dbg.value has its operand optimized out and cannot be; recovered, then a kill dbg.value is necessary to terminate earlier variable; locations. Additional kill dbg.values may be necessary when the debugger can; observe re-ordering of assignments. How variable location metadata is transformed during CodeGen; ============================================================. LLVM preserves debug information throughout mid-level and backend passes,; ultimately producing a mapping between source-level information and; instruction ranges. This; is relatively straightforwards for line number information, as mapping; instructions to line numbers is a simple association. For variable locations; however the story is more complex. As each ``llvm.dbg.value`` intrinsic; represents a source-level assignment of a value to a source variable, the; variable location intrinsics effectively embed a small imperative program; within the LLVM IR. By the end of CodeGen, this becomes a mapping from each; variable to their machine locations over ranges of in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:23605,optimiz,optimized,23605,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"double> %vec, <2 x i1> %mask, i32 %evl); declare <vscale x 4 x i32> @llvm.experimental.vp.reverse.nxv4i32(<vscale x 4 x i32> %vec, <vscale x 4 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.reverse.*``' intrinsic is the vector length; predicated version of the '``llvm.experimental.vector.reverse.*``' intrinsic. Arguments:; """""""""""""""""""". The result and the first argument ``vec`` are vectors with the same type.; The second argument ``mask`` is a vector mask and has the same number of; elements as the result. The third argument is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". This intrinsic reverses the order of the first ``evl`` elements in a vector.; The lanes in the result vector disabled by ``mask`` are ``poison``. The; elements past ``evl`` are poison. .. _int_vp_load:. '``llvm.vp.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.vp.load.v4f32.p0(ptr %ptr, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.vp.load.nxv2i16.p0(ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare <8 x float> @llvm.vp.load.v8f32.p1(ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p6(ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:783057,load,load,783057,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"dow values:. - Address in a CFI-instrumented DSO.; - Unchecked address (a trusted non-instrumented DSO). Encoded as; value 0xFFFF.; - Invalid address (everything else). Encoded as value 0. For a CFI-instrumented DSO, a shadow value encodes the address of the; __cfi_check function for all call targets in the corresponding memory; page. If Addr is the target address, and V is the shadow value, then; the address of __cfi_check is calculated as. .. code-block:: none. __cfi_check = AlignUpTo(Addr, 4096) - (V + 1) * 4096. This works as long as __cfi_check is aligned by 4096 bytes and located; below any call targets in its DSO, but not more than 256MB apart from; them. CFI_SlowPath; ------------. The slow path check is implemented in a runtime support library as. .. code-block:: none. void __cfi_slowpath(uint64 CallSiteTypeId, void *TargetAddr); void __cfi_slowpath_diag(uint64 CallSiteTypeId, void *TargetAddr, void *DiagData). These functions loads a shadow value for ``TargetAddr``, finds the; address of ``__cfi_check`` as described above and calls; that. ``DiagData`` is an opaque pointer to diagnostic data which is; passed verbatim to ``__cfi_check``, and ``__cfi_slowpath`` passes; ``nullptr`` instead. Compiler-RT library contains reference implementations of slowpath; functions, but they have unresolvable issues with correctness and; performance in the handling of dlopen(). It is recommended that; platforms provide their own implementations, usually as part of libc; or libdl. Position-independent executable requirement; -------------------------------------------. Cross-DSO CFI mode requires that the main executable is built as PIE.; In non-PIE executables the address of an external function (taken from; the main executable) is the address of that functions PLT record in; the main executable. This would break the CFI checks. Backward-edge CFI for return statements (RCFI); ==============================================. This section is a proposal. As of March 2017 it is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:23095,load,loads,23095,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['load'],['loads']
Performance,"dps	%xmm2, %xmm2, %xmm3; 1 3 1.00 vhaddps	%xmm3, %xmm3, %xmm4. Resources:; [0] - JALU0; [1] - JALU1; [2] - JDiv; [3] - JFPA; [4] - JFPM; [5] - JFPU0; [6] - JFPU1; [7] - JLAGU; [8] - JMul; [9] - JSAGU; [10] - JSTC; [11] - JVALU0; [12] - JVALU1; [13] - JVIMUL. Resource pressure per iteration:; [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13]; - - - 2.00 1.00 2.00 1.00 - - - - - - -. Resource pressure by instruction:; [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Instructions:; - - - - 1.00 - 1.00 - - - - - - - vmulps	%xmm0, %xmm1, %xmm2; - - - 1.00 - 1.00 - - - - - - - - vhaddps	%xmm2, %xmm2, %xmm3; - - - 1.00 - 1.00 - - - - - - - - vhaddps	%xmm3, %xmm3, %xmm4. According to this report, the dot-product kernel has been executed 300 times,; for a total of 900 simulated instructions. The total number of simulated micro; opcodes (uOps) is also 900. The report is structured in three main sections. The first section collects a; few performance numbers; the goal of this section is to give a very quick; overview of the performance throughput. Important performance indicators are; **IPC**, **uOps Per Cycle**, and **Block RThroughput** (Block Reciprocal; Throughput). Field *DispatchWidth* is the maximum number of micro opcodes that are dispatched; to the out-of-order backend every simulated cycle. For processors with an; in-order backend, *DispatchWidth* is the maximum number of micro opcodes issued; to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total; number of cycles. Field *Block RThroughput* is the reciprocal of the block throughput. Block; throughput is a theoretical quantity computed as the maximum number of blocks; (i.e. iterations) that can be executed per simulated clock cycle in the absence; of loop carried dependencies. Block throughput is superiorly limited by the; dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:15665,perform,performance,15665,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,3,"['perform', 'throughput']","['performance', 'throughput']"
Performance,"dr4.txt; main; /tmp/test.cpp:15:0. bar; 6295592 4. Example 6 - path-style options:. This example uses the same source file as above, but the source file's; full path is /tmp/foo/test.cpp and is compiled as follows. The first case; shows the default absolute path, the second --basenames, and the third; shows --relativenames. .. code-block:: console. $ pwd; /tmp; $ clang -g foo/test.cpp -o test.elf; $ llvm-symbolizer --obj=test.elf 0x4004a0; main; /tmp/foo/test.cpp:15:0; $ llvm-symbolizer --obj=test.elf 0x4004a0 --basenames; main; test.cpp:15:0; $ llvm-symbolizer --obj=test.elf 0x4004a0 --relativenames; main; foo/test.cpp:15:0. Example 7 - Addresses as symbol names:. .. code-block:: console. $ llvm-symbolizer --obj=test.elf main; main; /tmp/test.cpp:14:0; $ llvm-symbolizer --obj=test.elf ""CODE foz""; foz; /tmp/test.h:1:0. OPTIONS; -------. .. option:: --adjust-vma <offset>. Add the specified offset to object file addresses when performing lookups.; This can be used to perform lookups as if the object were relocated by the; offset. .. option:: --basenames, -s. Print just the file's name without any directories, instead of the; absolute path. .. option:: --build-id. Look up the object using the given build ID, specified as a hexadecimal; string. Mutually exclusive with :option:`--obj`. .. option:: --color [=<always|auto|never>]. Specify whether to use color in :option:`--filter-markup` mode. Defaults to; ``auto``, which detects whether standard output supports color. Specifying; ``--color`` alone is equivalent to ``--color=always``. .. option:: --debug-file-directory <path>. Provide a path to a directory with a `.build-id` subdirectory to search for; debug information for stripped binaries. Multiple instances of this argument; are searched in the order given. .. option:: --debuginfod, --no-debuginfod. Whether or not to try debuginfod lookups for debug binaries. Unless specified,; debuginfod is only enabled if libcurl was compiled in (``LLVM_ENABLE_CURL``); and at least on",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst:4915,perform,perform,4915,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst,1,['perform'],['perform']
Performance,"drawing uses full space on the right side; 6. Allow to open ROOT files in online session (via url parameter); 7. One could monitor simultaneously objects from server and root files; 8. Implement 'autocol' draw option - when superimposing histograms,; their line colors will be automatically assigned; 9. Implement 'nostat' draw option - disabled stat drawing; 10. Using '_same_' identifier in item name, one can easily draw or superimpose; similar items from different files. Could be used in URL like:; `...&files=[file1.root,file2.root]&items=[file1.root/hpx, file2.root/_same_]`; `...&files=[file1.root,file2.root]&item=file1.root/hpx+file2.root/_same_`; Main limitation - file names should have similar length.; 11. When 'autozoom' specified in draw options, histogram zoomed into; non-empty content. Same command available via context menu.; 12. Item of 'Text' kind can be created. It is displayed as; plain text in the browser. If property 'mathjax' specified,; MathJax.js library will be loaded and used for rendering.; See httpcontrol.C macro for example.; 13. When using foreignObject, provide workaround for absolute positioning; problem in Chrome/Safari, see <http://bit.ly/1wjqCQ9>. ## Changes in 3.2; 1. Support JSON objects embedding in html pages, produced by THttpServer; 2. For small resize of canvas use autoscale functionality of SVG. Only when; relative changes too large, redraw complete canvas again.; 3. Use touch-punch.min.js to process touch events with jquery-ui; 4. Even when several TH1/TGraph/TF1 objects with fill attribute overlap each other,; one able to get tooltip for underlying objects; 5. Use jquery-ui menu for context menu; 6. From context menu one could select several options for drawing; 7. Provide user interface for executing TTree::Draw on THttpServer; 8. 3D graphic (three.js) works only with IE11. ## Changes in 3.1; 1. Correctly show tooltips in case of overlapped objects; 2. Implement JSROOT.create() method to create supported; in JavaScript ROOT cla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:68381,load,loaded,68381,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loaded']
Performance,"dress itself. ###### Loads folded into data-invariant operations can be hardened after the operation. The first key to making this feasible is to recognize that many operations on; x86 are ""data-invariant"". That is, they have no (known) observable behavior; differences due to the particular input data. These instructions are often used; when implementing cryptographic primitives dealing with private key data; because they are not believed to provide any side-channels. Similarly, we can; defer hardening until after them as they will not in-and-of-themselves; introduce a speculative execution side-channel. This results in code sequences; that look like:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; orl %eax, %edi; ```. While an addition happens to the loaded (potentially secret) value, that; doesn't leak any data and we then immediately harden it. ###### Hardening of loaded values deferred down the data-invariant expression graph. We can generalize the previous idea and sink the hardening down the expression; graph across as many data-invariant operations as desirable. This can use very; conservative rules for whether something is data-invariant. The primary goal; should be to handle multiple loads with a single hardening instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:26032,load,loaded,26032,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"dress space to be user accessible may need to adjust the exact sequence used; above. Additionally, the low addresses will need to be marked unreadable by the; OS to fully harden the load. ###### RIP-relative addressing is even easier to break. There is a common addressing mode idiom that is substantially harder to check:; addressing relative to the instruction pointer. We cannot change the value of; the instruction pointer register and so we have the harder problem of forcing; `%base + scale * %index + offset` to be an invalid address, by *only* changing; `%index`. The only advantage we have is that the attacker also cannot modify; `%base`. If we use the fast instruction sequence above, but only apply it to; the index, we will always access `%rip + (scale * -1) + offset`. If the; attacker can find a load which with this address happens to point to secret; data, then they can reach it. However, the loader and base libraries can also; simply refuse to map the heap, data segments, or stack within 2gb of any of the; text in the program, much like it can reserve the low 2gb of address space. ###### The flag registers again make everything hard. Unfortunately, the technique of using `orq`-instructions has a serious flaw on; x86. The very thing that makes it easy to accumulate state, the flag registers; containing predicates, causes serious problems here because they may be alive; and used by the loading instruction or subsequent instructions. On x86, the; `orq` instruction **sets** the flags and will override anything already there.; This makes inserting them into the instruction stream very hazardous.; Unfortunately, unlike when hardening the loaded value, we have no fallback here; and so we must have a fully general approach available. The first thing we must do when generating these sequences is try to analyze; the surrounding code to prove that the flags are not in fact alive or being; used. Typically, it has been set by some other instruction which just happens; to s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:30974,load,loader,30974,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loader']
Performance,"dress. Most; of GEP's special aliasing rules do not apply to pointers computed from ptrtoint,; arithmetic, and inttoptr sequences. Can I compute the distance between two objects, and add that value to one address to compute the other address?; ---------------------------------------------------------------------------------------------------------------. As with arithmetic on null, you can use GEP to compute an address that way, but; you can't use that pointer to actually access the object if you do, unless the; object is managed outside of LLVM. Also as above, ptrtoint and inttoptr provide an alternative way to do this which; do not have this restriction. Can I do type-based alias analysis on LLVM IR?; ----------------------------------------------. You can't do type-based alias analysis using LLVM's built-in type system,; because LLVM has no restrictions on mixing types in addressing, loads or stores. LLVM's type-based alias analysis pass uses metadata to describe a different type; system (such as the C type system), and performs type-based aliasing on top of; that. Further details are in the; `language reference <LangRef.html#tbaa-metadata>`_. What happens if a GEP computation overflows?; --------------------------------------------. If the GEP lacks the ``inbounds`` keyword, the value is the result from; evaluating the implied two's complement integer computation. However, since; there's no guarantee of where an object will be allocated in the address space,; such values have limited meaning. If the GEP has the ``inbounds`` keyword, the result value is ``poison``; if the GEP overflows (i.e. wraps around the end of the address space). As such, there are some ramifications of this for inbounds GEPs: scales implied; by array/vector/pointer indices are always known to be ""nsw"" since they are; signed values that are scaled by the element size. These values are also; allowed to be negative (e.g. ""``gep i32, ptr %P, i32 -1``"") but the pointer; itself is logically treat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:17195,perform,performs,17195,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['perform'],['performs']
Performance,"driver. When; targeting Windows with the MSVC-compatible ``clang-cl`` driver, some; of the details are different. Tools; =====. .. FIXME: Describe DWARF-related tools. A complete compilation of C family programming languages typically; involves the following pipeline of tools, some of which are omitted; in some compilations:. * **Preprocessor**: This performs the actions of the C preprocessor:; expanding #includes and #defines.; The ``-E`` flag instructs Clang to stop after this step. * **Parsing**: This parses and semantically analyzes the source language and; builds a source-level intermediate representation (""AST""), producing a; :ref:`precompiled header (PCH) <usersmanual-precompiled-headers>`,; preamble, or; :doc:`precompiled module file (PCM) <Modules>`,; depending on the input.; The ``-precompile`` flag instructs Clang to stop after this step. This is; the default when the input is a header file. * **IR generation**: This converts the source-level intermediate representation; into an optimizer-specific intermediate representation (IR); for Clang, this; is LLVM IR.; The ``-emit-llvm`` flag instructs Clang to stop after this step. If combined; with ``-S``, Clang will produce textual LLVM IR; otherwise, it will produce; LLVM IR bitcode. * **Compiler backend**: This converts the intermediate representation; into target-specific assembly code.; The ``-S`` flag instructs Clang to stop after this step. * **Assembler**: This converts target-specific assembly code into; target-specific machine code object files.; The ``-c`` flag instructs Clang to stop after this step. * **Linker**: This combines multiple object files into a single image; (either a shared object or an executable). Clang provides all of these pieces other than the linker. When multiple; steps are performed by the same tool, it is common for the steps to be; fused together to avoid creating intermediate files. When given an output of one of the above steps as an input, earlier steps; are skipped (for inst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Toolchain.rst:1831,optimiz,optimizer-specific,1831,interpreter/llvm-project/clang/docs/Toolchain.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Toolchain.rst,1,['optimiz'],['optimizer-specific']
Performance,"ds are allowed; a load which is part of a race returns; ``undef``, but does not have undefined behavior. Atomic instructions; ===================. For cases where simple loads and stores are not sufficient, LLVM provides; various atomic instructions. The exact guarantees provided depend on the; ordering; see `Atomic orderings`_. ``load atomic`` and ``store atomic`` provide the same basic functionality as; non-atomic loads and stores, but provide additional guarantees in situations; where threads and signals are involved. ``cmpxchg`` and ``atomicrmw`` are essentially like an atomic load followed by an; atomic store (where the store is conditional for ``cmpxchg``), but no other; memory operation can happen on any thread between the load and store. A ``fence`` provides Acquire and/or Release ordering which is not part; of another operation; it is normally used along with Monotonic memory; operations. A Monotonic load followed by an Acquire fence is roughly; equivalent to an Acquire load, and a Monotonic store following a; Release fence is roughly equivalent to a Release; store. SequentiallyConsistent fences behave as both an Acquire and a; Release fence, and additionally provide a total ordering with some; complicated guarantees, see the C++ standard for details. Frontends generating atomic instructions generally need to be aware of the; target to some degree; atomic instructions are guaranteed to be lock-free, and; therefore an instruction which is wider than the target natively supports can be; impossible to generate. .. _Atomic orderings:. Atomic orderings; ================. In order to achieve a balance between performance and necessary guarantees,; there are six levels of atomicity. They are listed in order of strength; each; level includes all the guarantees of the previous level except for; Acquire/Release. (See also `LangRef Ordering <LangRef.html#ordering>`_.). .. _NotAtomic:. NotAtomic; ---------. NotAtomic is the obvious, a load or store which is not atomic. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:4963,load,load,4963,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['load']
Performance,"ds or by; adopting protocols. Moreover, because the method names are selected by; the type of the subscript, an object can be subscripted using both array; and dictionary styles. Array-Style Subscripting; ^^^^^^^^^^^^^^^^^^^^^^^^. When the subscript operand has an integral type, the expression is; rewritten to use one of two different selectors, depending on whether; the element is being read or written. When an expression reads an; element using an integral index, as in the following example:. .. code-block:: objc. NSUInteger idx = ...;; id value = object[idx];. it is translated into a call to ``objectAtIndexedSubscript:``. .. code-block:: objc. id value = [object objectAtIndexedSubscript:idx];. When an expression writes an element using an integral index:. .. code-block:: objc. object[idx] = newValue;. it is translated to a call to ``setObject:atIndexedSubscript:``. .. code-block:: objc. [object setObject:newValue atIndexedSubscript:idx];. These message sends are then type-checked and performed just like; explicit message sends. The method used for objectAtIndexedSubscript:; must be declared with an argument of integral type and a return value of; some Objective-C object pointer type. The method used for; setObject:atIndexedSubscript: must be declared with its first argument; having some Objective-C pointer type and its second argument having; integral type. The meaning of indexes is left up to the declaring class. The compiler; will coerce the index to the appropriate argument type of the method it; uses for type-checking. For an instance of ``NSArray``, reading an; element using an index outside the range ``[0, array.count)`` will raise; an exception. For an instance of ``NSMutableArray``, assigning to an; element using an index within this range will replace that element, but; assigning to an element using an index outside this range will raise an; exception; no syntax is provided for inserting, appending, or removing; elements for mutable arrays. A class need n",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ObjectiveCLiterals.rst:13041,perform,performed,13041,interpreter/llvm-project/clang/docs/ObjectiveCLiterals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ObjectiveCLiterals.rst,1,['perform'],['performed']
Performance,"ds will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. fl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:246727,load,loads,246727,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ds, since it is only for internal usage.; The reason why we create this intrinsic is that we still support IR form Stack; Protector in FastISel. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". On some platforms, the value returned by this intrinsic remains unchanged; between loads in the same thread. On other platforms, it returns the same; global variable value, if any, e.g. ``@__stack_chk_guard``. Currently some platforms have IR-level customized stack guard loading (e.g.; X86 Linux) that is not handled by ``llvm.stackguard()``, while they should be; in the future. '``llvm.objectsize``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i32 @llvm.objectsize.i32(ptr <object>, i1 <min>, i1 <nullunknown>, i1 <dynamic>); declare i64 @llvm.objectsize.i64(ptr <object>, i1 <min>, i1 <nullunknown>, i1 <dynamic>). Overview:; """""""""""""""""". The ``llvm.objectsize`` intrinsic is designed to provide information to the; optimizer to determine whether a) an operation (like memcpy) will overflow a; buffer that corresponds to an object, or b) that a runtime check for overflow; isn't necessary. An object in this context means an allocation of a specific; class, structure, array, or other object. Arguments:; """""""""""""""""""". The ``llvm.objectsize`` intrinsic takes four arguments. The first argument is a; pointer to or into the ``object``. The second argument determines whether; ``llvm.objectsize`` returns 0 (if true) or -1 (if false) when the object size is; unknown. The third argument controls how ``llvm.objectsize`` acts when ``null``; in address space 0 is used as its pointer argument. If it's ``false``,; ``llvm.objectsize`` reports 0 bytes available when given ``null``. Otherwise, if; the ``null`` is in a non-zero address space or if ``true`` is given for the; third argument of ``llvm.objectsize``, we assume its size is unknown. The fourth; argument to ``llvm.objectsize`` determines if the value should be evaluated at; runtime. The second, third, and fourth ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:931953,optimiz,optimizer,931953,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"dule cache pruning attempts to clear out old, unused module files so that the module cache itself does not grow without bound. The default delay is large (604,800 seconds, or 7 days) because this is an expensive operation. Set this value to 0 to turn off pruning. ``-fmodules-prune-after=seconds``; Specify the minimum time (in seconds) for which a file in the module cache must be unused (according to access time) before module pruning will remove it. The default delay is large (2,678,400 seconds, or 31 days) to avoid excessive module rebuilding. ``-module-file-info <module file name>``; Debugging aid that prints information about a given module file (with a ``.pcm`` extension), including the language and preprocessor options that particular module variant was built with. ``-fmodules-decluse``; Enable checking of module ``use`` declarations. ``-fmodule-name=module-id``; Consider a source file as a part of the given module. ``-fmodule-map-file=<file>``; Load the given module map file if a header from its directory or one of its subdirectories is loaded. ``-fmodules-search-all``; If a symbol is not found, search modules referenced in the current module maps but not imported for symbols, so the error message can reference the module by name. Note that if the global module index has not been built before, this might take some time as it needs to build all the modules. Note that this option doesn't apply in module builds, to avoid the recursion. ``-fno-implicit-modules``; All modules used by the build must be specified with ``-fmodule-file``. ``-fmodule-file=[<name>=]<file>``; Specify the mapping of module names to precompiled module files. If the; name is omitted, then the module file is loaded whether actually required; or not. If the name is specified, then the mapping is treated as another; prebuilt module search mechanism (in addition to ``-fprebuilt-module-path``); and the module is only loaded if required. Note that in this case the; specified file also overrides th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:16605,load,loaded,16605,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loaded']
Performance,"dule map file. ``-fmodules-cache-path=<directory>``; Specify the path to the modules cache. If not provided, Clang will select a system-appropriate default. ``-fno-autolink``; Disable automatic linking against the libraries associated with imported modules. ``-fmodules-ignore-macro=macroname``; Instruct modules to ignore the named macro when selecting an appropriate module variant. Use this for macros defined on the command line that don't affect how modules are built, to improve sharing of compiled module files. ``-fmodules-prune-interval=seconds``; Specify the minimum delay (in seconds) between attempts to prune the module cache. Module cache pruning attempts to clear out old, unused module files so that the module cache itself does not grow without bound. The default delay is large (604,800 seconds, or 7 days) because this is an expensive operation. Set this value to 0 to turn off pruning. ``-fmodules-prune-after=seconds``; Specify the minimum time (in seconds) for which a file in the module cache must be unused (according to access time) before module pruning will remove it. The default delay is large (2,678,400 seconds, or 31 days) to avoid excessive module rebuilding. ``-module-file-info <module file name>``; Debugging aid that prints information about a given module file (with a ``.pcm`` extension), including the language and preprocessor options that particular module variant was built with. ``-fmodules-decluse``; Enable checking of module ``use`` declarations. ``-fmodule-name=module-id``; Consider a source file as a part of the given module. ``-fmodule-map-file=<file>``; Load the given module map file if a header from its directory or one of its subdirectories is loaded. ``-fmodules-search-all``; If a symbol is not found, search modules referenced in the current module maps but not imported for symbols, so the error message can reference the module by name. Note that if the global module index has not been built before, this might take some time as it needs",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:15914,cache,cache,15914,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache']
Performance,"dule-path=. Although the two examples have inconsistent optimization and debugging level, both of them are accepted. Note that **currently** the compiler doesn't consider inconsistent macro definition a problem. For example:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 -DNDEBUG Use.cpp -fprebuilt-module-path=. Currently Clang would accept the above example. But it may produce surprising results if the; debugging code depends on consistent use of ``NDEBUG`` also in other translation units. Definitions consistency; ^^^^^^^^^^^^^^^^^^^^^^^. The C++ language defines that same declarations in different translation units should have; the same definition, as known as ODR (One Definition Rule). Prior to modules, the translation; units don't dependent on each other and the compiler itself can't perform a strong; ODR violation check. With the introduction of modules, now the compiler have; the chance to perform ODR violations with language semantics across translation units. However, in the practice, we found the existing ODR checking mechanism is not stable; enough. Many people suffers from the false positive ODR violation diagnostics, AKA,; the compiler are complaining two identical declarations have different definitions; incorrectly. Also the true positive ODR violations are rarely reported.; Also we learned that MSVC don't perform ODR check for declarations in the global module; fragment. So in order to get better user experience, save the time checking ODR and keep consistent; behavior with MSVC, we disabled the ODR check for the declarations in the global module; fragment by default. Users who want more strict check can still use the; ``-Xclang -fno-skip-odr-check-in-gmf`` flag to get the ODR check enabled. It is also; encouraged to report issues if users find false positive ODR violations or false negative ODR; violations with the flag enabled. ABI Impacts; -----------. The de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:17340,perform,perform,17340,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['perform'],['perform']
Performance,"duling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (StackPtr != 0 && i < LineLen); {; i++;; --StackPtr;; }; }; }; return StackPtr;; }. //===---------------------------------------------------------------------===//. This should compile to the mlas instruction:; int mlas(int x, int y, int z) { return ((x * y + z) < 0) ? 7 : 13; }. //===---------------------------------------------------------------------===//. At some point, we should triage these to see if they still apply to us:. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19598; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=18560; http://gcc.gnu.org/bugzi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9709,load,load,9709,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['load'],['load']
Performance,"dural protection](#interprocedural-checking).; * When hardening the address of a load, it uses a *destructive* or; *non-reversible* modification of the address to prevent an attacker from; reversing the check using attacker-controlled inputs.; * It does not completely block speculative execution, and merely prevents; *mis*-speculated paths from leaking secrets from memory (and stalls; speculation until this can be determined).; * It is completely general and makes no fundamental assumptions about the; underlying architecture other than the ability to do branchless conditional; data updates and a lack of value prediction.; * It does not require programmers to identify all possible secret data using; static source code annotations or code vulnerable to a variant #1 style; attack. Limitations of this approach:; * It requires re-compiling source code to insert hardening instruction; sequences. Only software compiled in this mode is protected.; * The performance is heavily dependent on a particular architecture's; implementation strategy. We outline a potential x86 implementation below and; characterize its performance.; * It does not defend against secret data already loaded from memory and; residing in registers or leaked through other side-channels in; non-speculative execution. Code dealing with this, e.g cryptographic; routines, already uses constant-time algorithms and code to prevent; side-channels. Such code should also scrub registers of secret data following; [these; guidelines](https://github.com/HACS-workshop/spectre-mitigations/blob/master/crypto_guidelines.md).; * To achieve reasonable performance, many loads may not be checked, such as; those with compile-time fixed addresses. This primarily consists of accesses; at compile-time constant offsets of global and local variables. Code which; needs this protection and intentionally stores secret data must ensure the; memory regions used for secret data are necessarily dynamic mappings or heap; allocations. This ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:6537,perform,performance,6537,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"during speculative; execution of the return, potentially directing speculative execution to; arbitrary gadgets in the binary. Let's look at an example.; ```; unsigned char local_buffer[4];; unsigned char *untrusted_data_from_caller = ...;; unsigned long untrusted_size_from_caller = ...;; if (untrusted_size_from_caller < sizeof(local_buffer)) {; // Speculative execution enters here with a too-large size.; memcpy(local_buffer, untrusted_data_from_caller,; untrusted_size_from_caller);; // The stack has now been smashed, writing an attacker-controlled; // address over the return address.; minor_processing(local_buffer);; return;; // Control will speculate to the attacker-written address.; }; ```. However, this can be mitigated by hardening the load of the return address just; like any other load. This is sometimes complicated because x86 for example; *implicitly* loads the return address off the stack. However, the; implementation technique below is specifically designed to mitigate this; implicit load by using the stack pointer to communicate misspeculation between; functions. This additionally causes a misspeculation to have an invalid stack; pointer and never be able to read the speculatively stored return address. See; the detailed discussion below. For variant #1.2, the attacker speculatively stores into the vtable or jump; table used to implement an indirect call or indirect jump. Because this is; speculative, this will often be possible even when these are stored in; read-only pages. For example:; ```; class FancyObject : public BaseObject {; public:; void DoSomething() override;; };; void f(unsigned long attacker_offset, unsigned long attacker_data) {; FancyObject object = getMyObject();; unsigned long *arr[4] = getFourDataPointers();; if (attacker_offset < 4) {; // We have bypassed the bounds check speculatively.; unsigned long *data = arr[attacker_offset];; // Now we have computed a pointer inside of `object`, the vptr.; *data = attacker_data;; // The vptr poin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:12465,load,load,12465,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"dware-loop count with a target specific instruction, usually a move of this; value to a special register or a hardware-loop instruction. '``llvm.test.set.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare i1 @llvm.test.set.loop.iterations.i32(i32); declare i1 @llvm.test.set.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.test.set.loop.iterations.*``' intrinsics are used to specify the; the loop trip count, and also test that the given count is not zero, allowing; it to control entry to a while-loop. They are placed in the loop preheader's; predecessor basic block, and are marked as ``IntrNoDuplicate`` to avoid; optimizers duplicating these instructions. Arguments:; """""""""""""""""""". The integer operand is the loop trip count of the hardware-loop, and thus; not e.g. the loop back-edge taken count. Semantics:; """""""""""""""""""". The '``llvm.test.set.loop.iterations.*``' intrinsics do not perform any; arithmetic on their operand. It's a hint to the backend that can use this to; set up the hardware-loop count with a target specific instruction, usually a; move of this value to a special register or a hardware-loop instruction.; The result is the conditional value of whether the given count is not zero. '``llvm.test.start.loop.iterations.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. ::. declare {i32, i1} @llvm.test.start.loop.iterations.i32(i32); declare {i64, i1} @llvm.test.start.loop.iterations.i64(i64). Overview:; """""""""""""""""". The '``llvm.test.start.loop.iterations.*``' intrinsics are similar to the; '``llvm.test.set.loop.iterations.*``' and '``llvm.start.loop.iterations.*``'; intrinsics, used to specify the hardware-loop trip count, but also produce a; value identical to the input that can be used as the input to the loop. The; second i1 output controls entry to a while-loop. Arguments:; """""""""""""""""""". The inte",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:646090,perform,perform,646090,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"dware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If not TgSplit execution; mode, omit glc=1. load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic glc=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - gene",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:243930,load,load,243930,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"dying Clang's CodeGen directory can also be a good source of ideas. Note; that Clang and LLVM are explicitly version locked so you'll need to make sure; you're using a Clang built from the same git revision or release as the LLVM; library you're using. As always, it's *strongly* recommended that you track; tip of tree development, particularly during bring up of a new project. The Basics; ^^^^^^^^^^^. #. Make sure that your Modules contain both a data layout specification and; target triple. Without these pieces, non of the target specific optimization; will be enabled. This can have a major effect on the generated code quality. #. For each function or global emitted, use the most private linkage type; possible (private, internal or linkonce_odr preferably). Doing so will; make LLVM's inter-procedural optimizations much more effective. #. Avoid high in-degree basic blocks (e.g. basic blocks with dozens or hundreds; of predecessors). Among other issues, the register allocator is known to; perform badly with confronted with such structures. The only exception to; this guidance is that a unified return block with high in-degree is fine. Use of allocas; ^^^^^^^^^^^^^^. An alloca instruction can be used to represent a function scoped stack slot,; but can also represent dynamic frame expansion. When representing function; scoped variables or locations, placing alloca instructions at the beginning of; the entry block should be preferred. In particular, place them before any; call instructions. Call instructions might get inlined and replaced with; multiple basic blocks. The end result is that a following alloca instruction; would no longer be in the entry basic block afterward. The SROA (Scalar Replacement Of Aggregates) and Mem2Reg passes only attempt; to eliminate alloca instructions that are in the entry basic block. Given; SSA is the canonical form expected by much of the optimizer; if allocas can; not be eliminated by Mem2Reg or SROA, the optimizer is likely to be les",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:1963,perform,perform,1963,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['perform'],['perform']
Performance,"e ""invisible"" at tracking time. Let us suppose that we need to group together two volumes `A` and `B`; into a structure and position this into several other volumes `D,E,` and; `F`. What we need to do is to create a virtual container volume `C`; holding `A` and `B`, then position `C` in the other volumes. Note that `C` is a volume having a determined medium. Since it is not a; real volume, we need to manually set its medium the same as that of; `D,E` or `F` in order to make it ""invisible"" (same physics properties).; In other words, the limitation in proceeding this way is that `D,E,` and; `F` must point to the same medium. If this was not the case, we would; have to define different virtual volumes for each placement: `C`, `C`'; and `C`\"", having the same shape but different media matching the; corresponding containers. This might not happen so often, but when it; does, it forces the creation of several extra virtual volumes. Other; limitation comes from the fact that any container is directly used by; navigation algorithms to optimize tracking. These must geometrically; contain their belongings (positioned volumes) so that these do not; extrude its shape boundaries. Not respecting this rule generally leads; to unpredictable results. Therefore `A` and `B` together must fit into; `C` that has to fit also into `D`, `E`, and `F`. This is not always; straightforward to accomplish, especially when instead of `A` and `B` we; have many more volumes. In order to avoid these problems, one can use for the difficult cases; the class TGeoVolumeAssembly, representing an assembly of volumes.; This behaves like a normal container volume supporting other volumes; positioned inside, but it has neither shape nor medium. It cannot be; used directly as a piece of the geometry, but just as a temporary; structure helping temporary assembling and positioning volumes. If we define now `C` as an assembly containing `A` and `B`, positioning; the assembly into `D,E` and `F` will actually posi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:31510,optimiz,optimize,31510,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimize']
Performance,"e <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21581,optimiz,optimized,21581,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"e <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or library replicates the linker's; capability to collect information for link time optimization. Not only is; this code duplication difficult to justify, but it also has several other; disadvantages. For example, the linking semantics and the features provided; by the linker on various platform are not unique. This means, this new tool; needs to support all such features and platforms in one super too",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:3133,optimiz,optimizer,3133,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"e Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214976,load,load,214976,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"e Configuration manager from the VS IDE or the; ``/property:Configuration`` command line option when using MSBuild.; * By default, the Visual Studio project files generated by CMake use the; 32-bit toolset. If you are developing on a 64-bit version of Windows and; want to use the 64-bit toolset, pass the ``-Thost=x64`` flag when; generating the Visual Studio solution. This requires CMake 3.8.0 or later. 13. Start Visual Studio and select configuration:. In the directory you created the project files will have an ``llvm.sln``; file, just double-click on that to open Visual Studio. The default Visual; Studio configuration is **Debug** which is slow and generates a huge amount; of debug information on disk. For now, we recommend selecting **Release**; configuration for the LLVM project which will build the fastest or; **RelWithDebInfo** which is also several time larger than Release.; Another technique is to build all of LLVM in Release mode and change; compiler flags, disabling optimization and enabling debug information, only; for specific libraries or source files you actually need to debug. 14. Test LLVM in Visual Studio:. You can run LLVM tests by merely building the project ""check-all"". The test; results will be shown in the VS output window. Once the build succeeds, you; have verified a working LLVM development environment!. You should not see any unexpected failures, but will see many unsupported; tests and expected failures:. ::. 114>Testing Time: 1124.66s; 114> Skipped : 39; 114> Unsupported : 21649; 114> Passed : 51615; 114> Expectedly Failed: 93; ========== Build: 114 succeeded, 0 failed, 321 up-to-date, 0 skipped ==========``. Alternatives to manual installation; ===================================; Instead of the steps above, to simplify the installation procedure you can use; `Chocolatey <https://chocolatey.org/>`_ as package manager.; After the `installation <https://chocolatey.org/install>`_ of Chocolatey,; run these commands in an admin shell to instal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedVS.rst:7309,optimiz,optimization,7309,interpreter/llvm-project/llvm/docs/GettingStartedVS.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedVS.rst,1,['optimiz'],['optimization']
Performance,"e Creation of Composite Location Descriptions; ----------------------------------------------------------. To allow composition of composite location descriptions, an explicit operation; that indicates the end of the definition of a composite location description is; required. This can be implied if the end of a DWARF expression is reached,; allowing current DWARF expressions to remain legal. See ``DW_OP_LLVM_piece_end`` in; :ref:`amdgpu-dwarf-composite-location-description-operations`. 2.7 Generalize DWARF Base Objects to Allow Any Location Description Kind; ------------------------------------------------------------------------. The number of registers and the cost of memory operations is much higher for; AMDGPU than a typical CPU. The compiler attempts to optimize whole variables and; arrays into registers. Currently DWARF only allows ``DW_OP_push_object_address`` and related operations; to work with a global memory location. To support AMDGPU optimized code it is; required to generalize DWARF to allow any location description to be used. This; allows registers, or composite location descriptions that may be a mixture of; memory, registers, or even implicit values. See ``DW_OP_push_object_address`` in; :ref:`amdgpu-dwarf-general-location-description-operations`. 2.8 General Support for Address Spaces; --------------------------------------. AMDGPU needs to be able to describe addresses that are in different kinds of; memory. Optimized code may need to describe a variable that resides in pieces; that are in different kinds of storage which may include parts of registers,; memory that is in a mixture of memory kinds, implicit values, or be undefined. DWARF has the concept of segment addresses. However, the segment cannot be; specified within a DWARF expression, which is only able to specify the offset; portion of a segment address. The segment index is only provided by the entity; that specifies the DWARF expression. Therefore, the segment index is a property; that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:16454,optimiz,optimized,16454,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimized']
Performance,"e MIPS Backend; ---------------------------. Changes to the PowerPC Backend; ------------------------------. * LLJIT's JIT linker now defaults to JITLink on 64-bit ELFv2 targets.; * Initial-exec TLS model is supported on AIX.; * Implemented new resource based scheduling model of POWER7 and POWER8.; * ``frexp`` libcall now references correct symbol name for ``fp128``.; * Optimized materialization of 64-bit immediates, code generation of; ``vec_promote`` and atomics.; * Global constant strings are pooled in the TOC under one entry to reduce the; number of entries in the TOC.; * Added a number of missing Power10 extended mnemonics.; * Added the SCV instruction.; * Fixed register class for the paddi instruction.; * Optimize VPERM and fix code order for swapping vector operands on LE.; * Added various bug fixes and code gen improvements. AIX Support/improvements:. * Support for a non-TOC-based access sequence for the local-exec TLS model (called small local-exec).; * XCOFF toc-data peephole optimization and bug fixes.; * Move less often used __ehinfo TOC entries to the end of the TOC section.; * Fixed problems when the AIX libunwind unwinds starting from a signal handler; and the function that raised the signal happens to be a leaf function that; shares the stack frame with its caller or a leaf function that does not store; the stack frame backchain. Changes to the RISC-V Backend; -----------------------------. * The Zfa extension version was upgraded to 1.0 and is no longer experimental.; * Zihintntl extension version was upgraded to 1.0 and is no longer experimental.; * Intrinsics were added for Zk*, Zbb, and Zbc. See https://github.com/riscv-non-isa/riscv-c-api-doc/blob/master/riscv-c-api.md#scalar-bit-manipulation-extension-intrinsics; * Default ABI with F but without D was changed to ilp32f for RV32 and to lp64f for RV64.; * The Zvbb, Zvbc, Zvkb, Zvkg, Zvkn, Zvknc, Zvkned, Zvkng, Zvknha, Zvknhb, Zvks,; Zvksc, Zvksed, Zvksg, Zvksh, and Zvkt extension version was upgra",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ReleaseNotes.rst:6389,optimiz,optimization,6389,interpreter/llvm-project/llvm/docs/ReleaseNotes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ReleaseNotes.rst,1,['optimiz'],['optimization']
Performance,"e MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:299784,load,load,299784,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"e MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2029,load,loads,2029,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,2,['load'],['loads']
Performance,"e TTree and this factor is used to give the cache; size. If option is set to zero auto cache creation is disabled and the default; cache size is the historical one (equivalent to factor 1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14334,cache,cache,14334,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,3,['cache'],['cache']
Performance,"e ThinLTO or full LTO respectively, further enhancing; the performance gains from a PGO build by enabling interprocedural; optimizations. For example, to run a CMake configuration for a PGO build; that also enables ThinTLO, use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; -DPGO_INSTRUMENT_LTO=Thin \; <path to source>/llvm. By default, clang will generate profile data by compiling a simple; hello world program. You can also tell clang use an external; project for generating profile data that may be a better fit for your; use case. The project you specify must either be a lit test suite; (use the CLANG_PGO_TRAINING_DATA option) or a CMake project (use the; CLANG_PERF_TRAINING_DATA_SOURCE_DIR option). For example, If you wanted to use the; `LLVM Test Suite <https://github.com/llvm/llvm-test-suite/>`_ to generate; profile data you would use the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; -DBOOTSTRAP_CLANG_PGO_TRAINING_DATA_SOURCE_DIR=<path to llvm-test-suite> \; -DBOOTSTRAP_CLANG_PGO_TRAINING_DEPS=runtimes. The BOOTSTRAP\_ prefixes tells CMake to pass the variables on to the instrumented; stage two build. And the CLANG_PGO_TRAINING_DEPS option let's you specify; additional build targets to build before building the external project. The; LLVM Test Suite requires compiler-rt to build, so we need to add the; `runtimes` target as a dependency. After configuration, building the stage2-instrumented-generate-profdata target; will automatically build the stage1 compiler, build the instrumented compiler; with the stage1 compiler, and then run the instrumented compiler against the; perf training data:. .. code-block:: console. $ ninja stage2-instrumented-generate-profdata. If you let that run for a few hours or so, it will place a profdata file in your; build directory. This takes a really long time because it builds clang twice,; and you ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:6818,cache,caches,6818,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['caches']
Performance,"e ``-scev-aa`` pass; ^^^^^^^^^^^^^^^^^^^^^. The ``-scev-aa`` pass implements AliasAnalysis queries by translating them into; ScalarEvolution queries. This gives it a more complete understanding of; ``getelementptr`` instructions and loop induction variables than other alias; analyses have. Alias analysis driven transformations; -------------------------------------. LLVM includes several alias-analysis driven transformations which can be used; with any of the implementations above. The ``-adce`` pass; ^^^^^^^^^^^^^^^^^^. The ``-adce`` pass, which implements Aggressive Dead Code Elimination uses the; ``AliasAnalysis`` interface to delete calls to functions that do not have; side-effects and are not used. The ``-licm`` pass; ^^^^^^^^^^^^^^^^^^. The ``-licm`` pass implements various Loop Invariant Code Motion related; transformations. It uses the ``AliasAnalysis`` interface for several different; transformations:. * It uses mod/ref information to hoist or sink load instructions out of loops if; there are no instructions in the loop that modifies the memory loaded. * It uses mod/ref information to hoist function calls out of loops that do not; write to memory and are loop-invariant. * It uses alias information to promote memory objects that are loaded and stored; to in loops to live in a register instead. It can do this if there are no may; aliases to the loaded/stored memory location. The ``-argpromotion`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-argpromotion`` pass promotes by-reference arguments to be passed in; by-value instead. In particular, if pointer arguments are only loaded from it; passes in the value loaded instead of the address to the function. This pass; uses alias information to make sure that the value loaded from the argument; pointer is not modified between the entry of the function and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis informati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:27772,load,load,27772,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,2,['load'],"['load', 'loaded']"
Performance,"e ``null`` is in a non-zero address space or if ``true`` is given for the; third argument of ``llvm.objectsize``, we assume its size is unknown. The fourth; argument to ``llvm.objectsize`` determines if the value should be evaluated at; runtime. The second, third, and fourth arguments only accept constants. Semantics:; """""""""""""""""""". The ``llvm.objectsize`` intrinsic is lowered to a value representing the size of; the object concerned. If the size cannot be determined, ``llvm.objectsize``; returns ``i32/i64 -1 or 0`` (depending on the ``min`` argument). '``llvm.expect``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.expect`` on any; integer bit width. ::. declare i1 @llvm.expect.i1(i1 <val>, i1 <expected_val>); declare i32 @llvm.expect.i32(i32 <val>, i32 <expected_val>); declare i64 @llvm.expect.i64(i64 <val>, i64 <expected_val>). Overview:; """""""""""""""""". The ``llvm.expect`` intrinsic provides information about expected (the; most probable) value of ``val``, which can be used by optimizers. Arguments:; """""""""""""""""""". The ``llvm.expect`` intrinsic takes two arguments. The first argument is; a value. The second argument is an expected value. Semantics:; """""""""""""""""""". This intrinsic is lowered to the ``val``. '``llvm.expect.with.probability``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This intrinsic is similar to ``llvm.expect``. This is an overloaded intrinsic.; You can use ``llvm.expect.with.probability`` on any integer bit width. ::. declare i1 @llvm.expect.with.probability.i1(i1 <val>, i1 <expected_val>, double <prob>); declare i32 @llvm.expect.with.probability.i32(i32 <val>, i32 <expected_val>, double <prob>); declare i64 @llvm.expect.with.probability.i64(i64 <val>, i64 <expected_val>, double <prob>). Overview:; """""""""""""""""". The ``llvm.expect.with.probability`` intrinsic provides information about; expected value of ``val`` with probability(or confidence) ``prob``, which can; be us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:933781,optimiz,optimizers,933781,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"e a compile-time constant positive power of two no greater than; target-specific atomic access size limit. For each of the input pointers ``align`` parameter attribute must be specified. It; must be a power of two no less than the ``element_size``. Caller guarantees that; both the source and destination pointers are aligned to that boundary. Semantics:; """""""""""""""""""". The '``llvm.memcpy.element.unordered.atomic.*``' intrinsic copies ``len`` bytes of; memory from the source location to the destination location. These locations are not; allowed to overlap. The memory copy is performed as a sequence of load/store operations; where each access is guaranteed to be a multiple of ``element_size`` bytes wide and; aligned at an ``element_size`` boundary. The order of the copy is unspecified. The same value may be read from the source; buffer many times, but only one write is issued to the destination buffer per; element. It is well defined to have concurrent reads and writes to both source and; destination provided those reads and writes are unordered atomic when specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered loads from the source location and stores to the; destination. Lowering:; """""""""""""""""". In the most general case call to the '``llvm.memcpy.element.unordered.atomic.*``' is; lowered to a call to the symbol ``__llvm_memcpy_element_unordered_atomic_*``. Where '*'; is replaced with an actual element size. See :ref:`RewriteStatepointsForGC intrinsic; lowering <RewriteStatepointsForGC_intrinsic_lowering>` for details on GC specific; lowering. Optimizer is allowed to inline memory copy when it's profitable to do so. '``llvm.memmove.element.unordered.atomic``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use; ``llvm.memmove.element.unordered.atomic`` on any integer bit width and for; different address spaces. Not all targets suppor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:959369,concurren,concurrent,959369,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['concurren'],['concurrent']
Performance,"e an `REntry` matching the new model.; 4. Writing continues as described in steps 2-5 above. ### Reading Case; The reverse process is performed on reading (e.g. `RNTupleReader::LoadEntry()`, `RNTupleView` call operator). By default, the page source uses an `RClusterPool` to asynchronously read-ahead data.; When a page of a certain cluster is required, the cluster pool reads pages of _active_ columns.; For instance, if only certain fields are used (e.g., through an imposed model), only the pages of columns connected to those fields are read.; Columns can be dynamically added (e.g. during event iteration, a new field view is created in a reader).; The cluster pool reads ahead a limited number of clusters given by the _cluster bunch size_ option (default = 1).; The read-ahead uses vector reads.; For the file backend, it additionally coalesces close read requests and uses uring reads when available. The page source can be restricted to a certain entry range.; This allows for optimizing the page lists that are being read.; Additionally, it allows for optimizing the cluster pool to not read-ahead beyond the limits. #### Late model extension; Reading an RNTuple with an extended model is transparent -- i.e., no additional interface calls are required.; Internally, columns that were created as part of late model extension will have synthesized zero-initialized column ranges for the clusters that were already written before the model was extended.; In addition, pages made up of 0x00 bytes are synthesized for deferred columns in the clusters that were already (partially) filled before the model was extended. Storage Backends; ----------------. Support for storage backends is implemented through derived classes of `RPageSink` and `RPageSource`.; The `RPage{Sink,Source}File` class provides a storage backend for RNTuple data in ROOT files, local or remote.; The `RPage{Sink,Source}Daos` class provides a storage backend for RNTuple data in the DAOS object store. Every new storage b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:22801,optimiz,optimizing,22801,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['optimiz'],['optimizing']
Performance,"e an inlined copy of the string values in the table; itself making the tables much larger than they need to be on disk, especially; for large C++ programs. Can't we just fix the sections by adding all of the names we need to this; table? No, because that is not what the tables are defined to contain and we; won't know the difference between the old bad tables and the new good tables.; At best we could make our own renamed sections that contain all of the data we; need. These tables are also insufficient for what a debugger like LLDB needs. LLDB; uses clang for its expression parsing where LLDB acts as a PCH. LLDB is then; often asked to look for type ""``foo``"" or namespace ""``bar``"", or list items in; namespace ""``baz``"". Namespaces are not included in the pubnames or pubtypes; tables. Since clang asks a lot of questions when it is parsing an expression,; we need to be very fast when looking up names, as it happens a lot. Having new; accelerator tables that are optimized for very quick lookups will benefit this; type of debugging experience greatly. We would like to generate name lookup tables that can be mapped into memory; from disk, and used as is, with little or no up-front parsing. We would also; be able to control the exact content of these different tables so they contain; exactly what we need. The Name Accelerator Tables were designed to fix these; issues. In order to solve these issues we need to:. * Have a format that can be mapped into memory from disk and used as is; * Lookups should be very fast; * Extensible table format so these tables can be made by many producers; * Contain all of the names needed for typical lookups out of the box; * Strict rules for the contents of tables. Table size is important and the accelerator table format should allow the reuse; of strings from common string tables so the strings for the names are not; duplicated. We also want to make sure the table is ready to be used as-is by; simply mapping the table into memory with min",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:59226,optimiz,optimized,59226,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"e and NOT incremental builds; Using ccache materially improves average build times. Incremental builds; can be slightly faster, but introduce the risk of build corruption due to; e.g. state changes, etc... At this point, the recommendation is not to; use incremental builds and instead use ccache as the latter captures the; majority of the benefit with less risk of false positives. One of the non-obvious benefits of using ccache is that it makes the; builder less sensitive to which projects are being monitored vs built.; If a change triggers a build request, but doesn't change the build output; (e.g. doc changes, python utility changes, etc..), the build will entirely; hit in cache and the build request will complete in just the testing time. With multiple workers, it is tempting to try to configure a shared cache; between the workers. Experience to date indicates this is difficult to; well, and that having local per-worker caches gets most of the benefit; anyways. We don't currently recommend shared caches. CCache does depend on the builder hardware having sufficient IO to access; the cache with reasonable access times - i.e. a fast disk, or enough memory; for a RAM cache, etc.. For builders without, incremental may be your best; option, but is likely to require higher ongoing involvement from the; sponsor. Enable batch builds; As a last resort, you can configure your builder to batch build requests.; This makes the build failure notifications markedly less actionable, and; should only be done once all other reasonable measures have been taken. Leave it on the staging buildmaster; While most of this section has been biased towards builders intended for; the main buildmaster, it is worth highlighting that builders can run; indefinitely on the staging buildmaster. Such a builder may still be; useful for the sponsoring organization, without concern of negatively; impacting the broader community. The sponsoring organization simply; has to take on the responsibility of a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst:12070,cache,caches,12070,interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToAddABuilder.rst,1,['cache'],['caches']
Performance,"e as the executable. OPTIONS; -------; .. option:: --accelerator=<accelerator type>. Specify the desired type of accelerator table. Valid options are 'Apple',; 'Dwarf', 'Default' and 'None'. .. option:: --arch <arch>. Link DWARF debug information only for specified CPU architecture types.; Architectures may be specified by name. When using this option, an error will; be returned if any architectures can not be properly linked. This option can; be specified multiple times, once for each desired architecture. All CPU; architectures will be linked by default and any architectures that can't be; properly linked will cause :program:`dsymutil` to return an error. .. option:: --build-variant-suffix <suffix=buildvariant>. Specify the build variant suffix used to build the executable file.; There can be multiple variants for the binary of a product, each built; slightly differently. The most common build variants are 'debug' and; 'profile'. Setting the DYLD_IMAGE_SUFFIX environment variable will; cause dyld to load the specified variant at runtime. .. option:: --dump-debug-map. Dump the *executable*'s debug-map (the list of the object files containing the; debug information) in YAML format and exit. No DWARF link will take place. .. option:: -D <path>. Specify a directory that contain dSYM files to search for.; This is used for mergeable libraries, so dsymutil knows where to look; for dSYM files with debug information about symbols present in those; libraries. .. option:: --fat64. Use a 64-bit header when emitting universal binaries. .. option:: --flat, -f. Produce a flat dSYM file. A ``.dwarf`` extension will be appended to the; executable name unless the output file is specified using the ``-o`` option. .. option:: --gen-reproducer. Generate a reproducer consisting of the input object files. Alias for; --reproducer=GenerateOnExit. .. option:: --help, -h. Print this help output. .. option:: --keep-function-for-static. Make a static variable keep the enclosing function even ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/dsymutil.rst:1518,load,load,1518,interpreter/llvm-project/llvm/docs/CommandGuide/dsymutil.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/dsymutil.rst,1,['load'],['load']
Performance,"e at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:271299,load,load,271299,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"e at; http://root.cern/files/brun_lcgapp09.pptx ; or; http://root.cern/files/brun_lcgapp09.pdf .; The baskets are flushed and the Tree header saved at regular intervals (See AutoFlush and OptimizeBaskets); When the amount of data written so far (fTotBytes) is greater than fAutoFlush (see SetAutoFlush) all the baskets are flushed to disk.; This makes future reading faster as it guarantees that baskets belonging to nearby entries will be on the same disk region.; When the first call to flush the baskets happens, we also take this opportunity to optimize the baskets buffers.; We also check if the number of bytes written is greater than fAutoSave (see SetAutoSave).; In this case we also write the Tree header. This makes the Tree recoverable up to this point in case the program writing the Tree crashes.; Note that the user can also decide to call FlushBaskets and AutoSave in her event loop on the base of the number of events written instead of the number of bytes written.; New function TTree::OptimizeBaskets. void TTree::OptimizeBaskets(Int_t maxMemory, Float_t minComp, Option_t *option). This function may be called after having filled some entries in a Tree; using the information in the existing branch buffers, it will reassign; new branch buffer sizes to optimize time and memory.; The function computes the best values for branch buffer sizes such that; the total buffer sizes is less than maxMemory and nearby entries written; at the same time.; In case the branch compression factor for the data written so far is less; than compMin, the compression is disabled. if option =""d"" an analysis report is printed.; This function may also be called on an existing Tree to figure out the best values; given the information in the Tree header. TFile f(""myfile.root"");; TTree *T = (TTree*)f.Get(""mytreename"");; T->Print(); //show the branch buffer sizes before optimization; T->OptimizeBaskets(10000000,1,""d"");; T->Print(); //show the branch buffer sizes after optimization. New interface ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:4319,Optimiz,OptimizeBaskets,4319,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,1,['Optimiz'],['OptimizeBaskets']
Performance,"e atomic with respect to calls to ``objc_storeWeak`` on ``src``. .. _arc.runtime.objc_release:. ``void objc_release(id value);``; --------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a; release operation exactly as if the object had been sent the ``release``; message. .. _arc.runtime.objc_retain:. ``id objc_retain(id value);``; -----------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation exactly as if the object had been sent the ``retain`` message. Always returns ``value``. .. _arc.runtime.objc_retainAutorelease:. ``id objc_retainAutorelease(id value);``; ----------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation followed by an autorelease operation. Equivalent to the following; code:. .. code-block:: objc. id objc_retainAutorelease(id value) {; return objc_autorelease(objc_retain(value));; }. Always returns ``value``. .. _arc.runtime.objc_retainAutoreleaseReturnValue:. ``id objc_retainAutoreleaseReturnValue(id value);``; ---------------------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation followed by the operation described in; :ref:`objc_autoreleaseReturnValue <arc.runtime.objc_autoreleaseReturnValue>`.; Equivalent to the following code:. .. code-block:: objc. id objc_retainAutoreleaseReturnValue(id value) {; return objc_autoreleaseReturnValue(objc_retain(value));; }. Always returns ``value``. .. _arc.runtime.objc_retainAutoreleasedReturnValue:. ``id objc_retainAutoreleasedReturnValue(id value);``; ----------------------------------------------------. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:113984,perform,performs,113984,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance,"e base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``llvm.masked.load``' operands do not have corresponding operands in; '``llvm.vp.load``': the '``passthru``' operand is implicitly ``poison``; the; '``alignment``' operand is taken as the ``align`` parameter attribute, if; provided. The default alignment is taken as the ABI alignment of the return; type as specified by the :ref:`datalayout string<langref_datalayout>`. Examples:; """""""""""""""""". .. code-block:: text. %r = call <8 x i8> @llvm.vp.load.v8i8.p0(ptr align 2 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %also.r = call <8 x i8> @llvm.masked.load.v8i8.p0(ptr %ptr, i32 2, <8 x i1> %mask, <8 x i8> poison). .. _int_vp_store:. '``llvm.vp.store``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare void @llvm.vp.store.v4f32.p0(<4 x float> %val, ptr %ptr, <4 x i1> %mask, i32 %evl); declare void @llvm.vp.store.nxv2i16.p0(<vscale x 2 x i16> %val, ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare void @llvm.vp.store.v8f32.p1(<8 x float> %val, ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare void @llvm.vp.store.nxv1i64.p6(<vscale x 1 x i64> %val, ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.store.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.store <int_mstore>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the vector value to be written to memory. The second; operand is the base pointer for the store. It has the same underlying type as; the value operand. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:784840,load,load,784840,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"e because we had conservatively banned the; synthesis in order to give ourselves exactly this leeway. Applying ``__attribute__((NSObject))`` to a property not of retainable object; pointer type has the same behavior it does outside of ARC: it requires the; property type to be some sort of pointer and permits the use of modifiers other; than ``assign``. These modifiers only affect the synthesized getter and; setter; direct accesses to the ivar (even if synthesized) still have primitive; semantics, and the value in the ivar will not be automatically released during; deallocation. .. _arc.ownership.semantics:. Semantics; ---------. There are five :arc-term:`managed operations` which may be performed on an; object of retainable object pointer type. Each qualifier specifies different; semantics for each of these operations. It is still undefined behavior to; access an object outside of its lifetime. A load or store with ""primitive semantics"" has the same semantics as the; respective operation would have on an ``void*`` lvalue with the same alignment; and non-ownership qualification. :arc-term:`Reading` occurs when performing a lvalue-to-rvalue conversion on an; object lvalue. * For ``__weak`` objects, the current pointee is retained and then released at; the end of the current full-expression. In particular, messaging a ``__weak``; object keeps the object retained until the end of the full expression. .. code-block:: objc. __weak MyObject *weakObj;. void foo() {; // weakObj is retained before the message send and released at the end of; // the full expression.; [weakObj m];; }. This must execute atomically with respect to assignments and to the final; release of the pointee.; * For all other objects, the lvalue is loaded with primitive semantics. :arc-term:`Assignment` occurs when evaluating an assignment operator. The; semantics vary based on the qualification:. * For ``__strong`` objects, the new pointee is first retained; second, the; lvalue is loaded with primitive se",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:37670,load,load,37670,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['load']
Performance,"e best values; given the information in the Tree header. TFile f(""myfile.root"");; TTree *T = (TTree*)f.Get(""mytreename"");; T->Print(); //show the branch buffer sizes before optimization; T->OptimizeBaskets(10000000,1,""d"");; T->Print(); //show the branch buffer sizes after optimization. New interface functions to customize the TreeCache; virtual void AddBranchToCache(const char *bname, Bool_t subbranches = kFALSE);; virtual void AddBranchToCache(TBranch *branch, Bool_t subbranches = kFALSE);; virtual void PrintCacheStats(Option_t* option = """") const;; virtual void SetParallelUnzip(Bool_t opt=kTRUE);; virtual void SetCacheEntryRange(Long64_t first, Long64_t last);; virtual void SetCacheLearnEntries(Int_t n=10);; virtual void StopCacheLearningPhase();; New functionality AutoFlush (and changes to AutoSave). Implement a new member fAutoFlush in TTree with its getter and setter:. void TTree::SetAutoFlush(Long64_t autof). The logic of the AutoFlush mechanism is optimized such that the TreeCache; will read always up to the point where FlushBaskets has been called.; This minimizes the number of cases where one has to seek backward when reading. This function may be called at the start of a program to change; the default value for fAutoFlush. CASE 1 : autof > 0. autof is the number of consecutive entries after which TTree::Fill will; flush all branch buffers to disk. CASE 2 : autof < 0. When filling the Tree the branch buffers will be flushed to disk when; more than autof bytes have been written to the file. At the first FlushBaskets; TTree::Fill will replace fAutoFlush by the current value of fEntries. Calling this function with autof < 0 is interesting when it is hard to estimate; the size of one entry. This value is also independent of the Tree. When calling SetAutoFlush with no arguments, the; default value is -30000000, ie that the first AutoFlush will be done when; 30 MBytes of data are written to the file. CASE 3 : autof = 0; The AutoFlush mechanism is disabled. Flushi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:5984,optimiz,optimized,5984,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,2,['optimiz'],['optimized']
Performance,"e big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving symbols; at process launch time, which can be very slow for C++ code. .. _shared_libs:. .. warning::; One very important note: Distributions should never be built using the; *BUILD_SHARED_LIBS* CMake option. That option exists for optimizing developer; workflow only. Due to design and implementation decisions, LLVM relies on; global data which can end up being duplicated across shared libraries; resulting in bugs. As such this is not a safe way to distribute LLVM or; LLVM-based tools. The simplest example of building a distribution with reasonable performance is; captured in the DistributionExample CMake cache file located at; clang/cmake/caches/DistributionExample.cmake. The following command will perform; and install the distribution build:. .. code-block:: console. $ cmake -G Ninja -C <path to clang>/cmake/caches/DistributionExample.cmake <path to LLVM source>; $ ninja stage2-distribution; $ ninja stage2-install-distribution. Difference between ``install`` and ``install-distribution``; -----------------------------------------------------------. One subtle but important thing to note is the difference between the ``install``; and ``install-distribution`` targets. The ``install`` target is expected to; install every",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:2231,optimiz,optimizing,2231,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['optimiz'],['optimizing']
Performance,"e call to ``foo``).; Of those, 31,977 were spent inside the body of ``bar``. The last line; of the profile (``2: 0``) corresponds to line 2 inside ``main``. No; samples were collected there. .. _prof_instr:. Profiling with Instrumentation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Clang also supports profiling via instrumentation. This requires building a; special instrumented version of the code and has some runtime; overhead during the profiling, but it provides more detailed results than a; sampling profiler. It also provides reproducible results, at least to the; extent that the code behaves consistently across runs. Clang supports two types of instrumentation: frontend-based and IR-based.; Frontend-based instrumentation can be enabled with the option ``-fprofile-instr-generate``,; and IR-based instrumentation can be enabled with the option ``-fprofile-generate``.; For best performance with PGO, IR-based instrumentation should be used. It has; the benefits of lower instrumentation overhead, smaller raw profile size, and; better runtime performance. Frontend-based instrumentation, on the other hand,; has better source correlation, so it should be used with source line-based; coverage testing. The flag ``-fcs-profile-generate`` also instruments programs using the same; instrumentation method as ``-fprofile-generate``. However, it performs a; post-inline late instrumentation and can produce context-sensitive profiles. Here are the steps for using profile guided optimization with; instrumentation:. 1. Build an instrumented version of the code by compiling and linking with the; ``-fprofile-generate`` or ``-fprofile-instr-generate`` option. .. code-block:: console. $ clang++ -O2 -fprofile-instr-generate code.cc -o code. 2. Run the instrumented executable with inputs that reflect the typical usage.; By default, the profile data will be written to a ``default.profraw`` file; in the current directory. You can override that default by using option; ``-fprofile-instr-generate=`` or b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:102938,perform,performance,102938,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performance']
Performance,"e changed by a TVectorD datatype,; //because the pointer has no meaning in the R environment.; //This is a generalization of the RosenBrock function, with the min xi=1 and i>0.; Double_t GenRosenBrock(const TVectorD xx ); {; int length=xx.GetNoElements();. Double_t result=0;; for(int i=0;i<(length-1);i++); {; result+=pow(1-xx[i],2)+100*pow(xx[i+1]-pow(xx[i],2),2);; }; return result;; }. //the min xi=0 i>0; Double_t Rastrigin(const TVectorD xx); {; int length=xx.GetNoElements();; Double_t result=10*length;; for(int i=0;i<length;i++); {; result+=xx[i]*xx[i]-10*cos(6.2831853*xx[i]);; }; return result;; }. void GlobalMinimization(); {; TBenchmark bench;; ROOT::R::TRInterface &r=ROOT::R::TRInterface::Instance();. Bool_t installed=r.Eval(""is.element('DEoptim', installed.packages()[,1])"");; if(!installed); {; std::cout<<""Package DEoptim no installed in R""<<std::endl;; std::cout<<""Run install.packages('DEoptim') in R's environment""<<std::endl;; return;; }. //loading DEoptim; r<<""suppressMessages(library(DEoptim, quietly = TRUE))"";. // passing RosenBrock function to R; r[""GenRosenBrock""]<<GenRosenBrock;. //maximun number of iterations; r[""MaxIter""]<<5000;; //n = size of vector that is an argument for GenRosenBrock; r[""n""]<<3;; //lower limits; r<<""ll<-rep(-25, n)"";; //upper limits; r<<""ul<-rep(25, n)"";. bench.Start(""GlobalMinimizationRosenBrock"");; //calling minimization and timing it.; r<<""result1<-DEoptim(fn=GenRosenBrock,lower=ll,upper=ul,control=list(NP=10*n,itermax=MaxIter,trace=FALSE))"";; std::cout<<""-----------------------------------------""<<std::endl;; std::cout<<""RosenBrock's minimum in: ""<<std::endl;; r<<""print(result1$optim$bestmem)"";; std::cout<<""Bechmark Times""<<std::endl;; // printing times; bench.Show(""GlobalMinimizationRosenBrock"");. //passing RosenBrock function to R; r[""Rastrigin""]<<Rastrigin;; //maximun number of iterations; r[""MaxIter""]<<2000;; //n = size of a vector which is an argument for Rastrigin; r[""n""]<<3;; //lower limits; r<<""ll<-rep(-5, n)"";; //up",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/r/doc/users-guide/ROOTR_Users_Guide.md:18554,load,loading,18554,bindings/r/doc/users-guide/ROOTR_Users_Guide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/r/doc/users-guide/ROOTR_Users_Guide.md,1,['load'],['loading']
Performance,"e changes are:. * `Straight-line scalar optimizations <https://goo.gl/4Rb9As>`_ -- These; reduce redundancy within straight-line code. * `Aggressive speculative execution; <https://llvm.org/docs/doxygen/html/SpeculativeExecution_8cpp_source.html>`_; -- This is mainly for promoting straight-line scalar optimizations, which are; most effective on code along dominator paths. * `Memory space inference; <https://llvm.org/doxygen/NVPTXInferAddressSpaces_8cpp_source.html>`_ --; In PTX, we can operate on pointers that are in a particular ""address space""; (global, shared, constant, or local), or we can operate on pointers in the; ""generic"" address space, which can point to anything. Operations in a; non-generic address space are faster, but pointers in CUDA are not explicitly; annotated with their address space, so it's up to LLVM to infer it where; possible. * `Bypassing 64-bit divides; <https://llvm.org/docs/doxygen/html/BypassSlowDivision_8cpp_source.html>`_ --; This was an existing optimization that we enabled for the PTX backend. 64-bit integer divides are much slower than 32-bit ones on NVIDIA GPUs.; Many of the 64-bit divides in our benchmarks have a divisor and dividend; which fit in 32-bits at runtime. This optimization provides a fast path for; this common case. * Aggressive loop unrolling and function inlining -- Loop unrolling and; function inlining need to be more aggressive for GPUs than for CPUs because; control flow transfer in GPU is more expensive. More aggressive unrolling and; inlining also promote other optimizations, such as constant propagation and; SROA, which sometimes speed up code by over 10x. (Programmers can force unrolling and inline using clang's `loop unrolling pragmas; <https://clang.llvm.org/docs/AttributeReference.html#pragma-unroll-pragma-nounroll>`_; and ``__attribute__((always_inline))``.). Publication; ===========. The team at Google published a paper in CGO 2016 detailing the optimizations; they'd made to clang/LLVM. Note that ""gpucc"" ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst:19389,optimiz,optimization,19389,interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,1,['optimiz'],['optimization']
Performance,"e compiler) dependencies and; tries to generate the relevant vfs file. ## State of the union. Preloading all modules at start up time turn our motivating example into:. ```cpp; // ROOT prompt; root [] S *s; // #1: does not require a definition.; root [] foo::bar *baz1; // #2: does not require a definition.; root [] foo::bar baz2; // #3: requires a definition.; root [] TCanvas* c = new TCanvas(); // #4 requires a definition. ```. becomes equivalent to. ```cpp; // ROOT prompt; root [] import ROOT.*;; root [] import Foo.*;; root [] S *s; // #1: does not require a definition.; root [] foo::bar *baz1; // #2: does not require a definition.; root [] foo::bar baz2; // #3: requires a definition.; root [] TCanvas* c = new TCanvas(); // #4 requires a definition; ```. The implementation avoids recursive actions and relies on a well-defined (by; the C++ standard) behavior. Currently, this comes with a constant performance; overhead which we go in details bellow. ROOT uses the global module index (GMI) to avoid the performance overhead. ROOT; only preloads the set of C++ modules which are not present in the GMI. The; example becomes equivalent to:. ```cpp; // ROOT prompt; root [] import Foo.*; // Preload Foo if it is not in the GMI.; root [] S *s; // #1: does not require a definition.; root [] foo::bar *baz1; // #2: does not require a definition.; root [] foo::bar baz2; // #3: requires a definition.; root [] TCanvas* c = new TCanvas(); // #4 requires a definition; ```. Line #4 forces cling to send ROOT a callback that TCanvas in unknown but; the GMI resolves it to module Gpad, loads it and returns the control to cling. ### Performance; This section compares ROOT PCH technology with C++ Modules which is important but; unfair comparison. As we noted earlier, PCH is very efficient, it cannot be; extended to the experiments software stacks because of its design constraints.; On the contrary, the C++ Modules can be used in third-party code where the PCH; is not available. The comparis",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:16676,perform,performance,16676,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['perform'],['performance']
Performance,"e conditional branch or ending with; one conditional and one unconditional branch), the operands returned in the; ``Cond`` parameter can be passed to methods of other instructions to create new; branches or perform other operations. An implementation of ``analyzeBranch``; requires the helper methods ``removeBranch`` and ``insertBranch`` to manage; subsequent operations. ``analyzeBranch`` should return false indicating success in most circumstances.; ``analyzeBranch`` should only return true when the method is stumped about what; to do, for example, if a block has three terminating branches.; ``analyzeBranch`` may return true if it encounters a terminator it cannot; handle, such as an indirect branch. .. _instruction-selector:. Instruction Selector; ====================. LLVM uses a ``SelectionDAG`` to represent LLVM IR instructions, and nodes of; the ``SelectionDAG`` ideally represent native target instructions. During code; generation, instruction selection passes are performed to convert non-native; DAG instructions into native target-specific instructions. The pass described; in ``XXXISelDAGToDAG.cpp`` is used to match patterns and perform DAG-to-DAG; instruction selection. Optionally, a pass may be defined (in; ``XXXBranchSelector.cpp``) to perform similar DAG-to-DAG operations for branch; instructions. Later, the code in ``XXXISelLowering.cpp`` replaces or removes; operations and data types not supported natively (legalizes) in a; ``SelectionDAG``. TableGen generates code for instruction selection using the following target; description input files:. * ``XXXInstrInfo.td`` --- Contains definitions of instructions in a; target-specific instruction set, generates ``XXXGenDAGISel.inc``, which is; included in ``XXXISelDAGToDAG.cpp``. * ``XXXCallingConv.td`` --- Contains the calling and return value conventions; for the target architecture, and it generates ``XXXGenCallingConv.inc``,; which is included in ``XXXISelLowering.cpp``. The implementation of an instruction ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:52262,perform,performed,52262,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['performed']
Performance,"e default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula class, which is still available in ROOT as =ROOT::v5::TFormula=.; Some of the TFormula member funtions available in version 5, such as =Analyze= and =AnalyzeFunction= are not available in the new TFormula class.; On the other hand formula expressions which were valid in version 5 are still valid in TFormula version 6; - TFormu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14903,cache,cache,14903,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance,"e definition of the outputfile. This allows to have complete URL and; to pass options to TFile::Open. XrdProofd plugin. Add automatically the line 'Path.ForceRemote 1' to the; session rootrc file if the ROOT version is < 5.24/00 ; this acts; as a workaround for the wrong TTreeCache initialization at the; transition between local and remote files fixed in 5.24/00 . Enable mass storage domain settings when working with; TChain's; in multi-master mode. The Mass Storage Domain must be specified as; option in the URL. ; chain.AddFile(""root:// .....?msd=CERN""). and the string must match the value specified in defining the; submaster node.; Improved performance monitoring: the 'Rate plot' button; in the dialog box has been renamed 'Performance Plot' and now shows up; to 4 plots as a function of the processing time:. Instantaneous processing rate, which is now better; estimated by a better estimation of the normalizing times; Average read chunck size, defined as; TFile::GetFileBytesRead() / TFile::GetFileReadCalls() during the last; unit of time; this allows to monitor the usage of the cache; this plot; is present only if some I/O is done, i.e. not for pure CPU tasks.; The number of active workers; The number of total and effecive sessions running; concurrently on the cluster (started by the same daemon); this plot is; present only is the number is at least onec different from 1. If enabled, send monitoring information from the master; at each GetNextPacket (at each call of TPerfStat::PacketEvent) to allow; extrnal real-time progress monitoring.; Save the status of a 'proofserv' session into a new file; in the 'activesessions' area. The full path of the new file is; ; <admin_path>/.xproofd.<port>/activesessions/<user>.<group>.<pid>.status. The status indicates whether the session is idle, running or queued.; The status is updated every 'checkfq' secs (see xpd.proofservmgr;; default 30 s). The status is dumped by the reader thread of TXProofServ; and t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:5058,cache,cache,5058,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,2,['cache'],['cache']
Performance,"e described in ""Color and Color Palettes"". ``` {.cpp}; root[] la->SetTextColor(color); ```. #### Setting Text Font. Use `TAttText::SetTextFont` to set the font. The parameter font is the; font code, combining the font and precision:; `font = 10 * fontID + precision`. ``` {.cpp}; root[] la->SetTextFont(font); ```. The table below lists the available fonts. The font IDs must be between; 1 and 14. The precision can be:. - Precision = 0 fast hardware fonts (steps in the size). - Precision = 1 scalable and rotate-able hardware fonts (see below). - Precision = 2 scalable and rotate-able hardware fonts. When precision 0 is used, only the original non-scaled system fonts are; used. The fonts have a minimum (4) and maximum (37) size in pixels.; These fonts are fast and are of good quality. Their size varies with; large steps and they cannot be rotated. Precision 1 and 2 fonts have a; different behavior depending if True Type Fonts (TTF) are used or not.; If TTF are used, you always get very good quality scalable and; rotate-able fonts. However, TTF are slow. Precision 1 and 2 fonts have a; different behavior for PostScript in case of **`TLatex`** objects:. - With precision 1, the PostScript text uses the old convention (see; **`TPostScript`**) for some special characters to draw sub and; superscripts or Greek text. - With precision 2, the ""PostScript"" special characters are drawn as; such. To draw sub and superscripts it is highly recommended to use; **`TLatex`** objects instead. For example: `font = 62` is the font with ID `6` and precision `2`. ![Font's examples](pictures/030000CF.png). The available fonts are:. +-----------+--------------------------+-----------------------+---------+------------+; | Font ID | X11 | True Type name | Is | ""boldness"" |; | | | | italic | |; +-----------+--------------------------+-----------------------+---------+------------+; | 1 | times-medium-i-normal | ""Times New Roman"" | Yes | 4 |; +-----------+--------------------------+--------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:71190,scalab,scalable,71190,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['scalab'],['scalable']
Performance,"e element is written as described below. If the expression; appears on the left-hand side of a compound assignment operator (e.g.; +=), the program is ill-formed, because the result of reading an element; is always an Objective-C object pointer and no binary operators are; legal on such pointers. If the expression appears in any other position,; the element is read as described below. It is an error to take the; address of a subscript expression, or (in C++) to bind a reference to; it. Programs can use object subscripting with Objective-C object pointers of; type ``id``. Normal dynamic message send rules apply; the compiler must; see *some* declaration of the subscripting methods, and will pick the; declaration seen first. Caveats; =======. Objects created using the literal or boxed expression syntax are not; guaranteed to be uniqued by the runtime, but nor are they guaranteed to; be newly-allocated. As such, the result of performing direct comparisons; against the location of an object literal (using ``==``, ``!=``, ``<``,; ``<=``, ``>``, or ``>=``) is not well-defined. This is usually a simple; mistake in code that intended to call the ``isEqual:`` method (or the; ``compare:`` method). This caveat applies to compile-time string literals as well.; Historically, string literals (using the ``@""...""`` syntax) have been; uniqued across translation units during linking. This is an; implementation detail of the compiler and should not be relied upon. If; you are using such code, please use global string constants instead; (``NSString * const MyConst = @""...""``) or use ``isEqual:``. Grammar Additions; =================. To support the new syntax described above, the Objective-C; ``@``-expression grammar has the following new productions:. ::. objc-at-expression : '@' (string-literal | encode-literal | selector-literal | protocol-literal | object-literal); ;. object-literal : ('+' | '-')? numeric-constant; | character-constant; | boolean-constant; | array-literal; | dictio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ObjectiveCLiterals.rst:17266,perform,performing,17266,interpreter/llvm-project/clang/docs/ObjectiveCLiterals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ObjectiveCLiterals.rst,1,['perform'],['performing']
Performance,"e element) in; order to save memory (by not having all the object in memory; at the same time). However for histograms, the default is to first load all the; objects and then merge them in one go ; this is customizable; when creating the TFileMerger object. Asynchronous Prefetching; The prefetching mechanism uses two new classes (TFilePrefetch; and TFPBlock) to prefetch in advance a block of tree entries. There; is a thread which takes care of actually transferring the blocks and; making them available to the main requesting thread. Therefore, the time; spent by the main thread waiting for the data before processing considerably; decreases. Besides the prefetching mechanisms there is also a local; caching option which can be enabled by the user. Both capabilities are; disabled by default and must be explicitly enabled by the user. In order to enable the prefetching the user must set the rootrc environment; variable TFile.AsyncPrefetching as follows:; gEnv->SetValue(""TFile.AsyncPrefetching"", 1). Only when the; prefetching is enabled can the user set the local cache directory in; which the file transferred will be saved. For subsequent reads of the; same file the system will use the local copy of the file from cache.; To set up a local cache directory, the client can use the following commands:. TString cachedir=""file:/tmp/xcache/"";; // or using xrootd on port 2000; // TString cachedir=""root://localhost:2000//tmp/xrdcache1/"";; gEnv->SetValue(""Cache.Directory"", cachedir.Data());. The TFilePrefetch class is responsible for actually reading; and storing the requests received from the main thread. It also creates; the working thread which will transfer all the information. Apart from; managing the block requests, it also deals with caching the blocks on; the local machine and retrieving them when necessary. The TFPBlock class represents the encapsulation of a block; request. It contains the chunks to be prefetched and also serves as a; container for the information read. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html:5654,cache,cache,5654,io/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html,13,"['Cache', 'cache']","['Cache', 'cache', 'cachedir']"
Performance,"e end-point URL for local; files; Improve realtime notification during 'verify'. TProofDraw. Fix a problem with the axis ranges of the underlying; histogram in PolyMarker3D; Allow to use the default pad instead of forcing; creation of one pad per object; Add wrapper to handle the feedback default canvas. TEventIter. Fix a problem with changing the tree cache size: the; size was reset to the default value after the first file. TDataSetManagerFile. Solve a consistency problem in checking URLs for; duplication when adding them to the relevant TFileInfo; During dataset validation, do not fail on duplications; but notify and add them to the bad file list. TPacketizerAdaptive, TPacketizer. Improve data node / worker matching by always using the; host FQDN. TPacketizerUnit, TEventIter. Make sure that the entry; number passed to TSelector::Process is unique and in increasing order; for non-data driven processing (packetizer TPacketizerUnit). This; allows to give a meaning to this variable, for example to related it to; one dimension of an integration. Fixes in PROOF-Lite:. Make sure that with envs settings via TProof::AddEnvVar; are effective; this enables, for example, the automatic valgrind setup; introduced in 5.24/00 or the experiment specific settings via the; script defined by the env PROOF_INIT; Fix a problem with TProof::Load so that now it can be; also be used for PROOF-Lite. TProofPlayerRemote. In SendSelector, add misisng; option kCpBin when sending the selector source; the binary files were; never retrieved, even if present and valid. TProofPlayerSlave. In; Process, fix a problem withcache directory locking while building; the selector; the net effect was that each worker process was; re-buidling its own selector binary. . TProofServ; Fix; the order in which the log file is sent in asynchronous processing; the; wrong order was screwing up an immediate synchronous query submission; after an asynchronous run; this case occured, for example, in; 'stressProof' . ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:9664,cache,cache,9664,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,3,"['Load', 'cache']","['Load', 'cache']"
Performance,"e exclude regex, but ``/usr/include/foo.h`` doesn't since it matches; the exclude regex. Controlling Debug Information; -----------------------------. Controlling Size of Debug Information; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Debug info kind generated by Clang can be set by one of the flags listed; below. If multiple flags are present, the last one is used. .. option:: -g0. Don't generate any debug info (default). .. option:: -gline-tables-only. Generate line number tables only. This kind of debug info allows to obtain stack traces with function names,; file names and line numbers (by such tools as ``gdb`` or ``addr2line``). It; doesn't contain any other data (e.g. description of local variables or; function parameters). .. option:: -fstandalone-debug. Clang supports a number of optimizations to reduce the size of debug; information in the binary. They work based on the assumption that; the debug type information can be spread out over multiple; compilation units. Specifically, the optimizations are:. - will not emit type definitions for types that are not needed by a; module and could be replaced with a forward declaration.; - will only emit type info for a dynamic C++ class in the module that; contains the vtable for the class.; - will only emit type info for a C++ class (non-trivial, non-aggregate); in the modules that contain a definition for one of its constructors.; - will only emit type definitions for types that are the subject of explicit; template instantiation declarations in the presence of an explicit; instantiation definition for the type. The **-fstandalone-debug** option turns off these optimizations.; This is useful when working with 3rd-party libraries that don't come; with debug information. Note that Clang will never emit type; information for types that are not referenced at all by the program. .. option:: -fno-standalone-debug. On Darwin **-fstandalone-debug** is enabled by default. The; **-fno-standalone-debug** option can be used to get to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:125184,optimiz,optimizations,125184,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"e files can include it simply with; **#include ""jazz/note.h""**. **tools**. This subdirectory should contain all of your source code for executables.; For each program that you build, you will have one directory in **tools**; that will contain that program's source code. **test**. This subdirectory should contain tests that verify that your code works; correctly. Automated tests are especially useful. Currently, the LLVM build system provides basic support for tests. The LLVM; system provides the following:. * LLVM contains regression tests in ``llvm/test``. These tests are run by the; :doc:`Lit <CommandGuide/lit>` testing tool. This test procedure uses ``RUN``; lines in the actual test case to determine how to run the test. See the; :doc:`TestingGuide` for more details. * LLVM contains an optional package called ``llvm-test``, which provides; benchmarks and programs that are known to compile with the Clang front; end. You can use these programs to test your code, gather statistical; information, and compare it to the current LLVM performance statistics. Currently, there is no way to hook your tests directly into the ``llvm/test``; testing harness. You will simply need to find a way to use the source; provided within that directory on your own. Typically, you will want to build your **lib** directory first followed by your; **tools** directory. Writing LLVM Style Makefiles; ============================. The LLVM build system provides a convenient way to build libraries and; executables. Most of your project Makefiles will only need to define a few; variables. Below is a list of the variables one can set and what they can; do:. Required Variables; ------------------. ``LEVEL``. This variable is the relative path from this ``Makefile`` to the top; directory of your project's source code. For example, if your source code; is in ``/tmp/src``, then the ``Makefile`` in ``/tmp/src/jump/high``; would set ``LEVEL`` to ``""../..""``. Variables for Building Subdirectories; ------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Projects.rst:3773,perform,performance,3773,interpreter/llvm-project/llvm/docs/Projects.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Projects.rst,1,['perform'],['performance']
Performance,"e files under the perf-training directory as training; data as long as the source files are marked up with LIT-style RUN lines. After it finishes you can use :code:`find . -name clang.profdata` to find it, but it; should be at a path something like:. .. code-block:: console. <build dir>/tools/clang/stage2-instrumented-bins/utils/perf-training/clang.profdata. You can feed that file into the LLVM_PROFDATA_FILE option when you build your; optimized compiler. It may be necessary to build additional targets before running perf training, such as; builtins and runtime libraries. You can use the :code:`CLANG_PGO_TRAINING_DEPS` CMake; variable for that purpose:. .. code-block:: cmake. set(CLANG_PGO_TRAINING_DEPS builtins runtimes CACHE STRING """"). The PGO cache has a slightly different stage naming scheme than other; multi-stage builds. It generates three stages: stage1, stage2-instrumented, and; stage2. Both of the stage2 builds are built using the stage1 compiler. The PGO cache generates the following additional targets:. **stage2-instrumented**; Builds a stage1 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. **stage2-instrumented-generate-profdata**; Depends on stage2-instrumented and will use the instrumented compiler to; generate profdata based on the training files in clang/utils/perf-training. **stage2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:8850,cache,cache,8850,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['cache']
Performance,"e final side-effecting node in the token chain. For example, in a single basic; block function it would be the return node. One important concept for SelectionDAGs is the notion of a ""legal"" vs.; ""illegal"" DAG. A legal DAG for a target is one that only uses supported; operations and supported types. On a 32-bit PowerPC, for example, a DAG with a; value of type i1, i8, i16, or i64 would be illegal, as would a DAG that uses a; SREM or UREM operation. The `legalize types`_ and `legalize operations`_ phases; are responsible for turning an illegal DAG into a legal DAG. .. _SelectionDAG-Process:. SelectionDAG Instruction Selection Process; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. SelectionDAG-based instruction selection consists of the following steps:. #. `Build initial DAG`_ --- This stage performs a simple translation from the; input LLVM code to an illegal SelectionDAG. #. `Optimize SelectionDAG`_ --- This stage performs simple optimizations on the; SelectionDAG to simplify it, and recognize meta instructions (like rotates; and ``div``/``rem`` pairs) for targets that support these meta operations.; This makes the resultant code more efficient and the `select instructions; from DAG`_ phase (below) simpler. #. `Legalize SelectionDAG Types`_ --- This stage transforms SelectionDAG nodes; to eliminate any types that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to clean up; redundancies exposed by type legalization. #. `Legalize SelectionDAG Ops`_ --- This stage transforms SelectionDAG nodes to; eliminate any operations that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to eliminate; inefficiencies introduced by operation legalization. #. `Select instructions from DAG`_ --- Finally, the target instruction selector; matches the DAG operations to target instructions. This process translates; the target-independent input DAG into another DAG of target instructions. #. `Sele",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:37101,perform,performs,37101,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['optimiz', 'perform']","['optimizations', 'performs']"
Performance,"e format; version. Versions 0, 1 and 2 are supported at this time. The difference between; version 0 and 1 is in the encoding of instruction operands in; each `FUNCTION_BLOCK`_. In version 0, each value defined by an instruction is assigned an ID; unique to the function. Function-level value IDs are assigned starting from; ``NumModuleValues`` since they share the same namespace as module-level; values. The value enumerator resets after each function. When a value is; an operand of an instruction, the value ID is used to represent the operand.; For large functions or large modules, these operand values can be large. The encoding in version 1 attempts to avoid large operand values; in common cases. Instead of using the value ID directly, operands are; encoded as relative to the current instruction. Thus, if an operand; is the value defined by the previous instruction, the operand; will be encoded as 1. For example, instead of. .. code-block:: none. #n = load #n-1; #n+1 = icmp eq #n, #const0; br #n+1, label #(bb1), label #(bb2). version 1 will encode the instructions as. .. code-block:: none. #n = load #1; #n+1 = icmp eq #1, (#n+1)-#const0; br #1, label #(bb1), label #(bb2). Note in the example that operands which are constants also use; the relative encoding, while operands like basic block labels; do not use the relative encoding. Forward references will result in a negative value.; This can be inefficient, as operands are normally encoded; as unsigned VBRs. However, forward references are rare, except in the; case of phi instructions. For phi instructions, operands are encoded as; `Signed VBRs`_ to deal with forward references. In version 2, the meaning of module records ``FUNCTION``, ``GLOBALVAR``,; ``ALIAS``, ``IFUNC`` and ``COMDAT`` change such that the first two operands; specify an offset and size of a string in a string table (see `STRTAB_BLOCK; Contents`_), the function name is removed from the ``FNENTRY`` record in the; value symbol table, and the top-level `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst:24216,load,load,24216,interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,1,['load'],['load']
Performance,"e globally unique scope names. A metadata; reference to the scope's domain is the second entry. A descriptive string may; optionally be provided as a third list entry. For example,. .. code-block:: llvm. ; Two scope domains:; !0 = !{!0}; !1 = !{!1}. ; Some scopes in these domains:; !2 = !{!2, !0}; !3 = !{!3, !0}; !4 = !{!4, !1}. ; Some scope lists:; !5 = !{!4} ; A list containing only scope !4; !6 = !{!4, !3, !2}; !7 = !{!3}. ; These two instructions don't alias:; %0 = load float, ptr %c, align 4, !alias.scope !5; store float %0, ptr %arrayidx.i, align 4, !noalias !5. ; These two instructions also don't alias (for domain !1, the set of scopes; ; in the !alias.scope equals that in the !noalias list):; %2 = load float, ptr %c, align 4, !alias.scope !5; store float %2, ptr %arrayidx.i2, align 4, !noalias !6. ; These two instructions may alias (for domain !0, the set of scopes in; ; the !noalias list is not a superset of, or equal to, the scopes in the; ; !alias.scope list):; %2 = load float, ptr %c, align 4, !alias.scope !6; store float %0, ptr %arrayidx.i, align 4, !noalias !7. '``fpmath``' Metadata; ^^^^^^^^^^^^^^^^^^^^^. ``fpmath`` metadata may be attached to any instruction of floating-point; type. It can be used to express the maximum acceptable error in the; result of that instruction, in ULPs, thus potentially allowing the; compiler to use a more efficient but less accurate method of computing; it. ULP is defined as follows:. If ``x`` is a real number that lies between two finite consecutive; floating-point numbers ``a`` and ``b``, without being equal to one; of them, then ``ulp(x) = |b - a|``, otherwise ``ulp(x)`` is the; distance between the two non-equal finite floating-point numbers; nearest ``x``. Moreover, ``ulp(NaN)`` is ``NaN``. The metadata node shall consist of a single positive float type number; representing the maximum relative error, for example:. .. code-block:: llvm. !0 = !{ float 2.5 } ; maximum acceptable inaccuracy is 2.5 ULPs. .. _range-metada",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:283720,load,load,283720,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"e have to define so many volumes to make an R?"". **A:** Well, in real life some objects have much more complex shapes; that an ""R"". The modeller cannot just know all of them; the idea; is to make a complex object by using elementary building blocks that; have known shapes (called ""primitive shapes""). Gluing these; together in the appropriate way is the user responsibility. **Q:** ""I am getting the global picture but not making much out of it... There; are also a lot of calls to TGeoVolume::AddNode() that I do not understand."". **A:** A volume is positioned inside another one by using this; method. The relative geometrical transformation as well as a copy number; must be specified. When positioned, a volume becomes a ""node"" of; its container and a new object of the class TGeoNode is; automatically created. This method is therefore the key element for the; creation of a hierarchical link between two volumes. As it will be; described further on in this document, there are few other methods; performing similar actions, but let us keep things simple for the time; being. In addition, notice that there are some visualization-related; calls in the example followed by a final TGeoVolume::Draw() call for; the top volume. These are explained in details in the section; ""Visualization Settings and Attributes"". At this point, you will; probably like to see how this geometry looks like. You just need to run; the example and you will get the following picture that you can rotate; using the mouse; or you can zoom / move it around (see what the Help; menu of the GL window displays). ~~~{.cpp}; % root rootgeom.C; ~~~. \image html geometry001.png width=600px. Now let us browse the hierarchy that was just created. Start a browser; and double-click on the item simple1 representing the; `gGeoManager` object. Note that right click opens the context menu; of the manager class where several global methods are available. ~~~{.cpp}; root[] new TBrowser;; ~~~. \image html geometry002.jpg width=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:10674,perform,performing,10674,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performing']
Performance,"e hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3659,optimiz,optimized,3659,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['optimiz'],['optimized']
Performance,"e image, such as the triple and architecture. All of these structures are combined to form a single binary blob, the order; does not matter because of the use of absolute offsets. This makes it easier to; extend in the future. As mentioned previously, multiple offloading images are; bundled together by simply concatenating them in this format. Because we have; the magic bytes and size of each image, we can extract them as-needed. Usage; =====. This tool can be used with the following arguments. Generally information is; passed as a key-value pair to the ``image=`` argument. The ``file`` and; ``triple``, arguments are considered mandatory to make a valid image.; The ``arch`` argument is suggested. .. code-block:: console. OVERVIEW: A utility for bundling several object files into a single binary.; The output binary can then be embedded into the host section table; to create a fatbinary containing offloading code. USAGE: clang-offload-packager [options]. OPTIONS:. Generic Options:. --help - Display available options (--help-hidden for more); --help-list - Display list of available options (--help-list-hidden for more); --version - Display the version of this program. clang-offload-packager options:. --image=<<key>=<value>,...> - List of key and value arguments. Required; keywords are 'file' and 'triple'.; -o <file> - Write output to <file>. Example; =======. This tool simply takes many input files from the ``image`` option and creates a; single output file with all the images combined. .. code-block:: console. clang-offload-packager -o out.bin --image=file=input.o,triple=nvptx64,arch=sm_70. The inverse operation can be performed instead by passing the packaged binary as; input. In this mode the matching images will either be placed in the output; specified by the ``file`` option. If no ``file`` argument is provided a name; will be generated for each matching image. .. code-block:: console. clang-offload-packager in.bin --image=file=output.o,triple=nvptx64,arch=sm_70; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadPackager.rst:8766,perform,performed,8766,interpreter/llvm-project/clang/docs/ClangOffloadPackager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadPackager.rst,1,['perform'],['performed']
Performance,"e implicit ``this`` or ``self`` argument comes first as a; pointer type.; 2. Followed by the function formal arguments in left to right source order. The source language result arguments are:. 1. The function result argument. The source language input or result struct type arguments that are less than or; equal to 16 bytes, are decomposed recursively into their base type fields, and; each field is passed as if a separate argument. For input arguments, if the; called function requires the struct to be in memory, for example because its; address is taken, then the function body is responsible for allocating a stack; location and copying the field arguments into it. Clang terms this *direct; struct*. The source language input struct type arguments that are greater than 16 bytes,; are passed by reference. The caller is responsible for allocating a stack; location to make a copy of the struct value and pass the address as the input; argument. The called function is responsible to perform the dereference when; accessing the input argument. Clang terms this *by-value struct*. A source language result struct type argument that is greater than 16 bytes, is; returned by reference. The caller is responsible for allocating a stack location; to hold the result value and passes the address as the last input argument; (before the implicit input arguments). In this case there are no result; arguments. The called function is responsible to perform the dereference when; storing the result value. Clang terms this *structured return (sret)*. *TODO: correct the ``sret`` definition.*. .. TODO::. Is this definition correct? Or is ``sret`` only used if passing in registers, and; pass as non-decomposed struct as stack argument? Or something else? Is the; memory location in the caller stack frame, or a stack memory argument and so; no address is passed as the caller can directly write to the argument stack; location? But then the stack location is still live after return. If an; argument sta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:392926,perform,perform,392926,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['perform']
Performance,"e in well-behaved Java programs, and typically the null; check can be folded into a nearby memory operation that operates on; the same memory location. The Fault Map Section; =====================. Information about implicit checks generated by LLVM are put in a; special ""fault map"" section. On Darwin this section is named; ``__llvm_faultmaps``. The format of this section is. .. code-block:: none. Header {; uint8 : Fault Map Version (current version is 1); uint8 : Reserved (expected to be 0); uint16 : Reserved (expected to be 0); }; uint32 : NumFunctions; FunctionInfo[NumFunctions] {; uint64 : FunctionAddress; uint32 : NumFaultingPCs; uint32 : Reserved (expected to be 0); FunctionFaultInfo[NumFaultingPCs] {; uint32 : FaultKind; uint32 : FaultingPCOffset; uint32 : HandlerPCOffset; }; }. FailtKind describes the reason of expected fault. Currently three kind; of faults are supported:. 1. ``FaultMaps::FaultingLoad`` - fault due to load from memory.; 2. ``FaultMaps::FaultingLoadStore`` - fault due to instruction load and store.; 3. ``FaultMaps::FaultingStore`` - fault due to store to memory. The ``ImplicitNullChecks`` pass; ===============================. The ``ImplicitNullChecks`` pass transforms explicit control flow for; checking if a pointer is ``null``, like:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %ptr_is_null = icmp i32* %ptr, null; br i1 %ptr_is_null, label %is_null, label %not_null, !make.implicit !0. not_null:; %t = load i32, i32* %ptr; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. !0 = !{}. to control flow implicit in the instruction loading or storing through; the pointer being null checked:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %t = load i32, i32* %ptr ;; handler-pc = label %is_null; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. This transform happens at the ``MachineInstr`` level, not the LLVM IR; level (so the above example is only representative, not literal). The; ``ImplicitNu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst:1915,load,load,1915,interpreter/llvm-project/llvm/docs/FaultMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst,1,['load'],['load']
Performance,"e instructions related to the memory model are given; additional; ``s_waitcnt`` instructions are required to ensure registers are defined before; being used. These may be able to be combined with the memory model ``s_waitcnt``; instructions as described above. The AMDGPU backend supports the following memory models:. HSA Memory Model [HSA]_; The HSA memory model uses a single happens-before relation for all address; spaces (see :ref:`amdgpu-address-spaces`).; OpenCL Memory Model [OpenCL]_; The OpenCL memory model which has separate happens-before relations for the; global and local address spaces. Only a fence specifying both global and; local address space, and seq_cst instructions join the relationships. Since; the LLVM ``memfence`` instruction does not allow an address space to be; specified the OpenCL fence has to conservatively assume both local and; global address space was specified. However, optimizations can often be; done to eliminate the additional ``s_waitcnt`` instructions when there are; no intervening memory instructions which access the corresponding address; space. The code sequences in the table indicate what can be omitted for the; OpenCL memory. The target triple environment is used to determine if the; source language is OpenCL (see :ref:`amdgpu-opencl`). ``ds/flat_load/store/atomic`` instructions to local memory are termed LDS; operations. ``buffer/global/flat_load/store/atomic`` instructions to global memory are; termed vector memory operations. Private address space uses ``buffer_load/store`` using the scratch V#; (GFX6-GFX8), or ``scratch_load/store`` (GFX9-GFX11). Since only a single thread; is accessing the memory, atomic memory orderings are not meaningful, and all; accesses are treated as non-atomic. Constant address space uses ``buffer/global_load`` instructions (or equivalent; scalar memory instructions). Since the constant address space contents do not; change during the execution of a kernel dispatch it is not legal to perform; stores",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:202428,optimiz,optimizations,202428,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['optimiz'],['optimizations']
Performance,"e is incompatible with the ``minsize``, ``optsize``, and; ``optnone`` attributes.; ``optforfuzzing``; This attribute indicates that this function should be optimized; for maximum fuzzing signal.; ``optnone``; This function attribute indicates that most optimization passes will skip; this function, with the exception of interprocedural optimization passes.; Code generation defaults to the ""fast"" instruction selector.; This attribute cannot be used together with the ``alwaysinline``; attribute; this attribute is also incompatible; with the ``minsize``, ``optsize``, and ``optdebug`` attributes. This attribute requires the ``noinline`` attribute to be specified on; the function as well, so the function is never inlined into any caller.; Only functions with the ``alwaysinline`` attribute are valid; candidates for inlining into the body of this function.; ``optsize``; This attribute suggests that optimization passes and code generator; passes make choices that keep the code size of this function low,; and otherwise do optimizations specifically to reduce code size as; long as they do not significantly impact runtime performance.; This attribute is incompatible with the ``optdebug`` and ``optnone``; attributes.; ``""patchable-function""``; This attribute tells the code generator that the code; generated for this function needs to follow certain conventions that; make it possible for a runtime function to patch over it later.; The exact effect of this attribute depends on its string value,; for which there currently is one legal possibility:. * ``""prologue-short-redirect""`` - This style of patchable; function is intended to support patching a function prologue to; redirect control away from the function in a thread safe; manner. It guarantees that the first instruction of the; function will be large enough to accommodate a short jump; instruction, and will be sufficiently aligned to allow being; fully changed via an atomic compare-and-swap instruction.; While the first requir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:96446,optimiz,optimization,96446,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['optimiz', 'perform']","['optimization', 'optimizations', 'performance']"
Performance,"e it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we int",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2840,optimiz,optimization,2840,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"e languages: C++ is much harder to parse, but once; parsed, it is much easier to optimize. Consequently, individual calls to; ROOT are typically faster from `PyROOT`, whereas loops are typically; slower. When programming in Python, the modus operandi is to consider; performance generally ""good enough"" on the outset, and when it turns out; that, it is not good enough; the performance critical part is converted; into C/C++ in an extension module. The school of thought where; pre-mature optimization is the root of all evil should find this way of; working very satisfying. In addition, if you look at their history, you; will see that many of the standard Python modules have followed this; path. Your code should always make maximum use of ROOT facilities; such that; most of the time is spending in compiled code. This goes even for very; simple things: e.g. do not compute invariant masses in Python, use; **`TLorentzVector`** instead. Moreover, before you start optimizing,; make sure that you have run a profiler to find out where the bottlenecks; are. Some performance, without cost in terms of programmer effort, may; be gained by using `psyco`, see the next link:; <http://psyco.sourceforge.net>, a Python just in time compiler (JIT).; Note, however, that `psyco` is limited to Intel i386 CPUs. Since `psyco`; optimizes Python, not `PyROOT` calls; it generally does not improve; performance that much if most of your code consists of ROOT API calls.; Mathematical computations in Python, on the other hand, benefit a lot. Every call to a Python member function results in a lookup of that; member function and an association of this method with `'self'`.; Furthermore, a temporary object is created during this process that is; discarded after the method call. In inner loops, it may be worth your; while (up to 30%), to short-cut this process by looking up and binding; the method before the loop, and discarding it afterwards. Here is an; example:. ``` {.cpp}; hpx = TH1F('hpx','px',100,-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:23456,optimiz,optimizing,23456,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,2,"['bottleneck', 'optimiz']","['bottlenecks', 'optimizing']"
Performance,"e left; mouse button. The cursor changes its shape to indicate what may be done:. Point the object or one part of it: ![](pictures/02000095.jpg); ![](pictures/02000096.jpg). Rotate: ![](pictures/02000097.jpg). Resize (exists also for the other directions):; ![](pictures/02000098.jpg) ![](pictures/02000099.jpg). Enlarge (used for text): ![](pictures/0200009A.jpg). Move: ![](pictures/0200009B.jpg). Here are some examples of:. Moving: ![](pictures/0200009C.jpg) Resizing: ![](pictures/0200009D.jpg). Rotating: ![](pictures/0200009E.jpg) ![](pictures/0300009F.png). #### With C++ Statements (Programmatically). How would one move an object in a script? Since there is a tight; correspondence between what is seen on the screen and the object in; memory, changing the object changes it on the screen. For example, try; to do:. ``` {.cpp}; root[] a.SetX1(0.9); ```. This should change one of the coordinates of our line, but nothing; happens on the screen. Why is that? In short, the canvas is not updated; with each change for performance reasons. See ""Updating the Pad"". ### Selecting Objects. #### The Middle Mouse Button. Objects in a canvas, as well as in a pad, are stacked on top of each; other in the order they were drawn. Some objects may become ""active""; objects, which mean they are reordered to be on top of the others. To; interactively make an object ""active"", you can use the middle mouse; button. In case of canvases or pads, the border becomes highlighted when; it is active. #### With C++ Statements (Programmatically). Frequently we want to draw in different canvases or pads. By default,; the objects are drawn in the active canvas. To activate a canvas you can; use the `TPad::cd()` method. ``` {.cpp}; root[] c1->cd(); ```. ### Context Menus: the Right Mouse Button. The context menus are a way to interactively call certain methods of an; object. When designing a class, the programmer can add methods to the; context menu of the object by making minor changes to the header fil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:3403,perform,performance,3403,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['performance']
Performance,"e likelihoods; Automatic handling of constraint terms; It is no longer necessary to add a Constrain() argument to fitTo() calls to have internal constraints; applied. Any pdf term appearing in a product that does not contain an observable and shares one or more parameters; with another pdf term in the same product that does contain an observable is automatically picked up as a constraint term.; For example given a dataset D(x) which defines variable x as observable, the default logic works out as follows. F(x,a,b)*G(a,a0,a1) --> G is constraint term (a also appears in F(x)); F(x,a,b)*G(y,c,d) --> G is dropped (factorizing term). A Constrain(y) term in the above example will still force term G(y,c,d) to be interpreted as constraint term; Automatic caching of numeric integral calculations; Integrals that require numeric integrations in two of more dimensions are now automatically cached in the expensive object store.; The expensive object store allows to cache such values between difference instance of integral objects that represent the; same configuration. If integrals are created from an object (function or pdf) that live in a RooWorkspace the ; expensive object cache of the workspace will be used instead of the global store instance, and values stored in the workspace; store will also be persisted if the workspace is persisted. The global caching behavior of integral objects can be ; controlled through RooRealIntegral::setCacheAllNumeric(Int_t nDimNumMin). Miscellaneous improvements data classes. The RooAbsData::tree() method has been restored. It will only return a TTree* pointer for datasets; that are based on a RooTreeDataStore implementation, i.e. not for the composite datasets mentioned below; A new composite data storage class RooCompositeDataStore has been added that allows to construct composite; RooDataSet objects without copying the input data. . // Make 2 input datasets and an index category; RooWorkspace w(""w"",true) ;; w->factory(""Gaussian::g(x[-10,10]",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:7895,cache,cache,7895,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,2,['cache'],['cache']
Performance,"e location of the builtin. When the invocation point of ``__builtin_FUNCTION`` is not a function scope the; empty string is returned. The builtin ``__builtin_source_location`` returns a pointer to constant static; data of type ``std::source_location::__impl``. This type must have already been; defined, and must contain exactly four fields: ``const char *_M_file_name``,; ``const char *_M_function_name``, ``<any-integral-type> _M_line``, and; ``<any-integral-type> _M_column``. The fields will be populated in the same; manner as the above four builtins, except that ``_M_function_name`` is populated; with ``__PRETTY_FUNCTION__`` rather than ``__FUNCTION__``. Alignment builtins; ------------------; Clang provides builtins to support checking and adjusting alignment of; pointers and integers.; These builtins can be used to avoid relying on implementation-defined behavior; of arithmetic on integers derived from pointers.; Additionally, these builtins retain type information and, unlike bitwise; arithmetic, they can perform semantic checking on the alignment value. **Syntax**:. .. code-block:: c. Type __builtin_align_up(Type value, size_t alignment);; Type __builtin_align_down(Type value, size_t alignment);; bool __builtin_is_aligned(Type value, size_t alignment);. **Example of use**:. .. code-block:: c++. char* global_alloc_buffer;; void* my_aligned_allocator(size_t alloc_size, size_t alignment) {; char* result = __builtin_align_up(global_alloc_buffer, alignment);; // result now contains the value of global_alloc_buffer rounded up to the; // next multiple of alignment.; global_alloc_buffer = result + alloc_size;; return result;; }. void* get_start_of_page(void* ptr) {; return __builtin_align_down(ptr, PAGE_SIZE);; }. void example(char* buffer) {; if (__builtin_is_aligned(buffer, 64)) {; do_fast_aligned_copy(buffer);; } else {; do_unaligned_copy(buffer);; }; }. // In addition to pointers, the builtins can also be used on integer types; // and are evaluatable inside constant",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:150768,perform,perform,150768,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['perform']
Performance,"e looks like this:. .. code-block:: c++. enum DebugLev {; nodebuginfo, quick, detailed; };. // Enable Debug Options to be specified on the command line; cl::opt<DebugLev> DebugLevel(""debug_level"", cl::desc(""Set the debugging level:""),; cl::values(; clEnumValN(nodebuginfo, ""none"", ""disable debug information""),; clEnumVal(quick, ""enable quick debug information""),; clEnumVal(detailed, ""enable detailed debug information"")));. This definition defines an enumerated command line variable of type ""``enum; DebugLev``"", which works exactly the same way as before. The difference here is; just the interface exposed to the user of your program and the help output by; the ""``-help``"" option:. ::. USAGE: compiler [options] <input file>. OPTIONS:; Choose optimization level:; -g - No optimizations, enable debugging; -O1 - Enable trivial optimizations; -O2 - Enable default optimizations; -O3 - Enable expensive optimizations; -debug_level - Set the debugging level:; =none - disable debug information; =quick - enable quick debug information; =detailed - enable detailed debug information; -f - Enable binary output on terminals; -help - display available options (-help-hidden for more); -o <filename> - Specify output filename; -quiet - Don't print informational messages. Again, the only structural difference between the debug level declaration and; the optimization level declaration is that the debug level declaration includes; an option name (``""debug_level""``), which automatically changes how the library; processes the argument. The CommandLine library supports both forms so that you; can choose the form most appropriate for your application. .. _lists:. Parsing a list of options; -------------------------. Now that we have the standard run-of-the-mill argument types out of the way,; lets get a little wild and crazy. Lets say that we want our optimizer to accept; a **list** of optimizations to perform, allowing duplicates. For example, we; might want to run: ""``compiler -dce -instsimpl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:18153,optimiz,optimization,18153,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,5,['optimiz'],"['optimization', 'optimizations']"
Performance,"e metadata node has no operands and all instances are; `distinct`; equality is checked for by comparing addresses. `llvm.dbg.assign` intrinsics use a `DIAssignID` metadata node instance as an; operand. This way it refers to any store-like instruction that has the same; `DIAssignID` attachment. E.g. For this test.cpp,. ```; int fun(int a) {; return a;; }; ```; compiled without optimisations:; ```; $ clang++ test.cpp -o test.ll -emit-llvm -S -g -O0 -Xclang -fexperimental-assignment-tracking; ```; we get:; ```; define dso_local noundef i32 @_Z3funi(i32 noundef %a) #0 !dbg !8 {; entry:; %a.addr = alloca i32, align 4, !DIAssignID !13; call void @llvm.dbg.assign(metadata i1 undef, metadata !14, metadata !DIExpression(), metadata !13, metadata i32* %a.addr, metadata !DIExpression()), !dbg !15; store i32 %a, i32* %a.addr, align 4, !DIAssignID !16; call void @llvm.dbg.assign(metadata i32 %a, metadata !14, metadata !DIExpression(), metadata !16, metadata i32* %a.addr, metadata !DIExpression()), !dbg !15; %0 = load i32, i32* %a.addr, align 4, !dbg !17; ret i32 %0, !dbg !18; }. ...; !13 = distinct !DIAssignID(); !14 = !DILocalVariable(name: ""a"", ...); ...; !16 = distinct !DIAssignID(); ```. The first `llvm.dbg.assign` refers to the `alloca` through `!DIAssignID !13`,; and the second refers to the `store` through `!DIAssignID !16`. ### Store-like instructions. In the absence of a linked `llvm.dbg.assign`, a store to an address that is; known to be the backing storage for a variable is considered to represent an; assignment to that variable. This gives us a safe fall-back in cases where `llvm.dbg.assign` intrinsics have; been deleted, the `DIAssignID` attachment on the store has been dropped, or the; optimiser has made a once-indirect store (not tracked with Assignment Tracking); direct. ### Middle-end: Considerations for pass-writers. #### Non-debug instruction updates. **Cloning** an instruction: nothing new to do. Cloning automatically clones a; `DIAssignID` attachment. Multip",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AssignmentTracking.md:4869,load,load,4869,interpreter/llvm-project/llvm/docs/AssignmentTracking.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AssignmentTracking.md,1,['load'],['load']
Performance,"e native ``__weak`` support to ensure; calling convention compatibility, this transfer is always handled; automatically by the compiler. .. admonition:: Rationale. In earlier releases, when non-trivial ownership was only permitted; on fields in Objective-C++, the ABI used for such classes was the; ordinary ABI for non-trivial C++ classes, which passes arguments and; returns indirectly and does not transfer responsibility for arguments.; When support for Objective-C structs was added, it was decided to; change to the current ABI for three reasons:. - It permits ARC / non-ARC compatibility for structs containing only; ``__strong`` references, as long as the non-ARC side is careful about; transferring ownership. - It avoids unnecessary indirection for sufficiently small types that; the C ABI would prefer to pass in registers. - Given that struct arguments must be produced at +1 to satisfy C's; semantics of initializing the local parameter variable, transferring; ownership of that copy to the callee is generally better for ARC; optimization, since otherwise there will be releases in the caller; that are much harder to pair with transfers in the callee. Breaking compatibility with existing Objective-C++ structures was; considered an acceptable cost, as most Objective-C++ code does not have; binary-compatibility requirements. Any existing code which cannot accept; this compatibility break, which is necessarily Objective-C++, should; force the use of the standard C++ ABI by declaring an empty (but; non-defaulted) destructor. .. _arc.ownership.inference:. Ownership inference; -------------------. .. _arc.ownership.inference.variables:. Objects; ^^^^^^^. If an object is declared with retainable object owner type, but without an; explicit ownership qualifier, its type is implicitly adjusted to have; ``__strong`` qualification. As a special case, if the object's base type is ``Class`` (possibly; protocol-qualified), the type is adjusted to have ``__unsafe_unretained``; qualifi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:64176,optimiz,optimization,64176,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"e object editor; should be the object class name concatenated with the word `Editor'`. (c) Provide a default constructor. (d) Use the signals/slots communication mechanism for event processing. (e) Implement the virtual method `SetModel(TObject *obj)` where all; widgets are set with the current object's attributes. This method is; called when the editor receives a signal from the canvas saying that an; object is the selected. (f) Implement all necessary slots and connect them to appropriate; signals that GUI widgets send out. The GUI classes in ROOT are developed; to emit signals whenever they change a state that others might be; interested. As we noted already, the signals/slots communication; mechanism allows total independence of the interacting classes. #### Creation and Destruction. GED-frames are constructed during traversal of class hierarchy of the; selected object, executed from method **`TGedEditor`**`::SetModel()`.; When a new object of a different class is selected, the unneeded; GED-frames are cached in memory for potential reuse. The frames are; deleted automatically when the editor is closed. Note: A deep cleanup is assumed for all frames put into the editor. This; implies:. - do not share the layout-hints among GUI components;. - do not delete child widgets in the destructor as this is done; automatically. #### Using Several Tabs. Sometimes you might need to use several tabs to organize properly your; class-editor. Each editor tab is a resource shared among all the; class-editors. Tabs must be created from the constructor of your; editor-class by using the method:. ``` {.cpp}; TGVerticalFrame* TGedFrame::CreateEditorTabSubFrame(const Text_t *name),; ```. It returns a pointer to a new tab container frame ready for use in your; class. If you need to hide/show this frame depending on the object's; status, you should store it in a data member. See for examples:; **`TH1Editor`**, **`TH2Editor`**. #### Base-Class Editors Control. Full control over base-cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:105677,cache,cached,105677,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['cache'],['cached']
Performance,"e of the most powerful features of LLVM is its library-first design mentality; and the way you can compose a wide variety of tools using different portions of; LLVM. Even in this situation using *BUILD_SHARED_LIBS* is not supported. If you; want to distribute LLVM as a shared library for use in a tool, the recommended; method is using *LLVM_BUILD_LLVM_DYLIB*, and you can use *LLVM_DYLIB_COMPONENTS*; to configure which LLVM components are part of libLLVM.; Note: *LLVM_BUILD_LLVM_DYLIB* is not available on Windows. Options for Optimizing LLVM; ===========================. There are four main build optimizations that our CMake build system supports.; When performing a bootstrap build it is not beneficial to do anything other than; setting *CMAKE_BUILD_TYPE* to ``Release`` for the stage-1 compiler. This is; because the more intensive optimizations are expensive to perform and the; stage-1 compiler is thrown away. All of the further options described should be; set on the stage-2 compiler either using a CMake cache file, or by prefixing the; option with *BOOTSTRAP_*. The first and simplest to use is the compiler optimization level by setting the; *CMAKE_BUILD_TYPE* option. The main values of interest are ``Release`` or; ``RelWithDebInfo``. By default the ``Release`` option uses the ``-O3``; optimization level, and ``RelWithDebInfo`` uses ``-O2``. If you want to generate; debug information and use ``-O3`` you can override the; *CMAKE_<LANG>_FLAGS_RELWITHDEBINFO* option for C and CXX.; DistributionExample.cmake does this. Another easy to use option is Link-Time-Optimization. You can set the; *LLVM_ENABLE_LTO* option on your stage-2 build to ``Thin`` or ``Full`` to enable; building LLVM with LTO. These options will significantly increase link time of; the binaries in the distribution, but it will create much faster binaries. This; option should not be used if your distribution includes static archives, as the; objects inside the archive will be LLVM bitcode, which is not po",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:7977,cache,cache,7977,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['cache'],['cache']
Performance,"e of; the core are loaded via the plugin manager plugin manager or; equivalent techniques, while the white ones are not. Of course, if one; wants to access a plugin library directly, it has to be explicitly; linked. An example of a plugin library is `libMinuit`. To create and; fill histograms you need to link `libHist.so`. If the code has a call; to fit the histogram, the ""fitter"" will dynamically load libMinuit if; it is not yet loaded. #### Plugins: Runtime Library Dependencies for Linking. plugin manager The Plugin Manager **`TPluginManager`** allows; postponing library dependencies to runtime: a plugin library will only; be loaded when it is needed. Non-plugins will need to be linked, and; are thus loaded at start-up. Plugins are defined by a base class (e.g.; **`TFile`**) that will be implemented in a plugin, a tag used to; identify the plugin (e.g. `^rfio:` as part of the protocol string),; the plugin class of which an object will be created; (e.g. **`TRFIOFile`**), the library to be loaded (in short; `libRFIO.so` to RFIO), and the constructor to be called (e.g.; ""`TRFIOFile()`""). This can be specified in the `.rootrc` which already; contains many plugin definitions, or by calls to; `gROOT->GetPluginManager()->AddHandler()`. #### Library AutoLoading. When using a class in Cling, e.g. in an interpreted source file, ROOT; will automatically load the library that defines this class. On; start-up, ROOT parses all files ending on `.rootmap` rootmap that are; in one of the `$LD_LIBRARY_PATH` (or `$DYLD_LIBRARY_PATH` for `MacOS`,; or `$PATH` for `Windows`). They contain class names and the library; names that the class depends on. After reading them, ROOT knows which; classes are available, and which libraries to load for them. When `TSystem::Load(""ALib"")` is called, ROOT uses this information to; determine which libraries `libALib.so` depends on. It will load these; libraries first. Otherwise, loading the requested library could cause; a system (dynamic loader) erro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:19481,load,loaded,19481,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,1,['load'],['loaded']
Performance,"e on below, we end up suggesting you use `or` with an all-ones mask,; making the x86 instruction sequence look like the following:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; movl (%rsi), %edi # Load potentially secret data from %rsi.; orl %eax, %edi; ```. Other useful patterns may be to fold the load into the `or` instruction itself; at the cost of a register-to-register copy. There are some challenges with deploying this approach:; 1. Many loads on x86 are folded into other instructions. Separating them would; add very significant and costly register pressure with prohibitive; performance cost.; 1. Loads may not target a general purpose register requiring extra instructions; to map the state value into the correct register class, and potentially more; expensive instructions to mask the value in some way.; 1. The flags registers on x86 are very likely to be live, and challenging to; preserve cheaply.; 1. There are many more values loaded than pointers & indices used for loads. As; a consequence, hardening the result of a load requires substantially more; instructions than hardening the address of the load (see below). Despite these challenges, hardening the result of the load critically allows; the load to proceed and thus has dramatically less impact on the total; speculative / out-of-order potential of the execution. There are also several; interesting techniques to try and mitigate these challenges and make hardening; the results of loads viable in at least some cases. However, we generally; expect to fall back when unprofitable from hardening the loaded value to the; next approach of hardening the address itself. ###### Loads folded into data-invariant operations can be hardened after the operation. The first key to making this feasible is to recognize that many operations on; x86 are ""data-invariant"". That is, they have no (known) observable behavior; differences due to the particular input data. These instructions ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:24367,load,loaded,24367,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],"['loaded', 'loads']"
Performance,"e once the parser has the; declaration context and determined the default attribute accordingly. This also requires the parser to reset the type of the declaration with the; newly created type with the right default attribute. Promotion expression; ====================. A new expression will be introduced to represent the conversion from a pointer; with an external bounds annotation, such as ``__counted_by``, to; ``__bidi_indexable``. This type of conversion cannot be handled by normal; CastExprs because it requires an extra subexpression(s) to provide the bounds; information necessary to create a wide pointer. Bounds check expression; =======================. Bounds checks are part of semantics defined in the ``-fbounds-safety`` language; model. Hence, exposing the bounds checks and other semantic actions in the AST; is desirable. A new expression for bounds checks has been added to the AST. The; bounds check expression has a ``BoundsCheckKind`` to indicate the kind of checks; and has the additional sub-expressions that are necessary to perform the check; according to the kind. Paired assignment check; =======================. ``-fbounds-safety`` enforces that variables or fields related with the same; external bounds annotation (e.g., ``buf`` and ``count`` related with; ``__counted_by`` in the example below) must be updated side by side within the; same basic block and without side effect in between. .. code-block:: c. typedef struct {; int *__counted_by(count) buf; size_t count;; } sized_buf_t;. void alloc_buf(sized_buf_t *sbuf, sized_t nelems) {; sbuf->buf = (int *)malloc(sizeof(int) * nelems);; sbuf->count = nelems;; }. To implement this rule, the compiler requires a linear representation of; statements to understand the ordering and the adjacency between the two or more; assignments. The Clang CFG is used to implement this analysis as Clang CFG; provides a linear view of statements within each ``CFGBlock`` (Clang; ``CFGBlock`` represents a single basic block i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst:4988,perform,perform,4988,interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafetyImplPlans.rst,1,['perform'],['perform']
Performance,"e operand.; def : InstAlias<""aad"", (AAD8i8 10)>;. // Fixed register operand.; def : InstAlias<""fcomi"", (COM_FIr ST1)>;. // Simple alias.; def : InstAlias<""fcomi $reg"", (COM_FIr RST:$reg)>;. Instruction aliases can also have a Requires clause to make them subtarget; specific. If the back-end supports it, the instruction printer can automatically emit the; alias rather than what's being aliased. It typically leads to better, more; readable code. If it's better to print out what's being aliased, then pass a '0'; as the third parameter to the InstAlias definition. Instruction Matching; --------------------. .. note::. To Be Written. .. _Implementations of the abstract target description interfaces:; .. _implement the target description:. Target-specific Implementation Notes; ====================================. This section of the document explains features or design decisions that are; specific to the code generator for a particular target. .. _tail call section:. Tail call optimization; ----------------------. Tail call optimization, callee reusing the stack of the caller, is currently; supported on x86/x86-64, PowerPC, AArch64, and WebAssembly. It is performed on; x86/x86-64, PowerPC, and AArch64 if:. * Caller and callee have the calling convention ``fastcc``, ``cc 10`` (GHC; calling convention), ``cc 11`` (HiPE calling convention), ``tailcc``, or; ``swifttailcc``. * The call is a tail call - in tail position (ret immediately follows call and; ret uses value of call or is void). * Option ``-tailcallopt`` is enabled or the calling convention is ``tailcc``. * Platform-specific constraints are met. x86/x86-64 constraints:. * No variable argument lists are used. * On x86-64 when generating GOT/PIC code only module-local calls (visibility =; hidden or protected) are supported. PowerPC constraints:. * No variable argument lists are used. * No byval parameters are used. * On ppc32/64 GOT/PIC only module-local calls (visibility = hidden or protected); are supported. WebAsse",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:86203,optimiz,optimization,86203,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"e operation (corresponding to the '``alignment``' operand of; '``llvm.masked.store``') is specified by the ``align`` parameter attribute (see; above). If it is not provided then the ABI alignment of the type of the; '``value``' operand as specified by the :ref:`datalayout; string<langref_datalayout>` is used instead. Examples:; """""""""""""""""". .. code-block:: text. call void @llvm.vp.store.v8i8.p0(<8 x i8> %val, ptr align 4 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, the call above is lane-wise equivalent to the call below. call void @llvm.masked.store.v8i8.p0(<8 x i8> %val, ptr %ptr, i32 4, <8 x i1> %mask). .. _int_experimental_vp_strided_load:. '``llvm.experimental.vp.strided.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.experimental.vp.strided.load.v4f32.i64(ptr %ptr, i64 %stride, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.experimental.vp.strided.load.nxv2i16.i64(ptr %ptr, i64 %stride, <vscale x 2 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, scalar values from; memory locations evenly spaced apart by '``stride``' number of bytes, starting from '``ptr``'. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the stride; value expressed in bytes. The third operand is a vector of boolean values; with the same number of elements as the return type. The fourth is the explicit; vector length of the operation. The base pointer underlying type matches the type of the scalar; elements of the return operand. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, multiple scalar; values from memory in the same way as the :ref:`llvm.vp.gather <int_vp_gather>` intrinsic,; where the vector of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:787366,load,load,787366,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"e operation (independent of module system). Thus, the API of each software library is only parsed once, reducing the *M x N* compilation problem to an *M + N* problem. * **Fragility**: Each module is parsed as a standalone entity, so it has a consistent preprocessor environment. This completely eliminates the need for ``__underscored`` names and similarly defensive tricks. Moreover, the current preprocessor definitions when an import declaration is encountered are ignored, so one software library can not affect how another software library is compiled, eliminating include-order dependencies. * **Tool confusion**: Modules describe the API of software libraries, and tools can reason about and present a module as a representation of that API. Because modules can only be built standalone, tools can rely on the module definition to ensure that they get the complete API for the library. Moreover, modules can specify which languages they work with, so, e.g., one can not accidentally attempt to load a C++ module into a C program. Problems modules do not solve; -----------------------------; Many programming languages have a module or package system, and because of the variety of features provided by these languages it is important to define what modules do *not* do. In particular, all of the following are considered out-of-scope for modules:. * **Rewrite the world's code**: It is not realistic to require applications or software libraries to make drastic or non-backward-compatible changes, nor is it feasible to completely eliminate headers. Modules must interoperate with existing software libraries and allow a gradual transition. * **Versioning**: Modules have no notion of version information. Programmers must still rely on the existing versioning mechanisms of the underlying language (if any exist) to version software libraries. * **Namespaces**: Unlike in some languages, modules do not imply any notion of namespaces. Thus, a struct declared in one module will still conflic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:6100,load,load,6100,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['load']
Performance,"e optimizations are:. - will not emit type definitions for types that are not needed by a; module and could be replaced with a forward declaration.; - will only emit type info for a dynamic C++ class in the module that; contains the vtable for the class.; - will only emit type info for a C++ class (non-trivial, non-aggregate); in the modules that contain a definition for one of its constructors.; - will only emit type definitions for types that are the subject of explicit; template instantiation declarations in the presence of an explicit; instantiation definition for the type. The **-fstandalone-debug** option turns off these optimizations.; This is useful when working with 3rd-party libraries that don't come; with debug information. Note that Clang will never emit type; information for types that are not referenced at all by the program. .. option:: -fno-standalone-debug. On Darwin **-fstandalone-debug** is enabled by default. The; **-fno-standalone-debug** option can be used to get to turn on the; vtable-based optimization described above. .. option:: -g. Generate complete debug info. .. option:: -feliminate-unused-debug-types. By default, Clang does not emit type information for types that are defined; but not used in a program. To retain the debug info for these unused types,; the negation **-fno-eliminate-unused-debug-types** can be used. Controlling Macro Debug Info Generation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Debug info for C preprocessor macros increases the size of debug information in; the binary. Macro debug info generated by Clang can be controlled by the flags; listed below. .. option:: -fdebug-macro. Generate debug info for preprocessor macros. This flag is discarded when; **-g0** is enabled. .. option:: -fno-debug-macro. Do not generate debug info for preprocessor macros (default). Controlling Debugger ""Tuning""; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. While Clang generally emits standard DWARF debug info (http://dwarfstd.org),; different debuggers may",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:126211,optimiz,optimization,126211,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"e optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_count``). Pragmas setting transformation options; imply the transformation is enabled, as if it was enabled via the corresponding; transformation pragma (e.g. ``vectorize(enable)``). If the transformation is; disabled (e.g. ``vectorize(disable)``), that takes precedence over; transformations option pragmas implying that transformation. Vectorization, Interleaving, and Predication; --------------------------------------------. A vectorized loop performs multiple iterations of the original loop; in parallel using vector instructions. The instruction set of the target; processor determines which vector instructions are available and their vector; widths. This restricts the types of loops that can be vectorized. The vectorizer; automatically determines if th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:162619,optimiz,optimization,162619,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"e plugins can be passed with; `-plugin-arg-<plugin-name>`. Note that those options must reach clang's cc1 process. There are two; ways to do so:. * Directly call the parsing process by using the `-cc1` option; this; has the downside of not configuring the default header search paths, so; you'll need to specify the full system path configuration on the command; line.; * Use clang as usual, but prefix all arguments to the cc1 process with; `-Xclang`. For example, to run the ``print-function-names`` plugin over a source file in; clang, first build the plugin, and then call clang with the plugin from the; source tree:. .. code-block:: console. $ export BD=/path/to/build/directory; $ (cd $BD && make PrintFunctionNames ); $ clang++ -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS \; -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D_GNU_SOURCE \; -I$BD/tools/clang/include -Itools/clang/include -I$BD/include -Iinclude \; tools/clang/tools/clang-check/ClangCheck.cpp -fsyntax-only \; -Xclang -load -Xclang $BD/lib/PrintFunctionNames.so -Xclang \; -plugin -Xclang print-fns. Also see the print-function-name plugin example's; `README <https://github.com/llvm/llvm-project/blob/main/clang/examples/PrintFunctionNames/README.txt>`_. Using the clang command line; ----------------------------. Using `-fplugin=plugin` on the clang command line passes the plugin; through as an argument to `-load` on the cc1 command line. If the plugin; class implements the ``getActionType`` method then the plugin is run; automatically. For example, to run the plugin automatically after the main AST; action (i.e. the same as using `-add-plugin`):. .. code-block:: c++. // Automatically run the plugin after the main AST action; PluginASTAction::ActionType getActionType() override {; return AddAfterMainAction;; }. Interaction with ``-clear-ast-before-backend``; ----------------------------------------------. To reduce peak memory usage of the compiler, plugins are recommended to run; *before* the main action, w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst:6434,load,load,6434,interpreter/llvm-project/clang/docs/ClangPlugins.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst,1,['load'],['load']
Performance,"e pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic stores. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic stores. The optional constant ``align`` argument specifies the alignment of the; operation (that is, the alignment of the memory address). It is the; responsibility of the code emitter to ensure that the alignment information is; correct. Overestimating the alignment results in undefined behavior.; Underestimating the alignment may produce less efficient code. An alignment of; 1 is always safe. The maximum possible alignment is ``1 << 32``. An alignment; value higher than the size of the loaded type implies memory up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for the target. The optional ``!nontemporal`` metadata must reference a single metadata; name ``<nontemp_node>`` corresponding to a metadata node with one ``i32`` entry; of value 1. The existence of the ``!nontemporal`` metadata on the instruction; tells the optimizer and code generator that this load is not expected to; be reused in the cache. The code generator may select special; instructions to save cache bandwidth, such as the ``MOVNT`` instruction on; x86. The optional ``!invariant.group`` metadata must reference a; single metadata name ``<empty_node>``. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:421442,load,loaded,421442,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],['loaded']
Performance,"e pointer for; multi-grid synchronization is; passed in the kernarg. ""ValueType"" string Unused and deprecated. This should no longer; be emitted, but is accepted for compatibility. ""PointeeAlign"" integer Alignment in bytes of pointee; type for pointer type kernel; argument. Must be a power; of 2. Only present if; ""ValueKind"" is; ""DynamicSharedPointer"".; ""AddrSpaceQual"" string Kernel argument address space; qualifier. Only present if; ""ValueKind"" is ""GlobalBuffer"" or; ""DynamicSharedPointer"". Values; are:. - ""Private""; - ""Global""; - ""Constant""; - ""Local""; - ""Generic""; - ""Region"". .. TODO::. Is GlobalBuffer only Global; or Constant? Is; DynamicSharedPointer always; Local? Can HCC allow Generic?; How can Private or Region; ever happen?. ""AccQual"" string Kernel argument access; qualifier. Only present if; ""ValueKind"" is ""Image"" or; ""Pipe"". Values; are:. - ""ReadOnly""; - ""WriteOnly""; - ""ReadWrite"". .. TODO::. Does this apply to; GlobalBuffer?. ""ActualAccQual"" string The actual memory accesses; performed by the kernel on the; kernel argument. Only present if; ""ValueKind"" is ""GlobalBuffer"",; ""Image"", or ""Pipe"". This may be; more restrictive than indicated; by ""AccQual"" to reflect what the; kernel actual does. If not; present then the runtime must; assume what is implied by; ""AccQual"" and ""IsConst"". Values; are:. - ""ReadOnly""; - ""WriteOnly""; - ""ReadWrite"". ""IsConst"" boolean Indicates if the kernel argument; is const qualified. Only present; if ""ValueKind"" is; ""GlobalBuffer"". ""IsRestrict"" boolean Indicates if the kernel argument; is restrict qualified. Only; present if ""ValueKind"" is; ""GlobalBuffer"". ""IsVolatile"" boolean Indicates if the kernel argument; is volatile qualified. Only; present if ""ValueKind"" is; ""GlobalBuffer"". ""IsPipe"" boolean Indicates if the kernel argument; is pipe qualified. Only present; if ""ValueKind"" is ""Pipe"". .. TODO::. Can GlobalBuffer be pipe; qualified?. ================= ============== ========= ================================. .. .. table:: AMDHSA",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:126368,perform,performed,126368,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"e pragma has an effect on functions only at the point of their definition; for; function templates, this means that the state of the pragma at the point of an; instantiation is not necessarily relevant. Consider the following example:. .. code-block:: c++. template<typename T> T twice(T t) {; return 2 * t;; }. #pragma clang optimize off; template<typename T> T thrice(T t) {; return 3 * t;; }. int container(int a, int b) {; return twice(a) + thrice(b);; }; #pragma clang optimize on. In this example, the definition of the template function ``twice`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``container`` function is also in the region and will not be optimized, but; it causes the instantiation of ``twice`` and ``thrice`` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161065,optimiz,optimize,161065,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['optimiz'],"['optimization-list', 'optimize']"
Performance,"e profile.; This option can only be used with sample-based profile in extbinary format. .. option:: --gen-partial-profile=[true|false]. Mark the profile to be a partial profile which only provides partial profile; coverage for the optimized target. This option can only be used with; sample-based profile in extbinary format. .. option:: --convert-sample-profile-layout=[nest|flat]. Convert the merged profile into a profile with a new layout. Supported; layout are ``nest`` (Nested profile, the input should be CS flat profile) and; ``flat`` (Profile with nested inlinees flattened out). .. option:: --supplement-instr-with-sample=<file>. Supplement an instrumentation profile with sample profile. The sample profile; is the input of the flag. Output will be in instrumentation format (only works; with -instr). .. option:: --zero-counter-threshold=<float>. For the function which is cold in instr profile but hot in sample profile, if; the ratio of the number of zero counters divided by the total number of; counters is above the threshold, the profile of the function will be regarded; as being harmful for performance and will be dropped. .. option:: --instr-prof-cold-threshold=<int>. User specified cold threshold for instr profile which will override the cold; threshold got from profile summary. .. option:: --suppl-min-size-threshold=<int>. If the size of a function is smaller than the threshold, assume it can be; inlined by PGO early inliner and it will not be adjusted based on sample; profile. .. option:: --debug-info=<path>. Specify the executable or ``.dSYM`` that contains debug info for the raw profile.; When ``--debug-info-correlate`` or ``--profile-correlate=debug-info`` was used ; for instrumentation, use this option to correlate the raw profile. .. option:: --binary-file=<path>. Specify the executable that contains profile data and profile name sections for; the raw profile. When ``-profile-correlate=binary`` was used for; instrumentation, use this option to correlate t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst:6034,perform,performance,6034,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-profdata.rst,1,['perform'],['performance']
Performance,"e range of offsets from the pointer parameter to bytes; which can be accessed by the function. This range does not include accesses by; function calls from ``calls`` list. where each ``Callee`` describes how parameter is forwarded into other; functions and looks like:. .. code-block:: text. callee: ^3, param: 5, offset: [-3, 3]. The ``callee`` refers to the summary entry id of the callee, ``param`` is; the number of the callee parameter which points into the callers parameter; with offset known to be inside of the ``offset`` range. ``calls`` will be; consumed and removed by thin link stage to update ``Param::offset`` so it; covers all accesses possible by ``calls``. Pointer parameter without corresponding ``Param`` is considered unsafe and we; assume that access with any offset is possible. Example:. If we have the following function:. .. code-block:: text. define i64 @foo(ptr %0, ptr %1, ptr %2, i8 %3) {; store ptr %1, ptr @x; %5 = getelementptr inbounds i8, ptr %2, i64 5; %6 = load i8, ptr %5; %7 = getelementptr inbounds i8, ptr %2, i8 %3; tail call void @bar(i8 %3, ptr %7); %8 = load i64, ptr %0; ret i64 %8; }. We can expect the record like this:. .. code-block:: text. params: ((param: 0, offset: [0, 7]),(param: 2, offset: [5, 5], calls: ((callee: ^3, param: 1, offset: [-128, 127])))). The function may access just 8 bytes of the parameter %0 . ``calls`` is empty,; so the parameter is either not used for function calls or ``offset`` already; covers all accesses from nested function calls.; Parameter %1 escapes, so access is unknown.; The function itself can access just a single byte of the parameter %2. Additional; access is possible inside of the ``@bar`` or ``^3``. The function adds signed; offset to the pointer and passes the result as the argument %1 into ``^3``.; This record itself does not tell us how ``^3`` will access the parameter.; Parameter %3 is not a pointer. .. _refs_summary:. Refs; ^^^^. The optional ``Refs`` field looks like:. .. code-block:: text.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:345531,load,load,345531,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],['load']
Performance,"e reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry points ``Foo`` and ``Bar`` in JITDylib; ``JD2`` by calling:. .. code-block:: c++. auto ReexportFlags = JITSymbolFlags::Exported | JITSymbolFlags::Callable;; JD2.define(; lazyReexports(CallThroughMgr, StubsMgr, JD,; SymbolAliasMap({; { Mangle(""foo""), { Mangle(""foo_body""), ReexportedFlags } },; { Mangle(""bar""), { Mangle(""bar_body""), ReexportedFlags } }; }));. A full example of how to use lazyReexports with the LLJIT class can be found at; ``llvm/examples/OrcV2Examples/LLJITWithLazyReexports``. Supporting Custom Compilers; ===========================. TBD. .. _transitioning_orcv1_to_orcv2:. Transitioning from ORCv1 to ORCv2; =================================. Since LLVM 7.0, new ORC development work has focused on adding support for; concurrent JIT compilation. The new APIs (including new layer interfaces and; implementations, and new utilities) that support concurrency are collectively; referred to as ORCv2, and the original, non-concurrent layers and utilities; are now referred to as ORCv1. The majority of the ORCv1 layers and utilities were renamed with a 'Legacy'; prefix in LLVM 8.0, and have deprecation warnings attached in LLVM 9.0. In LLVM; 12.0 ORCv1 will be removed entirely. Transitioning from ORCv1 to ORCv2 should be easy for most clients. Most of the; ORCv1 layers and utilities have ORCv2 counterparts [2]_ that can be directly; substituted. However there are some design differences between ORCv1 and ORCv2; to be aware of:. 1. ORCv2 fully adopts the JIT-as-linker model that began with MCJIT. Modules; (and other program representations, e.g. Object Files) are no longer added; directly to JIT classes or layers. Instead, they are added to ``JITDylib``; instances *by* layers. The ``JITDylib`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:19240,concurren,concurrent,19240,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['concurren'],['concurrent']
Performance,"e region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_count``). Pragmas setting transformation options; imply the transformation is enabled, as if it was enabled via the corre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161909,optimiz,optimization,161909,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"e register content is the same as it would have been on a little endian system. It may seem that ``LD1`` should suffice to perform vector loads on a big endian machine. However there are pros and cons to the two approaches that make it less than simple which register format to pick. There are two options:. 1. The content of a vector register is the same *as if* it had been loaded with an ``LDR`` instruction.; 2. The content of a vector register is the same *as if* it had been loaded with an ``LD1`` instruction. Because ``LD1 == LDR + REV`` and similarly ``LDR == LD1 + REV`` (on a big endian system), we can simulate either type of load with the other type of load plus a ``REV`` instruction. So we're not deciding which instructions to use, but which format to use (which will then influence which instruction is best to use). .. The 'clearer' container is required to make the following section header come after the floated; images above.; .. container:: clearer. Note that throughout this section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and vectors to have identical memory layouts - ``[4 x i8]`` and ``<4 x i8>`` should be represented the same in memory. Without this property there would be many special cases that the optimizer would have to cleverly handle. Use of ``LDR`` would break this lane ordering property. This doesn't preclude the use of ``LDR``, but we would have to do one of two things:. 1. Insert a ``REV`` instruction to reverse the lane order after every ``LDR``.; 2. Disable all optimizations that rely on lane layout, and for every access to an individual l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:4461,load,loads,4461,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loads']
Performance,"e result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is often more useful to; refer to optimization remarks post-compilation, typically during performance; analysis. For that, LLVM can serialize the remarks produced for each compilation unit to; a file that can be consumed later. By default, the format of the serialized remarks is :ref:`YAML; <yamlremarks>`, and it can be accompanied by a :ref:`section <remarkssection>`; in the object files to easily retrieve it. :doc:`llc <CommandGuide/llc>` and :doc:`opt <CommandGuide/opt>` support the; following options:. ``Basic options``. .. option:: -pass-remarks-output=<filename>. Enables the serialization of remarks to a file specified in <filename>. By default, the output is serialized to :ref:`YAML <yamlremarks>`. .. option:: -pass-remarks-format=<format>. Specifies the output format of the serialized remarks. Supported formats:. * :ref:`yaml <yamlrem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:1805,optimiz,optimization,1805,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,"e returned value. Overview:; """""""""""""""""""". The ``llvm.ssa.copy`` intrinsic can be used to attach information to; operations by copying them and giving them new names. For example,; the PredicateInfo utility uses it to build Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:937707,load,load,937707,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"e runtime library. |; +---------+------------+-----------------------------------------------------------------------------+; | int32_t | reserved | Reserved, to be used by the runtime library. |; +---------+------------+-----------------------------------------------------------------------------+; | char* | psource | Program source information, stored as "";filename;function;line;column;;\\0"" |; +---------+------------+-----------------------------------------------------------------------------+. If debugging information is enabled, we will also create strings to indicate the; names and declarations of variables mapped in target regions. These have the; same format as the source location in the :ref:`identifier structure; <table-ident_t_structure>`, but the function name is replaced with the variable; name. .. _Device Compilation:. Offload Device Compilation; --------------------------. The input file is compiled for each active device toolchain. The device; compilation stage is performed differently from the host stage. Namely, we do; not generate any offloading entries. This is set by passing the; ``-fopenmp-is-target-device`` flag to the front-end. We use the host bitcode to; determine which symbols to export from the device. The bitcode file is passed in; from the previous stage using the ``-fopenmp-host-ir-file-path`` flag.; Compilation is otherwise performed as it would be for any other target triple. When compiling for the OpenMP device, we set the visibility of all device; symbols to be ``protected`` by default. This improves performance and prevents a; class of errors where a symbol in the target device could preempt a host; library. The OpenMP runtime library is linked in during compilation to provide the; implementations for standard OpenMP functionality. For GPU targets this is done; by linking in a special bitcode library during compilation, (e.g.; ``libomptarget-nvptx64-sm_70.bc``) using the ``-mlink-builtin-bitcode`` flag.; Other device libraries, s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:10274,perform,performed,10274,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['perform'],['performed']
Performance,"e same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L0 and L1 caches at the start of each kernel dispatch.; * On dGPU the kernarg backing memory is accessed as MTYPE UC (uncached) to avoid; needing to invalidate the L2 cache.; * On APU the kernarg backing memory is accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC (non-coherent). Since the private address space is only accessed; by a single thread, and is always write-before-read, there is never a need to; invalidate these entries from the L0 or L1 caches. Wavefronts are executed in native mode with in-order reporting of loads and; sample instructions. In this mode vmcnt reports completion of load, atomic with; return and sample instructions in order, and the vscnt reports the completion of; store and atomic without return in order. See ``MEM_ORDERED`` field in; :ref:`amdgpu-amdhsa-compute_pgm_rsrc1-gfx6-gfx12-table`. Wavefronts can be executed in WGP or CU wavefront execution mode:. * In WGP wavefront execution mode the wavefronts of a work-group are executed; on the SIMDs of both CUs of the WGP. Therefore, explicit management of the per; CU L0 caches is required for work-group synchronization. Also accesses to L1; at work-group scope need to be explicitly ordered as the accesses from; different CUs are not ordered.; * In CU wavefront execution mode the wavefronts of a work-group are executed on; the SIMDs of a single CU of the WGP. Therefore, all",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:341810,cache,caches,341810,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"e set in one of the above; configuration files. `$VafConf_LocalPodLocation`; : Full path to the PoD installation on the client. > The `$VafConf_LocalPodLocation` variable must be set before the; > `PoD_env.sh` script gets sourced, so set it either in; > `common.before`, `local.before` or `local.conf`. Since PoD is; > usually system-wide installed, its location is normally; > system-wide set in either the `local.conf` file by the system; > administrator. `$VafConf_RemotePodLocation`; : Full path to the PoD installation on the VAF master node. *Note: this variable should be set in the configuration files for; the local environment despite it refers to a software present on the; remote nodes.*. `$VafConf_PodRms` *(optional)*; : Name of the Resource Management System used for submitting PoD jobs.; Run `pod-submit -l` to see the possible values. If not set, defaults to `condor`. `$VafConf_PodQueue` *(optional)*; : Queue name where to submit PoD jobs. If no queue has been given, the default one configured on your RMS; will be used. ### Remote environment configuration. All the PoD commands sent to the VAF master will live in the environment; loaded via using the following scripts. Similarly to the local environment, configuration is split in different files; to allow for a system-wide configuration, which has precedence over; user's configuration in the home directory. If a script cannot be found,; it will be silently skipped. - `<output_of_payload>`. - `common.before`. - `remote.before`. - `remote.conf`. - `common.after`. - `remote.after`. For an explanation on how to pass extra data to the workers safely; through the payload, see below. ### Payload: sending local files to the remote nodes. In many cases it is necessary to send some local data to the remote; workers: it is very common, for instance, to distribute a local Grid; authentication proxy on the remote workers to let them authenticate to; access a data storage. The `payload` file must be an executable generating",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/UsingVirtualAnalysisFacility.md:4331,queue,queue,4331,proof/doc/confman/UsingVirtualAnalysisFacility.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/UsingVirtualAnalysisFacility.md,1,['queue'],['queue']
Performance,"e short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1775,optimiz,optimization,1775,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,1,['optimiz'],['optimization']
Performance,"e show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows why it is such a popular; target for language designers and others who need high performance code; generation.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:4679,perform,performance,4679,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,1,['perform'],['performance']
Performance,"e sources are checked out from the CVS code repository,. 1. change to the directory:. $ cd MathLibs/Minuit. 2. run autogen:. $ ./autogen. 3. create a new directory:. $ cd ..; $ mkdir Minuit-BUILD; $ cd Minuit-BUILD/. 4. run configure:. $ ../Minuit/configure. 5. create the tar.gz:. $ make dist. This will create a Minuit-x.x.x.tar.gz which can be distributed and used; as described above. ## M versions ##. The version numbers of M follow the release numbers of the SEAL project; @bib-SEAL at CERN @bib-CERN. ### From to ###. The program is entirely written in standard portable . M does not depend; on any external library. In its minimal usage the user must only provide; an implementation of the FCNBase class to M and parameters and; uncertainties in form of std::vector containers. ### Memory allocation and thread safety ###. Differently to the version of M , the version has its own memory manager; (StackAllocator. The user can select between the standard dynamic memory; allocation and deallocation (default) and performance-optimized; stacklike allocation (optional). However, the library is not thread; save using stackallocation. ### M parameters ###. Differently to the version of M there is no limit on the number of; parameters, variable or non-variable. Memory allocation is done; dynamically according to the actual needs and ""on demand"". There is no; protection against an upper limit on the number of parameters, however; the ""technological"" limitations of M can be seen around a maximum of 15; free parameters at a time. ## Interference with other packages ##. The new M has been designed to interfere as little as possible with; other programs or packages which may be loaded at the same time. M is; thread safe by default. Optionally the user can select a different way; of dynamically allocating memory in the class StackAllacator for M , in; which case (and after an entire recompilation of the whole library) the; thread safety is lost. ## Floating-point precision ##. [ins",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:25505,perform,performance-optimized,25505,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['perform'],['performance-optimized']
Performance,"e split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0). -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:220740,perform,performing,220740,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"e stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes.; **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:257669,perform,performing,257669,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"e statistics; gathered, use the '``-stats``' option:. .. code-block:: none. $ opt -stats -mypassname < program.bc > /dev/null; ... statistics output ... Note that in order to use the '``-stats``' option, LLVM must be; compiled with assertions enabled. When running ``opt`` on a C file from the SPEC benchmark suite, it gives a; report that looks like this:. .. code-block:: none. 7646 bitcodewriter - Number of normal instructions; 725 bitcodewriter - Number of oversized instructions; 129996 bitcodewriter - Number of bitcode bytes written; 2817 raise - Number of insts DCEd or constprop'd; 3213 raise - Number of cast-of-self removed; 5046 raise - Number of expression trees converted; 75 raise - Number of other getelementptr's formed; 138 raise - Number of load/store peepholes; 42 deadtypeelim - Number of unused typenames removed from symtab; 392 funcresolve - Number of varargs functions resolved; 27 globaldce - Number of global variables removed; 2 adce - Number of basic blocks removed; 134 cee - Number of branches revectored; 49 cee - Number of setcc instruction eliminated; 532 gcse - Number of loads removed; 2919 gcse - Number of instructions removed; 86 indvars - Number of canonical indvars added; 87 indvars - Number of aux indvars removed; 25 instcombine - Number of dead inst eliminate; 434 instcombine - Number of insts combined; 248 licm - Number of load insts hoisted; 1298 licm - Number of insts hoisted to a loop pre-header; 3 licm - Number of insts hoisted to multiple loop preds (bad, no loop pre-header); 75 mem2reg - Number of alloca's promoted; 1444 cfgsimplify - Number of blocks simplified. Obviously, with so many optimizations, having a unified framework for this stuff; is very nice. Making your pass fit well into the framework makes it more; maintainable and useful. .. _DebugCounters:. Adding debug counters to aid in debugging your code; ---------------------------------------------------. Sometimes, when writing new passes, or trying to track down bugs, it; i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:48615,load,load,48615,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,3,['load'],"['load', 'loads']"
Performance,"e system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; ea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2245,scalab,scalable,2245,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,1,['scalab'],['scalable']
Performance,"e systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1407,optimiz,optimization,1407,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,3,"['optimiz', 'tune']","['optimization', 'tunes']"
Performance,"e than 5 we take the ones that are executed the most. We; verify our assumption that we picked a hot back-edge in first-level; instrumentation, by making sure that the number of times we took an; exit edge from the hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instruction",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3378,queue,queue,3378,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['queue'],['queue']
Performance,"e that the minimizer sees, thus setting it to zero for the minimizer.; Since this does not impact the derivative terms, it does not affect the fitting result, except for added numerical precision. ## Calculators; `RooFit::TestStatistics` provides two abstract base classes for likelihood calculation: `LikelihoodWrapper` and `LikelihoodGradientWrapper`.; These are used by the `RooAbsMinimizerFcn` implementation `MinuitFcnGrad` which expects them to, respectively, provide likelihood and likelihood gradient values for use by `Minuit2` in fitting the pdf to the dataset. The `Wrapper`s can be implemented for different kinds of algorithms, or with different kinds of optimization ""back-ends"" in mind.; One implementation of each is ready for use in `RooFit` currently:. 1. `LikelihoodSerial` is more or less simply a rewrite of the existing serial calculation of a `RooNLLVar`.; 2. `LikelihoodGradientJob` calculates the partial derivatives or the gradient in parallel on multiple CPUs/cores, based on `RooFit::MultiProcess`, which is a fork-based multi-processing task execution framework with dynamic load balancing. Other possible implementations could use the GPU or external tools like TensorFlow. The coupling of all these classes to `RooMinimizer` is made via the `MinuitFcnGrad` class, which owns the `Wrappers` that calculate the likelihood components. Note: a second `LikelihoodWrapper` class called `LikelihoodJob` is also available.; This class emulates the existing `NumCPU(>1)` functionality of the `RooAbsTestStatistic` tree, which is implemented based on `RooRealMPFE`.; This class is not yet thoroughly tested and should not be considered production ready. ### Usage example: `MultiProcess` enabled parallel gradient calculator. The main selling point of using `RooFit::TestStatistics` from a performance point of view is the implementation of the `RooFit::MultiProcess` based `LikelihoodGradientJob` calculator class.; To use it, one should create a `RooMinimizer` using the new con",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:7418,load,load,7418,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,1,['load'],['load']
Performance,"e the default policy is sufficient in most cases, it may break down when; ``T`` does not provide a default constructor. Also, in the case of many; instances of ``ilist``\ s, the memory overhead of the associated sentinels is; wasted. To alleviate the situation with numerous and voluminous; ``T``-sentinels, sometimes a trick is employed, leading to *ghostly sentinels*. Ghostly sentinels are obtained by specially-crafted ``ilist_traits<T>`` which; superpose the sentinel with the ``ilist`` instance in memory. Pointer; arithmetic is used to obtain the sentinel, which is relative to the ``ilist``'s; ``this`` pointer. The ``ilist`` is augmented by an extra pointer, which serves; as the back-link of the sentinel. This is the only field in the ghostly; sentinel which can be legally accessed. .. _dss_other:. Other Sequential Container options; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Other STL containers are available, such as ``std::string``. There are also various STL adapter classes such as ``std::queue``,; ``std::priority_queue``, ``std::stack``, etc. These provide simplified access; to an underlying container but don't affect the cost of the container itself. .. _ds_string:. String-like containers; ----------------------. There are a variety of ways to pass around and use strings in C and C++, and; LLVM adds a few new options to choose from. Pick the first option on this list; that will do what you need, they are ordered according to their relative cost. Note that it is generally preferred to *not* pass strings around as ``const; char*``'s. These have a number of problems, including the fact that they; cannot represent embedded nul (""\0"") characters, and do not have a length; available efficiently. The general replacement for '``const char*``' is; StringRef. For more information on choosing string containers for APIs, please see; :ref:`Passing Strings <string_apis>`. .. _dss_stringref:. llvm/ADT/StringRef.h; ^^^^^^^^^^^^^^^^^^^^. The StringRef class is a simple value class t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:70739,queue,queue,70739,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['queue'],['queue']
Performance,"e the option `-emit-pch`:. .. code-block:: bash. $ clang -cc1 test.h -emit-pch -o test.h.pch. This option is transparently used by ``clang`` when generating PCH files. The; resulting PCH file contains the serialized form of the compiler's internal; representation after it has completed parsing and semantic analysis. The PCH; file can then be used as a prefix header with the `-include-pch`; option:. .. code-block:: bash. $ clang -cc1 -include-pch test.h.pch test.c -o test.s. Design Philosophy; -----------------. Precompiled headers are meant to improve overall compile times for projects, so; the design of precompiled headers is entirely driven by performance concerns.; The use case for precompiled headers is relatively simple: when there is a; common set of headers that is included in nearly every source file in the; project, we *precompile* that bundle of headers into a single precompiled; header (PCH file). Then, when compiling the source files in the project, we; load the PCH file first (as a prefix header), which acts as a stand-in for that; bundle of headers. A precompiled header implementation improves performance when:. * Loading the PCH file is significantly faster than re-parsing the bundle of; headers stored within the PCH file. Thus, a precompiled header design; attempts to minimize the cost of reading the PCH file. Ideally, this cost; should not vary with the size of the precompiled header file. * The cost of generating the PCH file initially is not so large that it; counters the per-source-file performance improvement due to eliminating the; need to parse the bundled headers in the first place. This is particularly; important on multi-core systems, because PCH file generation serializes the; build when all compilations require the PCH file to be up-to-date. Modules, as implemented in Clang, use the same mechanisms as precompiled; headers to save a serialized AST file (one per module) and use those AST; modules. From an implementation standpoint, modules ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:1597,load,load,1597,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['load']
Performance,"e the requested PC sections. While relying on LLVM IR metadata to request PC sections makes the above; guarantee relatively trivial, propagation of metadata through the optimization; and code generation pipeline has the following guarantees. Metadata Propagation; --------------------. In general, LLVM *does not make any guarantees* about preserving IR metadata; (attached to an ``Instruction``) through IR transformations. When using PC; sections metadata, this guarantee is unchanged, and ``!pcsections`` metadata is; remains *optional* until lowering to machine IR (MIR). Note for Code Generation; ------------------------. As with other LLVM IR metadata, there are no requirements for LLVM IR; transformation passes to preserve ``!pcsections`` metadata, with the following; exceptions:. * The ``AtomicExpandPass`` shall preserve ``!pcsections`` metadata; according to the below rules 1-4. When translating LLVM IR to MIR, the ``!pcsections`` metadata shall be copied; from the source ``Instruction`` to the target ``MachineInstr`` (set with; ``MachineInstr::setPCSections()``). The instruction selectors and MIR; optimization passes shall preserve PC sections metadata as follows:. 1. Replacements will preserve PC sections metadata of the replaced; instruction. 2. Duplications will preserve PC sections metadata of the copied; instruction. 3. Merging will preserve PC sections metadata of one of the two; instructions (no guarantee on which instruction's metadata is used). 4. Deletions will loose PC sections metadata. This is similar to debug info, and the ``BuildMI()`` helper provides a; convenient way to propagate debug info and ``!pcsections`` metadata in the; ``MIMetadata`` bundle. Note for Metadata Users; -----------------------. Use cases for ``!pcsections`` metadata should either be fully tolerant to; missing metadata, or the passes inserting ``!pcsections`` metadata should run; *after* all LLVM IR optimization passes to preserve the metadata until being; translated to MIR.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst:3873,optimiz,optimization,3873,interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PCSectionsMetadata.rst,2,['optimiz'],['optimization']
Performance,"e thread. It is possible for the compiler to use both SIMT concurrency and iteration; concurrency techniques in the code of a single source language thread. Therefore, a DWARF operation is required to denote the current concurrent; iteration instance, much like ``DW_OP_push_object_address`` denotes the current; object. See ``DW_OP_LLVM_push_iteration`` in; :ref:`amdgpu-dwarf-literal-operations`. In addition, a way is needed for the compiler to communicate how many source; language loop iterations are executing concurrently. See; ``DW_AT_LLVM_iterations`` in :ref:`amdgpu-dwarf-low-level-information`. 2.20 DWARF Operation to Create Runtime Overlay Composite Location Description; -----------------------------------------------------------------------------. It is common in SIMD vectorization for the compiler to generate code that; promotes portions of an array into vector registers. For example, if the; hardware has vector registers with 8 elements, and 8 wide SIMD instructions, the; compiler may vectorize a loop so that is executes 8 iterations concurrently for; each vectorized loop iteration. On the first iteration of the generated vectorized loop, iterations 0 to 7 of; the source language loop will be executed using SIMD instructions. Then on the; next iteration of the generated vectorized loop, iteration 8 to 15 will be; executed, and so on. If the source language loop accesses an array element based on the loop; iteration index, the compiler may read the element into a register for the; duration of that iteration. Next iteration it will read the next element into; the register, and so on. With SIMD, this generalizes to the compiler reading; array elements 0 to 7 into a vector register on the first vectorized loop; iteration, then array elements 8 to 15 on the next iteration, and so on. The DWARF location description for the array needs to express that all elements; are in memory, except the slice that has been promoted to the vector register.; The starting positio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:35233,concurren,concurrently,35233,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['concurren'],['concurrently']
Performance,"e to a decision tree.; Variable importance ranking by counting the number of cuts made in each; dimension. The variable, for which the most cuts were done is ranked highest. Fixed the size of the sampling box in PDEFoam:; In TMVA 4.1.0 the size of the PDEFoam sampling box in each dimension was; 2*VolFrac times the foam size. This was contrary to the intention and the; documentation in the UserGuide and is now corrected: In TMVA 4.1.1 the size; of the PDEFoam sampling box in each dimension is now VolFrac times the foam; size. This implies that in TMVA 4.1.1 the VolFrac value for training a PDEFoam; must be doubled in order to give the same results as in TMVA 4.1.0. The default; VolFrac value was also changed from 0.0333 to 0.0666.; New configuration variable ""NbinsMVAoutput"" defining the bins of the MVA output; variables in the TMVA training plots produced via the GUI. As always, Config; settings can be modified in the training script via, eg, the command. (TMVA::gConfig().GetVariablePlotting()).fNbinsMVAoutput = 50;. to be called AFTER initialising the TMVA Factory object. Bug fixes. Requested number of training and testing events was not; correct when pre-selection cuts were applied. Now the number of; requested events scales with the preselection efficiency and hence; does not need to be adjusted with the pre-selection. This also; corrects the problems seen in the Category classifierm, where; pre-selection is used to build the categories.; Correct histogram boundaries in PlotVariable.; Correct scanning procedure in OptimizeTuningParameters.; Print the significance formula that is actually used; Small speed improvement for PDEFoam functions.; Fix for MethodBoost which ensures that the method options for the boosted; classifier are handled correctly during boosting.; Fixed problems in classification of some methods when booking background; training tree before signal one.; Fixed preprocessing transformation bug in HMatrix; Several minor bug fixes for version 4.1.2. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v530/index.html:2097,Optimiz,OptimizeTuningParameters,2097,tmva/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v530/index.html,1,['Optimiz'],['OptimizeTuningParameters']
Performance,"e type from scalable type; declare <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> %vec, i64 <idx>). ; Extract fixed type from fixed type; declare <2 x double> @llvm.vector.extract.v2f64.v4f64(<4 x double> %vec, i64 <idx>). Overview:; """""""""""""""""". The '``llvm.vector.extract.*``' intrinsics extract a vector from within another; vector starting from a given index. The return type must be explicitly; specified. Conceptually, this can be used to decompose a scalable vector into; non-scalable parts, however this intrinsic can also be used on purely fixed; types. Scalable vectors can only be extracted from other scalable vectors. Arguments:; """""""""""""""""""". The ``vec`` is the vector from which we will extract a subvector. The ``idx`` specifies the starting element number within ``vec`` from which a; subvector is extracted. ``idx`` must be a constant multiple of the known-minimum; vector length of the result type. If the result type is a scalable vector,; ``idx`` is first scaled by the result type's runtime scaling factor. Elements; ``idx`` through (``idx`` + num_elements(result_type) - 1) must be valid vector; indices. If this condition cannot be determined statically but is false at; runtime, then the result vector is a :ref:`poison value <poisonvalues>`. The; ``idx`` parameter must be a vector index constant type (for most targets this; will be an integer pointer type). '``llvm.experimental.vector.reverse``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <2 x i8> @llvm.experimental.vector.reverse.v2i8(<2 x i8> %a); declare <vscale x 4 x i32> @llvm.experimental.vector.reverse.nxv4i32(<vscale x 4 x i32> %a). Overview:; """""""""""""""""". The '``llvm.experimental.vector.reverse.*``' intrinsics reverse a vector.; The intrinsic takes a single vector and returns a vector of matching type but; with the original lane order reversed. These intrinsics work for both fixed; and scalable vec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:665481,scalab,scalable,665481,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"e typically based on profile data. .. _md_invariant.group:. '``invariant.group``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The experimental ``invariant.group`` metadata may be attached to; ``load``/``store`` instructions referencing a single metadata with no entries.; The existence of the ``invariant.group`` metadata on the instruction tells; the optimizer that every ``load`` and ``store`` to the same pointer operand; can be assumed to load or store the same; value (but see the ``llvm.launder.invariant.group`` intrinsic which affects; when two pointers are considered the same). Pointers returned by bitcast or; getelementptr with only zero indices are considered the same. Examples:. .. code-block:: llvm. @unknownPtr = external global i8; ...; %ptr = alloca i8; store i8 42, ptr %ptr, !invariant.group !0; call void @foo(ptr %ptr). %a = load i8, ptr %ptr, !invariant.group !0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.group !0 ; Can assume that %unknownValue == 42. call void @foo(ptr %ptr); %newPtr2 = call ptr @llvm.launder.invariant.group.p0(ptr %ptr); %d = load i8, ptr %newPtr2, !invariant.group !0 ; Can't step through launder.invariant.group to get value of %ptr. ...; declare void @foo(ptr); declare ptr @getPointer(ptr); declare ptr @llvm.launder.invariant.group.p0(ptr). !0 = !{}. The invariant.group metadata must be dropped when replacing one pointer by; another based on aliasing information. This is because invariant.group is tied; to the SSA value of the pointer operand. .. code-block:: llvm. %v = load i8, ptr %x, !invariant.group !0; ; if %x mustalias %y then we can replace the above instruction with; %v = load i8, ptr %y. Note that this is an experimental feature, which means that its sema",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:316793,load,load,316793,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"e used to avoid scanning the directory; too often. It does not impact the decision of which files to prune. A; value of 0 forces the scan to occur. The default is every 20 minutes. Clang Bootstrap; ---------------. To `bootstrap clang/LLVM <https://llvm.org/docs/AdvancedBuilds.html#bootstrap-builds>`_; with ThinLTO, follow these steps:. 1. The host compiler_ must be a version of clang that supports ThinLTO.; #. The host linker_ must support ThinLTO (and in the case of gold, must be; `configured with plugins enabled <https://llvm.org/docs/GoldPlugin.html>`_).; #. Use the following additional `CMake variables; <https://llvm.org/docs/CMake.html#options-and-variables>`_; when configuring the bootstrap compiler build:. * ``-DLLVM_ENABLE_LTO=Thin``; * ``-DCMAKE_C_COMPILER=/path/to/host/clang``; * ``-DCMAKE_CXX_COMPILER=/path/to/host/clang++``; * ``-DCMAKE_RANLIB=/path/to/host/llvm-ranlib``; * ``-DCMAKE_AR=/path/to/host/llvm-ar``. Or, on Windows:. * ``-DLLVM_ENABLE_LTO=Thin``; * ``-DCMAKE_C_COMPILER=/path/to/host/clang-cl.exe``; * ``-DCMAKE_CXX_COMPILER=/path/to/host/clang-cl.exe``; * ``-DCMAKE_LINKER=/path/to/host/lld-link.exe``; * ``-DCMAKE_RANLIB=/path/to/host/llvm-ranlib.exe``; * ``-DCMAKE_AR=/path/to/host/llvm-ar.exe``. #. To use additional linker arguments for controlling the backend; parallelism_ or enabling incremental_ builds of the bootstrap compiler,; after configuring the build, modify the resulting CMakeCache.txt file in the; build directory. Specify any additional linker options after; ``CMAKE_EXE_LINKER_FLAGS:STRING=``. Note the configure may fail if; linker plugin options are instead specified directly in the previous step. The ``BOOTSTRAP_LLVM_ENABLE_LTO=Thin`` will enable ThinLTO for stage 2 and; stage 3 in case the compiler used for stage 1 does not support the ThinLTO; option. More Information; ================. * From LLVM project blog:; `ThinLTO: Scalable and Incremental LTO; <http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html>`_; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:9345,scalab,scalable-and-incremental-lto,9345,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['scalab'],['scalable-and-incremental-lto']
Performance,"e values are all bfdnames. - `binary`; - `ihex`; - `elf32-i386`; - `elf32-x86-64`; - `elf64-x86-64`; - `elf32-iamcu`; - `elf32-littlearm`; - `elf64-aarch64`; - `elf64-littleaarch64`; - `elf32-littleriscv`; - `elf64-littleriscv`; - `elf32-powerpc`; - `elf32-powerpcle`; - `elf64-powerpc`; - `elf64-powerpcle`; - `elf32-bigmips`; - `elf32-ntradbigmips`; - `elf32-ntradlittlemips`; - `elf32-tradbigmips`; - `elf32-tradlittlemips`; - `elf64-tradbigmips`; - `elf64-tradlittlemips`; - `elf32-sparc`; - `elf32-sparcel`; - `elf32-hexagon`; - `elf32-loongarch`; - `elf64-loongarch`; - `elf64-s390`. Additionally, all targets except `binary` and `ihex` can have `-freebsd` as a; suffix. BINARY INPUT AND OUTPUT; -----------------------. If `binary` is used as the value for :option:`--input-target`, the input file; will be embedded as a data section in an ELF relocatable object, with symbols; ``_binary_<file_name>_start``, ``_binary_<file_name>_end``, and; ``_binary_<file_name>_size`` representing the start, end and size of the data,; where ``<file_name>`` is the path of the input file as specified on the command; line with non-alphanumeric characters converted to ``_``. If `binary` is used as the value for :option:`--output-target`, the output file; will be a raw binary file, containing the memory image of the input file.; Symbols and relocation information will be discarded. The image will start at; the address of the first loadable section in the output. EXIT STATUS; -----------. :program:`llvm-objcopy` exits with a non-zero exit code if there is an error.; Otherwise, it exits with code 0. BUGS; ----. To report bugs, please visit <https://github.com/llvm/llvm-project/labels/tools:llvm-objcopy/strip/>. There is a known issue with :option:`--input-target` and :option:`--target`; causing only ``binary`` and ``ihex`` formats to have any effect. Other values; will be ignored and :program:`llvm-objcopy` will attempt to guess the input; format. SEE ALSO; --------. :manpage:`llvm-strip(1)`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst:20100,load,loadable,20100,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,1,['load'],['loadable']
Performance,"e variables. The maximum alignment; is ``1 << 32``. For global variable declarations, as well as definitions that may be; replaced at link time (``linkonce``, ``weak``, ``extern_weak`` and ``common``; linkage types), the allocation size and alignment of the definition it resolves; to must be greater than or equal to that of the declaration or replaceable; definition, otherwise the behavior is undefined. Globals can also have a :ref:`DLL storage class <dllstorageclass>`,; an optional :ref:`runtime preemption specifier <runtime_preemption_model>`,; an optional :ref:`global attributes <glattrs>` and; an optional list of attached :ref:`metadata <metadata>`. Variables and aliases can have a; :ref:`Thread Local Storage Model <tls_model>`. Globals cannot be or contain :ref:`Scalable vectors <t_vector>` because their; size is unknown at compile time. They are allowed in structs to facilitate; intrinsics returning multiple values. Generally, structs containing scalable; vectors are not considered ""sized"" and cannot be used in loads, stores, allocas,; or GEPs. The only exception to this rule is for structs that contain scalable; vectors of the same type (e.g. ``{<vscale x 2 x i32>, <vscale x 2 x i32>}``; contains the same type while ``{<vscale x 2 x i32>, <vscale x 2 x i64>}``; doesn't). These kinds of structs (we may call them homogeneous scalable vector; structs) are considered sized and can be used in loads, stores, allocas, but; not GEPs. Syntax::. @<GlobalVarName> = [Linkage] [PreemptionSpecifier] [Visibility]; [DLLStorageClass] [ThreadLocal]; [(unnamed_addr|local_unnamed_addr)] [AddrSpace]; [ExternallyInitialized]; <global | constant> <Type> [<InitializerConstant>]; [, section ""name""] [, partition ""name""]; [, comdat [($name)]] [, align <Alignment>]; [, code_model ""model""]; [, no_sanitize_address] [, no_sanitize_hwaddress]; [, sanitize_address_dyninit] [, sanitize_memtag]; (, !name !N)*. For example, the following defines a global in a numbered address space; with an init",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:36168,scalab,scalable,36168,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['load', 'scalab']","['loads', 'scalable']"
Performance,"e when carefully adopted.; * Study performance bottlenecks -- the feature is designed with performance; considerations in mind. In this document we describe the current performance; bottlenecks and trade-offs.; * Understand if the gradual migration policy is sufficient -- C++ Modules in; ROOT support gradual migration. In particular, ROOT can enable C++ Modules for; itself and still run in legacy mode for the third-party code (generating; rootmap files and other scaffolding). C++ Modules are here and we would like to give a brief introduction of how the; feature works, what are its pros and cons, what's the current state of the; implementation and how third-party code can use it. Read more [[1]]. C++ Modules in ROOT are default since v6.20 (Unix) and v6.22 (OSX). ## Design Goals. * Coherence with standard C++ -- C++ Modules TS is advancing and will be; likely part the upcoming C++20 standard;; * Performance -- provide performance that is competitive to ROOT with PCH and; advance further the implementation of the C++ Modules in clang to optimize; memory footprint and execution time;; * Incremental adoption -- provide third-party code with an incremental; migration process for their codebases. ## Motivation. An implementation of the modules concepts exists in the LLVM frontend Clang used; as a library by ROOT [[2]]. Clang supports the Modules TS and hosts modules ; research and development work. The implementation encourages incremental, ; bottom-up [[3]] adoption of the modules feature. Modules in Clang are designed; to work for C, C++, ObjectiveC, ObjectiveC++ and Swift. Users can enable the; modules feature without modifications in header files. The LLVM compiler allows; users to specify module interfaces in dedicated file, called *module maps files*.; A module map file expresses the mapping between a module file and a collection; of header files. If the compiler finds such file in the include paths it; automatically generates, imports and uses module files. The mo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:1835,perform,performance,1835,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"e where we have used the unfolding method is the; decomposition of continuum of gamma-ray spectra. Using simulation and; interpolation techniques, we have synthesized the response matrix (size; `3400x3400` channels) of Gammasphere spectrometer (Figure 4.9). Its details are; presented in Figure 4.10. The original spectrum of Co^56 before and after; continuum decomposition are presented in Figures 4.11, 4.12, respectively. ![Response matrix (size 3400x3400 channels) of Gammasphere spectrometer](figures/image104.png). ![Detail of Figure 4.9](figures/image106.png). ![Original spectrum of Co56 before continuum decomposition](figures/image108.png). ![Original spectrum of Co56 after continuum decomposition](figures/image110.png). ## 2-DIMENSIONAL SPECTRA. We have extended the method of Gold deconvolution also for; 2-dimensional data. Again, the goal of the deconvolution methods is to; improve the resolution in the spectrum and to decompose multiplets. The method of optimized 2-dimensional deconvolution is described in details in [8]. Mathematical formulation of 2-dimensional convolution system is as; follows:. $$ y(i_1,i_2) = \sum_{k_1=0}^{N_1-1}\sum_{k_2=0}^{N_2-1}h(i_1-k_1,i_2-k_2)x(k_1,k_2), i_1=0,1,2,...,N_1-1, i_2=0,1,2,...,N_2-1 $$. Assuming we know the output; spectrum `y` and the response spectrum `h`, the task is to calculate the matrix `x`. The basic function has the form of. ``` {.cpp}; char *Deconvolution2(float **source,; const float **resp,; int sizex,; int sizey,; int niter);; ```. This function calculates deconvolution from the source spectrum according to; the response spectrum. The result is placed in the matrix pointed by the source; pointer. Function parameters:. - **`source`**: pointer to the matrix of the source spectrum; - **`resp`**: pointer to the matrix of the response spectrum; - **`sizex`**: x length of source and the response spectra; - **`sizey`**: y length of source and the response spectra; - **`number_of_iterations`**: see [8] for details. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/spectrum/Spectrum.md:32809,optimiz,optimized,32809,documentation/spectrum/Spectrum.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/spectrum/Spectrum.md,1,['optimiz'],['optimized']
Performance,"e with ``__has_builtin(__builtin_offsetof)``. ``__builtin_call_with_static_chain``; ------------------------------------. ``__builtin_call_with_static_chain`` is used to perform a static call while; setting updating the static chain register. **Syntax**:. .. code-block:: c++. T __builtin_call_with_static_chain(T expr, void* ptr). **Example of Use**:. .. code-block:: c++. auto v = __builtin_call_with_static_chain(foo(3), foo);. **Description**:. This builtin returns ``expr`` after checking that ``expr`` is a non-member; static call expression. The call to that expression is made while using ``ptr``; as a function pointer stored in a dedicated register to implement *static chain*; calling convention, as used by some language to implement closures or nested; functions. Query for this feature with ``__has_builtin(__builtin_call_with_static_chain)``. ``__builtin_readcyclecounter``; ------------------------------. ``__builtin_readcyclecounter`` is used to access the cycle counter register (or; a similar low-latency, high-accuracy clock) on those targets that support it. **Syntax**:. .. code-block:: c++. __builtin_readcyclecounter(). **Example of Use**:. .. code-block:: c++. unsigned long long t0 = __builtin_readcyclecounter();; do_something();; unsigned long long t1 = __builtin_readcyclecounter();; unsigned long long cycles_to_do_something = t1 - t0; // assuming no overflow. **Description**:. The ``__builtin_readcyclecounter()`` builtin returns the cycle counter value,; which may be either global or process/thread-specific depending on the target.; As the backing counters often overflow quickly (on the order of seconds) this; should only be used for timing small intervals. When not supported by the; target, the return value is always zero. This builtin takes no arguments and; produces an unsigned long long result. Query for this feature with ``__has_builtin(__builtin_readcyclecounter)``. Note; that even if present, its use may depend on run-time privilege or other OS; cont",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:102942,latency,latency,102942,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['latency'],['latency']
Performance,"e with pedantic enabled."" ON); option(LLVM_ENABLE_WERROR ""Fail and stop if a warning is triggered."" OFF). option(LLVM_ENABLE_DUMP ""Enable dump functions even when assertions are disabled"" OFF); option(LLVM_UNREACHABLE_OPTIMIZE ""Optimize llvm_unreachable() as undefined behavior (default), guaranteed trap when OFF"" ON). if( NOT uppercase_CMAKE_BUILD_TYPE STREQUAL ""DEBUG"" ); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" OFF); else(); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" ON); endif(). option(LLVM_ENABLE_EXPENSIVE_CHECKS ""Enable expensive checks"" OFF). # While adding scalable vector support to LLVM, we temporarily want to; # allow an implicit conversion of TypeSize to uint64_t, and to allow; # code to get the fixed number of elements from a possibly scalable vector.; # This CMake flag enables a more strict mode where it asserts that the type; # is not a scalable vector type.; #; # Enabling this flag makes it easier to find cases where the compiler makes; # assumptions on the size being 'fixed size', when building tests for; # SVE/SVE2 or other scalable vector architectures.; option(LLVM_ENABLE_STRICT_FIXED_SIZE_VECTORS; ""Enable assertions that type is not scalable in implicit conversion from TypeSize to uint64_t and calls to getNumElements"" OFF). set(LLVM_ABI_BREAKING_CHECKS ""WITH_ASSERTS"" CACHE STRING; ""Enable abi-breaking checks. Can be WITH_ASSERTS, FORCE_ON or FORCE_OFF.""). option(LLVM_FORCE_USE_OLD_TOOLCHAIN; ""Set to ON to force using an old, unsupported host toolchain."" OFF). set(LLVM_LOCAL_RPATH """" CACHE FILEPATH; ""If set, an absolute path added as rpath on binaries that do not already contain an executable-relative rpath.""). option(LLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN; ""Set to ON to only warn when using a toolchain which is about to be deprecated, instead of emitting an error."" OFF). option(LLVM_USE_INTEL_JITEVENTS; ""Use Intel JIT API to inform Intel(R) VTune(TM) Amplifier XE 2011 about JIT code""; OFF). if( LLVM_USE_INTEL_JITEVENTS ); # Verify we ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:25977,scalab,scalable,25977,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['scalab'],['scalable']
Performance,"e you have a reduced test-case, go to `the LLVM Bug Tracking System; <https://github.com/llvm/llvm-project/issues>`_ and fill out the form with the; necessary details (note that you don't need to pick a label, just use if you're; not sure). The bug description should contain the following information:. * All information necessary to reproduce the problem.; * The reduced test-case that triggers the bug.; * The location where you obtained LLVM (if not from our Git; repository). Thanks for helping us make LLVM better!. .. _crashes the compiler:. Crashing Bugs; =============. More often than not, bugs in the compiler cause it to crash---often due to; an assertion failure of some sort. The most important piece of the puzzle; is to figure out if it is crashing in the Clang front-end or if it is one of; the LLVM libraries (e.g. the optimizer or code generator) that has; problems. To figure out which component is crashing (the front-end, middle-end; optimizer, or backend code generator), run the ``clang`` command line as you; were when the crash occurred, but with the following extra command line; options:. * ``-emit-llvm -Xclang -disable-llvm-passes``: If ``clang`` still crashes when; passed these options (which disable the optimizer and code generator), then; the crash is in the front-end. Jump ahead to :ref:`front-end bugs; <frontend-crash>`. * ``-emit-llvm``: If ``clang`` crashes with this option (which disables; the code generator), you found a middle-end optimizer bug. Jump ahead to; :ref:`middle-end bugs <middleend-crash>`. * Otherwise, you have a backend code generator crash. Jump ahead to :ref:`code; generator bugs <backend-crash>`. .. _frontend-crash:. Front-end bugs; --------------. On a ``clang`` crash, the compiler will dump a preprocessed file and a script; to replay the ``clang`` command. For example, you should see something like. .. code-block:: text. PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:; Preprocessed source(s) and associated run script(s) ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst:1811,optimiz,optimizer,1811,interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSubmitABug.rst,1,['optimiz'],['optimizer']
Performance,"e {; return Error::success();; }; void notifyTransferringResources(ResourceKey DstKey,; ResourceKey SrcKey) override {}. // JITLink pass to print all defined symbols in G.; Error printAllSymbols(LinkGraph &G) {; for (auto *Sym : G.defined_symbols()); if (Sym->hasName()); dbgs() << Sym->getName() << ""\n"";; return Error::success();; }; };. // Create our LLJIT instance using a custom object linking layer setup.; // This gives us a chance to install our plugin.; auto J = ExitOnErr(LLJITBuilder(); .setObjectLinkingLayerCreator(; [](ExecutionSession &ES, const Triple &T) {; // Manually set up the ObjectLinkingLayer for our LLJIT; // instance.; auto OLL = std::make_unique<ObjectLinkingLayer>(; ES, std::make_unique<jitlink::InProcessMemoryManager>());. // Install our plugin:; OLL->addPlugin(std::make_unique<MyPlugin>());. return OLL;; }); .create());. // Add an object to the JIT. Nothing happens here: linking isn't triggered; // until we look up some symbol in our object.; ExitOnErr(J->addObject(loadFromDisk(""main.o"")));. // Plugin triggers here when our lookup of main triggers linking of main.o; auto MainSym = J->lookup(""main"");. LinkGraph; =========. JITLink maps all relocatable object formats to a generic ``LinkGraph`` type; that is designed to make linking fast and easy (``LinkGraph`` instances can; also be created manually. See :ref:`constructing_linkgraphs`). Relocatable object formats (e.g. COFF, ELF, MachO) differ in their details,; but share a common goal: to represent machine level code and data with; annotations that allow them to be relocated in a virtual address space. To; this end they usually contain names (symbols) for content defined inside the; file or externally, chunks of content that must be moved as a unit (sections; or subsections, depending on the format), and annotations describing how to; patch content based on the final address of some target symbol/section; (relocations). At a high level, the ``LinkGraph`` type represents these concepts as a deco",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:6440,load,loadFromDisk,6440,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['load'],['loadFromDisk']
Performance,"e(index) to return the name of a variable (parameter) given an index, and VariableIndex(name) to return the index of a variable given a name.; Set a status code in Minuit2Minimizer according to the following convention:; status = minimizeStatus + 10 * minosStatus + 100 * hesseStatus.; See the Minuit2Minimizer reference documentation for the possible values of minimizeStatus , minosStatus and hesseStatus.; In MnHesse. when the inversion of the hessian matrix failed, return MnInvertFailed instead of MnHesseFailed. Mathcore Fitting classes. Fix the fitting with the integral option in multi-dimensions.; Force the gradient calculation when requested in the minimizer; classes and avoid to perform the check when using TMinuit. This was; already the case in Minuit2.; Add new class ROOT::Fit::SparseData for dealing with binned sparse data. This class automatically merges the empty region, so they can be considered, whenever possible as a larger single bin. This improves the performances when doing likelihood fits on the sparse data.; Fix the likelihood fits for variable bin histograms. Now a correct normalization is applied according to the bin volume.; Add new methods in Minimizer class :. Minimizer::SetPrecision(double eps) to change in the minimizer the precision on which the objective functions are evaluated. By default the numerical double precision is used inside the minimizers. This method should be used only if the precision in the function evaluation is worse than the double precision.; std::string Minimizer::VariableName (unsigned int index) to return a name of the minimizer variable (i.e. a fitting parameter) given the integer index. Return an empty string if the variable is not found or of the minimizer does not re-implement this method.; int Minimizer::VariableIndex(const std::string & name) to return the index of a variable given a name. Return -1 if the variable is not found or if the specific minimizer does not re-implement this function. ROOT::Fit::FitResult",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v526/index.html:3111,perform,performances,3111,math/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v526/index.html,2,['perform'],['performances']
Performance,"e) info. This is a Mach-O-specific option. Valid values are:. * ``no-compact-unwind`` - Only emit DWARF unwind when compact unwind encodings; aren't available. This is the default for arm64.; * ``always`` - Always emit DWARF unwind regardless.; * ``default`` - Use the platform-specific default (``always`` for all; non-arm64-platforms). ``no-compact-unwind`` is a performance optimization -- Clang will emit smaller; object files that are more quickly processed by the linker. This may cause; binary compatibility issues on older x86_64 targets, however, so use it with; caution. .. _configuration-files:. Configuration files; -------------------. Configuration files group command-line options and allow all of them to be; specified just by referencing the configuration file. They may be used, for; example, to collect options required to tune compilation for particular; target, such as ``-L``, ``-I``, ``-l``, ``--sysroot``, codegen options, etc. Configuration files can be either specified on the command line or loaded; from default locations. If both variants are present, the default configuration; files are loaded first. The command line option ``--config=`` can be used to specify explicit; configuration files in a Clang invocation. If the option is used multiple times,; all specified files are loaded, in order. For example:. ::. clang --config=/home/user/cfgs/testing.txt; clang --config=debug.cfg --config=runtimes.cfg. If the provided argument contains a directory separator, it is considered as; a file path, and options are read from that file. Otherwise the argument is; treated as a file name and is searched for sequentially in the directories:. - user directory,; - system directory,; - the directory where Clang executable resides. Both user and system directories for configuration files are specified during; clang build using CMake parameters, ``CLANG_CONFIG_FILE_USER_DIR`` and; ``CLANG_CONFIG_FILE_SYSTEM_DIR`` respectively. The first file found is used.; It is an error ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:30923,load,loaded,30923,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['loaded']
Performance,"e, and additionally provide a total ordering with some; complicated guarantees, see the C++ standard for details. Frontends generating atomic instructions generally need to be aware of the; target to some degree; atomic instructions are guaranteed to be lock-free, and; therefore an instruction which is wider than the target natively supports can be; impossible to generate. .. _Atomic orderings:. Atomic orderings; ================. In order to achieve a balance between performance and necessary guarantees,; there are six levels of atomicity. They are listed in order of strength; each; level includes all the guarantees of the previous level except for; Acquire/Release. (See also `LangRef Ordering <LangRef.html#ordering>`_.). .. _NotAtomic:. NotAtomic; ---------. NotAtomic is the obvious, a load or store which is not atomic. (This isn't; really a level of atomicity, but is listed here for comparison.) This is; essentially a regular load or store. If there is a race on a given memory; location, loads from that location return undef. Relevant standard; This is intended to match shared variables in C/C++, and to be used in any; other context where memory access is necessary, and a race is impossible. (The; precise definition is in `LangRef Memory Model <LangRef.html#memmodel>`_.). Notes for frontends; The rule is essentially that all memory accessed with basic loads and stores; by multiple threads should be protected by a lock or other synchronization;; otherwise, you are likely to run into undefined behavior. If your frontend is; for a ""safe"" language like Java, use Unordered to load and store any shared; variable. Note that NotAtomic volatile loads and stores are not properly; atomic; do not try to use them as a substitute. (Per the C/C++ standards,; volatile does provide some limited guarantees around asynchronous signals, but; atomics are generally a better solution.). Notes for optimizers; Introducing loads to shared variables along a codepath where they would not; ot",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:6213,load,loads,6213,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"e, in one script you can use ACLiC to compile and load another; script. ``` {.cpp}; gROOT->ProcessLine("".L MyScript.C+""); gROOT->ProcessLine("".L MyScript.C++""); ```. ### Setting the Include Path. You can get the include path by typing:. ``` {.cpp}; root[] .include; ```. You can append to the include path by typing:. ``` {.cpp}; root[] .include $HOME/mypackage/include; ```. In a script you can append to the include path:. ``` {.cpp}; gSystem->AddIncludePath("" -I$HOME/mypackage/include ""); ```. You can also overwrite the existing include path:. ``` {.cpp}; gSystem->SetIncludePath("" -I$HOME/mypackage/include ""); ```. The `$ROOTSYS/include` directory is automatically appended to the; include path, so you do not have to worry about including it. To add; library that should be used during linking of the shared library use; something like:. ``` {.cpp}; gSystem->AddLinkedLibs(""-L/my/path -lanylib"");; ```. This is especially useful for static libraries. For shared ones you; can also simply load them before trying to compile the script:. ``` {.cpp}; gSystem->Load(""mydir/mylib"");; ```. ACLiC uses the directive `fMakeSharedLibs` to create the shared; library. If loading the shared library fails, it tries to output a; list of missing symbols by creating an executable (on some platforms; like OSF, this does not HAVE to be an executable) containing the; script. It uses the directive `fMakeExe` to do so. For both; directives, before passing them to `TSystem::Exec()`, it expands the; variables `$SourceFiles`, `$SharedLib`, `$LibName`, `$IncludePath`,; `$LinkedLibs`, `$ExeName `and` $ObjectFiles`. See `SetMakeSharedLib()`; for more information on those variables. When the file being passed to; ACLiC is on a read only file system, ACLiC warns the user and creates; the library in a temporary directory:. ``` {.cpp}; root[] .L readonly/t.C++; Warning in <ACLiC>: /scratch/aclic/subs/./readonly is not writable!; Warning in <ACLiC>: Output will be written to /tmp; Info in <TUnixSystem::ACLiC",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:17211,load,load,17211,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['load']
Performance,"e, omit.; - Must happen before the; following buffer_wbinvl1_vol.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:250157,load,load,250157,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"e, or ABI-required stack probes, if any.; It defines the size of the guard region. It ensures that if the function; may use more stack space than the size of the guard region, stack probing; sequence will be emitted. It takes one required integer value, which; is 4096 by default. If a function that has a ``""stack-probe-size""`` attribute is inlined into; a function with another ``""stack-probe-size""`` attribute, the resulting; function has the ``""stack-probe-size""`` attribute that has the lower; numeric value. If a function that has a ``""stack-probe-size""`` attribute is; inlined into a function that has no ``""stack-probe-size""`` attribute; at all, the resulting function has the ``""stack-probe-size""`` attribute; of the callee.; ``""no-stack-arg-probe""``; This attribute disables ABI-required stack probes, if any.; ``returns_twice``; This attribute indicates that this function can return twice. The C; ``setjmp`` is an example of such a function. The compiler disables; some optimizations (like tail calls) in the caller of these; functions.; ``safestack``; This attribute indicates that; `SafeStack <https://clang.llvm.org/docs/SafeStack.html>`_; protection is enabled for this function. If a function that has a ``safestack`` attribute is inlined into a; function that doesn't have a ``safestack`` attribute or which has an; ``ssp``, ``sspstrong`` or ``sspreq`` attribute, then the resulting; function will have a ``safestack`` attribute.; ``sanitize_address``; This attribute indicates that AddressSanitizer checks; (dynamic address safety analysis) are enabled for this function.; ``sanitize_memory``; This attribute indicates that MemorySanitizer checks (dynamic detection; of accesses to uninitialized memory) are enabled for this function.; ``sanitize_thread``; This attribute indicates that ThreadSanitizer checks; (dynamic thread safety analysis) are enabled for this function.; ``sanitize_hwaddress``; This attribute indicates that HWAddressSanitizer checks; (dynamic address safety a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:99870,optimiz,optimizations,99870,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"e, there is; a problem with the following scenario: a class `Foo` has a; **`TClonesArray`** of `Bar` objects the `Foo` object is written with; `split=0` to `Tree` `T1`. In this case the `StreamerInfo` for the class; `Bar` is created in optimized mode in such a way that data members of; the same type are written as an array improving the I/O performance. In; a new program, `T1` is read and a new `Tree` `T2` is created with the; object `Foo` in `split > 1`. When the `T2 `branch is created, the `StreamerInfo` for the class `Bar`; is created with no optimization (mandatory for the split mode). The; optimized Bar `StreamerInfo` is going to be used to read the; **`TClonesArray`** in `T1`. The result will be `Bar` objects with data; member values not in the right sequence. The solution to this problem is; to call `BypassStreamer(kFALSE)` for the **`TClonesArray`**. In this; case, the normal `Bar::Streamer` function will be called. The; `Bar::Streamer` function works OK independently if the `Bar`; `StreamerInfo `had been generated in optimized mode or not. ## Pointers and References in Persistency. An object pointer as a data member presents a challenge to the streaming; software. If the object pointed to is saved every time, it could create; circular dependencies and consume a large amount of disk space. The; network of references must be preserved on disk and recreated upon; reading the file. If you use independent I/O operations for pointers and their referenced; objects you can use the **`TRef`** class. Later in this section is an; example that compares disk space, memory usage, and I/O times of C++; pointers and **`TRef`**`s`. In general, a **`TRef`** is faster than C++; but the advantage of a C++ pointer is that it is already C++. ### Streaming C++ Pointers. When ROOT encounters a pointer data member it calls the `Streamer` of; the object and labels it with a unique object identifier. The object; identifier is unique for one I/O operation. If there is another pointer; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:55385,optimiz,optimized,55385,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['optimiz'],['optimized']
Performance,"e, whether the; non-positive-definiteness is real or only numerical is largely; irrelevant, since in both cases the error matrix will be unreliable; and the minimum suspicious. #### An Ill-posed Problem. For questions of parameter dependence, see the discussion above on; positive-definiteness. Possible other mathematical problems are the; following:. - Excessive numerical round off - be especially careful of; exponential and factorial functions which get big very quickly and; lose accuracy. - Starting too far from the solution - the function may have; unphysical local minima, especially at infinity in some variables. ## Minuit2 Package. `Minuit2` is a new object-oriented implementation, written in C++, of; the popular `MINUIT` minimization package. Compared with the; **`TMinuit`** class, which is a direct conversion from FORTRAN to C++,; `Minuit2` is a complete redesign and re-implementation of the package.; This new version provides all the functionality present in the old; FORTRAN version, with almost equivalent numerical accuracy and; computational performances.; Furthermore, it contains some fixes and small improvements and this new functionality:; * The possibility to set single side parameter limits; * the FUMILI algorithm (see the next paragraph ""FUMILI Minimization Package""),; which is an optimized method for least square and log; likelihood minimizations. Minuit2 has been originally developed by M.; Winkler and F. James in the SEAL project. More information can be found; on the [MINUIT Web Site](MINUIT Web Site) and in particular at the; following documentation page at; <http://www.cern.ch/minuit/doc/doc.html>. A detailed User Guide for Minuit2 exists, describing the API of the internal classes.; ROOT uses `Minuit2` for fitting via the `Minuit2Minimizer` class which implements; the `ROOT::Math::Minimizer` interface. `Minuit2` is also distributed as an independent package of ROOT and can be built; without any other dependency on the ROOT libraries. Examples o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:65032,perform,performances,65032,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['performances']
Performance,"e-manipulating intrinsics. Example:; """""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; %ptr = alloca i32, i32 4 ; yields ptr; %ptr = alloca i32, i32 4, align 1024 ; yields ptr; %ptr = alloca i32, align 1024 ; yields ptr. .. _i_load:. '``load``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = load [volatile] <ty>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.load !<empty_node>][, !invariant.group !<empty_node>][, !nonnull !<empty_node>][, !dereferenceable !<deref_bytes_node>][, !dereferenceable_or_null !<deref_bytes_node>][, !align !<align_node>][, !noundef !<empty_node>]; <result> = load atomic [volatile] <ty>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>]; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}; !<deref_bytes_node> = !{ i64 <dereferenceable_bytes> }; !<align_node> = !{ i64 <value_alignment> }. Overview:; """""""""""""""""". The '``load``' instruction is used to read from memory. Arguments:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:412555,load,load,412555,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"e-path=prebuilt. A trick to prebuild all modules required for our source file in one command is to generate implicit modules while using the ``-fdisable-module-hash`` option. .. code-block:: sh. rm -rf prebuilt ; mkdir prebuilt; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -fdisable-module-hash; ls prebuilt/*.pcm; # prebuilt/A.pcm prebuilt/B.pcm. Note that with explicit or prebuilt modules, we are responsible for, and should be particularly careful about the compatibility of our modules.; Using mismatching compilation options and modules may lead to issues. .. code-block:: sh. clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -DENABLE_A; # use.c:4:10: warning: implicit declaration of function 'a' is invalid in C99 [-Wimplicit-function-declaration]; # return a(x);; # ^; # 1 warning generated. So we need to maintain multiple versions of prebuilt modules. We can do so using a manual module mapping, or pointing to a different prebuilt module cache path. For example:. .. code-block:: sh. rm -rf prebuilt ; mkdir prebuilt ; rm -rf prebuilt_a ; mkdir prebuilt_a; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -fdisable-module-hash; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt_a -fdisable-module-hash -DENABLE_A; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt_a -DENABLE_A. Instead of managing the different module versions manually, we can build implicit modules in a given cache path (using ``-fmodules-cache-path``), and reuse them as prebuilt implicit modules by passing ``-fprebuilt-module-path`` and ``-fprebuilt-implicit-modules``. .. code-block:: sh. rm -rf prebuilt; mkdir prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-pa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:21588,cache,cache,21588,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache']
Performance,"e-time?; The obvious answer is compile-time: see the performance differences between; C++ and Python, for example.; Obvious, but completely wrong, however.; In fact, when it comes to Python, it is even the `wrong question.`. Everything in Python is run-time: modules, classes, functions, etc. are all; run-time constructs.; A Python module that defines a class is a set of instructions to the Python; interpreter that lead to the construction of the desired class object.; A C/C++ extension module that defines a class does the same thing by calling; a succession of Python interpreter Application Programming Interfaces (APIs;; the exact same that Python uses itself internally).; If you use a compile-time binder such as `SWIG`_ or `pybind11`_ to bind a C++; class, then what gets compiled is the series of API calls necessary to; construct a Python-side equivalent at `run-time` (when the module gets; loaded), not the Python class object.; In short, whether a binding is created at ""compile-time"" or at run-time has; no measurable bearing on performance. What does affect performance is the overhead to cross the language barrier.; This consists of unboxing Python objects to extract or convert the underlying; objects or data to something that matches what C++ expects; overload; resolution based on the unboxed arguments; offset calculations; and finally; the actual dispatch.; As a practical matter, overload resolution is the most costly part, followed; by the unboxing and conversion.; Best performance is achieved by specialization of the paths through the; run-time: recognize early the case at hand and select an optimized path.; For that reason, `PyPy`_ is so fast: JIT-ed traces operate on unboxed objects; and resolved overloads are baked into the trace, incurring no further cost.; Similarly, this is why pybind11 is so slow: its code generation is the C++; compiler's template engine, so complex path selection and specialization is; very hard to do in a performance-portable way. In",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:1724,perform,performance,1724,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,1,['perform'],['performance']
Performance,"e. First of all, you can't set a breakpoint in a shared object that; has not been loaded yet, and second of all there are problems with inlined; functions in shared objects. Here are some suggestions to debugging your pass; with GDB. For sake of discussion, I'm going to assume that you are debugging a; transformation invoked by :program:`opt`, although nothing described here; depends on that. Setting a breakpoint in your pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. First thing you do is start gdb on the opt process:. .. code-block:: console. $ gdb opt; GNU gdb 5.0; Copyright 2000 Free Software Foundation, Inc.; GDB is free software, covered by the GNU General Public License, and you are; welcome to change it and/or distribute copies of it under certain conditions.; Type ""show copying"" to see the conditions.; There is absolutely no warranty for GDB. Type ""show warranty"" for details.; This GDB was configured as ""sparc-sun-solaris2.6""...; (gdb). Note that :program:`opt` has a lot of debugging information in it, so it takes; time to load. Be patient. Since we cannot set a breakpoint in our pass yet; (the shared object isn't loaded until runtime), we must execute the process,; and have it stop before it invokes our pass, but after it has loaded the shared; object. The most foolproof way of doing this is to set a breakpoint in; ``PassManager::run`` and then run the process with the arguments you want:. .. code-block:: console. $ (gdb) break llvm::PassManager::run; Breakpoint 1 at 0x2413bc: file Pass.cpp, line 70.; (gdb) run test.bc -load $(LLVMTOP)/llvm/Debug+Asserts/lib/[libname].so -[passoption]; Starting program: opt test.bc -load $(LLVMTOP)/llvm/Debug+Asserts/lib/[libname].so -[passoption]; Breakpoint 1, PassManager::run (this=0xffbef174, M=@0x70b298) at Pass.cpp:70; 70 bool PassManager::run(Module &M) { return PM->run(M); }; (gdb). Once the :program:`opt` stops in the ``PassManager::run`` method you are now; free to set breakpoints in your pass so that you can trace thr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:53199,load,load,53199,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"e. Mandatory loop canonicalizations such; as loop rotation are still applied. It is recommended to use this metadata in addition to any llvm.loop.*; transformation directive. Also, any loop should have at most one; directive applied to it (and a sequence of transformations built using; followup-attributes). Otherwise, which transformation will be applied; depends on implementation details such as the pass pipeline order. See :ref:`transformation-metadata` for details. '``llvm.loop.vectorize``' and '``llvm.loop.interleave``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Metadata prefixed with ``llvm.loop.vectorize`` or ``llvm.loop.interleave`` are; used to control per-loop vectorization and interleaving parameters such as; vectorization width and interleave count. These metadata should be used in; conjunction with ``llvm.loop`` loop identification metadata. The; ``llvm.loop.vectorize`` and ``llvm.loop.interleave`` metadata are only; optimization hints and the optimizer will only interleave and vectorize loops if; it believes it is safe to do so. The ``llvm.loop.parallel_accesses`` metadata; which contains information about loop-carried memory dependencies can be helpful; in determining the safety of these transformations. '``llvm.loop.interleave.count``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata suggests an interleave count to the loop interleaver.; The first operand is the string ``llvm.loop.interleave.count`` and the; second operand is an integer specifying the interleave count. For; example:. .. code-block:: llvm. !0 = !{!""llvm.loop.interleave.count"", i32 4}. Note that setting ``llvm.loop.interleave.count`` to 1 disables interleaving; multiple iterations of the loop. If ``llvm.loop.interleave.count`` is set to 0; then the interleave count will be determined automatically. '``llvm.loop.vectorize.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables vectorization for the loo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:296486,optimiz,optimization,296486,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"e. These are explained in details in the section; ""Visualization Settings and Attributes"". At this point, you will; probably like to see how this geometry looks like. You just need to run; the example and you will get the following picture that you can rotate; using the mouse; or you can zoom / move it around (see what the Help; menu of the GL window displays). ~~~{.cpp}; % root rootgeom.C; ~~~. \image html geometry001.png width=600px. Now let us browse the hierarchy that was just created. Start a browser; and double-click on the item simple1 representing the; `gGeoManager` object. Note that right click opens the context menu; of the manager class where several global methods are available. ~~~{.cpp}; root[] new TBrowser;; ~~~. \image html geometry002.jpg width=600px. The folders `Materials`, `Media` and `Local transformations` are in fact; the containers where the geometry manager stores the corresponding; objects. The `Illegal overlaps` folder is empty but can be filled after; performing a geometry validity check (see section: ""Checking the; Geometry""). If tracking is performed using `TGeo`, the folder; `Tracks` might contain user-defined tracks that can be; visualized/animated in the geometry context (see section: ""Creating and; Visualizing Tracks""). Since for the time being we are interested more in; the geometrical hierarchy, we will focus on the last two displayed items; `TOP `and `TOP_1`. These are the top volume and the corresponding top; node in the hierarchy. Double clicking on the `TOP` volume will unfold all different volumes; contained by the top volume. In the right panel, we will see all the; volumes contained by `TOP` (if the same is positioned 4 times we will; get 4 identical items). This rule will apply to any clicked volume in; the hierarchy. Note that right clicking a volume item activates the; volume context menu containing several specific methods. We will call; the volume hierarchy developed in this way as the; `logical geometry graph`. The vo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:11893,perform,performing,11893,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performing']
Performance,"e. Variants of the intrinsic with non-void return; type also return a value according to calling convention. On PowerPC, note that ``<target>`` must be the ABI function pointer for the; intended target of the indirect call. Specifically, when compiling for the; ELF V1 ABI, ``<target>`` is the function-descriptor address normally used as; the C/C++ function-pointer representation. Requesting zero patch point arguments is valid. In this case, all; variable operands are handled just like; ``llvm.experimental.stackmap.*``. The difference is that space will; still be reserved for patching, a call will be emitted, and a return; value is allowed. The location of the arguments are not normally recorded in the stack; map because they are already fixed by the calling convention. The; remaining ``live values`` will have their location recorded, which; could be a register, stack location, or constant. A special calling; convention has been introduced for use with stack maps, anyregcc,; which forces the arguments to be loaded into registers but allows; those register to be dynamically allocated. These argument registers; will have their register locations recorded in the stack map in; addition to the remaining ``live values``. The patch point also emits nops to cover at least ``<numBytes>`` of; instruction encoding space. Hence, the client must ensure that; ``<numBytes>`` is enough to encode a call to the target address on the; supported targets. If the call target is constant null, then there is; no minimum requirement. A zero-byte null target patchpoint is; valid. The runtime may patch the code emitted for the patch point, including; the call sequence and nops. However, the runtime may not assume; anything about the code LLVM emits within the reserved space. Partial; patching is not allowed. The runtime must patch all reserved bytes,; padding with nops if necessary. This example shows a patch point reserving 15 bytes, with one argument; in $rdi, and a return value in $rax per n",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:9628,load,loaded,9628,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['load'],['loaded']
Performance,"e., more than a single background and signal; class) has been enabled for these methods: MLP (NN), BDTG,; FDA.; The multiclass; functionality can be enabled with the Factory option; ""AnalysisType=multiclass"". Training data is; specified with an additional classname, e.g. via; factory->AddTree(tree,""classname"");. After the; training a genetic algorithm is invoked to determine the best; cuts for selecting a specific class, based on the figure of; merit: purity*efficiency. TMVA comes with two examples in; $ROOTSYS/tmva/test: TMVAMulticlass.C; and TMVAMulticlassApplication.C. New TMVA event vector building. The code; for splitting the input data into training and test samples for; all classes and the mixing of those samples to one training and; one test sample has been rewritten completely. The new code is; more performant and has a clearer structure. This fixes several; bugs which have been reported by some users of TMVA.; Code and performance test framework: A unit; test framework for daily software and method performance; validation has been implemented.; . Methods. BDT Automatic parameter optimisation for building the; tree architecture: The optimisation procedure uses the; performance of the trained classifier on the ""test sample"" for; finding the set of optimal parameters. Two different methods to; traverse the parameter space are available (scanning, genetic; algorithm). Currently parameter optimization is implemented only; for these three parameters that influence the tree architectur:; the maximum depth of a tree, MaxDepth, the minimum; number of events in each node, NodeMinEvents, and; the number of tress, NTrees. Optimization can; is invoked by calling; factory->OptimizeAllMethods(); prior to the call; factory->TrainAllMethods();. Automated and configurable parameter optimization is soon to; be enabled for all methods (for those parameters where; optimization is applicable).; . BDT node splitting: While Decision Trees; typically have only univariate splits, in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html:1418,perform,performance,1418,tmva/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html,4,['perform'],['performance']
Performance,"e.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:363268,load,load,363268,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"e/id/name. Enable printing category information in diagnostic line. This option, which defaults to ""none"", controls whether or not Clang; prints the category associated with a diagnostic when emitting it.; Each diagnostic may or many not have an associated category, if it; has one, it is listed in the diagnostic categorization field of the; diagnostic line (in the []'s). For example, a format string warning will produce these three; renditions based on the setting of this option:. ::. t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,1]; t.c:3:11: warning: conversion specifies type 'char *' but the argument has type 'int' [-Wformat,Format String]. This category can be used by clients that want to group diagnostics; by category, so it should be a high level category. We want dozens; of these, not hundreds or thousands of them. .. _opt_fsave-optimization-record:. .. option:: -f[no-]save-optimization-record[=<format>]. Enable optimization remarks during compilation and write them to a separate; file. This option, which defaults to off, controls whether Clang writes; optimization reports to a separate file. By recording diagnostics in a file,; users can parse or sort the remarks in a convenient way. By default, the serialization format is YAML. The supported serialization formats are:. - .. _opt_fsave_optimization_record_yaml:. ``-fsave-optimization-record=yaml``: A structured YAML format. - .. _opt_fsave_optimization_record_bitstream:. ``-fsave-optimization-record=bitstream``: A binary format based on LLVM; Bitstream. The output file is controlled by :option:`-foptimization-record-file`. In the absence of an explicit output file, the file is chosen using the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:10785,optimiz,optimization-record,10785,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization-record']
Performance,"e1; compiler is a balance of optimization vs build time because it is a throwaway.; The stage2 compiler is the fully optimized compiler intended to ship to users. Setting up these compilers requires a lot of options. To simplify the; configuration the Apple Clang build settings are contained in CMake Cache files.; You can build an Apple Clang compiler using the following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/Apple-stage1.cmake <path to source>/llvm; $ ninja stage2-distribution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts is common in our more advanced; build configurations. Multi-stage PGO; ===============. Profile-Guided Optimizations (PGO) is a really great way to optimize the code; clang generates. Our multi-stage PGO builds are a workflow for generating PGO; profiles that can be used to optimize clang. At a high level, the way PGO works is that you build an instrumented compiler,; then you run the instrumented compiler against sample source files. While the; instrumented compiler runs it will output a bunch of files containing; performance counters (.profraw files). After generating all the profraw files; you use llvm-profdata to merge the files into a single profdata file that you; can feed into the LLVM_PROFDATA_FILE option. Our PGO.cmake cache automates that whole process. You can use it for; configuration with CMake with the following command:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/PGO.cmake \; <path to source>/llv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:4544,cache,cache,4544,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,2,['cache'],['cache']
Performance,"e:Children:Grade:... *; * | Cost *; *Entries :3354 : Total Size = 154237 bytes File Size = 32316 *; *Baskets : 3 : Basket Size = 32000 bytes Compression= 2.97 *; ```. ## Scan a Variable the Tree with TTree::Scan. The `TTree::Scan` method shows all values of the list of leaves; separated by a colon. ``` {.cpp}; root[] T->Scan(""Cost:Age:Children""); ************************************************; * Row * Cost * Age * Children *; ************************************************; * 0 * 11975 * 58 * 0 *; * 1 * 10228 * 63 * 0 *; * 2 * 10730 * 56 * 2 *; * 3 * 9311 * 61 * 0 *; * 4 * 9966 * 52 * 2 *; * 5 * 7599 * 60 * 0 *; * 6 * 9868 * 53 * 1 *; * 7 * 8012 * 60 * 1 *; ...; ```. ## The Tree Viewer. ![Activating the tree viewer](pictures/030000EF.png). The tree viewer is a quick and easy way to examine a tree. To start the; tree viewer, open a file and object browser. Right click on a; **`TTree`** and select `StartViewer`. You can also start the tree viewer; from the command line. First load the viewer library. ``` {.cpp}; root[] TFile f(""cernstaff.root""); root[] T->StartViewer(); ```. If you want to start a tree viewer without a tree, you need to load the; tree player library first:. ``` {.cpp}; root[] gSystem->Load(""libTreeViewer.so""); root[] new TTreeViewer(); ```. The figure above shows how the tree viewer looks like for the example file; `cernstaff.root`. The left panel contains the list of trees and their; branches; in this case there is only one tree. You can add more trees; with the File-Open command to open the file containing the new tree,; then use the context menu on the right panel, select `SetTreeName` and; enter the name of the tree to add. On the right are the leaves or; variables in the tree. You can double click on any leaf to a histogram; it. The toolbar in the upper part can be used for user commands, changing; the drawing option and the histogram name. The lower part contains three; picture buttons that draw a histogram, stop the current command, and; refr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:6622,load,load,6622,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"e; (named *Average Wait times*) reports useful timing statistics, which should; help diagnose performance bottlenecks caused by long data dependencies and; sub-optimal usage of hardware resources. An instruction in the timeline view is identified by a pair of indices, where; the first index identifies an iteration, and the second index is the; instruction index (i.e., where it appears in the code sequence). Since this; example was generated using 3 iterations: ``-iterations=3``, the iteration; indices range from 0-2 inclusively. Excluding the first and last column, the remaining columns are in cycles.; Cycles are numbered sequentially starting from 0. From the example output above, we know the following:. * Instruction [1,0] was dispatched at cycle 1.; * Instruction [1,0] started executing at cycle 2.; * Instruction [1,0] reached the write back stage at cycle 4.; * Instruction [1,0] was retired at cycle 10. Instruction [1,0] (i.e., vmulps from iteration #1) does not have to wait in the; scheduler's queue for the operands to become available. By the time vmulps is; dispatched, operands are already available, and pipeline JFPU1 is ready to; serve another instruction. So the instruction can be immediately issued on the; JFPU1 pipeline. That is demonstrated by the fact that the instruction only; spent 1cy in the scheduler's queue. There is a gap of 5 cycles between the write-back stage and the retire event.; That is because instructions must retire in program order, so [1,0] has to wait; for [0,2] to be retired first (i.e., it has to wait until cycle 10). In the example, all instructions are in a RAW (Read After Write) dependency; chain. Register %xmm2 written by vmulps is immediately used by the first; vhaddps, and register %xmm3 written by the first vhaddps is used by the second; vhaddps. Long data dependencies negatively impact the ILP (Instruction Level; Parallelism). In the dot-product example, there are anti-dependencies introduced by; instructions from different i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:24280,queue,queue,24280,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"e; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgk",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:362220,load,load,362220,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"e; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkmcnt(0) &; vm/vscnt(0). - If CU wavefront execution; mode, omit vm/vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:350080,load,loads,350080,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"e; calling function or; * the autorelease pool is restored to a previous state. .. admonition:: Rationale. Rationale: not all memory and resources are managed with reference counts; it; is common for objects to manage private resources in their own, private way.; Typically these resources are completely encapsulated within the object, but; some classes offer their users direct access for efficiency. If ARC is not; aware of methods that return such ""interior"" pointers, its optimizations can; cause the owning object to be reclaimed too soon. This attribute informs ARC; that it must tread lightly. The extension rules are somewhat intentionally vague. The autorelease pool; limit is there to permit a simple implementation to simply retain and; autorelease the receiver. The other limit permits some amount of; optimization. The phrase ""derived from"" is intended to encompass the results; both of pointer transformations, such as casts and arithmetic, and of loading; from such derived pointers; furthermore, it applies whether or not such; derivations are applied directly in the calling code or by other utility code; (for example, the C library routine ``strchr``). However, the implementation; never need account for uses after a return from the code which calls the; method returning an interior pointer. As an exception, no extension is required if the receiver is loaded directly; from a ``__strong`` object with :ref:`precise lifetime semantics; <arc.optimization.precise>`. .. admonition:: Rationale. Implicit autoreleases carry the risk of significantly inflating memory use,; so it's important to provide users a way of avoiding these autoreleases.; Tying this to precise lifetime semantics is ideal, as for local variables; this requires a very explicit annotation, which allows ARC to trust the user; with good cheer. .. _arc.misc.c-retainable:. C retainable pointer types; --------------------------. A type is a :arc-term:`C retainable pointer type` if it is a pointer to; (possibl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:101023,load,loading,101023,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loading']
Performance,"e; file. This option, which defaults to off, controls whether Clang writes; optimization reports to a separate file. By recording diagnostics in a file,; users can parse or sort the remarks in a convenient way. By default, the serialization format is YAML. The supported serialization formats are:. - .. _opt_fsave_optimization_record_yaml:. ``-fsave-optimization-record=yaml``: A structured YAML format. - .. _opt_fsave_optimization_record_bitstream:. ``-fsave-optimization-record=bitstream``: A binary format based on LLVM; Bitstream. The output file is controlled by :option:`-foptimization-record-file`. In the absence of an explicit output file, the file is chosen using the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when used with `-c` or `-S`.; For example:. * ``clang -fsave-optimization-record -c in.c -o out.o`` will generate; ``out.opt.yaml``. * ``clang -fsave-optimization-record -c in.c`` will generate; ``in.opt.yaml``. When targeting (Thin)LTO, the base is derived from the output filename, and; the extension is not dropped. When targeting ThinLTO, the following scheme is used:. ``<base>.opt.<format>.thin.<num>.<format>``. Darwin-only: when used for generating a linked binary from a source file; (through an intermediate object file), the driver will invoke `cc1` to; generate a temporary object file. The temporary remark file will be emitted; next to the object file, which will then be picked up by `dsymutil` and; emitted in the .dSYM bundle. This is available for all formats except YAML. For example:. ``clang -fsave-optimization-record=bitstream in.c -o out`` will generate. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.o``. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.opt.bitstream``. * ``out``. * ``out.dSYM/Contents/Resources/Remarks/out``. Darwin-only: compiling for multiple architectures will use the followin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:11933,optimiz,optimization-record,11933,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization-record']
Performance,"e; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; local load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:377894,load,load,377894,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"e; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; local load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:377027,load,load,377027,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"e; individuals and organizations will contribute back their work if they do not; want to have to maintain a fork forever (which is time consuming and expensive; when merges are involved). Further, nobody makes money on compilers these days,; but many people need them to get bigger goals accomplished: it makes sense for; everyone to work together.; For more information about the LLVM/clang license, please see the LLVM License; Description for more information. Internal Design and Implementation. A real-world, production quality compiler. Clang is designed and built by experienced compiler developers who are; increasingly frustrated with the problems that existing open source; compilers have. Clang is carefully and thoughtfully designed and; built to provide the foundation of a whole new generation of; C/C++/Objective C development tools, and we intend for it to be; production quality.; Being a production quality compiler means many things: it means being high; performance, being solid and (relatively) bug free, and it means eventually; being used and depended on by a broad range of people. While we are still in; the early development stages, we strongly believe that this will become a; reality. A simple and hackable code base. Our goal is to make it possible for anyone with a basic understanding; of compilers and working knowledge of the C/C++/ObjC languages to understand and; extend the clang source base. A large part of this falls out of our decision to; make the AST mirror the languages as closely as possible: you have your friendly; if statement, for statement, parenthesis expression, structs, unions, etc, all; represented in a simple and explicit way.; In addition to a simple design, we work to make the source base approachable; by commenting it well, including citations of the language standards where; appropriate, and designing the code for simplicity. Beyond that, clang offers; a set of AST dumpers, printers, and visualizers that make it easy to put code in; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html:11696,perform,performance,11696,interpreter/llvm-project/clang/www/features.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html,2,['perform'],['performance']
Performance,"e; it may make more sense to have the definitions added for you on demand by a; *definition generator*.A definition generator is an object that can be attached; to a JITDylib, receiving a callback whenever a lookup within that JITDylib fails; to find one or more symbols. The definition generator is given a chance to; produce a definition of the missing symbol(s) before the lookup proceeds. ORC provides the ``DynamicLibrarySearchGenerator`` utility for reflecting symbols; from the process (or a specific dynamic library) for you. For example, to reflect; the whole interface of a runtime library:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; auto &JD = ES.createJITDylib(""main"");. if (auto DLSGOrErr =; DynamicLibrarySearchGenerator::Load(""/path/to/lib""; DL.getGlobalPrefix())); JD.addGenerator(std::move(*DLSGOrErr);; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symboli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:33544,load,loadModule,33544,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loadModule']
Performance,"e; linker. The ``Module`` class keeps track of a list of :ref:`Function; <c_Function>`\ s, a list of GlobalVariable_\ s, and a SymbolTable_.; Additionally, it contains a few helpful member functions that try to make common; operations easy. .. _m_Module:. Important Public Members of the ``Module`` class; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. * ``Module::Module(std::string name = """")``. Constructing a Module_ is easy. You can optionally provide a name for it; (probably based on the name of the translation unit). * | ``Module::iterator`` - Typedef for function list iterator; | ``Module::const_iterator`` - Typedef for const_iterator.; | ``begin()``, ``end()``, ``size()``, ``empty()``. These are forwarding methods that make it easy to access the contents of a; ``Module`` object's :ref:`Function <c_Function>` list. * ``Module::FunctionListType &getFunctionList()``. Returns the list of :ref:`Function <c_Function>`\ s. This is necessary to use; when you need to update the list or perform a complex action that doesn't have; a forwarding method. ----------------. * | ``Module::global_iterator`` - Typedef for global variable list iterator; | ``Module::const_global_iterator`` - Typedef for const_iterator.; | ``Module::insertGlobalVariable()`` - Inserts a global variable to the list.; | ``Module::removeGlobalVariable()`` - Removes a global variable from the list.; | ``Module::eraseGlobalVariable()`` - Removes a global variable from the list and deletes it.; | ``global_begin()``, ``global_end()``, ``global_size()``, ``global_empty()``. These are forwarding methods that make it easy to access the contents of a; ``Module`` object's GlobalVariable_ list. ----------------. * ``SymbolTable *getSymbolTable()``. Return a reference to the SymbolTable_ for this ``Module``. ----------------. * ``Function *getFunction(StringRef Name) const``. Look up the specified function in the ``Module`` SymbolTable_. If it does not; exist, return ``null``. * ``FunctionCallee getOrInsertFunct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:138841,perform,perform,138841,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['perform']
Performance,"e; remainder state follows the syntax {State1;State2} where State1; and State2 are the state names of the two spitting categories. Additional; functionality exists to work with multiple prototype p.d.f.s simultaneously. ; Improved infrastructure for caching p.d.f and functions. The infrastructure that exists for caching p.d.f.s, i.e. p.d.f that precalculate their value; for all observable values at one and cache those in a histogram that is returned as p.d.f shape; (with optional interpolation), has been expanded. This infrastructure comprises RooAbsCached; the base class for all caching p.d.fs, RooAbsSelfCachedPdf a base class for end-user; caching p.d.f implementations that simply cache the result of evaluate() and RooCachedPdf; that can wrap and cache any input p.d.f specified in its constructor. . By default a p.d.f is sampled and cached in all observables in any; given use context, with no need to specify what those are in advance.; The internal code has also been changed such that all cache; histograms now store pre-normalized p.d.f, which is more efficient; than 'raw' p.d.f histograms that are explicitly post-normalized; through integration. Multiple different use cases (e.g. definitions; of what are observables vs parameters) can be cached; simultaneously. Now it is also possible to specify that p.d.f.s; should be sampled and cached in one or more parameter dimensions; in addition to the automatically determined set of observables.; as well. Also a complete new line of classes with similar functionality has been added inheriting from RooAbsReal.; These are RooAbsCachedReal,RooAbsSelfCachedReal and RooCachedReal. A newly; added class RooHistFunc presents these shapes and is capable of handling negative entries. New PDF error handling structure. New infrastructure has been put into place to propagate and process p.d.f evaluation errors during fitting.; Previously evaluation errors were marked with a zero p.d.f value and propagated as a special condition; in Roo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html:14238,cache,cache,14238,roofit/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v520/index.html,2,['cache'],['cache']
Performance,"e; stale data. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkmcnt(0) &; vm/vscnt(0). - If CU wavefront execution; mode, omit vm/vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always ge",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:350555,load,loads,350555,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"e; stubs). Materialization is the blanket term for any actions (compiling, linking,; splatting bits, registering with runtimes, etc.) that are required to generate a; symbol definition that is safe to call or access. As each materializer completes its work it notifies the JITDylib, which in turn; notifies any query objects that are waiting on the newly materialized; definitions. Each query object maintains a count of the number of symbols that; it is still waiting on, and once this count reaches zero the query object calls; the query handler with a *SymbolMap* (a map of symbol names to addresses); describing the result. If any symbol fails to materialize the query immediately; calls the query handler with an error. The collected materialization units are sent to the ExecutionSession to be; dispatched, and the dispatch behavior can be set by the client. By default each; materializer is run on the calling thread. Clients are free to create new; threads to run materializers, or to send the work to a work queue for a thread; pool (this is what LLJIT/LLLazyJIT do). Top Level APIs; ==============. Many of ORC's top-level APIs are visible in the example above:. - *ExecutionSession* represents the JIT'd program and provides context for the; JIT: It contains the JITDylibs, error reporting mechanisms, and dispatches the; materializers. - *JITDylibs* provide the symbol tables. - *Layers* (ObjLinkingLayer and CXXLayer) are wrappers around compilers and; allow clients to add uncompiled program representations supported by those; compilers to JITDylibs. - *ResourceTrackers* allow you to remove code. Several other important APIs are used explicitly. JIT clients need not be aware; of them, but Layer authors will use them:. - *MaterializationUnit* - When XXXLayer::add is invoked it wraps the given; program representation (in this example, C++ source) in a MaterializationUnit,; which is then stored in the JITDylib. MaterializationUnits are responsible for; describing the definitions ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:11183,queue,queue,11183,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['queue'],['queue']
Performance,"e] ptr <pointer>, <ty> <cmp>, <ty> <new> [syncscope(""<target-scope>"")] <success ordering> <failure ordering>[, align <alignment>] ; yields { ty, i1 }. Overview:; """""""""""""""""". The '``cmpxchg``' instruction is used to atomically modify memory. It; loads a value in memory and compares it to a given value. If they are; equal, it tries to store a new value into the memory. Arguments:; """""""""""""""""""". There are three arguments to the '``cmpxchg``' instruction: an address; to operate on, a value to compare to the value currently be at that; address, and a new value to place at that address if the compared values; are equal. The type of '<cmp>' must be an integer or pointer type whose; bit width is a power of two greater than or equal to eight and less; than or equal to a target-specific size limit. '<cmp>' and '<new>' must; have the same type, and the type of '<pointer>' must be a pointer to; that type. If the ``cmpxchg`` is marked as ``volatile``, then the; optimizer is not allowed to modify the number or order of execution of; this ``cmpxchg`` with other :ref:`volatile operations <volatile>`. The success and failure :ref:`ordering <ordering>` arguments specify how this; ``cmpxchg`` synchronizes with other atomic operations. Both ordering parameters; must be at least ``monotonic``, the failure ordering cannot be either; ``release`` or ``acq_rel``. A ``cmpxchg`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. The pointer passed into cmpxchg must have alignment greater ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:426590,optimiz,optimizer,426590,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"e` and `offset` are fixed immediate values. `scale` must be `1`, `2`,; `4`, or `8`, and `offset` can be any 32-bit sign extended value. The exact; computation performed to find the address is then: `%base + (scale * %index) +; offset` under 64-bit 2's complement modular arithmetic. One issue with this approach is that, after hardening, the `%base + (scale *; %index)` subexpression will compute a value near zero (`-1 + (scale * -1)`) and; then a large, positive `offset` will index into memory within the first two; gigabytes of address space. While these offsets are not attacker controlled,; the attacker could chose to attack a load which happens to have the desired; offset and then successfully read memory in that region. This significantly; raises the burden on the attacker and limits the scope of attack but does not; eliminate it. To fully close the attack we must work with the operating system; to preclude mapping memory in the low two gigabytes of address space. ###### 64-bit load checking instructions. We can use the following instruction sequences to check loads. We set up `%r8`; in these examples to hold the special value of `-1` which will be `cmov`ed over; `%rax` in misspeculated paths. Single register addressing mode:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rsi # Mask the pointer if misspeculating.; movl (%rsi), %edi; ```. Two register addressing mode:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rsi # Mask the pointer if misspeculating.; orq %rax, %rcx # Mask the index if misspeculating.; movl (%rsi,%rcx), %edi; ```. This will result in a negative address near zero or in `offset` wrapping the; address space back to a small positive address. Small, negative addresses will; fault in user-mode for most operating systems, but targets which need the high; address space to be user accessible may need to adjust the exact sequence used; above. Additio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:29149,load,load,29149,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"e`` can cast to ``__single`` including when they have different; pointee types as long as it is allowed in the underlying C standard.; ``-fbounds-safety`` doesn't guarantee type safety. * ``__bidi_indexable`` and ``__indexable`` can cast to ``__single``. The; compiler may insert run-time checks to ensure the pointer has at least a; single element or is a null pointer. * ``__bidi_indexable`` casts to ``__indexable`` if the pointer does not have an; underflow. The compiler may insert run-time checks to ensure the pointer is; not below the lower bound. * ``__indexable`` casts to ``__bidi_indexable``. The resulting; ``__bidi_indexable`` gets the lower bound same as the pointer value. * A type conversion may involve both a bitcast and a bounds annotation cast. For; example, casting from ``int *__bidi_indexable`` to ``char *__single`` involve; a bitcast (``int *`` to ``char *``) and a bounds annotation cast; (``__bidi_indexable`` to ``__single``). In this case, the compiler performs; the bitcast and then converts the bounds annotation. This means, ``int; *__bidi_indexable`` will be converted to ``char *__bidi_indexable`` and then; to ``char *__single``. * ``__terminated_by(T)`` cannot cast to any safe pointer type without the same; ``__terminated_by(T)`` attribute. To perform the cast, programmers can use an; intrinsic function such as ``__unsafe_terminated_by_to_indexable(P)`` to force; the conversion. * ``__terminated_by(T)`` can cast to ``__unsafe_indexable``. * Any type without ``__terminated_by(T)`` cannot cast to ``__terminated_by(T)``; without explicitly using an intrinsic function to allow it. + ``__unsafe_terminated_by_from_indexable(T, PTR [, PTR_TO_TERM])`` casts any; safe pointer PTR to a ``__terminated_by(T)`` pointer. ``PTR_TO_TERM`` is an; optional argument where the programmer can provide the exact location of the; terminator. With this argument, the function can skip reading the entire; array in order to locate the end of the pointer (or the upper bound).",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst:43873,perform,performs,43873,interpreter/llvm-project/clang/docs/BoundsSafety.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/BoundsSafety.rst,1,['perform'],['performs']
Performance,"e`` must be equivalent to sending ``release`` when; one of the autorelease pools currently in scope is popped. It may not throw an; exception. When the semantics call for performing one of these operations on a retainable; object pointer, if that pointer is ``null`` then the effect is a no-op. All of the semantics described in this document are subject to additional; :ref:`optimization rules <arc.optimization>` which permit the removal or; optimization of operations based on local knowledge of data flow. The; semantics describe the high-level behaviors that the compiler implements, not; an exact sequence of operations that a program will be compiled into. .. _arc.objects.operands:. Retainable object pointers as operands and arguments; ----------------------------------------------------. In general, ARC does not perform retain or release operations when simply using; a retainable object pointer as an operand within an expression. This includes:. * loading a retainable pointer from an object with non-weak :ref:`ownership; <arc.ownership>`,; * passing a retainable pointer as an argument to a function or method, and; * receiving a retainable pointer as the result of a function or method call. .. admonition:: Rationale. While this might seem uncontroversial, it is actually unsafe when multiple; expressions are evaluated in ""parallel"", as with binary operators and calls,; because (for example) one expression might load from an object while another; writes to it. However, C and C++ already call this undefined behavior; because the evaluations are unsequenced, and ARC simply exploits that here to; avoid needing to retain arguments across a large number of calls. The remainder of this section describes exceptions to these rules, how those; exceptions are detected, and what those exceptions imply semantically. .. _arc.objects.operands.consumed:. Consumed parameters; ^^^^^^^^^^^^^^^^^^^. A function or method parameter of retainable object pointer type may be marked; as :arc-te",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:15652,load,loading,15652,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['loading']
Performance,"eached a ``llvm.coro.suspend.retcon`` has undefined behavior. The remainder of this section describes the behavior under switched-resume; lowering. This intrinsic is lowered when a coroutine is split into; the start, resume and destroy parts. In the start part, it is a no-op,; in resume and destroy parts, it is replaced with `ret void` instruction and; the rest of the block containing `coro.end` instruction is discarded.; In landing pads it is replaced with an appropriate instruction to unwind to; caller. The handling of coro.end differs depending on whether the target is; using landingpad or WinEH exception model. For landingpad based exception model, it is expected that frontend uses the; `coro.end`_ intrinsic as follows:. .. code-block:: llvm. ehcleanup:; %InResumePart = call i1 @llvm.coro.end(ptr null, i1 true, token none); br i1 %InResumePart, label %eh.resume, label %cleanup.cont. cleanup.cont:; ; rest of the cleanup. eh.resume:; %exn = load ptr, ptr %exn.slot, align 8; %sel = load i32, ptr %ehselector.slot, align 4; %lpad.val = insertvalue { ptr, i32 } undef, ptr %exn, 0; %lpad.val29 = insertvalue { ptr, i32 } %lpad.val, i32 %sel, 1; resume { ptr, i32 } %lpad.val29. The `CoroSpit` pass replaces `coro.end` with ``True`` in the resume functions,; thus leading to immediate unwind to the caller, whereas in start function it; is replaced with ``False``, thus allowing to proceed to the rest of the cleanup; code that is only needed during initial invocation of the coroutine. For Windows Exception handling model, a frontend should attach a funclet bundle; referring to an enclosing cleanuppad as follows:. .. code-block:: llvm. ehcleanup:; %tok = cleanuppad within none []; %unused = call i1 @llvm.coro.end(ptr null, i1 true, token none) [ ""funclet""(token %tok) ]; cleanupret from %tok unwind label %RestOfTheCleanup. The `CoroSplit` pass, if the funclet bundle is present, will insert; ``cleanupret from %tok unwind to caller`` before; the `coro.end`_ intrinsic and will rem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:45014,load,load,45014,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['load'],['load']
Performance,"ead *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx90a:. Memory Model GFX90A; +++++++++++++++++++. For GFX90A:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; glo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:234811,perform,performed,234811,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"ead *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; - agent all instructions even; - system for OpenCL.*; ============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx942:. Memory Model GFX942; +++++++++++++++++++. For GFX942:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; glo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:284959,perform,performed,284959,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"ead when compiling many small files. The driver doesn't do much; work compared to a compilation, but we have tried to keep it as; efficient as possible by following a few simple principles:. - Avoid memory allocation and string copying when possible.; - Don't parse arguments more than once.; - Provide a few simple interfaces for efficiently searching arguments. Simple; ------. Finally, the driver was designed to be ""as simple as possible"", given; the other goals. Notably, trying to be completely compatible with the; gcc driver adds a significant amount of complexity. However, the design; of the driver attempts to mitigate this complexity by dividing the; process into a number of independent stages instead of a single; monolithic task. Internal Design and Implementation; ==================================. .. contents::; :local:; :depth: 1. Internals Introduction; ----------------------. In order to satisfy the stated goals, the driver was designed to; completely subsume the functionality of the gcc executable; that is, the; driver should not need to delegate to gcc to perform subtasks. On; Darwin, this implies that the Clang driver also subsumes the gcc; driver-driver, which is used to implement support for building universal; images (binaries and object files). This also implies that the driver; should be able to call the language specific compilers (e.g. cc1); directly, which means that it must have enough information to forward; command line arguments to child processes correctly. Design Overview; ---------------. The diagram below shows the significant components of the driver; architecture and how they relate to one another. The orange components; represent concrete data structures built by the driver, the green; components indicate conceptually distinct stages which manipulate these; data structures, and the blue components are important helper classes. .. image:: DriverArchitecture.png; :align: center; :alt: Driver Architecture Diagram. Driver Stages; --------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:2888,perform,perform,2888,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['perform'],['perform']
Performance,"eam; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a state that would never have existed; in the execution of the unoptimized program, given the same input. Doing so; risks misleading the developer by reporting a state that does not exist,; damaging their understanding of the optimized program and undermining their; trust in the debugger. Sometimes perfectly preserving variable locations is not possible, often when a; redundant calculation is optimized out. In such cases, a ``llvm.dbg.value``; with operand ``poison`` should be used, to terminate earlier variable locations; and let the debugger present ``optimized out`` to the developer. Withholding; these potentially stale variable values from the developer diminishes the; amount of available debug information, but increases the reliability of the; remaining information. To illustrate some potential issues, consider the following example:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); br i1 %cond, label %truebr, label %falsebr; truebr:; %tval = add i32 %bar, 1; call @llvm.dbg.value(metadata i32 %tval, metadata !1, metadata !2); %g1 = call i32 @gazonk(); br label %exit; falsebr:; %fval = add i32 %bar, 2; call @llvm.dbg.value(metadata i32 %fval, metadata !1, metadata !2); %g2 = call i32 @gazonk(); br label %exit; exit:; %merge = phi [ %tval, %truebr ], [ %fval, %falsebr ]; %g = phi [ %g1, %truebr ], [ %g2, %falsebr ]; call @llvm.dbg.value(metadata i32 %merge, metadata !1, metadata !2); call @llvm.dbg.value(m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:19628,optimiz,optimized,19628,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"eans automatic convergence check is turned off). ConvergenceTests No -1  Number of steps (without improvement) required for convergence (<0 means automatic convergence check is turned off). UseRegulator No False  Use regulator to avoid over-training. UpdateLimit No 10000  Maximum times of regulator update. CalculateErrors No False  Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an MVA value. WeightRange No 1  Take the events for the estimator calculations from small deviations from the desired value to large deviations only over the weight range. Configuration options for MVA method :. Configuration options reference for MVA method: Cuts. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). FitMethod No GA GA, SA, MC, MCEvents, MINUIT, EventScan Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated). EffMethod No EffSel EffSel, EffPDF Selection Method. CutRangeMin Yes -1  Minimum of allowed cut range (set per variable). CutRangeMax Yes -1  Maximum of allowed cut range (set per variable). VarProp Yes NotEnforced ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:24588,perform,performed,24588,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"earFitter), for solving linear least square fits.; - \ref Minuit2Page ""Minuit2"": new object-oriented implementation of MINUIT, with the same minimization algorithms (such as Migrad or Simplex). In addition it provides a new implementation of the Fumili algorithm, a specialized method for finding the minimum of a standard least square or likelihood functions.; - **Fumili**: library providing the implementation of the original Fumili fitting algorithm (class TFumili). - **Linear algebra**. Two libraries are contained in %ROOT for describing linear algebra matrices and vector classes:. - Matrix: general matrix package providing matrix classes (TMatrixD and TMatrixF) and vector classes (TVectorD and TVectorF) and the complete environment to perform linear algebra calculations, like equation solving and eigenvalue decompositions.; - \ref SMatrixPage ""SMatrix"": package optimized for high performances matrix and vector computations of small and fixed size. It is based on expression templates to achieve an high level optimization. - **Physics Vectors**: Classes for describing vectors in 2, 3 and 4 dimensions (relativistic vectors) and their rotation and transformation algorithms. Two package exist in %ROOT:. - Physics: library with the TVector3 and TLorentzVector classes.; - GenVector: new library providing generic class templates for modeling the vectors. See the \ref GenVector ""GenVector"" page. - \ref Unuran ""UNURAN"": Package with universal algorithms for generating non-uniform pseudo-random numbers, from a large classes of continuous or discrete distributions in one or multi-dimensions. - **Foam** Multi-dimensional general purpose Monte Carlo event generator (and integrator). It generates randomly points (vectors) according to an arbitrary probability distribution in n dimensions. - **FFTW** Library with implementation of the fast Fourier transform (FFT) using the FFTW package. It requires a previous installation of [FFTW](http://www.fftw.org). - **MLP** Library with the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/index.md:3016,optimiz,optimization,3016,math/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/index.md,1,['optimiz'],['optimization']
Performance,"earch=product%3Aclang+component%3A-New%2BBugs%2CAST%2CBasic%2CDriver%2CHeaders%2CLLVM%2BCodeGen%2Cparser%2Cpreprocessor%2CSemantic%2BAnalyzer>`_; for known existing bugs (FIXME: Is there a section for bug-reporting; guidelines somewhere?). Intentionally unsupported GCC extensions; ----------------------------------------. - clang does not support the gcc extension that allows variable-length; arrays in structures. This is for a few reasons: one, it is tricky to; implement, two, the extension is completely undocumented, and three,; the extension appears to be rarely used. Note that clang *does*; support flexible array members (arrays with a zero or unspecified; size at the end of a structure).; - GCC accepts many expression forms that are not valid integer constant; expressions in bit-field widths, enumerator constants, case labels,; and in array bounds at global scope. Clang also accepts additional; expression forms in these contexts, but constructs that GCC accepts due to; simplifications GCC performs while parsing, such as ``x - x`` (where ``x`` is a; variable) will likely never be accepted by Clang.; - clang does not support ``__builtin_apply`` and friends; this extension; is extremely obscure and difficult to implement reliably. .. _c_ms:. Microsoft extensions; --------------------. clang has support for many extensions from Microsoft Visual C++. To enable these; extensions, use the ``-fms-extensions`` command-line option. This is the default; for Windows targets. Clang does not implement every pragma or declspec provided; by MSVC, but the popular ones, such as ``__declspec(dllexport)`` and ``#pragma; comment(lib)`` are well supported. clang has a ``-fms-compatibility`` flag that makes clang accept enough; invalid C++ to be able to parse most Microsoft headers. For example, it; allows `unqualified lookup of dependent base class members; <https://clang.llvm.org/compatibility.html#dep_lookup_bases>`_, which is; a common compatibility issue with clang. This flag is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:136235,perform,performs,136235,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performs']
Performance,"eared at the; top of the source file.; * ``fast`` Behaves identically to specifying both ``-ffast-math`` and; ``ffp-contract=fast``. Note: If your command line specifies multiple instances; of the ``-ffp-model`` option, or if your command line option specifies; ``-ffp-model`` and later on the command line selects a floating point; option that has the effect of negating part of the ``ffp-model`` that; has been selected, then the compiler will issue a diagnostic warning; that the override has occurred. .. option:: -ffp-exception-behavior=<value>. Specify the floating-point exception behavior. Valid values are: ``ignore``, ``maytrap``, and ``strict``.; The default value is ``ignore``. Details:. * ``ignore`` The compiler assumes that the exception status flags will not be read and that floating point exceptions will be masked.; * ``maytrap`` The compiler avoids transformations that may raise exceptions that would not have been raised by the original code. Constant folding performed by the compiler is exempt from this option.; * ``strict`` The compiler ensures that all transformations strictly preserve the floating point exception semantics of the original code. .. option:: -ffp-eval-method=<value>. Specify the floating-point evaluation method for intermediate results within; a single expression of the code. Valid values are: ``source``, ``double``, and ``extended``.; For 64-bit targets, the default value is ``source``. For 32-bit x86 targets; however, in the case of NETBSD 6.99.26 and under, the default value is; ``double``; in the case of NETBSD greater than 6.99.26, with NoSSE, the; default value is ``extended``, with SSE the default value is ``source``.; Details:. * ``source`` The compiler uses the floating-point type declared in the source program as the evaluation method.; * ``double`` The compiler uses ``double`` as the floating-point evaluation method for all float expressions of type that is narrower than ``double``.; * ``extended`` The compiler uses ``long doubl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:63886,perform,performed,63886,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performed']
Performance,"eate miscompiles if mixing of constrained and normal; operations is done. The correct way to mix constrained and less constrained; operations is to use the rounding mode and exception handling metadata to; mark constrained intrinsics as having LLVM's default behavior. Each of these intrinsics corresponds to a normal floating-point operation. The; data arguments and the return value are the same as the corresponding FP; operation. The rounding mode argument is a metadata string specifying what; assumptions, if any, the optimizer can make when transforming constant; values. Some constrained FP intrinsics omit this argument. If required; by the intrinsic, this argument must be one of the following strings:. ::. ""round.dynamic""; ""round.tonearest""; ""round.downward""; ""round.upward""; ""round.towardzero""; ""round.tonearestaway"". If this argument is ""round.dynamic"" optimization passes must assume that the; rounding mode is unknown and may change at runtime. No transformations that; depend on rounding mode may be performed in this case. The other possible values for the rounding mode argument correspond to the; similarly named IEEE rounding modes. If the argument is any of these values; optimization passes may perform transformations as long as they are consistent; with the specified rounding mode. For example, 'x-0'->'x' is not a valid transformation if the rounding mode is; ""round.downward"" or ""round.dynamic"" because if the value of 'x' is +0 then; 'x-0' should evaluate to '-0' when rounding downward. However, this; transformation is legal for all other rounding modes. For values other than ""round.dynamic"" optimization passes may assume that the; actual runtime rounding mode (as defined in a target-specific manner) matches; the specified rounding mode, but this is not guaranteed. Using a specific; non-dynamic rounding mode which does not match the actual rounding mode at; runtime results in undefined behavior. The exception behavior argument is a metadata string describing th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:868866,perform,performed,868866,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"eature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma clang optimize on. If no ``on`` is found to close an ``off`` region, the end of the region is the; end of the compilation unit. Note that a stray ``#pragma clang optimize on`` does not selectively enable; additional optimizations when compiling at low optimization levels. This feature; can only be used to selectively disable optimizations",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:159022,optimiz,optimization,159022,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"eatures;; features.Set(ROOT::Experimental::EIOFeatures::kGenerateOffsetMap);; ttree_ref.SetIOFeatures(features);; ```; - Added `GetAutoSave()` and `SetAutoSave()` methods to `TBufferMerger`, to allow; it to accumulate several buffers in memory before merging, to reduce the; amount of compression work done due to `TTree` metadata. - Added a non-blocking callback mechanism to `TBufferMerger` to allow users to; control the rate at which data is pushed into the merging queue. The callback; mechanism can be used, for example, to launch tasks asynchronously whenever a; buffer is done processing. ## TTree Libraries. - Resolved O(N^2) scaling problem in ```TTree::Draw()``` observed when a branch that contains a; large TClonesArray where each element contains another small vector container.; - `TTree::TTree()` now takes the `TDirectory*` that the tree should be constructed in.; Defaults to `gDirectory`, i.e. the default behavior did not change.; - To prepare for multi-threaded workflows, a preloading and retaining clusters feature is introduced.; This change will prevent additional reads from occurring when reading events out of sequence.; By setting TTree::SetClusterPrefetch(), an entire clusters will be loaded into memory, rather than single baskets.; By setting the MaxVirtualSize of the tree to a negative value, previous clusters will be retained; (the absolute value of MaxVirtualSize indicates how many additional clusters will be kept in memory).; - Added ```TBranchProxy::GetEntries``` to support leaflist variable size array and added ```TBranchProxy::GetArrayLength```.; - In ```TBranch::Streamer``` insured that we never steam already basket already written to disk. ### TDataFrame. #### New features; - Add `Alias`, a facility to specify an alternative name for a given column: `auto histo = mytdf.Alias(""myAlias"", ""myColumn"").Histo1D(""myAlias"");`. Especially useful for pyROOT users to deal with column names that are not valid C++ identifiers (e.g. `Filter(""1branch > 0"") -->",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:9862,multi-thread,multi-threaded,9862,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['multi-thread'],['multi-threaded']
Performance,"ebug symbols; usually this means; giving the `-g` option to compiler. ``` {.cpp}; root[] .L MyScript.C++O; ```. will compile `MyScript.C` with optimizations; usually this means; giving the `-O` option to compiler. The syntax:. ``` {.cpp}; root[] .L MyScript.C++; ```. is using the default optimization level. The initial default is to; compile with the same level of optimization as the root executable; itself. The default can be changed by:. ``` {.cpp}; root[] gSystem->SetAclicMode(TSystem::kDebug);; root[] gSystem->SetAclicMode(TSystem::kOpt);; ```. Note that the commands:. ``` {.cpp}; root[] .L MyScript.C+g; root[] .L MyScript.C+O; ```. respectively compile `MyScript.C` with debug and optimization if the; library does not exist yet; they will not change the debug and the; optimization level if the library already exist and it is up to date.; To use ACLiC from compiled code or from inside another macro, we; recommend using `gROOT->ProcessLine()`. For; example, in one script you can use ACLiC to compile and load another; script. ``` {.cpp}; gROOT->ProcessLine("".L MyScript.C+""); gROOT->ProcessLine("".L MyScript.C++""); ```. ### Setting the Include Path. You can get the include path by typing:. ``` {.cpp}; root[] .include; ```. You can append to the include path by typing:. ``` {.cpp}; root[] .include $HOME/mypackage/include; ```. In a script you can append to the include path:. ``` {.cpp}; gSystem->AddIncludePath("" -I$HOME/mypackage/include ""); ```. You can also overwrite the existing include path:. ``` {.cpp}; gSystem->SetIncludePath("" -I$HOME/mypackage/include ""); ```. The `$ROOTSYS/include` directory is automatically appended to the; include path, so you do not have to worry about including it. To add; library that should be used during linking of the shared library use; something like:. ``` {.cpp}; gSystem->AddLinkedLibs(""-L/my/path -lanylib"");; ```. This is especially useful for static libraries. For shared ones you; can also simply load them before trying to compile",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:16265,load,load,16265,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['load']
Performance,"ecede the instructions they map; to (if such an instruction exists). For example, ``1 = MemoryDef(liveOnEntry)``; is a ``MemoryAccess`` (specifically, a ``MemoryDef``), and it describes the LLVM; instruction ``store i8 0, ptr %p3``. Other places in ``MemorySSA`` refer to this; particular ``MemoryDef`` as ``1`` (much like how one can refer to ``load i8, ptr; %p1`` in LLVM with ``%1``). Again, ``MemoryPhi``\ s don't correspond to any LLVM; Instruction, so the line directly below a ``MemoryPhi`` isn't special. Going from the top down:. - ``6 = MemoryPhi({entry,1},{if.end,4})`` notes that, when entering; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; =======",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7206,optimiz,optimization,7206,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimization']
Performance,"ecifically; recognizes the form of this intrinsic and the constant initializers it may; load from; if a loaded constant initializer is known to have the form; ``i32 trunc(x - %ptr)``, the intrinsic call is folded to ``x``. LLVM provides that the calculation of such a constant initializer will; not overflow at link time under the medium code model if ``x`` is an; ``unnamed_addr`` function. However, it does not provide this guarantee for; a constant initializer folded into a function body. This intrinsic can be; used to avoid the possibility of overflows when loading from such a constant. .. _llvm_sideeffect:. '``llvm.sideeffect``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.sideeffect() inaccessiblememonly nounwind willreturn. Overview:; """""""""""""""""". The ``llvm.sideeffect`` intrinsic doesn't perform any operation. Optimizers; treat it as having side effects, so it can be inserted into a loop to; indicate that the loop shouldn't be assumed to terminate (which could; potentially lead to the loop being optimized away entirely), even if it's; an infinite loop with no other side effects. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". This intrinsic actually does nothing, but optimizers must assume that it; has externally observable side effects. '``llvm.is.constant.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use llvm.is.constant with any argument type. ::. declare i1 @llvm.is.constant.i32(i32 %operand) nounwind memory(none); declare i1 @llvm.is.constant.f32(float %operand) nounwind memory(none); declare i1 @llvm.is.constant.TYPENAME(TYPE %operand) nounwind memory(none). Overview:; """""""""""""""""". The '``llvm.is.constant``' intrinsic will return true if the argument; is known to be a manifest compile-time constant. It is guaranteed to; fold to either true or false before generating machine code. Semantics:; """""""""""""""""""". This intrinsic generates no code. If its argumen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:952112,optimiz,optimized,952112,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"ecified in *LLVM_RUNTIME_DISTRIBUTION_COMPONENTS* are not; automatically added to any distribution. Instead, you must include the targets; explicitly in some *LLVM_<distribution>_DISTRIBUTION_COMPONENTS* list. By default, each target can appear in multiple distributions; a target will be; installed as part of all distributions it appears in, and it'll be exported by; the last distribution it appears in (the order of distributions is the order; they appear in *LLVM_DISTRIBUTIONS*). We also define some umbrella targets (e.g.; ``llvm-libraries`` to install all LLVM libraries); a target can appear in a; different distribution than its umbrella, in which case the target will be; exported by the distribution it appears in (and not the distribution its; umbrella appears in). Set *LLVM_STRICT_DISTRIBUTIONS* to ``On`` if you want to; enforce a target appearing in only one distribution and umbrella distributions; being consistent with target distributions. We strongly encourage looking at ``clang/cmake/caches/MultiDistributionExample.cmake``; as an example of configuring multiple distributions. Special Notes for Library-only Distributions; --------------------------------------------. One of the most powerful features of LLVM is its library-first design mentality; and the way you can compose a wide variety of tools using different portions of; LLVM. Even in this situation using *BUILD_SHARED_LIBS* is not supported. If you; want to distribute LLVM as a shared library for use in a tool, the recommended; method is using *LLVM_BUILD_LLVM_DYLIB*, and you can use *LLVM_DYLIB_COMPONENTS*; to configure which LLVM components are part of libLLVM.; Note: *LLVM_BUILD_LLVM_DYLIB* is not available on Windows. Options for Optimizing LLVM; ===========================. There are four main build optimizations that our CMake build system supports.; When performing a bootstrap build it is not beneficial to do anything other than; setting *CMAKE_BUILD_TYPE* to ``Release`` for the stage-1 compiler",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:6769,cache,caches,6769,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['cache'],['caches']
Performance,"ecified; by the integer value in the metadata node. The alignment must be a power of 2.; This is analogous to the ''align'' attribute on parameters and return values.; This metadata can only be applied to loads of a pointer type. If the returned; value is not appropriately aligned at runtime, a poison value is returned; instead. The optional ``!noundef`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a node with no entries. The existence of; ``!noundef`` metadata on the instruction tells the optimizer that the value; loaded is known to be :ref:`well defined <welldefinedvalues>`.; If the value isn't well defined, the behavior is undefined. If the ``!noundef``; metadata is combined with poison-generating metadata like ``!nonnull``,; violation of that metadata constraint will also result in undefined behavior. Semantics:; """""""""""""""""""". The location of memory pointed to is loaded. If the value being loaded; is of scalar type then the number of bytes read does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, loading an ``i24`` reads at most three bytes. When loading a; value of a type like ``i20`` with a size that is not an integral number; of bytes, the result is undefined if the value was not originally; written using a store of the same type.; If the value being loaded is of aggregate type, the bytes that correspond to; padding may be accessed but are ignored, because it is impossible to observe; padding from the loaded aggregate value.; If ``<pointer>`` is not a well-defined value, the behavior is undefined. Examples:; """""""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; store i32 3, ptr %ptr ; yields void; %val = load i32, ptr %ptr ; yields i32:val = i32 3. .. _i_store:. '``store``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. store [volatile] <ty> <value>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.group !<empty_node>] ; yields v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:418179,load,loaded,418179,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"ecifies the number of bytes to be used when; copying the variable to the output buffer. The line below describes; `ntrack` to be written as a 16-bit integer (rather than a 32-bit; integer). ``` {.cpp}; ""ntrack/I2""; ```. With this Branch method, you can also add a leaf that holds an entire; array of variables. To add an array of floats use the `f[n]` notation; when describing the leaf. ``` {.cpp}; Float_t f[10];; tree->Branch(""fBranch"",f,""f[10]/F"");; ```. You can also add an array of variable length:. ``` {.cpp}; {; TFile *f = new TFile(""peter.root"",""recreate"");; Int_t nPhot;; Float_t E[500];; TTree* nEmcPhotons = new TTree(""nEmcPhotons"",""EMC Photons"");; nEmcPhotons->Branch(""nPhot"",&nPhot,""nPhot/I"");; nEmcPhotons->Branch(""E"",E,""E[nPhot]/F"");; }; ```. See ""Example 2: A Tree with a C Structure"" below; (`$ROOTSYS/tutorials/tree/tree2.C`) and `staff.C` at the beginning of; this chapter. ## Adding a TBranch to Hold an Object. To write a branch to hold an event object, we need to load the; definition of the `Event` class, which is in `$ROOTSYS/test/libEvent.so`; (if it doesn't exist type make in `$ROOTSYS/test`). An object can be; saved in a tree if a ROOT dictionary for its class has been generated; and loaded. ``` {.cpp}; root[] .L libEvent.so; ```. First, we need to open a file and create a tree. ``` {.cpp}; root[] TFile *f = new TFile(""AFile.root"",""RECREATE""); root[] TTree *tree = new TTree(""T"",""A Root Tree""); ```. We need to create a pointer to an `Event` object that will be used as a; reference in the `TTree::Branch` method. Then we create a branch; with the `TTree::Branch` method. ``` {.cpp}; root[] Event *event = new Event(); root[] tree->Branch(""EventBranch"",""Event"",&event,32000,99); ```. To add a branch to hold an object we use the signature above. The first; parameter is the name of the branch. The second parameter is the name of; the class of the object to be stored. The third parameter is the address; of a pointer to the object to be stored. Note that it is an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:23177,load,load,23177,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"ecify any custom filter (e.g. *.png) in the filter combo box; Enable the new (flat) button style. This can be enabled/disabled via the GUI.Style entry in $ROOTSYS/etc/system.rootrc (or in a user defined $HOME/.rootrc); Create special icons for symlinks (shortcuts) in the browser (add a small arrow on bottom left corner of the original icon). TGFileDialog. Implemented the wish #78935: Longer ""File of type:"" selector is wanted (make more combo box entries visible); Enable the new (flat) button style. This can be enabled/disabled via the GUI.Style entry in $ROOTSYS/etc/system.rootrc (or in a user defined $HOME/.rootrc). TGFSContainer. The shortcuts are now working on Windows. TGColorDialog, TGFontDialog, TGTextEditDialogs. Several improvements in the layout when increasing the font size. TGTextEditor. Added a ""Close"" menu entry; Properly ask the user to save the currently opened file (if modified) when trying to open a new file; Moved the IsSaved() part of the code in the LoadFile() method, to make sure it works also when the text editor is used as a plugin in the browser; Change the text highlighing color; Cleanup the text when quitting root (avoid potential crash on Linux). TGFrame. Allow to override CTRL+S behavior by using the TGMainFrame::BindKey() function. TVirtualDragManager. Renamed TVirtualDragManager::GetDragType() to TVirtualDragManager::GetEDragType(), to avoid potential clash between two classes (TGFrame and TVirtualDragManager) having both GetDragType method with different return types. And they are both inherited by one class (TGuiBldDragManager) which doesn't define GetDragType. TGSlider. Added mouse wheel handling. TGToolTip. Properly set the text color of the tooltip label, using the value of Gui.TooltipForegroundColor in $ROOTSYS/etc/system.rootrc (or in a user defined $HOME/.rootrc). GUIHtml; TGHtmlBrowser. Only add non-empty strings (urls) in the combo box, to avoid empty entries; Enable the new (flat) button style. This can be enabled/disabled vi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/doc/v530/index.html:3262,Load,LoadFile,3262,gui/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/doc/v530/index.html,1,['Load'],['LoadFile']
Performance,"ecise lifetime semantics and, at some point after ``T`` but; before the next store to ``S``, the computation history features a; load from ``S`` and in some way depends on the value loaded, or. * ``X`` is a value described as being released at the end of the; current full-expression and, at some point after ``T`` but before; the end of the full-expression, the computation history depends; on that value. .. admonition:: Rationale. The intent of the second rule is to say that objects held in normal; ``__strong`` local variables may be released as soon as the value in; the variable is no longer being used: either the variable stops; being used completely or a new value is stored in the variable. The intent of the third rule is to say that return values may be; released after they've been used. A computation history depends on a pointer value ``P`` if it:. * performs a pointer comparison with ``P``,; * loads from ``P``,; * stores to ``P``,; * depends on a pointer value ``Q`` derived via pointer arithmetic; from ``P`` (including an instance-variable or field access), or; * depends on a pointer value ``Q`` loaded from ``P``. Dependency applies only to values derived directly or indirectly from; a particular expression result and does not occur merely because a; separate pointer value dynamically aliases ``P``. Furthermore, this; dependency is not carried by values that are stored to objects. .. admonition:: Rationale. The restrictions on dependency are intended to make this analysis; feasible by an optimizer with only incomplete information about a; program. Essentially, dependence is carried to ""obvious"" uses of a; pointer. Merely passing a pointer argument to a function does not; itself cause dependence, but since generally the optimizer will not; be able to prove that the function doesn't depend on that parameter,; it will be forced to conservatively assume it does. Dependency propagates to values loaded from a pointer because those; values might be invalidated by deal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:79875,perform,performs,79875,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,3,"['load', 'perform']","['loaded', 'loads', 'performs']"
Performance,"ecision floating-point values. |; +-----------------------------+----------------------------------------------------------+; | ``[2 x [3 x [4 x i16]]]`` | 2x3x4 array of 16-bit integer values. |; +-----------------------------+----------------------------------------------------------+. There is no restriction on indexing beyond the end of the array implied; by a static type (though there are restrictions on indexing beyond the; bounds of an allocated object in some cases). This means that; single-dimension 'variable sized array' addressing can be implemented in; LLVM with a zero length array type. An implementation of 'pascal style; arrays' in LLVM could use the type ""``{ i32, [0 x float]}``"", for; example. .. _t_struct:. Structure Type; """""""""""""""""""""""""""". :Overview:. The structure type is used to represent a collection of data members; together in memory. The elements of a structure may be any type that has; a size. Structures in memory are accessed using '``load``' and '``store``' by; getting a pointer to a field with the '``getelementptr``' instruction.; Structures in registers are accessed using the '``extractvalue``' and; '``insertvalue``' instructions. Structures may optionally be ""packed"" structures, which indicate that; the alignment of the struct is one byte, and that there is no padding; between the elements. In non-packed structs, padding between field types; is inserted as defined by the DataLayout string in the module, which is; required to match what the underlying code generator expects. Structures can either be ""literal"" or ""identified"". A literal structure; is defined inline with other types (e.g. ``[2 x {i32, i32}]``) whereas; identified types are always defined at the top level with a name.; Literal types are uniqued by their contents and can never be recursive; or opaque since there is no way to write one. Identified types can be; recursive, can be opaqued, and are never uniqued. :Syntax:. ::. %T1 = type { <type list> } ; Identified normal struct ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:181436,load,load,181436,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"eck at the level of a single volume by using `option`=""`d`"" or; `option`=""`d<number>`"" to perform overlap checking by sampling the; volume with \<`number`\> random points (default 1 million). This; produces also a picture showing in red the overlapping region and; estimates the volume of the overlaps. An extrusion *A)* is declared in any of the following cases:. - At least one of the vertices of the daughter mesh representation is; outside the mother volume (in fact its shape) and having a safety; distance to the mother greater than the desired value;; - At least one of the mother vertices is contained also by one of its; daughters, in the same conditions. An overlap *B)* is declared if:. - At least one vertex of a positioned volume mesh is contained (having; a safety bigger than the accepted maximum value) by other positioned; volume inside the same container. The check is performed also by; inverting the candidates. The code is highly optimized to avoid checking candidates that are far; away in space by performing a fast check on their bounding boxes. Once; the checking tool is fired-up inside a volume or at top level, the list; of overlaps (visible as Illegal overlaps inside a TBrowser) held; by the manager class will be filled with TGeoOverlap objects; containing a full description of the detected overlaps. The list is; sorted in the decreasing order of the overlapping distance, extrusions; coming first. An overlap object name represents the full description of; the overlap, containing both candidate node names and a letter; (x-extrusion, o-overlap) representing the type. Double-clicking an; overlap item in a TBrowser produces a picture of the overlap; containing only the two overlapping nodes (one in blue and one in green); and having the critical vertices represented by red points. The picture; can be rotated/zoomed or drawn in X3d as any other view. Calling; gGeoManager->PrintOverlaps() prints the list of overlaps. \anchor GP03b; ### Graphical Checking Method",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:94726,optimiz,optimized,94726,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['optimiz', 'perform']","['optimized', 'performing']"
Performance,"eclarations as a string and JITs them (bindings; are not created until actual use).; The code is loaded into the global scope, thus any previously loaded code; is available from one ``cppdef`` call to the next, as are all standard; C++ headers that have been loaded through pre-compiled headers.; Example::. >>> cppyy.cppdef(r""""""\; ... void hello() {; ... std::cout << ""Hello, World!"" << std::endl;; ... }""""""); True; >>> cppyy.gbl.hello(); Hello, World!; >>> . * ``cppexec``: direct access to the interpreter.; This function accepts C++ statements as a string, JITs and executes them.; Just like ``cppdef``, execution is in the global scope and all previously; loaded code is available.; If the statements are declarations, the effect is the same as ``cppdef``,; but ``cppexec`` also accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); Hello, World!; True; >>> . * ``include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Files are located through include paths given to the Cling.; Example::. >>> cppyy.include(""vector"") # equivalent to ""#include <vector>""; True; >>> . * ``c_include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Name mangling is an important difference between C and C++ code.; The use of ``c_include`` instead of ``include`` prevents mangling. * ``load_library``: load compiled C++ into the interpreter.; This function takes the name of a shared library and loads it into current; process, exposing all external symbols to Cling.; Libraries are located through load paths given to Cling, either through the; ""-L"" compiler flag or the dynamic search path environment variable (system; dependent).; Any method that brings symbols into the process (including normal linking,; e.g. when embedding Python in a C++ application) is suit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:2041,load,load,2041,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['load']
Performance,"eclared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5843,load,load,5843,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['load']
Performance,"ecover=; <controlling-code-generation>` flag. Forward-Edge CFI for Virtual Calls; ==================================. This scheme checks that virtual calls take place using a vptr of the correct; dynamic type; that is, the dynamic type of the called object must be a; derived class of the static type of the object used to make the call.; This CFI scheme can be enabled on its own using ``-fsanitize=cfi-vcall``. For this scheme to work, all translation units containing the definition; of a virtual member function (whether inline or not), other than members; of :ref:`ignored <cfi-ignorelist>` types or types with public :doc:`LTO; visibility <LTOVisibility>`, must be compiled with ``-flto`` or ``-flto=thin``; enabled and be statically linked into the program. Performance; -----------. A performance overhead of less than 1% has been measured by running the; Dromaeo benchmark suite against an instrumented version of the Chromium; web browser. Another good performance benchmark for this mechanism is the; virtual-call-heavy SPEC 2006 xalancbmk. Note that this scheme has not yet been optimized for binary size; an increase; of up to 15% has been observed for Chromium. Bad Cast Checking; =================. This scheme checks that pointer casts are made to an object of the correct; dynamic type; that is, the dynamic type of the object must be a derived class; of the pointee type of the cast. The checks are currently only introduced; where the class being casted to is a polymorphic class. Bad casts are not in themselves control flow integrity violations, but they; can also create security vulnerabilities, and the implementation uses many; of the same mechanisms. There are two types of bad cast that may be forbidden: bad casts; from a base class to a derived class (which can be checked with; ``-fsanitize=cfi-derived-cast``), and bad casts from a pointer of; type ``void*`` or another unrelated type (which can be checked with; ``-fsanitize=cfi-unrelated-cast``). The difference betwe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:4968,perform,performance,4968,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,1,['perform'],['performance']
Performance,"ect file.; This type of reference (DW_FORM_ref_addr) is an offset from the; beginning of the .debug_info section of the target executable or shared; object file, or, for references within a supplementary object file, an; offset from the beginning of the local .debug_info section; it is; relocatable in a relocatable object file and frequently relocated in an; executable or shared object file. In the 32-bit DWARF format, this; offset is a 4-byte unsigned value; in the 64-bit DWARF format, it is an; 8-byte unsigned value (see; :ref:`amdgpu-dwarf-32-bit-and-64-bit-dwarf-formats`). *A debugging information entry that may be referenced by another; compilation unit using DW_FORM_ref_addr must have a global symbolic; name.*. *For a reference from one executable or shared object file to another,; the reference is resolved by the debugger to identify the executable or; shared object file and the offset into that file's* ``.debug_info``; *section in the same fashion as the run time loader, either when the; debug information is first read, or when the reference is used.*. A.7.7 DWARF Expressions; ~~~~~~~~~~~~~~~~~~~~~~~. .. note::. Rename DWARF Version 5 section 7.7 to reflect the unification of location; descriptions into DWARF expressions. A.7.7.1 Operation Expressions; +++++++++++++++++++++++++++++. .. note::. Rename DWARF Version 5 section 7.7.1 and delete section 7.7.2 to reflect the; unification of location descriptions into DWARF expressions. This augments DWARF Version 5 section 7.7.1 and Table 7.9, and adds a new; table describing vendor extension operations for ``DW_OP_LLVM_user``. A DWARF operation expression is stored in a block of contiguous bytes. The bytes; form a sequence of operations. Each operation is a 1-byte code that identifies; that operation, followed by zero or more bytes of additional data. The encoding; for the operation ``DW_OP_LLVM_user`` is described in; :ref:`amdgpu-dwarf-operation-encodings-table`, and the encoding of all; ``DW_OP_LLVM_user`` vend",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:218465,load,loader,218465,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['load'],['loader']
Performance,"ect metrics such as benchmark runtime, compilation time and; code size. The test-suite is divided into several directories:. - `SingleSource/`. Contains test programs that are only a single source file in size. A; subdirectory may contain several programs. - `MultiSource/`. Contains subdirectories which entire programs with multiple source files.; Large benchmarks and whole applications go here. - `MicroBenchmarks/`. Programs using the [google-benchmark](https://github.com/google/benchmark); library. The programs define functions that are run multiple times until the; measurement results are statistically significant. - `External/`. Contains descriptions and test data for code that cannot be directly; distributed with the test-suite. The most prominent members of this; directory are the SPEC CPU benchmark suites.; See [External Suites](#external-suites). - `Bitcode/`. These tests are mostly written in LLVM bitcode. - `CTMark/`. Contains symbolic links to other benchmarks forming a representative sample; for compilation performance measurements. ### Benchmarks. Every program can work as a correctness test. Some programs are unsuitable for; performance measurements. Setting the `TEST_SUITE_BENCHMARKING_ONLY` CMake; option to `ON` will disable them. Configuration; -------------. The test-suite has configuration options to customize building and running the; benchmarks. CMake can print a list of them:. ```bash; % cd test-suite-build; # Print basic options:; % cmake -LH; # Print all options:; % cmake -LAH; ```. ### Common Configuration Options. - `CMAKE_C_FLAGS`. Specify extra flags to be passed to C compiler invocations. The flags are; also passed to the C++ compiler and linker invocations. See; [https://cmake.org/cmake/help/latest/variable/CMAKE_LANG_FLAGS.html](https://cmake.org/cmake/help/latest/variable/CMAKE_LANG_FLAGS.html). - `CMAKE_C_COMPILER`. Select the C compiler executable to be used. Note that the C++ compiler is; inferred automatically i.e. when specifying ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:3484,perform,performance,3484,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['perform'],['performance']
Performance,"ect to fall back when unprofitable from hardening the loaded value to the; next approach of hardening the address itself. ###### Loads folded into data-invariant operations can be hardened after the operation. The first key to making this feasible is to recognize that many operations on; x86 are ""data-invariant"". That is, they have no (known) observable behavior; differences due to the particular input data. These instructions are often used; when implementing cryptographic primitives dealing with private key data; because they are not believed to provide any side-channels. Similarly, we can; defer hardening until after them as they will not in-and-of-themselves; introduce a speculative execution side-channel. This results in code sequences; that look like:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; orl %eax, %edi; ```. While an addition happens to the loaded (potentially secret) value, that; doesn't leak any data and we then immediately harden it. ###### Hardening of loaded values deferred down the data-invariant expression graph. We can generalize the previous idea and sink the hardening down the expression; graph across as many data-invariant operations as desirable. This can use very; conservative rules for whether something is data-invariant. The primary goal; should be to handle multiple loads with a single hardening instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are nar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:25914,load,loaded,25914,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"ect. Be very careful if you take the address of; stack objects. As we shall see soon, they are deleted automatically,; which could leave you with an illegal pointer. Using it will corrupt; and may as well crash the program!. It is time to look at the destruction of objects. A destructor is a; special C++ function that releases resources for (or destroys) an; object of a class. It is the opposite of a constructor that creates the; object of a class when it is called. The compiler will provide a; destructor that does nothing if none is provided. We will add one to; our Quad class so that we can see when it is called. The class names; the destructor but with a prefix \~ which is the C++ one's complement; i.e. bit wise complement, and hence has destruction overtones! We; declare it in the .h file and define it in the `.cxx` file. It does; not do much except print out that it has been called (still a useful; debug technique despite today's powerful debuggers!). Now run root, load the Quad class and create a heap object:. ``` {.cpp}; root[] .L Quad.cxx; root[] Quad *my_objptr = new Quad(1.,2.,-3.);; ```. To delete the object:. ``` {.cpp}; root[] delete my_objptr;; root[] my_objptr = 0;; ```. You should see the print out from its destructor. Setting the pointer; to zero afterwards is not strictly necessary (and Cling does it; automatically), but the object is no more accessible, and any attempt; to use the pointer again will, as has already been stated, cause; grief. So much for heap objects, but how are stack objects deleted? In; C++, a stack object is deleted as soon as control leaves the innermost; compound statement that encloses it. Therefore, it is singularly; futile to do something like:. ``` {.cpp}; root[] { Quad my_object(1.,2.,-3.); }; ```. Cling does not follow this rule; if you type in the above line, you; will not see the destructor message. As explained in the Script; lesson, you can load in compound statements, which would be a bit; pointless if everything di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ALittleC++.md:12885,load,load,12885,documentation/users-guide/ALittleC++.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ALittleC++.md,1,['load'],['load']
Performance,"ect: RE: LLVM Concerns... > 1. Reference types; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:1077,perform,performance,1077,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,1,['perform'],['performance']
Performance,"ect`` intrinsic provides information about expected (the; most probable) value of ``val``, which can be used by optimizers. Arguments:; """""""""""""""""""". The ``llvm.expect`` intrinsic takes two arguments. The first argument is; a value. The second argument is an expected value. Semantics:; """""""""""""""""""". This intrinsic is lowered to the ``val``. '``llvm.expect.with.probability``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This intrinsic is similar to ``llvm.expect``. This is an overloaded intrinsic.; You can use ``llvm.expect.with.probability`` on any integer bit width. ::. declare i1 @llvm.expect.with.probability.i1(i1 <val>, i1 <expected_val>, double <prob>); declare i32 @llvm.expect.with.probability.i32(i32 <val>, i32 <expected_val>, double <prob>); declare i64 @llvm.expect.with.probability.i64(i64 <val>, i64 <expected_val>, double <prob>). Overview:; """""""""""""""""". The ``llvm.expect.with.probability`` intrinsic provides information about; expected value of ``val`` with probability(or confidence) ``prob``, which can; be used by optimizers. Arguments:; """""""""""""""""""". The ``llvm.expect.with.probability`` intrinsic takes three arguments. The first; argument is a value. The second argument is an expected value. The third; argument is a probability. Semantics:; """""""""""""""""""". This intrinsic is lowered to the ``val``. .. _int_assume:. '``llvm.assume``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.assume(i1 %cond). Overview:; """""""""""""""""". The ``llvm.assume`` allows the optimizer to assume that the provided; condition is true. This information can then be used in simplifying other parts; of the code. More complex assumptions can be encoded as; :ref:`assume operand bundles <assume_opbundles>`. Arguments:; """""""""""""""""""". The argument of the call is the condition which the optimizer may assume is; always true. Semantics:; """""""""""""""""""". The intrinsic allows the optimizer to assume that the provided condition is; always true wheneve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:934735,optimiz,optimizers,934735,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"ected; 7525 events out of the 283813 events in the chain of files. (2.65 per; cent). ``` {.cpp}; root[] chain.Process(""h1analysis.C"",""fillList""); ```. *Step D:* Process only entries in the event list. The event list is read; from the file in `elist.root` generated by step C. ``` {.cpp}; root[] chain.Process(""h1analysis.C"",""useList""); ```. *Step E*: The above steps have been executed with the interpreter. You; can repeat the steps B, C, and D using ACLiC by replacing; ""`h1analysis.C`"" by ""`h1analysis.C+`"" or ""`h1analysis.C++`"". *Step F:* If you want to see the differences between the interpreter; speed and ACLiC speed start a new session, create the chain as in step; 1, then execute. ``` {.cpp}; root[] chain.Process(""h1analysis.C+"",""useList""); ```. The commands executed with the four different methods B, C, D and E; produce two canvases shown below:. ![](pictures/030001FD.png) ![](pictures/030001FE.png). ## Script. This is the `h1analsysis.C` file that was generated by; `TTree::MakeSelector` and then modified to perform the analysis. ``` {.cpp}; #include ""h1analysis.h""; #include ""TH2.h""; #include ""TF1.h""; #include ""TStyle.h""; #include ""TCanvas.h""; #include ""TLine.h""; #include ""TEventList.h"". const Double_t dxbin = (0.17-0.13)/40; // Bin-width; const Double_t sigma = 0.0012;; TEventList *elist = 0;; Bool_t useList, fillList;; TH1F *hdmd;; TH2F *h2;. //_________________________________________________________; Double_t fdm5(Double_t *xx, Double_t *par); {; Double_t x = xx[0];; if (x <= 0.13957) return 0;; Double_t xp3 = (x-par[3])*(x-par[3]);; Double_t res = dxbin*(par[0]*TMath::Power(x-0.13957,par[1]); + par[2]/2.5066/par[4]*TMath::Exp(-xp3/2/par[4]/par[4]));; return res;; }. //_________________________________________________________; Double_t fdm2(Double_t *xx, Double_t *par); {; Double_t x = xx[0];; if (x <= 0.13957) return 0;; Double_t xp3 = (x-0.1454)*(x-0.1454);; Double_t res = dxbin*(par[0]*TMath::Power(x-0.13957,0.25); + par[1]/2.5066/sigma*TMath::Exp(-xp3/2/si",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ExampleAnalysis.md:5201,perform,perform,5201,documentation/users-guide/ExampleAnalysis.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ExampleAnalysis.md,1,['perform'],['perform']
Performance,"ection. Optionally, a pass may be defined (in; ``XXXBranchSelector.cpp``) to perform similar DAG-to-DAG operations for branch; instructions. Later, the code in ``XXXISelLowering.cpp`` replaces or removes; operations and data types not supported natively (legalizes) in a; ``SelectionDAG``. TableGen generates code for instruction selection using the following target; description input files:. * ``XXXInstrInfo.td`` --- Contains definitions of instructions in a; target-specific instruction set, generates ``XXXGenDAGISel.inc``, which is; included in ``XXXISelDAGToDAG.cpp``. * ``XXXCallingConv.td`` --- Contains the calling and return value conventions; for the target architecture, and it generates ``XXXGenCallingConv.inc``,; which is included in ``XXXISelLowering.cpp``. The implementation of an instruction selection pass must include a header that; declares the ``FunctionPass`` class or a subclass of ``FunctionPass``. In; ``XXXTargetMachine.cpp``, a Pass Manager (PM) should add each instruction; selection pass into the queue of passes to run. The LLVM static compiler (``llc``) is an excellent tool for visualizing the; contents of DAGs. To display the ``SelectionDAG`` before or after specific; processing phases, use the command line options for ``llc``, described at; :ref:`SelectionDAG-Process`. To describe instruction selector behavior, you should add patterns for lowering; LLVM code into a ``SelectionDAG`` as the last parameter of the instruction; definitions in ``XXXInstrInfo.td``. For example, in ``SparcInstrInfo.td``,; this entry defines a register store operation, and the last parameter describes; a pattern with the store DAG operator. .. code-block:: text. def STrr : F3_1< 3, 0b000100, (outs), (ins MEMrr:$addr, IntRegs:$src),; ""st $src, [$addr]"", [(store i32:$src, ADDRrr:$addr)]>;. ``ADDRrr`` is a memory mode that is also defined in ``SparcInstrInfo.td``:. .. code-block:: text. def ADDRrr : ComplexPattern<i32, 2, ""SelectADDRrr"", [], []>;. The definition of ``ADDRrr``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:53495,queue,queue,53495,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['queue'],['queue']
Performance,"ection. This phase takes a legal SelectionDAG as input, pattern matches the; instructions supported by the target to this DAG, and produces a new DAG of; target code. For example, consider the following LLVM fragment:. .. code-block:: llvm. %t1 = fadd float %W, %X; %t2 = fmul float %t1, %Y; %t3 = fadd float %t2, %Z. This LLVM code corresponds to a SelectionDAG that looks basically like this:. .. code-block:: text. (fadd:f32 (fmul:f32 (fadd:f32 W, X), Y), Z). If a target supports floating point multiply-and-add (FMA) operations, one of; the adds can be merged with the multiply. On the PowerPC, for example, the; output of the instruction selector might look like this DAG:. ::. (FMADDS (FADDS W, X), Y, Z). The ``FMADDS`` instruction is a ternary instruction that multiplies its first; two operands and adds the third (as single-precision floating-point numbers).; The ``FADDS`` instruction is a simple binary single-precision add instruction.; To perform this pattern match, the PowerPC backend includes the following; instruction definitions:. .. code-block:: text; :emphasize-lines: 4-5,9. def FMADDS : AForm_1<59, 29,; (ops F4RC:$FRT, F4RC:$FRA, F4RC:$FRC, F4RC:$FRB),; ""fmadds $FRT, $FRA, $FRC, $FRB"",; [(set F4RC:$FRT, (fadd (fmul F4RC:$FRA, F4RC:$FRC),; F4RC:$FRB))]>;; def FADDS : AForm_2<59, 21,; (ops F4RC:$FRT, F4RC:$FRA, F4RC:$FRB),; ""fadds $FRT, $FRA, $FRB"",; [(set F4RC:$FRT, (fadd F4RC:$FRA, F4RC:$FRB))]>;. The highlighted portion of the instruction definitions indicates the pattern; used to match the instructions. The DAG operators (like ``fmul``/``fadd``); are defined in the ``include/llvm/Target/TargetSelectionDAG.td`` file.; ""``F4RC``"" is the register class of the input and result values. The TableGen DAG instruction selector generator reads the instruction patterns; in the ``.td`` file and automatically builds parts of the pattern matching code; for your target. It has the following strengths:. * At compiler-compile time, it analyzes your instruction patterns and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:47649,perform,perform,47649,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['perform']
Performance,"ections, replaces DWARF Version 5 section 2.5; and section 2.6. The new DWARF expression operation extensions are defined as; well as clarifying the extensions to already existing DWARF Version 5; operations. It is based on the text of the existing DWARF Version 5 standard. DWARF expressions describe how to compute a value or specify a location. *The evaluation of a DWARF expression can provide the location of an object, the; value of an array bound, the length of a dynamic string, the desired value; itself, and so on.*. If the evaluation of a DWARF expression does not encounter an error, then it can; either result in a value (see :ref:`amdgpu-dwarf-expression-value`) or a; location description (see :ref:`amdgpu-dwarf-location-description`). When a; DWARF expression is evaluated, it may be specified whether a value or location; description is required as the result kind. If a result kind is specified, and the result of the evaluation does not match; the specified result kind, then the implicit conversions described in; :ref:`amdgpu-dwarf-memory-location-description-operations` are performed if; valid. Otherwise, the DWARF expression is ill-formed. If the evaluation of a DWARF expression encounters an evaluation error, then the; result is an evaluation error. .. note::. Decided to define the concept of an evaluation error. An alternative is to; introduce an undefined value base type in a similar way to location; descriptions having an undefined location description. Then operations that; encounter an evaluation error can return the undefined location description or; value with an undefined base type. All operations that act on values would return an undefined entity if given an; undefined value. The expression would then always evaluate to completion, and; can be tested to determine if it is an undefined entity. However, this would add considerable additional complexity and does not match; that GDB throws an exception when these evaluation errors occur. If a DWARF exp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:43721,perform,performed,43721,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performed']
Performance,"ectly align them up to the |; | | | page boundary? By default (false), we round up allocation size to the nearest |; | | | power of two (2, 4, 8, 16) up to a maximum of 16-byte alignment for |; | | | performance reasons. Setting this to true can find single byte |; | | | buffer-overflows at the cost of performance, and may be incompatible with |; | | | some architectures. |; +----------------------------+---------+--------------------------------------------------------------------------------+; | MaxSimultaneousAllocations | 16 | Number of simultaneously-guarded allocations available in the pool. |; +----------------------------+---------+--------------------------------------------------------------------------------+; | SampleRate | 5000 | The probability (1 / SampleRate) that a page is selected for GWP-ASan |; | | | sampling. Sample rates up to (2^31 - 1) are supported. |; +----------------------------+---------+--------------------------------------------------------------------------------+; | InstallSignalHandlers | true | Install GWP-ASan signal handlers for SIGSEGV during dynamic loading. This |; | | | allows better error reports by providing stack traces for allocation and |; | | | deallocation when reporting a memory error. GWP-ASan's signal handler will |; | | | forward the signal to any previously-installed handler, and user programs |; | | | that install further signal handlers should make sure they do the same. Note, |; | | | if the previously installed SIGSEGV handler is SIG_IGN, we terminate the |; | | | process after dumping the error report. |; +----------------------------+---------+--------------------------------------------------------------------------------+. Example; -------. The below code has a use-after-free bug, where the ``string_view`` is created as; a reference to the temporary result of the ``string+`` operator. The; use-after-free occurs when ``sv`` is dereferenced on line 8. .. code:: cpp. 1: #include <iostream>; 2: #include <stri",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst:8486,load,loading,8486,interpreter/llvm-project/llvm/docs/GwpAsan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst,1,['load'],['loading']
Performance,"ectly by how much; information it has about the behaviors of the passes it is scheduling. For; example, the ""preserved"" set is intentionally conservative in the face of an; unimplemented :ref:`getAnalysisUsage <writing-an-llvm-pass-getAnalysisUsage>`; method. Not implementing when it should be implemented will have the effect of; not allowing any analysis results to live across the execution of your pass. The ``PassManager`` class exposes a ``--debug-pass`` command line options that; is useful for debugging pass execution, seeing how things work, and diagnosing; when you should be preserving more analyses than you currently are. (To get; information about all of the variants of the ``--debug-pass`` option, just type; ""``opt -help-hidden``""). By using the --debug-pass=Structure option, for example, we can see how our; :ref:`Hello World <writing-an-llvm-pass-basiccode>` pass interacts with other; passes. Lets try it out with the gvn and licm passes:. .. code-block:: console. $ opt -load lib/LLVMHello.so -gvn -licm --debug-pass=Structure < hello.bc > /dev/null; ModulePass Manager; FunctionPass Manager; Dominator Tree Construction; Basic Alias Analysis (stateless AA impl); Function Alias Analysis Results; Memory Dependence Analysis; Global Value Numbering; Natural Loop Information; Canonicalize natural loops; Loop-Closed SSA Form Pass; Basic Alias Analysis (stateless AA impl); Function Alias Analysis Results; Scalar Evolution Analysis; Loop Pass Manager; Loop Invariant Code Motion; Module Verifier; Bitcode Writer. This output shows us when passes are constructed.; Here we see that GVN uses dominator tree information to do its job. The LICM pass; uses natural loop information, which uses dominator tree as well. After the LICM pass, the module verifier runs (which is automatically added by; the :program:`opt` tool), which uses the dominator tree to check that the; resultant LLVM code is well formed. Note that the dominator tree is computed; once, and shared by three passes",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:43724,load,load,43724,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"ector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_inv sc0`` is required which will invalidate; the L1 cache. * A ``buffer_inv sc0`` is required to invalidate the L1 cache for coherence; between wavefronts executing in different work-groups as they may be; executing on different CUs. * Atomic read-modify-write instructions implicitly bypass the L1 cache.; Therefore, they do not use the sc0 bit for coherence and instead use it to; indicate if the instruction returns the original value being updated. They; do use sc1 to indicate system or agent scope coherence. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache. * The gfx942 can be configured as a number of smaller agents with each having; a single L2 shared by all CUs on the same agent, or as fewer (possibly one); larger agents with groups of CUs on each agent each sharing separate L2; caches.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensure",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:287003,cache,cache,287003,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"ector<unique_ptr<int>> v = {std::unique_ptr(pi)}; // pi is Compatible; print(*pi);; use(v);; }; // pi is Compatible; }; ```. We can refactor this code to use `unique_ptr`, however we would have to; introduce a non-owning pointer variable, since we can't use the moved-from; `unique_ptr` to access the object:. ```c++; void UniqueOwnership3() {; std::unique_ptr<int> pi = std::make_unique<int>();; if (...) {; Borrow(pi);; } else {; int *pi_non_owning = pi.get();; vector<unique_ptr<int>> v = {std::move(pi)};; print(*pi_non_owning);; use(v);; }; }; ```. If the original code didn't call `delete` at the very end of the function, then; our refactoring may change the point at which we run the destructor and release; memory. Specifically, if there is some user code after `delete`, then extending; the lifetime of the object until the end of the function may hold locks for; longer than necessary, introduce memory overhead etc. One solution is to always replace `delete` with a call to `reset()`, and then; perform another analysis that removes unnecessary `reset()` calls. ```c++; void AddedMemoryOverhead() {; HugeObject *ho = new HugeObject();; use(ho);; delete ho; // Release the large amount of memory quickly.; LongRunningFunction();; }; ```. This analysis will refuse to refactor code that mixes borrowed pointer values; and unique ownership. In the following code, `GetPtr()` returns a borrowed; pointer, which is assigned to `pi`. Then, `pi` is used to hold a uniquely-owned; pointer. We don't distinguish between these two assignments, and we want each; assignment to be paired with a corresponding sink; otherwise, we transition the; pointer to a `Conflicting` state, like in this example. ```c++; void ConflictingOwnership() {; int *pi; // pi is Compatible; pi = GetPtr(); // pi is Defined; Borrow(pi); // pi is Defined. pi = new int; // pi is Conflicting; Borrow(pi);; delete pi;; // pi is Conflicting; }; ```. We could still handle this case by finding a maximal range in the code where",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md:24568,perform,perform,24568,interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowAnalysisIntro.md,1,['perform'],['perform']
Performance,"ectorty %A, vectorty %B, i32 <OuterRows>, i32 <Inner>, i32 <OuterColumns>). Overview:; """""""""""""""""". The '``llvm.matrix.multiply.*``' intrinsics treat ``%A`` as a ``<OuterRows> x; <Inner>`` matrix, ``%B`` as a ``<Inner> x <OuterColumns>`` matrix, and; multiplies them. The result matrix is returned in the result vector. Arguments:; """""""""""""""""""". The first vector argument ``%A`` corresponds to a matrix with ``<OuterRows> *; <Inner>`` elements, and the second argument ``%B`` to a matrix with; ``<Inner> * <OuterColumns>`` elements. Arguments ``<OuterRows>``,; ``<Inner>`` and ``<OuterColumns>`` must be positive, constant integers. The; returned vector must have ``<OuterRows> * <OuterColumns>`` elements.; Vectors ``%A``, ``%B``, and the returned vector all have the same float or; integer element type. '``llvm.matrix.column.major.load.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare vectorty @llvm.matrix.column.major.load.*(; ptrty %Ptr, i64 %Stride, i1 <IsVolatile>, i32 <Rows>, i32 <Cols>). Overview:; """""""""""""""""". The '``llvm.matrix.column.major.load.*``' intrinsics load a ``<Rows> x <Cols>``; matrix using a stride of ``%Stride`` to compute the start address of the; different columns. The offset is computed using ``%Stride``'s bitwidth. This; allows for convenient loading of sub matrixes. If ``<IsVolatile>`` is true, the; intrinsic is considered a :ref:`volatile memory access <volatile>`. The result; matrix is returned in the result vector. If the ``%Ptr`` argument is known to; be aligned to some boundary, this can be specified as an attribute on the; argument. Arguments:; """""""""""""""""""". The first argument ``%Ptr`` is a pointer type to the returned vector type, and; corresponds to the start address to load from. The second argument ``%Stride``; is a positive, constant integer with ``%Stride >= <Rows>``. ``%Stride`` is used; to compute the column memory addresses. I.e., for a column ``C``, its start; memor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:677773,load,load,677773,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ects an error near a memory tag between 1 and `TG-1`, it; will show both the memory tag and the last byte of the granule. Currently,; it is up to the user to disambiguate the two possibilities. Instrumentation; ===============. Memory Accesses; ---------------; In the majority of cases, memory accesses are prefixed with a call to; an outlined instruction sequence that verifies the tags. The code size; and performance overhead of the call is reduced by using a custom calling; convention that. * preserves most registers, and; * is specialized to the register containing the address, and the type and; size of the memory access. Currently, the following sequence is used:. .. code-block:: none. // int foo(int *a) { return *a; }; // clang -O2 --target=aarch64-linux-android30 -fsanitize=hwaddress -S -o - load.c; [...]; foo:; stp x30, x20, [sp, #-16]!; adrp x20, :got:__hwasan_shadow // load shadow address from GOT into x20; ldr x20, [x20, :got_lo12:__hwasan_shadow]; bl __hwasan_check_x0_2_short_v2 // call outlined tag check; // (arguments: x0 = address, x20 = shadow base;; // ""2"" encodes the access type and size); ldr w0, [x0] // inline load; ldp x30, x20, [sp], #16; ret. [...]; __hwasan_check_x0_2_short_v2:; sbfx x16, x0, #4, #52 // shadow offset; ldrb w16, [x20, x16] // load shadow tag; cmp x16, x0, lsr #56 // extract address tag, compare with shadow tag; b.ne .Ltmp0 // jump to short tag handler on mismatch; .Ltmp1:; ret; .Ltmp0:; cmp w16, #15 // is this a short tag?; b.hi .Ltmp2 // if not, error; and x17, x0, #0xf // find the address's position in the short granule; add x17, x17, #3 // adjust to the position of the last byte loaded; cmp w16, w17 // check that position is in bounds; b.ls .Ltmp2 // if not, error; orr x16, x0, #0xf // compute address of last byte of granule; ldrb w16, [x16] // load tag from it; cmp x16, x0, lsr #56 // compare with pointer tag; b.eq .Ltmp1 // if matches, continue; .Ltmp2:; stp x0, x1, [sp, #-256]! // save original x0, x1 on stack (they will be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:3853,load,load,3853,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,2,['load'],['load']
Performance,"ects) can be; deallocated by calling the ``JITLinkMemoryManager::dealloc`` method. This method; takes a vector of ``FinalizedAlloc`` objects, since it is common to deallocate; multiple objects at the same time and this allows us to batch these requests for; transmission to the executing process. JITLink provides a simple in-process implementation of this interface:; ``InProcessMemoryManager``. It allocates pages once and re-uses them as both; working and target memory. ORC provides a cross-process-capable ``MapperJITLinkMemoryManager`` that can use; shared memory or ORC-RPC-based communication to transfer content to the executing; process. JITLinkMemoryManager and Security; ---------------------------------. JITLink's ability to link JIT'd code for a separate executor process can be; used to improve the security of a JIT system: The executor process can be; sandboxed, run within a VM, or even run on a fully separate machine. JITLink's memory manager interface is flexible enough to allow for a range of; trade-offs between performance and security. For example, on a system where code; pages must be signed (preventing code from being updated), the memory manager; can deallocate working memory pages after linking to free memory in the process; running JITLink. Alternatively, on a system that allows RWX pages, the memory; manager may use the same pages for both working and target memory by marking; them as RWX, allowing code to be modified in place without further overhead.; Finally, if RWX pages are not permitted but dual-virtual-mappings of; physical memory pages are, then the memory manager can dual map physical pages; as RW- in the JITLink process and R-X in the executor process, allowing; modification from the JITLink process but not from the executor (at the cost of; extra administrative overhead for the dual mapping). Error Handling; --------------. JITLink makes extensive use of the ``llvm::Error`` type (see the error handling; section of :doc:`ProgrammersManual`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:31051,perform,performance,31051,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['perform'],['performance']
Performance,"eculation-on-AMD-Processors.pdf)). However, this relies on finding and enumerating all possible points in code; which could be attacked to leak information. While in some cases static; analysis is effective at doing this at scale, in many cases it still relies on; human judgement to evaluate whether code might be vulnerable. Especially for; software systems which receive less detailed scrutiny but remain sensitive to; these attacks, this seems like an impractical security model. We need an; automatic and systematic mitigation strategy. ### Automatic `lfence` on Conditional Edges. A natural way to scale up the existing hand-coded mitigations is simply to; inject an `lfence` instruction into both the target and fallthrough; destinations of every conditional branch. This ensures that no predicate or; bounds check can be bypassed speculatively. However, the performance overhead; of this approach is, simply put, catastrophic. Yet it remains the only truly; ""secure by default"" approach known prior to this effort and serves as the; baseline for performance. One attempt to address the performance overhead of this and make it more; realistic to deploy is [MSVC's /Qspectre; switch](https://blogs.msdn.microsoft.com/vcblog/2018/01/15/spectre-mitigations-in-msvc/).; Their technique is to use static analysis within the compiler to only insert; `lfence` instructions into conditional edges at risk of attack. However,; [initial](https://arstechnica.com/gadgets/2018/02/microsofts-compiler-level-spectre-fix-shows-how-hard-this-problem-will-be-to-solve/); [analysis](https://www.paulkocher.com/doc/MicrosoftCompilerSpectreMitigation.html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:45572,perform,performance,45572,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"ecution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If not TgSplit execution; mode, omit glc=1. 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:245648,load,load,245648,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ed (expected to be 0); uint16 : Reserved (expected to be 0); }; uint32 : NumFunctions; FunctionInfo[NumFunctions] {; uint64 : FunctionAddress; uint32 : NumFaultingPCs; uint32 : Reserved (expected to be 0); FunctionFaultInfo[NumFaultingPCs] {; uint32 : FaultKind; uint32 : FaultingPCOffset; uint32 : HandlerPCOffset; }; }. FailtKind describes the reason of expected fault. Currently three kind; of faults are supported:. 1. ``FaultMaps::FaultingLoad`` - fault due to load from memory.; 2. ``FaultMaps::FaultingLoadStore`` - fault due to instruction load and store.; 3. ``FaultMaps::FaultingStore`` - fault due to store to memory. The ``ImplicitNullChecks`` pass; ===============================. The ``ImplicitNullChecks`` pass transforms explicit control flow for; checking if a pointer is ``null``, like:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %ptr_is_null = icmp i32* %ptr, null; br i1 %ptr_is_null, label %is_null, label %not_null, !make.implicit !0. not_null:; %t = load i32, i32* %ptr; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. !0 = !{}. to control flow implicit in the instruction loading or storing through; the pointer being null checked:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %t = load i32, i32* %ptr ;; handler-pc = label %is_null; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. This transform happens at the ``MachineInstr`` level, not the LLVM IR; level (so the above example is only representative, not literal). The; ``ImplicitNullChecks`` pass runs during codegen, if; ``-enable-implicit-null-checks`` is passed to ``llc``. The ``ImplicitNullChecks`` pass adds entries to the; ``__llvm_faultmaps`` section described above as needed. ``make.implicit`` metadata; --------------------------. Making null checks implicit is an aggressive optimization, and it can; be a net performance pessimization if too many memory operations end; up faulting because of it. A language runtime typically needs to; ensure ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst:2350,load,load,2350,interpreter/llvm-project/llvm/docs/FaultMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst,1,['load'],['load']
Performance,"ed C++ entities as if the header was loaded. The dictionary shared library can be relocated, as long as it can be found; by the dynamic loader (e.g. through ``LD_LIBRARY_PATH``) and the header file; is fully embedded or still accessible (e.g. through a path added to; ``cppyy.add_include_path`` at run-time, or with ``-I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manually find and load dictionaries (assuming these are; locatable by the dynamic loader). The class loader utilizes so-called rootmap files, which by convention should; live alongside the dictionary shared library (and C++ module file).; These are simple text files, which map C++ entities (such as classes) to the; dictionaries and other libraries that need to be loaded for their use. With ``genreflex``, the mapping file can be automatically created with; ``--rootmap-lib=MyClassDict``, where ""MyClassDict"" is the name of the shared; library (without the extension) build from the dictionary file.; With ``rootcling``, create the same mapping file with; ``-rmf MyClassDict.rootmap -rml MyClassDict``.; It is necessary to provide the final library name explicitly, since it is; only in the separate linking step where these names are fixed and those names; may not match the default choice. With the mapping file in place, the above example can be rerun without; explicit loading of the dict",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:8350,load,loader,8350,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,3,['load'],"['load', 'loader']"
Performance,"ed by the Free Software; Foundation. 10. If you wish to incorporate parts of the Program into other free; programs whose distribution conditions are different, write to the author; to ask for permission. For software which is copyrighted by the Free; Software Foundation, write to the Free Software Foundation; we sometimes; make exceptions for this. Our decision will be guided by the two goals; of preserving the free status of all derivatives of our free software and; of promoting the sharing and reuse of software generally. NO WARRANTY. 11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY; FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN; OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES; PROVIDE THE PROGRAM ""AS IS"" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED; OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF; MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS; TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE; PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,; REPAIR OR CORRECTION. 12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING; WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR; REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,; INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING; OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED; TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY; YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER; PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE; POSSIBILITY OF SUCH DAMAGES. END OF TERMS AND CONDITIONS. How to Apply These Terms to Your New Programs. If you develop a new program, and you want it to be of the greatest; possible use to the public, the best way to achieve this is to make it; free software",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/misc/rootql/LICENSE.txt:14237,PERFORM,PERFORMANCE,14237,misc/rootql/LICENSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/rootql/LICENSE.txt,2,['PERFORM'],['PERFORMANCE']
Performance,"ed by the end of 2024. Thus, we appreciate feedback and suggestions for improvement. ## Histogram Libraries. - Implement the FLT_MAX mechanism for `THStack::GetMaximum()` and `THStack::GetMiniumum()`.; - Print a warning when the range given to `TAxis::SetRange` is invalid.; - Fix projection name in `TH3` as requested [here](https://root-forum.cern.ch/t/project3d-letter-d-in-name-option/57612). ## Parallelism; - The ROOT::Experimental::TFuture template has been removed. ## RooFit Libraries. ### New CPU likelihood evaluation backend by default. The new vectorizing CPU evaluation backend is not the default for RooFit likelihoods.; Likelihood minimization is now up to 10x faster on a single CPU core. If you experience unexpected problems related to the likelihood evaluation, you; can revert back to the old backend by passing `RooFit::EvalBackend(""legacy"")`; to `RooAbsPdf::fitTo()` or `RooAbsPdf::createNLL()`. In case you observe any slowdowns with the new likelihood evaluation, please; open a GitHub issue about this, as such a performance regression is considered; a bug. ### Asymptotically correct uncertainties for extended unbinned likelihood fits. Added correct treatment of extended term in asymptotically correct method for uncertainty determination in the presence of weights.; This improvement will allow for extended unbinned maximum likelihood fits to use the asymptotically correct method when using the `RooFit::AsymptoticError()` command argument in [RooAbsPdf::fitTo()](https://root.cern.ch/doc/master/classRooAbsPdf.html#ab0721374836c343a710f5ff92a326ff5).; See also this [writeup on extended weighted fits](https://root.cern/files/extended_weighted_fits.pdf) that is also linked from the reference guide.; The [pull request](https://github.com/root-project/root/pull/14751) that introduced this feature might also be a good reference. ### Compile your code with memory safe interfaces. If you define the `ROOFIT_MEMORY_SAFE_INTERFACES` preprocessor macro, the; RooFit inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:7553,perform,performance,7553,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['perform'],['performance']
Performance,"ed code from misspeculation in; an unmitigated caller. There is also an advantage to using this form of interprocedural mitigation: by; forming these invalid stack pointer addresses we can prevent speculative; returns from successfully reading speculatively written values to the actual; stack. This works first by forming a data-dependency between computing the; address of the return address on the stack and our predicate state. And even; when satisfied, if a misprediction causes the state to be poisoned the; resulting stack pointer will be invalid. ##### Rewrite API of internal functions to directly propagate predicate state. (Not yet implemented.). We have the option with internal functions to directly adjust their API to; accept the predicate as an argument and return it. This is likely to be; marginally cheaper than embedding into `%rsp` for entering functions. ##### Use `lfence` to guard function transitions. An `lfence` instruction can be used to prevent subsequent loads from; speculatively executing until all prior mispredicted predicates have resolved.; We can use this broader barrier to speculative loads executing between; functions. We emit it in the entry block to handle calls, and prior to each; return. This approach also has the advantage of providing the strongest degree; of mitigation when mixed with unmitigated code by halting all misspeculation; entering a function which is mitigated, regardless of what occurred in the; caller. However, such a mixture is inherently more risky. Whether this kind of; mixture is a sufficient mitigation requires careful analysis. Unfortunately, experimental results indicate that the performance overhead of; this approach is very high for certain patterns of code. A classic example is; any form of recursive evaluation engine. The hot, rapid call and return; sequences exhibit dramatic performance loss when mitigated with `lfence`. This; component alone can regress performance by 2x or more, making it an unpleasant; tradeoff",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:41333,load,loads,41333,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"ed from the ReadySet to the IssuedSet. There,; instructions wait until they reach the write-back stage. At that point, they; get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as; ""ready to retire."". Instructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:39665,queue,queues,39665,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queues']
Performance,"ed function at this call site is in. If; the loaded or returned value is not in the specified range, a poison value is; returned instead. The ranges are represented with a flattened list of integers.; The loaded value or the value returned is known to be in the union of the ranges; defined by each consecutive pair. Each pair has the following properties:. - The type must match the scalar type of the instruction.; - The pair ``a,b`` represents the range ``[a,b)``.; - Both ``a`` and ``b`` are constants.; - The range is allowed to wrap.; - The range should not represent the full or empty set. That is,; ``a!=b``. In addition, the pairs must be in signed order of the lower bound and; they must be non-contiguous. For vector-typed instructions, the range is applied element-wise. Examples:. .. code-block:: llvm. %a = load i8, ptr %x, align 1, !range !0 ; Can only be 0 or 1; %b = load i8, ptr %y, align 1, !range !1 ; Can only be 255 (-1), 0 or 1; %c = call i8 @foo(), !range !2 ; Can only be 0, 1, 3, 4 or 5; %d = invoke i8 @bar() to label %cont; unwind label %lpad, !range !3 ; Can only be -2, -1, 3, 4 or 5; %e = load <2 x i8>, ptr %x, !range 0 ; Can only be <0 or 1, 0 or 1>; ...; !0 = !{ i8 0, i8 2 }; !1 = !{ i8 255, i8 2 }; !2 = !{ i8 0, i8 2, i8 3, i8 6 }; !3 = !{ i8 -2, i8 0, i8 3, i8 6 }. '``absolute_symbol``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``absolute_symbol`` metadata may be attached to a global variable; declaration. It marks the declaration as a reference to an absolute symbol,; which causes the backend to use absolute relocations for the symbol even; in position independent code, and expresses the possible ranges that the; global variable's *address* (not its value) is in, in the same format as; ``range`` metadata, with the extension that the pair ``all-ones,all-ones``; may be used to represent the full set. Example (assuming 64-bit pointers):. .. code-block:: llvm. @a = external global i8, !absolute_symbol !0 ; Absolute symbol in range [0,256); @b = extern",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:285800,load,load,285800,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,['load'],['load']
Performance,"ed in a; session; therefore, users must not try to control their deletion. It; contains lists of media, materials, transformations, shapes and volumes.; A special case is the one of geometrical transformations. When creating; a matrix or a translation, this is by default owned by external objects.; The manager class becomes owner of all transformations used for; positioning volumes. In order to force the ownership for other; transformations, one can use `TGeoMatrix::RegisterYourself()` method. Do; not be therefore surprised that some transformations cannot be found by; name when creating a composite shape for instance if you did not; register them after creation. Logical nodes (positioned volumes) are created and destroyed by the; **`TGeoVolume`** class. Physical nodes and their global transformations; are subjected to a caching mechanism due to the sometimes very large; memory requirements of logical graph expansion. The total number of; physical instances of volumes triggers the caching mechanism and the; cache manager is a client of **`TGeoManager`**. The manager class also; controls the drawing/checking package (**`TGeoPainter`** client). This; is linked with ROOT graphical libraries loaded on demand in order to; control visualization actions. ## Navigation and Tracking. Tracking is the feature allowing the transport of a given particle; knowing its kinematics. A state is determined by any combination of the; position $\vec{r}$ and direction $\vec{n}$ with respect to the world; reference frame. The direction $\vec{n}$ must be a unit vector having as; components the director cosines. The full classification of a given; state will provide the following information: the deepest physical node; containing the position vector, the distance to the closest boundary; along the direction vector, the next physical node after propagating the; current point with this distance and the safety distance to the nearest; boundary. This information allows the propagation of particle",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:100620,cache,cache,100620,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['cache'],['cache']
Performance,"ed return type and arguments. If a class override `TFile::GetStreamerInfoList` you will now see a compilation error like:. ```; /opt/build/root_builds/rootcling.cmake/include/TSQLFile.h:225:19: error: declaration of 'GetStreamerInfoList' overrides a 'final' function; virtual TList *GetStreamerInfoList();; ^; /opt/build/root_builds/rootcling.cmake/include/TFile.h:231:24: note: overridden virtual function is here; virtual TList *GetStreamerInfoList() final; // Note: to override behavior, please override GetStreamerInfoListImpl; ^; ```. Instead you need to override the protected method:. ```; InfoListRet GetStreamerInfoListImpl(bool lookupSICache);; ```. which can be implemented as. ```; InfoListRet DerivedClass::GetStreamerInfoListImpl(bool /*lookupSICache*/) {; ROOT::Internal::RConcurrentHashColl::HashValue hash;; TList *infolist = nullptr;; //; // Body of the former Derived::GetStreamerInfoList with the; // return statement replaced with something like:. // The second element indicates success or failure of the load.; // (i.e. {nullptr, 0, hash} indicates the list has already been processed; // {nullptr, 1, hash} indicates the list failed to be loaded; return {infolist, 0, hash};; }; ```. See `TFile::GetStreamerInfoListImpl` implementation for an example on how to implement the caching. * ZLIB (with compression level 1) is now the default compression algorithm for new ROOT files (LZ4 was default compression algorithm in 6.14). Because of reported ""corner cases"" for LZ4, we are working on the fix to be landed in a next release and return back LZ4 as a default compression algorithm. * Introducing a possibility for ROOT to use generic compression algorithm/level/settings, by introducing new generic class RCompressionSetting together with new structs ELevel (compression level), EDefaults (default compression settings) and EAlgorithm (compression algorithm). These changes are the first step in generalization of setup of ROOT compression algorithm. It also provides correc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md:5165,load,load,5165,README/ReleaseNotes/v616/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md,1,['load'],['load']
Performance,"ed type used for stack operations; defined in DWARF Version 4 and before.*. An integral type is a base type that has an encoding of ``DW_ATE_signed``,; ``DW_ATE_signed_char``, ``DW_ATE_unsigned``, ``DW_ATE_unsigned_char``,; ``DW_ATE_boolean``, or any target architecture defined integral encoding in the; inclusive range ``DW_ATE_lo_user`` to ``DW_ATE_hi_user``. .. note::. It is unclear if ``DW_ATE_address`` is an integral type. GDB does not seem to; consider it as integral. .. _amdgpu-dwarf-location-description:. A.2.5.3 DWARF Location Description; ++++++++++++++++++++++++++++++++++. *Debugging information must provide consumers a way to find the location of; program variables, determine the bounds of dynamic arrays and strings, and; possibly to find the base address of a subprograms call frame or the return; address of a subprogram. Furthermore, to meet the needs of recent computer; architectures and optimization techniques, debugging information must be able to; describe the location of an object whose location changes over the objects; lifetime, and may reside at multiple locations simultaneously during parts of an; object's lifetime.*. Information about the location of program objects is provided by location; descriptions. Location descriptions can consist of one or more single location descriptions. A single location description specifies the location storage that holds a; program object and a position within the location storage where the program; object starts. The position within the location storage is expressed as a bit; offset relative to the start of the location storage. A location storage is a linear stream of bits that can hold values. Each; location storage has a size in bits and can be accessed using a zero-based bit; offset. The ordering of bits within a location storage uses the bit numbering; and direction conventions that are appropriate to the current language on the; target architecture. There are five kinds of location storage:. *memory locat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:57126,optimiz,optimization,57126,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['optimiz'],['optimization']
Performance,"ed with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that; returns an unspecified value without causing side effects. If the function's return value's second element is false, the value of the; first element is undefined. .. _type.checked.load.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer to a; function from a virtual table pointer using metadata. Otherwise, its semantic is; identical to the ``llvm.type.checked.load`` intrinsic. A relative pointer is a pointer to an offset to the pointed to value. The; address of the underlying pointer of the relative pointer is obtained by adding; the offset to the address of the offset value. '``llvm.arithmetic.fence``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.arithmetic.fence(<type> <op>). Overview:; """""""""""""""""". The purpose of the ``llvm.arithmetic.fence`` intrinsic; is to prevent the optimizer from performing fast-math optimizations,; particularly reassociation,; between the argument and the expression that contains the argument.; It can be used to preserve the parentheses in the source language. Arguments:; """""""""""""""""""". The ``llvm.arithmetic.fence`` intrinsic takes only one argument.; The argument and the return value are floating-poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:940070,load,load,940070,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ed! The zero'th item as laid out in memory becomes the n'th lane in the vector. .. figure:: ARM-BE-ld1.png; :align: right. Big endian vector load using ``LD1``. Note that the lanes retain the correct ordering. Because of this, the instruction ``LD1`` performs a vector load but performs byte swapping not on the entire 64 bits, but on the individual items within the vector. This means that the register content is the same as it would have been on a little endian system. It may seem that ``LD1`` should suffice to perform vector loads on a big endian machine. However there are pros and cons to the two approaches that make it less than simple which register format to pick. There are two options:. 1. The content of a vector register is the same *as if* it had been loaded with an ``LDR`` instruction.; 2. The content of a vector register is the same *as if* it had been loaded with an ``LD1`` instruction. Because ``LD1 == LDR + REV`` and similarly ``LDR == LD1 + REV`` (on a big endian system), we can simulate either type of load with the other type of load plus a ``REV`` instruction. So we're not deciding which instructions to use, but which format to use (which will then influence which instruction is best to use). .. The 'clearer' container is required to make the following section header come after the floated; images above.; .. container:: clearer. Note that throughout this section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and vectors to have identical memory layouts - ``[4 x i8]`` and ``<4 x i8>`` should be represented the same in memory. Without this property there ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:4076,load,load,4076,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,2,['load'],['load']
Performance,"ed, with alias analysis being; used to make sure accesses don't alias. Run-time checks can also be added on; pointer access to structure members. Many variations are supported, but some that rely on undefined behaviour being; ignored (as other compilers do) are still being left un-vectorized. .. code-block:: c++. struct { int A[100], K, B[100]; } Foo;. void foo() {; for (int i = 0; i < 100; ++i); Foo.A[i] = Foo.B[i] + 100;; }. Vectorization of function calls; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The Loop Vectorizer can vectorize intrinsic math functions.; See the table below for a list of these functions. +-----+-----+---------+; | pow | exp | exp2 |; +-----+-----+---------+; | sin | cos | sqrt |; +-----+-----+---------+; | log |log2 | log10 |; +-----+-----+---------+; |fabs |floor| ceil |; +-----+-----+---------+; |fma |trunc|nearbyint|; +-----+-----+---------+; | | | fmuladd |; +-----+-----+---------+. Note that the optimizer may not be able to vectorize math library functions; that correspond to these intrinsics if the library calls access external state; such as ""errno"". To allow better optimization of C/C++ math library functions,; use ""-fno-math-errno"". The loop vectorizer knows about special instructions on the target and will; vectorize a loop containing a function call that maps to the instructions. For; example, the loop below will be vectorized on Intel x86 if the SSE4.1 roundps; instruction is available. .. code-block:: c++. void foo(float *f) {; for (int i = 0; i != 1024; ++i); f[i] = floorf(f[i]);; }. Partial unrolling during vectorization; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Modern processors feature multiple execution units, and only programs that contain a; high degree of parallelism can fully utilize the entire width of the machine.; The Loop Vectorizer increases the instruction level parallelism (ILP) by; performing partial-unrolling of loops. In the example below the entire array is accumulated into the variable 'sum'.; This is inefficient becaus",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:9664,optimiz,optimizer,9664,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimizer']
Performance,"ed-atomic).; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:218298,load,load,218298,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ed. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:269532,load,load,269532,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ed. The core LLVM classes are defined in header files in; the ``include/llvm/IR`` directory, and implemented in the ``lib/IR``; directory. It's worth noting that, for historical reasons, this library is; called ``libLLVMCore.so``, not ``libLLVMIR.so`` as you might expect. .. _Type:. The Type class and Derived Types; --------------------------------. ``Type`` is a superclass of all type classes. Every ``Value`` has a ``Type``.; ``Type`` cannot be instantiated directly but only through its subclasses.; Certain primitive types (``VoidType``, ``LabelType``, ``FloatType`` and; ``DoubleType``) have hidden subclasses. They are hidden because they offer no; useful functionality beyond what the ``Type`` class offers except to distinguish; themselves from other subclasses of ``Type``. All other types are subclasses of ``DerivedType``. Types can be named, but this; is not a requirement. There exists exactly one instance of a given shape at any; one time. This allows type equality to be performed with address equality of; the Type Instance. That is, given two ``Type*`` values, the types are identical; if the pointers are identical. .. _m_Type:. Important Public Methods; ^^^^^^^^^^^^^^^^^^^^^^^^. * ``bool isIntegerTy() const``: Returns true for any integer type. * ``bool isFloatingPointTy()``: Return true if this is one of the five; floating point types. * ``bool isSized()``: Return true if the type has known size. Things; that don't have a size are abstract types, labels and void. .. _derivedtypes:. Important Derived Types; ^^^^^^^^^^^^^^^^^^^^^^^. ``IntegerType``; Subclass of DerivedType that represents integer types of any bit width. Any; bit width between ``IntegerType::MIN_INT_BITS`` (1) and; ``IntegerType::MAX_INT_BITS`` (~8 million) can be represented. * ``static const IntegerType* get(unsigned NumBits)``: get an integer; type of a specific bit width. * ``unsigned getBitWidth() const``: Get the bit width of an integer type. ``SequentialType``; This is subclassed by ArrayTy",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:135224,perform,performed,135224,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performed']
Performance,"ed.; =================== =============== =============== =======================================. .. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V4 and Above; :name: amdgpu-trap-handler-for-amdhsa-os-v4-onwards-table. =================== =============== ================ ================= =======================================; Usage Code Sequence GFX6-GFX8 Inputs GFX9-GFX11 Inputs Description; =================== =============== ================ ================= =======================================; reserved ``s_trap 0x00`` Reserved by hardware.; debugger breakpoint ``s_trap 0x01`` *none* *none* Reserved for debugger to use for; breakpoints. Causes wave to be halted; with the PC at the trap instruction.; The debugger is responsible to resume; the wave, including the instruction; that the breakpoint overwrote.; ``llvm.trap`` ``s_trap 0x02`` ``SGPR0-1``: *none* Causes wave to be halted with the PC at; ``queue_ptr`` the trap instruction. The associated; queue is signalled to put it into the; error state. When the queue is put in; the error state, the waves executing; dispatches on the queue will be; terminated.; ``llvm.debugtrap`` ``s_trap 0x03`` *none* *none* - If debugger not enabled then behaves; as a no-operation. The trap handler; is entered and immediately returns to; continue execution of the wavefront.; - If the debugger is enabled, causes; the debug trap to be reported by the; debugger and the wavefront is put in; the halt state with the PC at the; instruction. The debugger must; increment the PC and resume the wave.; reserved ``s_trap 0x04`` Reserved.; reserved ``s_trap 0x05`` Reserved.; reserved ``s_trap 0x06`` Reserved.; reserved ``s_trap 0x07`` Reserved.; reserved ``s_trap 0x08`` Reserved.; reserved ``s_trap 0xfe`` Reserved.; reserved ``s_trap 0xff`` Reserved.; =================== =============== ================ ================= =======================================. .. _amdgpu-amdhsa-function-call-convention:. Call Convention; ~~~~~~",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:384858,queue,queue,384858,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"ed:. ~~~{.cpp}; const TGeoHMatrix *global = gGeoManager->GetCurrentMatrix();; TGeoHMatrix *copy = new TGeoHMatrix(*global);; ~~~. - One often needs to perform `master-to-local` and `local-to-master`; point and vector conversions to get from `MARS` to the local node; coordinates. This can be done by using the global transformation or; directly the **`TGeoManager`** corresponding interfaces:. ~~~{.cpp}; Double_t *glob_pt = gGeoManager->GetCurrentPoint();; Double_t *glob_dir = gGeoManager->GetCurrentDirection();; Double_t loc_pt[3], loc_dir[3];; // Go from MARS to local coordinates:; gGeoManager->MasterToLocal(glob_pt,loc_pt); // or:; global->MasterToLocal(glob_pt,loc_pt); // will be omitted from now; ~~~. \anchor GP02f; ### Saving and Restoring the Current State. As we already described, saving and restoring modeller states can be; quite useful during tracking and is a feature extensively used by; external tracking engines. We will call this navigation history; management, which in most of the cases can be performed by handling the; state identifiers. For quite big geometries, state indexing is not; possible anymore and will be automatically disabled by the modeller.; Fortunately there is a backup solution working in any condition: the; modeller maintains a stack of states that is internally used by its own; navigation algorithms, but user code is also allowed to access it. This; works on any stack principle by using PUSH and POP calls and user code; is responsible for popping the pushed states in order to keep the stack; clean. ~~~{.cpp}; // push the current state in the stack; Int_t index = gGeoManager->PushPath();; // push state and current point; Int_t index = gGeoManager->PushPoint();; // retrieves the last pushed state (decrements stack index); gGeoManager->PopPath();; // the same but retrieves also the point location; gGeoManager->PopPoint();; // just decrement stack index without changing state; gGeoManager->PopDummy();; // retrieves a state at given index wit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:72984,perform,performed,72984,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"ed; before invalidating; the; caches. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; ato",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:355319,load,load,355319,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"edecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collecto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1788,load,load,1788,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,"edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-extending the value to the full word size; and then shifting right by at least the number of original bits using the BMI2; `shrx` instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate 32 bits of data.; shrxq %rax, %rdi, %rdi # Shift out all 32 bits loaded.; ```. Because on x86 the zero-extend is free, this can efficiently harden the loaded; value. ##### Hardening the address of the load. When hardening the loaded value is inapplicable, most often because the; instruction directly leaks information (like `cmp` or `jmpq`), we switch to; hardening the _address_ of the load instead of the loaded value. This avoids; increasing register pressure by unfolding the load or paying some other high; cost. To understand how this works in practice, we need to examine the exact; semantics of the x86 addressing modes which, in its fully general form, looks; like `(%base,%index,scale)offset`. Here `%base` and `%index` are 64-bit; registers that can potentially be any value, and may be attacker controlled,; and `scale` and `offset` are fixed immediate values. `scale` must be `1`, `2`,; `4`, or `8`, and `offset` can be any 32-bit sign extended value. The exact; computation performed to find the address is then: `%base + (scale * %index) +; offset` under 64-bit 2's complement modular arithmetic. One issue with this approach is that, after hardening, the `%base + (scal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:27526,load,load,27526,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"eduled for release in 2018. For more information, see:. [http://root.cern.ch](http://root.cern.ch). The following people have contributed to this new version:. Kim Albertsson, CERN/EP-ADP-OS,\; Guilherme Amadio, CERN/SFT,\; Bertrand Bellenot, CERN/SFT,\; Brian Bockelman, UNL,\; Rene Brun, CERN/SFT,\; Philippe Canal, FNAL,\; Olivier Couet, CERN/SFT,\; Gerri Ganis, CERN/SFT,\; Andrei Gheata, CERN/SFT,\; Enrico Guiraud, CERN/SFT,\; Raphael Isemann, Chalmers Univ. of Tech.,\; Vladimir Ilievski, GSOC 2017,\; Sergey Linev, GSI,\; Pere Mato, CERN/SFT,\; Lorenzo Moneta, CERN/SFT,\; Axel Naumann, CERN/SFT,\; Danilo Piparo, CERN/SFT,\; Fons Rademakers, CERN/SFT,\; Enric Tejedor Saavedra, CERN/SFT,\; Oksana Shadura, UNL,\; Saurav Shekhar, GSOC 2017,\; Xavier Valls Pla, UJI, CERN/SFT,\; Vassil Vassilev, Princeton/CMS,\; Wouter Verkerke, NIKHEF/Atlas, RooFit,\; Stefan Wunsch, CERN/SFT, \; Zhe Zhang, UNL. ## Important Notice. The default compression algorithm used when writing ROOT files has been updated to use LZ4 in particular to improve read (decompression) performance. You can change this default for each file through (for example) the `TFile constructor` or `TFile::SetCompressionAlgorithm`. It should be noted that ROOT files written with LZ4 compression can not be read with older release of ROOT. Support for LZ4 was however back-ported to the patch branches of previous releases and the following tags (and later release in the same patch series) can read ROOT files written with LZ4 compression:. * v5.34/38; * v6.08/06 [not yet released]; * v6.10/08; * v6.12/02. ## Removed interfaces. ## Core Libraries; - Optimize away redundant deserialization of template specializations. This reduces the memory footprint for hsimple by around 30% while improving the runtime performance for various cases by around 15%.; - When ROOT is signaled with a SIGUSR2 (i.e. on Linux and MacOS X) it will now print a backtrace.; - Move RStringView.h to ROOT/RStringView.hxx and always include ROOT/RString",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:1182,perform,performance,1182,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['perform'],['performance']
Performance,"ee comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw-with-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; atomicrmw-no-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; caches. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any prece",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:354336,cache,caches,354336,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"eed for each run; default value '1'). EstimatorType No MSE MSE, CE, linear, sigmoid, tanh, radial MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood. NeuronInputType No sum sum, sqsum, abssum Neuron input function type. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). TrainingMethod No BP BP, GA, BFGS Train with Back-Propagation (BP), BFGS Algorithm (BFGS), or Genetic Algorithm (GA - slower and worse). LearningRate No 0.02  ANN learning rate parameter. DecayRate No 0.01  Decay rate for learning parameter. TestRate No 10  Test for overtraining performed at each #th epochs. EpochMonitoring No False  Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output file!). Sampling No 1  Only 'Sampling' (randomly selected) events are trained each epoch. SamplingEpoch No 1  Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training. SamplingImportance No 1  The sampling weights of events in epochs which successful (worse estimator than before) are multiplied with SamplingImportance, else they are divided. SamplingTraining No True  The training sample is sampled. SamplingTesting No False  The testing sa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:22231,perform,performance,22231,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"eedups for mixed code, beyond the Numba; compilation of Python code itself.; The current implementation integrates compiled C++ through function pointers,; object pointers, and pointer offsets, into the intermediate representation; (IR) as generated by Numba.; A future version may integrate Cling-generated IR directly into Numba IR (or; vice versa), e.g. if the C++ code is exposed from (precompiled) headers.; This would allow inlining of C++ code into Numba traces, for further; expected speedups. Why Numba?; ----------. The advertised premise of Numba is that it ""makes Python code fast.""; However, there is a much more compelling reason: Numba allows developers to; stay in their chosen ecosystem, be it Python or C++, in mixed environments,; without paying for their choice in lost performance.; For example, a Python developer using Numba does not need to rewrite a kernel; into C++ just to run performantly in a C++ framework.; Similarly, a C++ developer can use Numba to compile and create function; pointers to Python code for easy, performant, access.; This becomes even more compelling if the deployment target is a GPU, which; would otherwise certainly require a rewrite of the Python code.; Add that Numba, as a JIT-compiler, is fully run-time just like ``cppyy``,; and the use case for integration is clear.; (Numba does not currently provide support for C++.). Usage; -------. ``cppyy`` does not use Numba extension hooks to minimize accidental; dependencies.; Instead, it requires that the extensions are loaded explicitly by any code; that uses it::. import cppyy.numba_ext. After that, Numba is able to trace ``cppyy`` bound code when applying the; usual ``numba.njit`` decorator. Numba type declarations are done lazily, with the ``numba_ext`` module only; initially registering hooks on proxy base classes, to keep overheads in; Numba's type-resolution to a minimum.; On use in a JITed trace, each C++ type or function call is refined to the; actual, concrete types and type-sp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:1900,perform,performant,1900,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['perform'],['performant']
Performance,"een wavefronts executing in different work-groups as they may be; executing on different CUs. * Atomic read-modify-write instructions implicitly bypass the L1 cache.; Therefore, they do not use the sc0 bit for coherence and instead use it to; indicate if the instruction returns the original value being updated. They; do use sc1 to indicate system or agent scope coherence. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache. * The gfx942 can be configured as a number of smaller agents with each having; a single L2 shared by all CUs on the same agent, or as fewer (possibly one); larger agents with groups of CUs on each agent each sharing separate L2; caches.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:287544,cache,cache,287544,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"ef redefinition; N1360; Clang 3.1. Thread-local storage; N1364; Clang 3.3. Constant expressions; N1365; Unknown. Contractions and expression evaluation methods; N1367; Unknown. FLT_EVAL_METHOD and return; N1382; Unknown. Floating-point to int/_Bool conversions; N1391; Yes. Analyzability (along the lines); N1394; Unknown. Wide function returns (alternate proposal); N1396; Unknown. Alignment. N1397; Clang 3.2. N1447; Clang 3.2. Anonymous member-structures and unions (modulo ""name lookup""); N1406; Yes. Completeness of types; N1439; Yes. Generic macro facility; N1441; Yes. Dependency ordering for C memory model; N1444; Unknown. Subsetting the standard; N1460; Yes. Assumed types in F.9.2; N1468; Unknown. Supporting the 'noreturn' property in C1x; N1478; Clang 3.3. Updates to C++ memory model based on formalization; N1480; Unknown. Explicit initializers for atomics; N1482; Unknown. Atomics proposal (minus ternary op); N1485; Yes. UTF-8 string literals; N1488; Clang 3.3. Optimizing away infinite loops; N1509; Yes. Conditional normative status for Annex G; N1514; Unknown. Creation of complex value; N1464; Unknown. Recommendations for extended identifier characters for C and C++; N1518; Unknown. Atomic C1x/C++0x compatibility refinements (1st part only); N1526; Yes. Atomic bitfields implementation defined; N1530; Yes. Small fix for the effect of alignment on struct/union type compatibility; N1532; Yes. Synthesis re _Atomic; N1537; Unknown. Clarification for wide evaluation; N1531; Unknown. C17 implementation status; There are no major changes in this edition, only technical corrections and clarifications that are tracked by Defect Report.; You can use Clang in C17 mode with the -std=c17 or -std=c18 options (available in Clang 6 and later).; C23 implementation status; Clang has support for some of the features of the C standard following C17, informally referred to as C23.; You can use Clang in C23 mode with the -std=c23 option (available in Clang 18 and later) or with the; -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/c_status.html:6017,Optimiz,Optimizing,6017,interpreter/llvm-project/clang/www/c_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/c_status.html,1,['Optimiz'],['Optimizing']
Performance,"ef:`llvm.is.fpclass <llvm.is.fpclass>`. Arguments:; """""""""""""""""""". The first operand is a floating-point vector, the result type is a vector of; boolean with the same number of elements as the first argument. The second; operand specifies, which tests to perform :ref:`llvm.is.fpclass <llvm.is.fpclass>`.; The third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.is.fpclass``' intrinsic performs llvm.is.fpclass (:ref:`llvm.is.fpclass <llvm.is.fpclass>`). Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <2 x i1> @llvm.vp.is.fpclass.v2f16(<2 x half> %x, i32 3, <2 x i1> %m, i32 %evl); %t = call <vscale x 2 x i1> @llvm.vp.is.fpclass.nxv2f16(<vscale x 2 x half> %x, i32 3, <vscale x 2 x i1> %m, i32 %evl). .. _int_mload_mstore:. Masked Vector Load and Store Intrinsics; ---------------------------------------. LLVM provides intrinsics for predicated vector load and store operations. The predicate is specified by a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits of the mask are on, the intrinsic is identical to a regular vector load or store. When all bits are off, no memory is accessed. .. _int_mload:. '``llvm.masked.load.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data is a vector of any integer, floating-point or pointer data type. ::. declare <16 x float> @llvm.masked.load.v16f32.p0(ptr <ptr>, i32 <alignment>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x double> @llvm.masked.load.v2f64.p0(ptr <ptr>, i32 <alignment>, <2 x i1> <mask>, <2 x double> <passthru>); ;; The data is a vector of pointers; declare <8 x ptr> @llvm.masked.load.v8p0.p0(ptr <ptr>, i32 <alignment>, <8 x i1> <mask>, <8 x ptr> <passthru>).",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:842739,load,load,842739,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"efault algorithm provides a more sensible result for p.d.f.s with significant; spillover issues, provided that the p.d.f. can be continuated beyond its original domain.; Convolution in non-observables is also explicitly supported now. One can e.g. construct a p.d.f; of the form G(x) = Int[dy] ( F(x,y) (*) H(y) ). A new tutorial macro rf211_paramconv illustrates; how such convolutions can be constructed; It is now also possible to express FFT convolutions in terms of other observables than the; convolution observable itself. A common occurrence of that situation is a (circular) convolution a polar; angle theta, for a p.d.f. that is ultimately expressed in terms of cos(theta).; A new tutorial macro rf210_angularconv illustrates how to convolutions of angular observable; with or without an optional cosine transformation for the final observable. Option for improved calculation of errors in weighted likelihood fits. A new option SumW2Error() has been added to RooAbsPdf::fitTo() that will; perform an improved error calculation for weighted unbinned likelihood fits. In their unmodified; form, an ML fit to a weighted dataset will correctly estimate the parameters, but the errors will; scale with the sum of the weights, rather than the number of the events in the dataset (i.e.; if you double all event weights, all parameter errors will go down with sqrt(2)). In chi-squared; fits event weights can processed correctly by using both the sum of the weights and the; sum of the weights-squared for each bin. The newly added option SumW2Error() implements a similar; strategy for (unbinned) weighted ML fits by applying a correction to the covariance matrix; as follows. V' = V C-1 V. where V is the covariance matrix from the fit to weighted data, and C-1 is the inverse of the; covariance matrix calculated from a similar likelihood that constructed with the event weights applied squared. Redesign of RooFit dataset class structure. The original class structure of RooFit featured an abst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:12506,perform,perform,12506,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,2,['perform'],['perform']
Performance,"efault and any specified library search paths; (specified to this point). - libpath. The parameter identifies an additional library search path to be considered; when looking up libraries after the inclusion of this option. ``SHT_LLVM_DEPENDENT_LIBRARIES`` Section (Dependent Libraries); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This section contains strings specifying libraries to be added to the link by; the linker. The section should be consumed by the linker and not written to the output. The strings are encoded as standard null-terminated UTF-8 strings. For example:. .. code-block:: gas. .section "".deplibs"",""MS"",@llvm_dependent_libraries,1; .asciz ""library specifier 1""; .asciz ""library specifier 2"". The interpretation of the library specifiers is defined by the consuming linker. ``SHT_LLVM_CALL_GRAPH_PROFILE`` Section (Call Graph Profile); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This section is used to pass a call graph profile to the linker which can be; used to optimize the placement of sections. It contains a sequence of; (from symbol, to symbol, weight) tuples. It shall have a type of ``SHT_LLVM_CALL_GRAPH_PROFILE`` (0x6fff4c02), shall; have the ``SHF_EXCLUDE`` flag set, the ``sh_link`` member shall hold the section; header index of the associated symbol table, and shall have a ``sh_entsize`` of; 16. It should be named ``.llvm.call-graph-profile``. The contents of the section shall be a sequence of ``Elf_CGProfile`` entries. .. code-block:: c. typedef struct {; Elf_Word cgp_from;; Elf_Word cgp_to;; Elf_Xword cgp_weight;; } Elf_CGProfile;. cgp_from; The symbol index of the source of the edge. cgp_to; The symbol index of the destination of the edge. cgp_weight; The weight of the edge. This is represented in assembly as:. .. code-block:: gas. .cg_profile from, to, 42. ``.cg_profile`` directives are processed at the end of the file. It is an error; if either ``from`` or ``to`` are undefined temporary symbols. If either",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Extensions.rst:8262,optimiz,optimize,8262,interpreter/llvm-project/llvm/docs/Extensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Extensions.rst,1,['optimiz'],['optimize']
Performance,"eference frame of the current volume and to; compute the distance to exit its shape from inside. The returned value; is again compared to the maximum allowed step (the proposed one) and in; case the distance is safe no other action is performed and the proposed; step is approved. In case the boundary is closer, the computed distance; is taken as maximum allowed step. For optimization purposed, for; particles starting very close to the current volume boundary (less than; 0.01 microns) and exiting the algorithm stops here. After computing the distance to exit the current node, the distance to; the daughter of the current volume which is crossed next is computed by; TGeoManager::FindNextDaughterBoundary(). This computes the; distance to all daughter candidates that can be possibly crossed by; using volume voxelization. The algorithm is efficient in average only in; case the number of daughters is greater than 4. For fewer nodes, a; simple loop is performed and the minimum distance (from a point outside; each shape) is taken and compared to the maximum allowed step. The step; value is again updated if `step<stepmax` . A special case is when the current node is declared as possibly; overlapping with something else. If this is the case, the distance is; computed for all possibly overlapping candidates, taking into account; the overlapping priorities (see also: "" Overlapping volumes ""). The global matrix describing the next crossed physical node is; systematically computed in case the value of the proposed step is; negative. In this case, one can subsequently call; TGeoManager::ComputeNormalFast() to get the normal vector to the; crossed surface, after propagating the current point with the; TGeoManager::GetStep() value. This propagation can be done like:. ~~~{.cpp}; Double_t *current_point = gGeoManager->GetCurrentPoint();; Double_t *current_dir = gGeoManager->GetCurrentDirection();; for (Int_t i=0; i<3; i++); current_point[i] += step * current_dir[I];; ~~~. Note: The met",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:124058,perform,performed,124058,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['perform'],['performed']
Performance,"efgroup roofit_dev_docs_batchcompute RooBatchCompute library guide; \ingroup roofit_dev_docs; \date September 2021; \author Emmanouil Michalainas; \brief Overview of the RooBatchCompute library. ## RooBatchCompute Library; _Contains optimized computation functions for PDFs that enable significantly faster fittings._; #### Note: This library is still at an experimental stage. Tests are being conducted continuously to ensure correctness of the results, but the interfaces and the instructions on how to use might change. ### Purpose; While fitting, a significant amount of time and processing power is spent on computing the probability function for every event and PDF involved in the fitting model. To speed up this process, roofit can use the computation functions provided in this library. The functions provided here process whole data arrays (batches) instead of a single event at a time, as in the legacy evaluate() function in roofit. In addition, the code is written in a manner that allows for compiler optimizations, notably auto-vectorization. This library is compiled multiple times for different [vector instruction set architectures](https://en.wikipedia.org/wiki/SIMD) and the optimal code is executed during runtime, as a result of an automatic hardware detection mechanism that this library contains. **As a result, fits can benefit by a speedup of 3x-16x.**. As of ROOT v6.26, RooBatchComputes also provides multithread and [CUDA](https://en.wikipedia.org/wiki/CUDA) instances of the computation functions, resulting in even greater improvements for fitting times. ### How to use; This library is an internal component of RooFit, so users are not supposed to actively interact with it. Instead, they can benefit from significantly faster times for fitting by calling `fitTo()` and providing a `BatchMode(""cpu"")` or a `BatchMode(""cuda"")` option.; ``` {.cpp}; // fit using the most efficient library that the computer's CPU can support; RooMyPDF.fitTo(data, BatchMode(""cpu""));. // f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md:1017,optimiz,optimizations,1017,roofit/doc/developers/batchcompute.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md,1,['optimiz'],['optimizations']
Performance,"efined mapping to an; integer representation. This semantic quirk allows the runtime to pick a; integer mapping for each point in the program allowing relocations of objects; without visible effects. This high level abstract machine model is used for most of the optimizer. As; a result, transform passes do not need to be extended to look through explicit; relocation sequence. Before starting code generation, we switch; representations to an explicit form. The exact location chosen for lowering; is an implementation detail. Note that most of the value of the abstract machine model comes for collectors; which need to model potentially relocatable objects. For a compiler which; supports only a non-relocating collector, you may wish to consider starting; with the fully explicit form. Warning: There is one currently known semantic hole in the definition of; non-integral pointers which has not been addressed upstream. To work around; this, you need to disable speculation of loads unless the memory type; (non-integral pointer vs anything else) is known to unchanged. That is, it is; not safe to speculate a load if doing causes a non-integral pointer value to; be loaded as any other type or vice versa. In practice, this restriction is; well isolated to isSafeToSpeculate in ValueTracking.cpp. Explicit Representation; ^^^^^^^^^^^^^^^^^^^^^^^. A frontend could directly generate this low level explicit form, but; doing so may inhibit optimization. Instead, it is recommended that; compilers with relocating collectors target the abstract machine model just; described. The heart of the explicit approach is to construct (or rewrite) the IR in a; manner where the possible updates performed by the garbage collector are; explicitly visible in the IR. Doing so requires that we:. #. create a new SSA value for each potentially relocated pointer, and; ensure that no uses of the original (non relocated) value is; reachable after the safepoint,; #. specify the relocation in a way which is op",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:5835,load,loads,5835,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['load'],['loads']
Performance,"efined when one of; ``-fprofile-use``/``-fprofile-instr-use`` is in effect. The two macros can be used to provide more flexibiilty so a user program; can execute code specifically intended for profile generate or profile use.; For example, a user program can have special logging during profile generate:. .. code-block:: c. #if __LLVM_INSTR_PROFILE_GENERATE; expensive_logging_of_full_program_state();; #endif. The logging is automatically excluded during a normal build of the program,; hence it does not impact performance during a normal execution. It is advised to use such fine tuning only in a program's cold regions. The weak; symbols can introduce extra control flow (the ``if`` checks), while the macros; (hence declarations they guard in ``profile/instr_prof_interface.h``); can change the control flow of the functions that use them between profile; generation and profile use (which can lead to discarded counters in such; functions). Using these APIs in the program's cold regions introduces less; overhead and leads to more optimized code. Disabling Instrumentation; ^^^^^^^^^^^^^^^^^^^^^^^^^. In certain situations, it may be useful to disable profile generation or use; for specific files in a build, without affecting the main compilation flags; used for the other files in the project. In these cases, you can use the flag ``-fno-profile-instr-generate`` (or; ``-fno-profile-generate``) to disable profile generation, and; ``-fno-profile-instr-use`` (or ``-fno-profile-use``) to disable profile use. Note that these flags should appear after the corresponding profile; flags to have an effect. .. note::. When none of the translation units inside a binary is instrumented, in the; case of Fuchsia the profile runtime will not be linked into the binary and; no profile will be produced, while on other platforms the profile runtime; will be linked and profile will be produced but there will not be any; counters. Instrumenting only selected files or functions; ^^^^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:114913,optimiz,optimized,114913,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimized']
Performance,"efined. The argument itself is never; evaluated, so any side effects of the expression will be discarded. Query for this feature with ``__has_builtin(__builtin_assume)``. .. _langext-__builtin_assume_separate_storage:. ``__builtin_assume_separate_storage``; -------------------------------------. ``__builtin_assume_separate_storage`` is used to provide the optimizer with the; knowledge that its two arguments point to separately allocated objects. **Syntax**:. .. code-block:: c++. __builtin_assume_separate_storage(const volatile void *, const volatile void *). **Example of Use**:. .. code-block:: c++. int foo(int *x, int *y) {; __builtin_assume_separate_storage(x, y);; *x = 0;; *y = 1;; // The optimizer may optimize this to return 0 without reloading from *x.; return *x;; }. **Description**:. The arguments to this function are assumed to point into separately allocated; storage (either different variable definitions or different dynamic storage; allocations). The optimizer may use this fact to aid in alias analysis. If the; arguments point into the same storage, the behavior is undefined. Note that the; definition of ""storage"" here refers to the outermost enclosing allocation of any; particular object (so for example, it's never correct to call this function; passing the addresses of fields in the same struct, elements of the same array,; etc.). Query for this feature with ``__has_builtin(__builtin_assume_separate_storage)``. ``__builtin_offsetof``; ----------------------. ``__builtin_offsetof`` is used to implement the ``offsetof`` macro, which; calculates the offset (in bytes) to a given member of the given type. **Syntax**:. .. code-block:: c++. __builtin_offsetof(type-name, member-designator). **Example of Use**:. .. code-block:: c++. struct S {; char c;; int i;; struct T {; float f[2];; } t;; };. const int offset_to_i = __builtin_offsetof(struct S, i);; const int ext1 = __builtin_offsetof(struct U { int i; }, i); // C extension; const int offset_to_subobject = __",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:100324,optimiz,optimizer,100324,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizer']
Performance,"efore; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:264244,load,load,264244,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"efore; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:271200,cache,caches,271200,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"efore; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:251642,cache,cache,251642,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"eger value. The fourth operand, mask, is a vector of boolean values. The types of the mask and the value operand must have the same number of vector elements. Semantics:; """""""""""""""""""". The '``llvm.masked.store``' intrinsics is designed for conditional writing of selected vector elements in a single IR operation. It is useful for targets that support vector masked store and allows vectorizing predicated basic blocks on these targets. Other targets may support this intrinsic differently, for example by lowering it into a sequence of branches that guard scalar store operations.; The result of this operation is equivalent to a load-modify-store sequence. However, using this intrinsic prevents exceptions and data races on memory access to masked-off lanes. ::. call void @llvm.masked.store.v16f32.p0(<16 x float> %value, ptr %ptr, i32 4, <16 x i1> %mask). ;; The result of the following instructions is identical aside from potential data races and memory access exceptions; %oldval = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %value, <16 x float> %oldval; store <16 x float> %res, ptr %ptr, align 4. Masked Vector Gather and Scatter Intrinsics; -------------------------------------------. LLVM provides intrinsics for vector gather and scatter operations. They are similar to :ref:`Masked Vector Load and Store <int_mload_mstore>`, except they are designed for arbitrary memory accesses, rather than sequential memory accesses. Gather and scatter also employ a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits are off, no memory is accessed. .. _int_mgather:. '``llvm.masked.gather.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data are multiple scalar values of any integer, floating-point or pointer data type gathered together into one vector",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:847642,load,load,847642,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"egisters around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3817,cache,cache,3817,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,2,"['cache', 'optimiz']","['cache', 'optimized']"
Performance,"egments: TGeoConeSeg class; - Sphere: TGeoSphere class; - Torus: TGeoTorus class; - Paraboloid: TGeoParaboloid class; - Polycone: TGeoPcon class; - Polygon: TGeoPgon class; - Polygonal extrusion: TGeoXtru class; - Half Spaces: TGeoHalfSpace class; - Composite Shapes: TGeoCompositeShape class. \anchor SHAPES02; ### Navigation Methods Performed By Shapes. Shapes are named objects and register themselves to the `manager class`; at creation time. This is responsible for their final deletion. Shapes; can be created without name if their retrieval by name is no needed.; Generally shapes are objects that are useful only at geometry creation; stage. The pointer to a shape is in fact needed only when referring to a; given volume and it is always accessible at that level. Several volumes; may reference a single shape; therefore its deletion is not possible; once volumes were defined based on it. The navigation features related for instance to tracking particles are; performed in the following way: Each shape implement its specific; algorithms for all required tasks in its local reference system. Note; that the manager class handles global queries related to geometry.; However, shape-related queries might be sometimes useful:. ~~~ {.cpp}; Bool_t TGeoShape::Contains(Double_t *point[3]);; ~~~. The method above returns `kTRUE` if the point \*point is actually inside; the shape. The point has to be defined in the local shape reference. For; instance, for a box having `DX,DY` and `DZ `half-lengths a point will be; considered inside if:. `-DX <= point[0] <= DX`. `-DY <= point[1] <= DY`. `-DZ <= point[2] <= DZ`. ~~~ {.cpp}; Double_t TGeoShape::DistFromInside(Double_t *point[3],; Double_t *dir[3], Int_t iact,Double_t step,Double_t *safe);; ~~~. The method computes the distance to exiting a shape from a given point; `inside`, along a given direction. This direction is given by its; director cosines with respect to the local shape coordinate system. This; method provides additional info",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/shapes.md:3790,perform,performed,3790,geom/geom/doc/shapes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/shapes.md,1,['perform'],['performed']
Performance,"eir; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:375639,load,load,375639,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"el.; $ clang++ -std=c++20 -O3 Use.cpp -fprebuilt-module-path=.; # Inconsistent debugging level.; $ clang++ -std=c++20 -g Use.cpp -fprebuilt-module-path=. Although the two examples have inconsistent optimization and debugging level, both of them are accepted. Note that **currently** the compiler doesn't consider inconsistent macro definition a problem. For example:. .. code-block:: console. $ clang++ -std=c++20 M.cppm --precompile -o M.pcm; # Inconsistent optimization level.; $ clang++ -std=c++20 -O3 -DNDEBUG Use.cpp -fprebuilt-module-path=. Currently Clang would accept the above example. But it may produce surprising results if the; debugging code depends on consistent use of ``NDEBUG`` also in other translation units. Definitions consistency; ^^^^^^^^^^^^^^^^^^^^^^^. The C++ language defines that same declarations in different translation units should have; the same definition, as known as ODR (One Definition Rule). Prior to modules, the translation; units don't dependent on each other and the compiler itself can't perform a strong; ODR violation check. With the introduction of modules, now the compiler have; the chance to perform ODR violations with language semantics across translation units. However, in the practice, we found the existing ODR checking mechanism is not stable; enough. Many people suffers from the false positive ODR violation diagnostics, AKA,; the compiler are complaining two identical declarations have different definitions; incorrectly. Also the true positive ODR violations are rarely reported.; Also we learned that MSVC don't perform ODR check for declarations in the global module; fragment. So in order to get better user experience, save the time checking ODR and keep consistent; behavior with MSVC, we disabled the ODR check for the declarations in the global module; fragment by default. Users who want more strict check can still use the; ``-Xclang -fno-skip-odr-check-in-gmf`` flag to get the ODR check enabled. It is also; encouraged to repor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:17230,perform,perform,17230,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['perform'],['perform']
Performance,"elect <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_ctlz:. '``llvm.vp.ctlz.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.ctlz.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>); declare <vscale x 4 x i32> @llvm.vp.ctlz.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>); declare <256 x i64> @llvm.vp.ctlz.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>). Overview:; """""""""""""""""". Predicated ctlz of a vector of integers. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of integer type. The; second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.ctlz``' intrinsic performs ctlz (:ref:`ctlz <int_ctlz>`) of the first operand on each; enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.ctlz.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl, i1 false); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.ctlz.v4i32(<4 x i32> %a, i1 false); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_cttz:. '``llvm.vp.cttz.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.cttz.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>); declare <vscale x 4 x i32> @llvm.vp.cttz.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>); declare <256 x i64> @llvm.vp.cttz.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>). Overview:; """"""""""""""""""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:836268,perform,performs,836268,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"elect <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_cttz:. '``llvm.vp.cttz.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.cttz.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>); declare <vscale x 4 x i32> @llvm.vp.cttz.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>); declare <256 x i64> @llvm.vp.cttz.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>, i1 <is_zero_poison>). Overview:; """""""""""""""""". Predicated cttz of a vector of integers. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of integer type. The; second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.cttz``' intrinsic performs cttz (:ref:`cttz <int_cttz>`) of the first operand on each; enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.cttz.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl, i1 false); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.cttz.v4i32(<4 x i32> %a, i1 false); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_fshl:. '``llvm.vp.fshl.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.fshl.v16i32 (<16 x i32> <left_op>, <16 x i32> <middle_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.fshl.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <middle_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.fshl.v256i64 (<256 x i64> <left_op>,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:837665,perform,performs,837665,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"electively enables or disables vectorization for the loop. The; first operand is the string ``llvm.loop.vectorize.enable`` and the second operand; is a bit. If the bit operand value is 1 vectorization is enabled. A value of; 0 disables vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.enable"", i1 1}. '``llvm.loop.vectorize.predicate.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables creating predicated instructions; for the loop, which can enable folding of the scalar epilogue loop into the; main loop. The first operand is the string; ``llvm.loop.vectorize.predicate.enable`` and the second operand is a bit. If; the bit operand value is 1 vectorization is enabled. A value of 0 disables; vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.predicate.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.predicate.enable"", i1 1}. '``llvm.loop.vectorize.scalable.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata selectively enables or disables scalable vectorization for the; loop, and only has any effect if vectorization for the loop is already enabled.; The first operand is the string ``llvm.loop.vectorize.scalable.enable``; and the second operand is a bit. If the bit operand value is 1 scalable; vectorization is enabled, whereas a value of 0 reverts to the default fixed; width vectorization:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.scalable.enable"", i1 0}; !1 = !{!""llvm.loop.vectorize.scalable.enable"", i1 1}. '``llvm.loop.vectorize.width``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata sets the target width of the vectorizer. The first; operand is the string ``llvm.loop.vectorize.width`` and the second; operand is an integer specifying the width. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.vectorize.width"", i32 4}. Note that setting ``llvm.loop.vectorize.width`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:298467,scalab,scalable,298467,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"elihood estimator; rf606_nllerrorhandling.C - Understanding and customizing error handling in likelihood evaluations; rf607_fitresult.C - Demonstration of options of the RooFitResult class; ; SPECIAL PDFS. rf701_efficiencyfit.C - Unbinned maximum likelihood fit of an efficiency eff(x) function; rf702_efficiencyfit_2D.C - Unbinned maximum likelihood fit of an efficiency eff(x) function to; rf703_effpdfprod.C - Using a product of an (acceptance) efficiency and a p.d.f as p.d.f.; rf704_amplitudefit.C - Using a p.d.f defined by a sum of real-valued amplitude components; rf705_linearmorph.C - Linear interpolation between p.d.f shapes using the 'Alex Read' algorithm; rf706_histpdf.C - Histogram based p.d.f.s and functions; rf707_kernelestimation.C - Using non-parametric (multi-dimensional) kernel estimation p.d.f.s; rf708_bphysics.C - Special decay pdf for B physics with mixing and/or CP violation; ; VALIDATION AND MC STUDIES. rf801_mcstudy.C - A Toy Monte Carlo study that perform cycles of event generation and fittting; rf802_mcstudy_addons.C - RooMCStudy: using separate fit and generator models, using the chi^2 calculator model; rf803_mcstudy_addons2.C - RooMCStudy: Using the randomizer and profile likelihood add-on models; rf804_mcstudy_constr.C - Using RooMCStudy on models with constrains; ; Miscellaneous small improvements. A very large number of small fixes and interface improvements have been made in the context of the systematic review of all methods for the new tutorial macros and updated Users Manual.; Listed below are the most significant functionality upgrades that were introduced in the process. ; Runtime binding of C++ functions - You can now trivially bind at run time any C++ functions as a RooFit function or p.d.f. objects, e.g. RooAbsReal* erfx = bindFunction(""erfx"",TMath::erf,x). See rf105_funcbinding.C for details; Runtime binding of TFx functions - You can now trivially bind at run time any ROOT TFx function as a RooFit function or p.d.f. objects, e.g. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html:6074,perform,perform,6074,roofit/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html,2,['perform'],['perform']
Performance,"eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM that increase live-ranges. This pass uses alias analysis for two purposes:. #. Moving loop invariant loads and calls out of loops. If we can determine; that a load or call inside of a loop never aliases anything stored to, we; can hoist it or sink it like any other instruction. #. Scalar Promotion of Memory. If there is a store instruction inside of the; loop, we try to move the store to happen AFTER the loop instead of inside of; the loop. This can only happen if a few conditions are true:. #. The pointer stored through is loop invariant.; #. There are no stores or loads in the loop which *may* alias the pointer.; There are no calls in the loop which mod/ref the pointer. If these conditions are true, we can promote the loads and stores in the; loop of the pointer to use a temporary alloca'd variable. We then use the; :ref:`mem2reg <passes-mem2reg>` functionality to construct the appropriat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:24205,optimiz,optimizations,24205,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimizations']
Performance,"ell defined. #. It constrained and/or prohibited the use of features requiring runtime; support, e.g. static initializers or thread local storage. As a result of these restrictions not all language features supported by LLVM; worked under MCJIT, and objects to be loaded under the JIT had to be compiled to; target it (precluding the use of precompiled code from other sources under the; JIT). RuntimeDyld also provided very limited visibility into the linking process; itself: Clients could access conservative estimates of section size; (RuntimeDyld bundled stub size and padding estimates into the section size; value) and the final relocated bytes, but could not access RuntimeDyld's; internal object representations. Eliminating these restrictions and limitations was one of the primary motivations; for the development of JITLink. The llvm-jitlink tool; =====================. The ``llvm-jitlink`` tool is a command line wrapper for the JITLink library.; It loads some set of relocatable object files and then links them using; JITLink. Depending on the options used it will then execute them, or validate; the linked memory. The ``llvm-jitlink`` tool was originally designed to aid JITLink development by; providing a simple environment for testing. Basic usage; -----------. By default, ``llvm-jitlink`` will link the set of objects passed on the command; line, then search for a ""main"" function and execute it:. .. code-block:: sh. % cat hello-world.c; #include <stdio.h>. int main(int argc, char *argv[]) {; printf(""hello, world!\n"");; return 0;; }. % clang -c -o hello-world.o hello-world.c; % llvm-jitlink hello-world.o; Hello, World!. Multiple objects may be specified, and arguments may be provided to the JIT'd; main function using the -args option:. .. code-block:: sh. % cat print-args.c; #include <stdio.h>. void print_args(int argc, char *argv[]) {; for (int i = 0; i != argc; ++i); printf(""arg %i is \""%s\""\n"", i, argv[i]);; }. % cat print-args-main.c; void print_args(int argc, ch",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:37062,load,loads,37062,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['load'],['loads']
Performance,"else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now :) :. ::. ready> def foo(x) x + 1;; ready> foo(2);; Evaluated to 3.000000. ready> def foo(x) x + 2;; ready> foo(2);; Evaluated to 4.000000. It works!. Even with this simple code, we get some surprisingly powerful capabilities -; check this out:. ::. ready> extern sin(x);; Read extern:; declare double @sin(double). ready> extern cos(x);; Read extern:; declare double @cos(double). ready> sin(1.0);; Read top-level expression:; define double @2() {; entry:; ret double 0x3FEAED548F090CEE; }. Evaluated to 0.841471. ready> def foo(x) sin(x)*sin(x) + cos(x)*cos(x);; Read function definition:; define double @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltm",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:21300,concurren,concurrent,21300,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['concurren'],['concurrent']
Performance,"else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partitions of the; memory (that is, if you have one variable, as above, it represents the entire; memory, and if you have multiple variables, each one represents some; disjoint portion of the memory). First, because alias analysis results conflict with each other, and; each result may be what an analysis wants (IE; TBAA may say no-alias, and something else may say must-alias), it is; not possible to partition the memory the way every optimization wants.; Second, some alias analysis results are not transitive (IE A noalias B,; and B noalias C, does not mean A noalias C), so it is not possible to; come up with a precise partitioning in all cases without variables to; represent every pair of possible aliases. Thus, partitioning; precisely may require introducing at least N^2 new v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:16542,load,load,16542,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['load'],['load']
Performance,"em --- A method of instruction selection for code; generation. An example is the `BURG; <http://www.program-transformation.org/Transform/BURG>`_ tool. C; -. **CFI**; This abbreviation has two meanings.; Either:; Call Frame Information. Used in DWARF debug info and in C++ unwind info; to show how the function prolog lays out the stack frame. Or:; Control Flow Integrity. A general term for computer security techniques; that prevent a wide variety of malware attacks from redirecting the flow; of execution (the control flow) of a program. **CIE**; Common Information Entry. A kind of CFI used to reduce the size of FDEs.; The compiler creates a CIE which contains the information common across all; the FDEs. Each FDE then points to its CIE. **CSE**; Common Subexpression Elimination. An optimization that removes common; subexpression computation. For example ``(a+b)*(a+b)`` has two; subexpressions that are the same: ``(a+b)``. This optimization would; perform the addition only once and then perform the multiply (but only if; it's computationally correct/safe). D; -. **DAG**; Directed Acyclic Graph. .. _derived pointer:; .. _derived pointers:. **Derived Pointer**; A pointer to the interior of an object, such that a garbage collector is; unable to use the pointer for reachability analysis. While a derived pointer; is live, the corresponding object pointer must be kept in a root, otherwise; the collector might free the referenced object. With copying collectors,; derived pointers pose an additional hazard that they may be invalidated at; any `safe point`_. This term is used in opposition to `object pointer`_. **DSA**; Data Structure Analysis. **DSE**; Dead Store Elimination. E; -. **ento**; This namespace houses the; `Clang Static Analyzer <https://clang.llvm.org/docs/ClangStaticAnalyzer.html>`_.; It is an abbreviation of `entomology <https://en.wikipedia.org/wiki/Entomology>`_. *""Entomology is the scientific study of insects.""*. In the past, this namespace had not only the na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Lexicon.rst:2222,optimiz,optimization,2222,interpreter/llvm-project/llvm/docs/Lexicon.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Lexicon.rst,3,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"em to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:232870,load,load,232870,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,6,['load'],['load']
Performance,"em to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:366817,load,loads,366817,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"em. You need the `cvmfs` package,; you *don't* need the `cvmfs-devel` or `cvmfs-server` ones. - As root user, run:. # cvmfs_config setup. - Start the `autofs` service: how to to this depends on your operating; system. On Ubuntu using Upstart:. # restart autofs. On RHEL-based or older Ubuntus:. # service autofs restart. - Prepare a `/etc/cvmfs/default.local` file (create it if it does not; exists) with the following configuration bits:. ``` {.bash}; CVMFS_HTTP_PROXY=http://your-proxy-server.domain.ch:3128,DIRECT; CVMFS_REPOSITORIES=your-experiment.cern.ch,sft.cern.ch; CVMFS_QUOTA_LIMIT=50000; ```. You need to properly specify your closest HTTP caching proxy:; separate many of them via commas. The last fallback value, `DIRECT`,; tells cvmfs to connect directly without using any proxy at all. Among the list of repositories (comma-separated), always specify; `sft.cern.ch` and the one containing the software to your experiment; (e.g., `cms.cern.ch`). The quota limit is, in Megabytes, the amount of local disk space to; use as cache. - Check the configuration and repositories with:. # cvmfs_config chksetup; OK; # cvmfs_config probe; Probing /cvmfs/cms.cern.ch... OK; Probing /cvmfs/sft.cern.ch... OK. > You might need special configurations for some custom software; > repositories! Special cases are not covered in this guide. ### Firewall configuration. [PROOF on Demand](http://pod.gsi.de/) is very flexible in handling; various cases of network topologies. The best solution would be to allow; all TCP communications between the cluster machines. No other incoming communication is required from the outside. Configuration steps for the head node only; ------------------------------------------. ### Setup HTTPS+SSH (sshcertauth) authentication. > Latest recommended sshcertauth version is 0.8.5.; >; > [Download](https://github.com/dberzano/sshcertauth/archive/v0.8.5.zip); > and [read the; > instructions](http://newton.ph.unito.it/~berzano/w/doku.php?id=proof:sshcertauth). If you w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md:3366,cache,cache,3366,proof/doc/confman/ConfigProofPoD.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md,1,['cache'],['cache']
Performance,"emantics this is optimized to the following equivalent code:. .. code-block:: c. r1 = ...; if (!r0); // Incorrect functionality! The data in r1; // have not been computed by all threads yet.; r3 = my_sub_group_shuffle(r1, r2);; else {; r1 = computeA();; r3 = my_sub_group_shuffle(r1, r2);; r3 = computeB();; }. Declaring the function ``my_sub_group_shuffle`` with the convergent attribute; would prevent this:. .. code-block:: c. my_sub_group_shuffle() __attribute__((convergent));. Using ``convergent`` guarantees correct execution by keeping CFG equivalence; wrt operations marked as ``convergent``. CFG ``G`` is equivalent to ``G`` wrt; node ``Ni`` : ``iff  Nj (ij)`` domination and post-domination relations with; respect to ``Ni`` remain the same in both ``G`` and ``G``. noduplicate; ^^^^^^^^^^^. ``noduplicate`` is more restrictive with respect to optimizations than; ``convergent`` because a convergent function only preserves CFG equivalence.; This allows some optimizations to happen as long as the control flow remains; unmodified. .. code-block:: c. for (int i=0; i<4; i++); my_sub_group_shuffle(). can be modified to:. .. code-block:: c. my_sub_group_shuffle();; my_sub_group_shuffle();; my_sub_group_shuffle();; my_sub_group_shuffle();. while using ``noduplicate`` would disallow this. Also ``noduplicate`` doesn't; have the same safe semantics of CFG as ``convergent`` and can cause changes in; CFG that modify semantics of the original program. ``noduplicate`` is kept for backwards compatibility only and it considered to be; deprecated for future uses. .. _cxx_for_opencl:. C++ for OpenCL; --------------. Starting from clang 9 kernel code can contain C++17 features: classes, templates,; function overloading, type deduction, etc. Please note that this is not an; implementation of `OpenCL C++; <https://www.khronos.org/registry/OpenCL/specs/2.2/pdf/OpenCL_Cxx.pdf>`_ and; there is no plan to support it in clang in any new releases in the near future. Clang currently supports ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:151583,optimiz,optimizations,151583,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"embly printer:. .. code-block:: c++. extern ""C"" void LLVMInitializeSparcAsmPrinter() {; RegisterAsmPrinter<SparcAsmPrinter> X(getTheSparcTarget());; }. For more information, see ""`llvm/Target/TargetRegistry.h; </doxygen/TargetRegistry_8h-source.html>`_"". Register Set and Register Classes; =================================. You should describe a concrete target-specific class that represents the; register file of a target machine. This class is called ``XXXRegisterInfo``; (where ``XXX`` identifies the target) and represents the class register file; data that is used for register allocation. It also describes the interactions; between registers. You also need to define register classes to categorize related registers. A; register class should be added for groups of registers that are all treated the; same way for some instruction. Typical examples are register classes for; integer, floating-point, or vector registers. A register allocator allows an; instruction to use any register in a specified register class to perform the; instruction in a similar manner. Register classes allocate virtual registers; to instructions from these sets, and register classes let the; target-independent register allocator automatically choose the actual; registers. Much of the code for registers, including register definition, register; aliases, and register classes, is generated by TableGen from; ``XXXRegisterInfo.td`` input files and placed in ``XXXGenRegisterInfo.h.inc``; and ``XXXGenRegisterInfo.inc`` output files. Some of the code in the; implementation of ``XXXRegisterInfo`` requires hand-coding. Defining a Register; -------------------. The ``XXXRegisterInfo.td`` file typically starts with register definitions for; a target machine. The ``Register`` class (specified in ``Target.td``) is used; to define an object for each register. The specified string ``n`` becomes the; ``Name`` of the register. The basic ``Register`` object does not have any; subregisters and does not specify any a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:14774,perform,perform,14774,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['perform']
Performance,"ement <8 x i32> %value, i32 7; %ptr0 = extractelement <8 x ptr> %ptrs, i32 0; %ptr1 = extractelement <8 x ptr> %ptrs, i32 1; ..; %ptr7 = extractelement <8 x ptr> %ptrs, i32 7; ;; Note: the order of the following stores is important when they overlap:; store i32 %val0, ptr %ptr0, align 4; store i32 %val1, ptr %ptr1, align 4; ..; store i32 %val7, ptr %ptr7, align 4. Masked Vector Expanding Load and Compressing Store Intrinsics; -------------------------------------------------------------. LLVM provides intrinsics for expanding load and compressing store operations. Data selected from a vector according to a mask is stored in consecutive memory addresses (compressed store), and vice-versa (expanding load). These operations effective map to ""if (cond.i) a[j++] = v.i"" and ""if (cond.i) v.i = a[j++]"" patterns, respectively. Note that when the mask starts with '1' bits followed by '0' bits, these operations are identical to :ref:`llvm.masked.store <int_mstore>` and :ref:`llvm.masked.load <int_mload>`. .. _int_expandload:. '``llvm.masked.expandload.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. Several values of integer, floating point or pointer data type are loaded from consecutive memory addresses and stored into the elements of a vector according to the mask. ::. declare <16 x float> @llvm.masked.expandload.v16f32 (ptr <ptr>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x i64> @llvm.masked.expandload.v2i64 (ptr <ptr>, <2 x i1> <mask>, <2 x i64> <passthru>). Overview:; """""""""""""""""". Reads a number of scalar values sequentially from memory location provided in '``ptr``' and spreads them in a vector. The '``mask``' holds a bit for each vector lane. The number of elements read from memory is equal to the number of '1' bits in the mask. The loaded elements are positioned in the destination vector according to the sequence of '1' and '0' bits in the mask. E.g., if the mask vector is '10010001', ""expandload",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:855069,load,load,855069,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ementation for the element type, but it provides some novel; characteristics. In particular, it can efficiently store polymorphic objects,; the traits class is informed when an element is inserted or removed from the; list, and ``ilist``\ s are guaranteed to support a constant-time splice; operation. An ``ilist`` and an ``iplist`` are ``using`` aliases to one another and the; latter only currently exists for historical purposes. These properties are exactly what we want for things like ``Instruction``\ s and; basic blocks, which is why these are implemented with ``ilist``\ s. Related classes of interest are explained in the following subsections:. * :ref:`ilist_traits <dss_ilist_traits>`. * :ref:`llvm/ADT/ilist_node.h <dss_ilist_node>`. * :ref:`Sentinels <dss_ilist_sentinel>`. .. _dss_packedvector:. llvm/ADT/PackedVector.h; ^^^^^^^^^^^^^^^^^^^^^^^. Useful for storing a vector of values using only a few number of bits for each; value. Apart from the standard operations of a vector-like container, it can; also perform an 'or' set operation. For example:. .. code-block:: c++. enum State {; None = 0x0,; FirstCondition = 0x1,; SecondCondition = 0x2,; Both = 0x3; };. State get() {; PackedVector<State, 2> Vec1;; Vec1.push_back(FirstCondition);. PackedVector<State, 2> Vec2;; Vec2.push_back(SecondCondition);. Vec1 |= Vec2;; return Vec1[0]; // returns 'Both'.; }. .. _dss_ilist_traits:. ilist_traits; ^^^^^^^^^^^^. ``ilist_traits<T>`` is ``ilist<T>``'s customization mechanism. ``ilist<T>``; publicly derives from this traits class. .. _dss_ilist_node:. llvm/ADT/ilist_node.h; ^^^^^^^^^^^^^^^^^^^^^. ``ilist_node<T>`` implements the forward and backward links that are expected; by the ``ilist<T>`` (and analogous containers) in the default manner. ``ilist_node<T>``\ s are meant to be embedded in the node type ``T``, usually; ``T`` publicly derives from ``ilist_node<T>``. .. _dss_ilist_sentinel:. Sentinels; ^^^^^^^^^. ``ilist``\ s have another specialty that must be considered. To be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:67919,perform,perform,67919,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['perform']
Performance,"ementing methods follow the signature of the static type.; It is undefined behavior if ARC is exposed to an invalid pointer. For ARC's purposes, a valid object is one with ""well-behaved"" retaining; operations. Specifically, the object must be laid out such that the; Objective-C message send machinery can successfully send it the following; messages:. * ``retain``, taking no arguments and returning a pointer to the object.; * ``release``, taking no arguments and returning ``void``.; * ``autorelease``, taking no arguments and returning a pointer to the object. The behavior of these methods is constrained in the following ways. The term; :arc-term:`high-level semantics` is an intentionally vague term; the intent is; that programmers must implement these methods in a way such that the compiler,; modifying code in ways it deems safe according to these constraints, will not; violate their requirements. For example, if the user puts logging statements; in ``retain``, they should not be surprised if those statements are executed; more or less often depending on optimization settings. These constraints are; not exhaustive of the optimization opportunities: values held in local; variables are subject to additional restrictions, described later in this; document. It is undefined behavior if a computation history featuring a send of; ``retain`` followed by a send of ``release`` to the same object, with no; intervening ``release`` on that object, is not equivalent under the high-level; semantics to a computation history in which these sends are removed. Note that; this implies that these methods may not raise exceptions. It is undefined behavior if a computation history features any use whatsoever; of an object following the completion of a send of ``release`` that is not; preceded by a send of ``retain`` to the same object. The behavior of ``autorelease`` must be equivalent to sending ``release`` when; one of the autorelease pools currently in scope is popped. It may not throw a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:13888,optimiz,optimization,13888,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"emit diagnostics from passes describing whether an optimization; has been performed or missed for a particular reason, which should give more; insight to users about what the compiler did during the compilation pipeline. There are three main remark types:. ``Passed``. Remarks that describe a successful optimization performed by the compiler. :Example:. ::. foo inlined into bar with (cost=always): always inline attribute. ``Missed``. Remarks that describe an attempt to an optimization by the compiler that; could not be performed. :Example:. ::. foo not inlined into bar because it should never be inlined; (cost=never): noinline function attribute. ``Analysis``. Remarks that describe the result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is often more useful to; refer to optimization remarks post-compilation, typically during performance; analysis. For that, LLVM can serialize the remarks produced",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:1139,optimiz,optimization,1139,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,"emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27535,optimiz,optimized,27535,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"emory operations. Private address space uses ``buffer_load/store`` using the scratch V#; (GFX6-GFX8), or ``scratch_load/store`` (GFX9-GFX11). Since only a single thread; is accessing the memory, atomic memory orderings are not meaningful, and all; accesses are treated as non-atomic. Constant address space uses ``buffer/global_load`` instructions (or equivalent; scalar memory instructions). Since the constant address space contents do not; change during the execution of a kernel dispatch it is not legal to perform; stores, and atomic memory orderings are not meaningful, and all accesses are; treated as non-atomic. A memory synchronization scope wider than work-group is not meaningful for the; group (LDS) address space and is treated as work-group. The memory model does not support the region address space which is treated as; non-atomic. Acquire memory ordering is not meaningful on store atomic instructions and is; treated as non-atomic. Release memory ordering is not meaningful on load atomic instructions and is; treated a non-atomic. Acquire-release memory ordering is not meaningful on load or store atomic; instructions and is treated as acquire and release respectively. The memory order also adds the single thread optimization constraints defined in; table; :ref:`amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table`. .. table:: AMDHSA Memory Model Single Thread Optimization Constraints; :name: amdgpu-amdhsa-memory-model-single-thread-optimization-constraints-table. ============ ==============================================================; LLVM Memory Optimization Constraints; Ordering; ============ ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:203986,load,load,203986,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"emory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consiste",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:229165,load,load,229165,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"emory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: These are predicates on; orderings. They can be useful for passes that are aware of atomics, for; example to do DSE across a single atomic access, but not across a; release-acquire pair (see MemoryDependencyAnalysis for an example of this). * Alias analysis: Note that AA will return ModRef for anyt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15686,load,loads,15686,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"ems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked abo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:5952,load,load-linked,5952,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,1,['load'],['load-linked']
Performance,"en ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_count``). Pragmas setting transformation options; imply the transformation is enabled, as if it was enabled via the corresponding; transformation pragma (e.g. ``vectorize(enable)``). If the transformation is; disabled (e.g. ``vectorize(disable)``), that takes precedence over; transformations option pragmas implying that transformation. Vectorization, Interleaving, and Predication; --------------------------------------------. A vectorized loop performs multiple iterations of the original loop; in parallel using vector instru",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:162360,optimiz,optimizing,162360,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizing']
Performance,"en after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:359778,load,load,359778,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"en an object is stored in a split branch; the rule is associtated with the branch of the last of the rule's sources rather; than the last of the object's data member. - Properly support TStreamerInfo written by ROOT v4.00. - Fix the ordering of the keys in a TFile being written; in particular fixing the result of GetKey and FindKey which were no longer returning the lastest cycle for a TFile being written since v5.34/11. ## Networking Libraries. ### HTTP Server. ##### Command Interface; One can now register an arbitrary command to the server, which become visible in the web browser. Then, when the item is clicked by the user, the command ends-up in a gROOT->ProcessLineSync() call. ##### Custom Properties ; Custom properties can be configured for any item in the server. For example, one could configure an icon for each item visible in the browser. Or one could 'hide' any item from the user (but keep access with normal http requests). With such properties one could specify which item is drawn when web page is loaded, or configure monitoring. See tutorials/http/httpcontrol.C macro for more details. ##### Method Calls; Implement exe.json requests to be able to execute any method of registered objects. This request is used to provide remote TTree::Draw() functionality. ##### Misc; Correctly set 'Cache-Control' headers when replying to http requests.; Better support of STL containers when converting objects into json with TBufferJSON class. ## JavaScript ROOT. - Several files can now be loaded simultaneously; - Use d3.time.scale to display time scales; - Implemented drag and drop to superimpose histograms or graphs; - Allow selection of drawing option via context menu; - Better support of touch devices; - Provide simple layout, making it default; - Allow to open ROOT files in online session (via url parameter); - One could monitor simultaneously objects from server and root files; - Implement 'autocol' draw option - when superimposing histograms,; their line colors will be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:9922,load,loaded,9922,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['load'],['loaded']
Performance,"en be confined to the region by reading their leaf; inputs via volatile loads and writing their root outputs via volatile stores. '``llvm.seh.scope.begin``' and '``llvm.seh.scope.end``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.seh.scope.begin(); declare void @llvm.seh.scope.end(). Overview:; """""""""""""""""". The '``llvm.seh.scope.begin``' and '``llvm.seh.scope.end``' intrinsics mark; the boundary of a CPP object lifetime for Windows SEH Asynchrous Exception; Handling (MSVC option -EHa). Semantics:; """""""""""""""""""". LLVM's ordinary exception-handling representation associates EH cleanups and; handlers only with ``invoke``s, which normally correspond only to call sites. To; support arbitrary faulting instructions, it must be possible to recover the current; EH scope for any instruction. Turning every operation in LLVM that could fault; into an ``invoke`` of a new, potentially-throwing intrinsic would require adding a; large number of intrinsics, impede optimization of those operations, and make; compilation slower by introducing many extra basic blocks. These intrinsics can; be used instead to mark the region protected by a cleanup, such as for a local; C++ object with a non-trivial destructor. ``llvm.seh.scope.begin`` is used to mark; the start of the region; it is always called with ``invoke``, with the unwind block; being the desired unwind destination for any potentially-throwing instructions; within the region. `llvm.seh.scope.end` is used to mark when the scope ends; and the EH cleanup is no longer required (e.g. because the destructor is being; called). .. _int_read_register:; .. _int_read_volatile_register:; .. _int_write_register:. '``llvm.read_register``', '``llvm.read_volatile_register``', and '``llvm.write_register``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i32 @llvm.read_register.i32(",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:516868,optimiz,optimization,516868,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"en before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; sc0=1 sc1=1; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of Ope",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:313505,load,load,313505,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"en creating; a matrix or a translation, this is by default owned by external objects.; The manager class becomes owner of all transformations used for; positioning volumes. In order to force the ownership for other; transformations, one can use `TGeoMatrix::RegisterYourself()` method. Do; not be therefore surprised that some transformations cannot be found by; name when creating a composite shape for instance if you did not; register them after creation. Logical nodes (positioned volumes) are created and destroyed by the; **`TGeoVolume`** class. Physical nodes and their global transformations; are subjected to a caching mechanism due to the sometimes very large; memory requirements of logical graph expansion. The total number of; physical instances of volumes triggers the caching mechanism and the; cache manager is a client of **`TGeoManager`**. The manager class also; controls the drawing/checking package (**`TGeoPainter`** client). This; is linked with ROOT graphical libraries loaded on demand in order to; control visualization actions. ## Navigation and Tracking. Tracking is the feature allowing the transport of a given particle; knowing its kinematics. A state is determined by any combination of the; position $\vec{r}$ and direction $\vec{n}$ with respect to the world; reference frame. The direction $\vec{n}$ must be a unit vector having as; components the director cosines. The full classification of a given; state will provide the following information: the deepest physical node; containing the position vector, the distance to the closest boundary; along the direction vector, the next physical node after propagating the; current point with this distance and the safety distance to the nearest; boundary. This information allows the propagation of particles inside a; detector geometry by taking into account both geometrical and physical; constraints. We will hereby describe the user interface of **`TGeo`** to access; tracking functionality. This allows either devel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:100804,load,loaded,100804,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,"en geometry is shooting random; points. This can be called with the method; TGeoVolume::RandomPoints() and it draws a volume with the current; visualization settings. Random points are generated in the bounding box; of the drawn volume. The points are drawn with the color of their; deepest container. Only points inside visible nodes are drawn. \image html geometry011.png ""Random rays"" width=500px. A ray tracing method can be called TGeoVolume::RandomRays(). This; shoots rays from a given point in the local reference frame with random; directions. The intersections with displayed nodes appear as segments; having the color of the touched node. \anchor GP04; ## The Drawing Package. \image html geometry012.png. The modeller provides a powerful drawing; package, supporting several different options of visualization. A; library separated from the main one provides all functionality being; linked with the underlying ROOT visualization system. This library is; dynamically loaded by the plug-in manager only when drawing features are; requested. The geometrical structures that can be visualized are volumes; and volume hierarchies. The main component of the visualization system is volume primitive; painting in a ROOT pad. Starting from this one, several specific options; or subsystems are available, like: X3D viewing using hidden line and; surface removal algorithms, OpenGL viewing\* or ray tracing. The method TGeoManager::GetGeomPainter() loads the painting library in; memory. This is generally not needed since it is called automatically by; TGeoVolume::Draw() as well as by few other methods setting; visualization attributes. \anchor GP04a; ### Drawing Volumes and Hierarchies of Volumes. The first thing one would like to do after building some geometry is to; visualize the volume tree. This provides the fastest validation check; for most common coding or design mistakes. As soon as the geometry is; successfully closed, one should draw it starting from the top-level; volume:. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:97195,load,loaded,97195,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['loaded']
Performance,"en identifying a; switched-resume coroutine. Arguments:; """""""""""""""""""". The first argument provides information on the alignment of the memory returned; by the allocation function and given to `coro.begin` by the first argument. If; this argument is 0, the memory is assumed to be aligned to 2 * sizeof(ptr).; This argument only accepts constants. The second argument, if not `null`, designates a particular alloca instruction; to be a `coroutine promise`_. The third argument is `null` coming out of the frontend. The CoroEarly pass sets; this argument to point to the function this coro.id belongs to. The fourth argument is `null` before coroutine is split, and later is replaced; to point to a private global constant array containing function pointers to; outlined resume and destroy parts of the coroutine. Semantics:; """""""""""""""""""". The purpose of this intrinsic is to tie together `coro.id`, `coro.alloc` and; `coro.begin` belonging to the same coroutine to prevent optimization passes from; duplicating any of these instructions unless entire body of the coroutine is; duplicated. A frontend should emit exactly one `coro.id` intrinsic per coroutine. A frontend should emit function attribute `presplitcoroutine` for the coroutine. .. _coro.id.async:. 'llvm.coro.id.async' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; ::. declare token @llvm.coro.id.async(i32 <context size>, i32 <align>,; ptr <context arg>,; ptr <async function pointer>). Overview:; """""""""""""""""". The '``llvm.coro.id.async``' intrinsic returns a token identifying an async coroutine. Arguments:; """""""""""""""""""". The first argument provides the initial size of the `async context` as required; from the frontend. Lowering will add to this size the size required by the frame; storage and store that value to the `async function pointer`. The second argument, is the alignment guarantee of the memory of the; `async context`. The frontend guarantees that the memory will be aligned by this; value. The third argument is the `async",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:38028,optimiz,optimization,38028,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['optimiz'],['optimization']
Performance,"en, it will destroy; the value of the guard. When the function exits, the guard on the stack is; checked against the original guard by ``llvm.stackprotectorcheck``. If they are; different, then ``llvm.stackprotectorcheck`` causes the program to abort by; calling the ``__stack_chk_fail()`` function. '``llvm.stackguard``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.stackguard(). Overview:; """""""""""""""""". The ``llvm.stackguard`` intrinsic returns the system stack guard value. It should not be generated by frontends, since it is only for internal usage.; The reason why we create this intrinsic is that we still support IR form Stack; Protector in FastISel. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". On some platforms, the value returned by this intrinsic remains unchanged; between loads in the same thread. On other platforms, it returns the same; global variable value, if any, e.g. ``@__stack_chk_guard``. Currently some platforms have IR-level customized stack guard loading (e.g.; X86 Linux) that is not handled by ``llvm.stackguard()``, while they should be; in the future. '``llvm.objectsize``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i32 @llvm.objectsize.i32(ptr <object>, i1 <min>, i1 <nullunknown>, i1 <dynamic>); declare i64 @llvm.objectsize.i64(ptr <object>, i1 <min>, i1 <nullunknown>, i1 <dynamic>). Overview:; """""""""""""""""". The ``llvm.objectsize`` intrinsic is designed to provide information to the; optimizer to determine whether a) an operation (like memcpy) will overflow a; buffer that corresponds to an object, or b) that a runtime check for overflow; isn't necessary. An object in this context means an allocation of a specific; class, structure, array, or other object. Arguments:; """""""""""""""""""". The ``llvm.objectsize`` intrinsic takes four arguments. The first argument is a; pointer to or into the ``object``. The second argument determines whether; ``llvm.objectsize`` returns 0 (if true) o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:931477,load,loading,931477,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"en/LinkAllCodegenComponents.h``. Creating new registries; -----------------------. The easiest way to get started is to clone one of the existing registries; we; recommend ``llvm/CodeGen/RegAllocRegistry.h``. The key things to modify are; the class name and the ``FunctionPassCtor`` type. Then you need to declare the registry. Example: if your pass registry is; ``RegisterMyPasses`` then define:. .. code-block:: c++. MachinePassRegistry<RegisterMyPasses::FunctionPassCtor> RegisterMyPasses::Registry;. And finally, declare the command line option for your passes. Example:. .. code-block:: c++. cl::opt<RegisterMyPasses::FunctionPassCtor, false,; RegisterPassParser<RegisterMyPasses> >; MyPassOpt(""mypass"",; cl::init(&createDefaultMyPass),; cl::desc(""my pass option help""));. Here the command option is ""``mypass``"", with ``createDefaultMyPass`` as the; default creator. Using GDB with dynamically loaded passes; ----------------------------------------. Unfortunately, using GDB with dynamically loaded passes is not as easy as it; should be. First of all, you can't set a breakpoint in a shared object that; has not been loaded yet, and second of all there are problems with inlined; functions in shared objects. Here are some suggestions to debugging your pass; with GDB. For sake of discussion, I'm going to assume that you are debugging a; transformation invoked by :program:`opt`, although nothing described here; depends on that. Setting a breakpoint in your pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. First thing you do is start gdb on the opt process:. .. code-block:: console. $ gdb opt; GNU gdb 5.0; Copyright 2000 Free Software Foundation, Inc.; GDB is free software, covered by the GNU General Public License, and you are; welcome to change it and/or distribute copies of it under certain conditions.; Type ""show copying"" to see the conditions.; There is absolutely no warranty for GDB. Type ""show warranty"" for details.; This GDB was configured as ""sparc-sun-solaris2.6""...; (gdb). Note",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:52113,load,loaded,52113,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['loaded']
Performance,"enCL Features; ===============. Clang can be used to compile OpenCL kernels for execution on a device; (e.g. GPU). It is possible to compile the kernel into a binary (e.g. for AMDGPU); that can be uploaded to run directly on a device (e.g. using; `clCreateProgramWithBinary; <https://www.khronos.org/registry/OpenCL/specs/opencl-1.1.pdf#111>`_) or; into generic bitcode files loadable into other toolchains. Compiling to a binary using the default target from the installation can be done; as follows:. .. code-block:: console. $ echo ""kernel void k(){}"" > test.cl; $ clang test.cl. Compiling for a specific target can be done by specifying the triple corresponding; to the target, for example:. .. code-block:: console. $ clang --target=nvptx64-unknown-unknown test.cl; $ clang --target=amdgcn-amd-amdhsa -mcpu=gfx900 test.cl. Compiling to bitcode can be done as follows:. .. code-block:: console. $ clang -c -emit-llvm test.cl. This will produce a file `test.bc` that can be used in vendor toolchains; to perform machine code generation. Note that if compiled to bitcode for generic targets such as SPIR/SPIR-V,; portable IR is produced that can be used with various vendor; tools as well as open source tools such as `SPIRV-LLVM Translator; <https://github.com/KhronosGroup/SPIRV-LLVM-Translator>`_; to produce SPIR-V binary. More details are provided in `the offline; compilation from OpenCL kernel sources into SPIR-V using open source; tools; <https://github.com/KhronosGroup/OpenCL-Guide/blob/main/chapters/os_tooling.md>`_.; From clang 14 onwards SPIR-V can be generated directly as detailed in; :ref:`the SPIR-V support section <spir-v>`. Clang currently supports OpenCL C language standards up to v2.0. Clang mainly; supports full profile. There is only very limited support of the embedded; profile.; From clang 9 a C++ mode is available for OpenCL (see; :ref:`C++ for OpenCL <cxx_for_opencl>`). OpenCL v3.0 support is complete but it remains in experimental state, see more; details about",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:141376,perform,perform,141376,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['perform']
Performance,"enVector_exception class is created only when the throwing of exception is enabled. This avoids the allocation of an un-needed std::string. This problem was observed in CMS when converting from 4D-vectors based on mass to standard (x,y,z,t) vectors, when the mass is zero. In this case, a numerical error creates artificially small negative masses returned by the (x,y,z,t) vector. Eventually a protection could be added when calculating M2(), to avoid negative values due to numerical rounding.; ; Fix a problem in the assignment operator of the ROOT::Math::PxPyPzM4D class. Avoid having nan when converting for example from PxPyPzME4D to PxPyPzM4D when the mass is negative. ; Throw always exception in the non-supported setters (i.e. SetPt on a PxPyPzEVector) methods, which are generated only for the CINT dictionary. These methods flag a compiled-error when running in C++ mode. SMatrix. Change implementation of the SMatrix::Invert and SMatrix::Inverse methods. Now the optimized method based on the Cramer rule is used only for matrix up to sizes 2x2. The standard methods based on LU (for ordinary square matrix) or Bunch-Kaufman factorization (for square matrix) are used. The factorization method, although slower for small size matrices, they suffer much less from numerical precision problems.; New methods SMatrix::Invert and SMatrix::InverseFast are added for using the Cramer rule for up to matrix of sizes 5x5. This method has exactly the same implementation as the Invert and Inverse of the previous ROOT version.; ; Physics. TLorentzVector:Change in the implementation of the function SetPtEtaPhi and SetPtEtaPhiM the algorithm to calculate z from pt and eta. Use now, as in the GenVector package, the expression z = pt * sinh(eta) instead of using the tangent and the arc-tangent. This is is more efficient and avoids a problem found on 64 bit machines when eta=0. by Dariusz Miskowiec. Unuran. New version (1.3) from Josef Leydold fixing some warnings on Windows Visual Studio 9. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v522/index.html:5070,optimiz,optimized,5070,math/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v522/index.html,2,['optimiz'],['optimized']
Performance,"en_complete, it indicates; the coroutine must reach the final suspend point when it get destroyed. This attribute only works for switched-resume coroutines now. Metadata; ========. '``coro.outside.frame``' Metadata; ---------------------------------. ``coro.outside.frame`` metadata may be attached to an alloca instruction to; to signify that it shouldn't be promoted to the coroutine frame, useful for; filtering allocas out by the frontend when emitting internal control mechanisms.; Additionally, this metadata is only used as a flag, so the associated; node must be empty. .. code-block:: text. %__coro_gro = alloca %struct.GroType, align 1, !coro.outside.frame !0. ...; !0 = !{}. Areas Requiring Attention; =========================; #. When coro.suspend returns -1, the coroutine is suspended, and it's possible; that the coroutine has already been destroyed (hence the frame has been freed).; We cannot access anything on the frame on the suspend path.; However there is nothing that prevents the compiler from moving instructions; along that path (e.g. LICM), which can lead to use-after-free. At the moment; we disabled LICM for loops that have coro.suspend, but the general problem still; exists and requires a general solution. #. Take advantage of the lifetime intrinsics for the data that goes into the; coroutine frame. Leave lifetime intrinsics as is for the data that stays in; allocas. #. The CoroElide optimization pass relies on coroutine ramp function to be; inlined. It would be beneficial to split the ramp function further to; increase the chance that it will get inlined into its caller. #. Design a convention that would make it possible to apply coroutine heap; elision optimization across ABI boundaries. #. Cannot handle coroutines with `inalloca` parameters (used in x86 on Windows). #. Alignment is ignored by coro.begin and coro.free intrinsics. #. Make required changes to make sure that coroutine optimizations work with; LTO. #. More tests, more tests, more tests; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:58929,optimiz,optimization,58929,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,3,['optimiz'],"['optimization', 'optimizations']"
Performance,"enabled. It is not; rounded up to the; allocation; granularity.; "".vgpr_count"" integer Required Number of vector; registers required by; each work-item for; GFX6-GFX9. A register; is required if it is; used explicitly, or; if a higher numbered; register is used; explicitly.; "".agpr_count"" integer Required Number of accumulator; registers required by; each work-item for; GFX90A, GFX908.; "".max_flat_workgroup_size"" integer Required Maximum flat; work-group size; supported by the; kernel in work-items.; Must be >=1 and; consistent with; ReqdWorkGroupSize if; not 0, 0, 0.; "".sgpr_spill_count"" integer Number of stores from; a scalar register to; a register allocator; created spill; location.; "".vgpr_spill_count"" integer Number of stores from; a vector register to; a register allocator; created spill; location.; "".kind"" string The kind of the kernel; with the following; values:. ""normal""; Regular kernels. ""init""; These kernels must be; invoked after loading; the containing code; object and must; complete before any; normal and fini; kernels in the same; code object are; invoked. ""fini""; These kernels must be; invoked before; unloading the; containing code object; and after all init and; normal kernels in the; same code object have; been invoked and; completed. If omitted, ""normal"" is; assumed.; =================================== ============== ========= ================================. .. .. table:: AMDHSA Code Object V3 Kernel Argument Metadata Map; :name: amdgpu-amdhsa-code-object-kernel-argument-metadata-map-table-v3. ====================== ============== ========= ================================; String Key Value Type Required? Description; ====================== ============== ========= ================================; "".name"" string Kernel argument name.; "".type_name"" string Kernel argument type name.; "".size"" integer Required Kernel argument size in bytes.; "".offset"" integer Required Kernel argument offset in; bytes. The offset must be a; multiple of the alignme",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:135709,load,loading,135709,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loading']
Performance,"enation,; the preprocessor just returns distinct ``tok::string_literal`` and; ``tok::wide_string_literal`` tokens and the parser eats a sequence of them; wherever the grammar indicates that a string literal can occur. In order to do this, whenever the parser expects a ``tok::identifier`` or; ``tok::coloncolon``, it should call the ``TryAnnotateTypeOrScopeToken`` or; ``TryAnnotateCXXScopeToken`` methods to form the annotation token. These; methods will maximally form the specified annotation tokens and replace the; current token with them, if applicable. If the current tokens is not valid for; an annotation token, it will remain an identifier or ""``::``"" token. .. _Lexer:. The ``Lexer`` class; -------------------. The ``Lexer`` class provides the mechanics of lexing tokens out of a source; buffer and deciding what they mean. The ``Lexer`` is complicated by the fact; that it operates on raw buffers that have not had spelling eliminated (this is; a necessity to get decent performance), but this is countered with careful; coding as well as standard performance techniques (for example, the comment; handling code is vectorized on X86 and PowerPC hosts). The lexer has a couple of interesting modal features:. * The lexer can operate in ""raw"" mode. This mode has several features that; make it possible to quickly lex the file (e.g., it stops identifier lookup,; doesn't specially handle preprocessor tokens, handles EOF differently, etc).; This mode is used for lexing within an ""``#if 0``"" block, for example.; * The lexer can capture and return comments as tokens. This is required to; support the ``-C`` preprocessor mode, which passes comments through, and is; used by the diagnostic checker to identifier expect-error annotations.; * The lexer can be in ``ParsingFilename`` mode, which happens when; preprocessing after reading a ``#include`` directive. This mode changes the; parsing of ""``<``"" to return an ""angled string"" instead of a bunch of tokens; for each thing within the fi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:53637,perform,performance,53637,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,2,['perform'],['performance']
Performance,enationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.h; clang-tools-extra/clang-tidy/readability/ContainerDataPoint,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66094,perform,performance,66094,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"enced or copied:. ``` {.cpp}; const TGeoHMatrix *global = gGeoManager->GetCurrentMatrix();; TGeoHMatrix *copy = new TGeoHMatrix(*global);; ```. - One often needs to perform `master-to-local` and `local-to-master`; point and vector conversions to get from `MARS` to the local node; coordinates. This can be done by using the global transformation or; directly the **`TGeoManager`** corresponding interfaces:. ``` {.cpp}; Double_t *glob_pt = gGeoManager->GetCurrentPoint();; Double_t *glob_dir = gGeoManager->GetCurrentDirection();; Double_t loc_pt[3], loc_dir[3];; // Go from MARS to local coordinates:; gGeoManager->MasterToLocal(glob_pt,loc_pt); // or:; global->MasterToLocal(glob_pt,loc_pt); // will be omitted from now; ```. ### Saving and Restoring the Current State. As we already described, saving and restoring modeller states can be; quite useful during tracking and is a feature extensively used by; external tracking engines. We will call this navigation history; management, which in most of the cases can be performed by handling the; state identifiers. For quite big geometries, state indexing is not; possible anymore and will be automatically disabled by the modeller.; Fortunately there is a backup solution working in any condition: the; modeller maintains a stack of states that is internally used by its own; navigation algorithms, but user code is also allowed to access it. This; works on any stack principle by using PUSH and POP calls and user code; is responsible for popping the pushed states in order to keep the stack; clean. ``` {.cpp}; // push the current state in the stack; Int_t index = gGeoManager->PushPath();; // push state and current point; Int_t index = gGeoManager->PushPoint();; // retrieves the last pushed state (decrements stack index); gGeoManager->PopPath();; // the same but retrieves also the point location; gGeoManager->PopPoint();; // just decrement stack index without changing state; gGeoManager->PopDummy();; // retrieves a state at given index wi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:112569,perform,performed,112569,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"ences used to implement the memory model for GFX940, GFX941, GFX942; are defined in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table`. .. table:: AMDHSA Memory Model Code Sequences GFX940, GFX941, GFX942; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX940, GFX941, GFX942; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; nt=1. - volatile. 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. GFX940, GFX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:292916,load,load,292916,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ences; Unknown. 2530; C++23; Multiple definitions of enumerators; Unknown. 2531; DR; Static data members redeclared as constexpr; Unknown. 2532; open; Kind of pointer value returned by new T[0]; Not resolved. 2533; review; Storage duration of implicitly created objects; Not resolved. 2534; CD6; Value category of pseudo-destructor expression; Unknown. 2535; CD6; Type punning in class member access; Unknown. 2536; open; Partially initialized variables during constant initialization; Not resolved. 2537; drafting; Overbroad grammar for parameter-declaration; Not resolved. 2538; C++23; Can standard attributes be syntactically ignored?; Unknown. 2539; C++23; Three-way comparison requiring strong ordering for floating-point types; Unknown. 2540; CD6; Unspecified interpretation of numeric-escape-sequence; Unknown. 2541; open; Linkage specifications, module purview, and module attachment; Not resolved. 2542; DRWP; Is a closure type a structural type?; Unknown. 2543; C++23; constinit and optimized dynamic initialization; Unknown. 2544; open; Address of past-the-end of a potentially-overlapping subobject; Not resolved. 2545; open; Transparently replacing objects in constant expressions; Not resolved. 2546; tentatively ready; Defaulted secondary comparison operators defined as deleted; Unknown. 2547; tentatively ready; Defaulted comparison operator function for non-classes; Unknown. 2548; NAD; Array prvalues and additive operators; Unknown. 2549; review; Implicitly moving the operand of a throw-expression in unevaluated contexts; Not resolved. 2550; DRWP; Type ""reference to cv void"" outside of a declarator; Unknown. 2551; review; ""Refers to allocated storage"" has no meaning; Not resolved. 2552; DRWP; Constant evaluation of non-defining variable declarations; Unknown. 2553; review; Restrictions on explicit object member functions; Clang 18. 2554; review; Overriding virtual functions, also with explicit object parameters; Clang 18. 2555; drafting; Ineffective redeclaration preven",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:174567,optimiz,optimized,174567,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,2,['optimiz'],['optimized']
Performance,"encl-2.0-openclc.pdf#49>`_. opencl_unroll_hint; ^^^^^^^^^^^^^^^^^^. The implementation of this feature mirrors the unroll hint for C.; More details on the syntax can be found in the specification; `section 6.11.5; <https://www.khronos.org/registry/cl/specs/opencl-2.0-openclc.pdf#61>`_. convergent; ^^^^^^^^^^. To make sure no invalid optimizations occur for single program multiple data; (SPMD) / single instruction multiple thread (SIMT) Clang provides attributes that; can be used for special functions that have cross work item semantics.; An example is the subgroup operations such as `intel_sub_group_shuffle; <https://www.khronos.org/registry/cl/extensions/intel/cl_intel_subgroups.txt>`_. .. code-block:: c. // Define custom my_sub_group_shuffle(data, c); // that makes use of intel_sub_group_shuffle; r1 = ...; if (r0) r1 = computeA();; // Shuffle data from r1 into r3; // of threads id r2.; r3 = my_sub_group_shuffle(r1, r2);; if (r0) r3 = computeB();. with non-SPMD semantics this is optimized to the following equivalent code:. .. code-block:: c. r1 = ...; if (!r0); // Incorrect functionality! The data in r1; // have not been computed by all threads yet.; r3 = my_sub_group_shuffle(r1, r2);; else {; r1 = computeA();; r3 = my_sub_group_shuffle(r1, r2);; r3 = computeB();; }. Declaring the function ``my_sub_group_shuffle`` with the convergent attribute; would prevent this:. .. code-block:: c. my_sub_group_shuffle() __attribute__((convergent));. Using ``convergent`` guarantees correct execution by keeping CFG equivalence; wrt operations marked as ``convergent``. CFG ``G`` is equivalent to ``G`` wrt; node ``Ni`` : ``iff  Nj (ij)`` domination and post-domination relations with; respect to ``Ni`` remain the same in both ``G`` and ``G``. noduplicate; ^^^^^^^^^^^. ``noduplicate`` is more restrictive with respect to optimizations than; ``convergent`` because a convergent function only preserves CFG equivalence.; This allows some optimizations to happen as long as the control fl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:150626,optimiz,optimized,150626,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimized']
Performance,"end; on the floating-point environment. If the floating-point environment; has a zeroing treatment of subnormal input values (such as indicated; by the ``""denormal-fp-math""`` attribute), a subnormal value will be; observed (will not be implicitly treated as zero). General Intrinsics; ------------------. This class of intrinsics is designed to be generic and has no specific; purpose. '``llvm.var.annotation``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.var.annotation(ptr <val>, ptr <str>, ptr <str>, i32 <int>). Overview:; """""""""""""""""". The '``llvm.var.annotation``' intrinsic. Arguments:; """""""""""""""""""". The first argument is a pointer to a value, the second is a pointer to a; global string, the third is a pointer to a global string which is the; source file name, and the last argument is the line number. Semantics:; """""""""""""""""""". This intrinsic allows annotation of local variables with arbitrary; strings. This can be useful for special purpose optimizations that want; to look for these annotations. These have no other defined use; they are; ignored by code generation and optimization. '``llvm.ptr.annotation.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use '``llvm.ptr.annotation``' on a; pointer to an integer of any width. *NOTE* you must specify an address space for; the pointer. The identifier for the default address space is the integer; '``0``'. ::. declare ptr @llvm.ptr.annotation.p0(ptr <val>, ptr <str>, ptr <str>, i32 <int>); declare ptr @llvm.ptr.annotation.p1(ptr addrspace(1) <val>, ptr <str>, ptr <str>, i32 <int>). Overview:; """""""""""""""""". The '``llvm.ptr.annotation``' intrinsic. Arguments:; """""""""""""""""""". The first argument is a pointer to an integer value of arbitrary bitwidth; (result of some expression), the second is a pointer to a global string, the; third is a pointer to a global string which is the source file name, and the; last argument is the line num",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:924709,optimiz,optimizations,924709,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"ending; on the users of the result, some ``atomicrmw`` operations can be translated into; operations like ``LOCK AND``, but that does not work in general. On ARM (before v8), MIPS, and many other RISC architectures, Acquire, Release,; and SequentiallyConsistent semantics require barrier instructions for every such; operation. Loads and stores generate normal instructions. ``cmpxchg`` and; ``atomicrmw`` can be represented using a loop with LL/SC-style instructions; which take some sort of exclusive lock on a cache line (``LDREX`` and ``STREX``; on ARM, etc.). It is often easiest for backends to use AtomicExpandPass to lower some of the; atomic constructs. Here are some lowerings it can do:. * cmpxchg -> loop with load-linked/store-conditional; by overriding ``shouldExpandAtomicCmpXchgInIR()``, ``emitLoadLinked()``,; ``emitStoreConditional()``; * large loads/stores -> ll-sc/cmpxchg; by overriding ``shouldExpandAtomicStoreInIR()``/``shouldExpandAtomicLoadInIR()``; * strong atomic accesses -> monotonic accesses + fences by overriding; ``shouldInsertFencesForAtomic()``, ``emitLeadingFence()``, and; ``emitTrailingFence()``; * atomic rmw -> loop with cmpxchg or load-linked/store-conditional; by overriding ``expandAtomicRMWInIR()``; * expansion to __atomic_* libcalls for unsupported sizes.; * part-word atomicrmw/cmpxchg -> target-specific intrinsic by overriding; ``shouldExpandAtomicRMWInIR``, ``emitMaskedAtomicRMWIntrinsic``,; ``shouldExpandAtomicCmpXchgInIR``, and ``emitMaskedAtomicCmpXchgIntrinsic``. For an example of these look at the ARM (first five lowerings) or RISC-V (last; lowering) backend. AtomicExpandPass supports two strategies for lowering atomicrmw/cmpxchg to; load-linked/store-conditional (LL/SC) loops. The first expands the LL/SC loop; in IR, calling target lowering hooks to emit intrinsics for the LL and SC; operations. However, many architectures have strict requirements for LL/SC; loops to ensure forward progress, such as restrictions on the number and t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:20907,load,load-linked,20907,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,3,['load'],"['load-linked', 'loads']"
Performance,"ends for various; format / architecture combinations (as of July 2023). Support levels:. * None: No backend. JITLink will return an ""architecture not supported"" error.; Represented by empty cells in the table below.; * Skeleton: A backend exists, but does not support commonly used relocations.; Even simple programs are likely to trigger an ""unsupported relocation"" error.; Backends in this state may be easy to improve by implementing new relocations.; Consider getting involved!; * Basic: The backend supports simple programs, isn't ready for general use yet.; * Usable: The backend is useable for general use for at least one code and; relocation model.; * Good: The backend supports almost all relocations. Advanced features like; native thread local storage may not be available yet.; * Complete: The backend supports all relocations and object format features. .. list-table:: Availability and Status; :widths: 10 30 30 30; :header-rows: 1; :stub-columns: 1. * - Architecture; - ELF; - COFF; - MachO; * - arm32; - Skeleton; -; -; * - arm64; - Usable; -; - Good; * - LoongArch; - Good; -; -; * - PowerPC 64; - Usable; -; -; * - RISC-V; - Good; -; -; * - x86-32; - Basic; -; -; * - x86-64; - Good; - Usable; - Good. .. [1] See ``llvm/examples/OrcV2Examples/LLJITWithObjectLinkingLayerPlugin`` for; a full worked example. .. [2] If not for *hidden* scoped symbols we could eliminate the; ``JITLinkDylib*`` argument to ``JITLinkMemoryManager::allocate`` and; treat every object as a separate simulated dylib for the purposes of; memory layout. Hidden symbols break this by generating in-range accesses; to external symbols, requiring the access and symbol to be allocated; within range of one another. That said, providing a pre-reserved address; range pool for each simulated dylib guarantees that the relaxation; optimizations will kick in for all intra-dylib references, which is good; for performance (at the cost of whatever overhead is introduced by; reserving the address-range up-front).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:47186,optimiz,optimizations,47186,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"ened to the other string you registered, try running; :program:`opt` with the :option:`-help` option:. .. code-block:: console. $ opt -load lib/LLVMHello.so -help; OVERVIEW: llvm .bc -> .bc modular optimizer and analysis printer. USAGE: opt [subcommand] [options] <input bitcode file>. OPTIONS:; Optimizations available:; ...; -guard-widening - Widen guards; -gvn - Global Value Numbering; -gvn-hoist - Early GVN Hoisting of Expressions; -hello - Hello World Pass; -indvars - Induction Variable Simplification; -inferattrs - Infer set function attributes; ... The pass name gets added as the information string for your pass, giving some; documentation to users of :program:`opt`. Now that you have a working pass,; you would go ahead and make it do the cool transformations you want. Once you; get it all working and tested, it may become useful to find out how fast your; pass is. The :ref:`PassManager <writing-an-llvm-pass-passmanager>` provides a; nice command line option (:option:`-time-passes`) that allows you to get; information about the execution time of your pass along with the other passes; you queue up. For example:. .. code-block:: console. $ opt -load lib/LLVMHello.so -hello -time-passes < hello.bc > /dev/null; Hello: __main; Hello: puts; Hello: main; ===-------------------------------------------------------------------------===; ... Pass execution timing report ...; ===-------------------------------------------------------------------------===; Total Execution Time: 0.0007 seconds (0.0005 wall clock). ---User Time--- --User+System-- ---Wall Time--- --- Name ---; 0.0004 ( 55.3%) 0.0004 ( 55.3%) 0.0004 ( 75.7%) Bitcode Writer; 0.0003 ( 44.7%) 0.0003 ( 44.7%) 0.0001 ( 13.6%) Hello World Pass; 0.0000 ( 0.0%) 0.0000 ( 0.0%) 0.0001 ( 10.7%) Module Verifier; 0.0007 (100.0%) 0.0007 (100.0%) 0.0005 (100.0%) Total. As you can see, our implementation above is pretty fast. The additional; passes listed are automatically inserted by the :program:`opt` tool to verify; that the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:9879,queue,queue,9879,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['queue'],['queue']
Performance,"enerate good code) to; generate a reference output. Once ``bugpoint`` has a reference output for the; test program, it tries executing it with the selected code generator. If the; selected code generator crashes, ``bugpoint`` starts the `crash debugger`_ on; the code generator. Otherwise, if the resulting output differs from the; reference output, it assumes the difference resulted from a code generator; failure, and starts the `code generator debugger`_. Finally, if the output of the selected code generator matches the reference; output, ``bugpoint`` runs the test program after all of the LLVM passes have; been applied to it. If its output differs from the reference output, it assumes; the difference resulted from a failure in one of the LLVM passes, and enters the; `miscompilation debugger`_. Otherwise, there is no problem ``bugpoint`` can; debug. .. _crash debugger:. Crash debugger; --------------. If an optimizer or code generator crashes, ``bugpoint`` will try as hard as it; can to reduce the list of passes (for optimizer crashes) and the size of the; test program. First, ``bugpoint`` figures out which combination of optimizer; passes triggers the bug. This is useful when debugging a problem exposed by; ``opt``, for example, because it runs over 38 passes. Next, ``bugpoint`` tries removing functions from the test program, to reduce its; size. Usually it is able to reduce a test program to a single function, when; debugging intraprocedural optimizations. Once the number of functions has been; reduced, it attempts to delete various edges in the control flow graph, to; reduce the size of the function as much as possible. Finally, ``bugpoint``; deletes any individual LLVM instructions whose absence does not eliminate the; failure. At the end, ``bugpoint`` should tell you what passes crash, give you a; bitcode file, and give you instructions on how to reproduce the failure with; ``opt`` or ``llc``. .. _code generator debugger:. Code generator debugger; -------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:3071,optimiz,optimizer,3071,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,2,['optimiz'],['optimizer']
Performance,"enerating the PCH file initially is not so large that it; counters the per-source-file performance improvement due to eliminating the; need to parse the bundled headers in the first place. This is particularly; important on multi-core systems, because PCH file generation serializes the; build when all compilations require the PCH file to be up-to-date. Modules, as implemented in Clang, use the same mechanisms as precompiled; headers to save a serialized AST file (one per module) and use those AST; modules. From an implementation standpoint, modules are a generalization of; precompiled headers, lifting a number of restrictions placed on precompiled; headers. In particular, there can only be one precompiled header and it must; be included at the beginning of the translation unit. The extensions to the; AST file format required for modules are discussed in the section on; :ref:`modules <pchinternals-modules>`. Clang's AST files are designed with a compact on-disk representation, which; minimizes both creation time and the time required to initially load the AST; file. The AST file itself contains a serialized representation of Clang's; abstract syntax trees and supporting data structures, stored using the same; compressed bitstream as `LLVM's bitcode file format; <https://llvm.org/docs/BitCodeFormat.html>`_. Clang's AST files are loaded ""lazily"" from disk. When an AST file is initially; loaded, Clang reads only a small amount of data from the AST file to establish; where certain important data structures are stored. The amount of data read in; this initial load is independent of the size of the AST file, such that a; larger AST file does not lead to longer AST load times. The actual header data; in the AST file --- macros, functions, variables, types, etc. --- is loaded; only when it is referenced from the user's code, at which point only that; entity (and those entities it depends on) are deserialized from the AST file.; With this approach, the cost of using an AST fil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:3124,load,load,3124,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['load']
Performance,"eneric; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:216539,cache,cache,216539,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"eneric; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; followi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:297285,load,load,297285,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"eneric; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic. - Must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:230992,load,load,230992,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"eneric; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/stor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:275894,cache,cache,275894,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,enmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65407,perform,performance,65407,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"enseMap that you should be aware of, however.; The iterators in a DenseMap are invalidated whenever an insertion occurs,; unlike map. Also, because DenseMap allocates space for a large number of; key/value pairs (it starts with 64 by default), it will waste a lot of space if; your keys or values are large. Finally, you must implement a partial; specialization of DenseMapInfo for the key that you want, if it isn't already; supported. This is required to tell DenseMap about two special marker values; (which can never be inserted into the map) that it needs internally. DenseMap's find_as() method supports lookup operations using an alternate key; type. This is useful in cases where the normal key type is expensive to; construct, but cheap to compare against. The DenseMapInfo is responsible for; defining the appropriate comparison and hashing methods for each alternate key; type used. DenseMap.h also contains a SmallDenseMap variant, that similar to; :ref:`SmallVector <dss_smallvector>` performs no heap allocation until the; number of elements in the template parameter N are exceeded. .. _dss_valuemap:. llvm/IR/ValueMap.h; ^^^^^^^^^^^^^^^^^^^. ValueMap is a wrapper around a :ref:`DenseMap <dss_densemap>` mapping; ``Value*``\ s (or subclasses) to another type. When a Value is deleted or; RAUW'ed, ValueMap will update itself so the new version of the key is mapped to; the same value, just as if the key were a WeakVH. You can configure exactly how; this happens, and what else happens on these two events, by passing a ``Config``; parameter to the ValueMap template. .. _dss_intervalmap:. llvm/ADT/IntervalMap.h; ^^^^^^^^^^^^^^^^^^^^^^. IntervalMap is a compact map for small keys and values. It maps key intervals; instead of single keys, and it will automatically coalesce adjacent intervals.; When the map only contains a few intervals, they are stored in the map object; itself to avoid allocations. The IntervalMap iterators are quite big, so they should not be passed around as",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:91985,perform,performs,91985,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performs']
Performance,"ent *elem,; Double_t weight);; void TGeoMixture::DefineElement(Int_t iel, Int_t z, Int_t natoms);; ```. or:. ``` {.cpp}; void AddElement(TGeoMaterial* mat, Double_t weight);; void AddElement(TGeoElement* elem, Double_t weight);; void AddElement(TGeoElement* elem, Int_t natoms);; void AddElement(Double_t a, Double_t z, Double_t weight); ```. - `iel:` index of the element` [0,nel-1]`; - `a` and `z:` the atomic mass and charge; - `weight:` proportion by mass of the elements; - `natoms`: number of atoms of the element in the molecule making the; mixture. The radiation length is automatically computed when all elements are; defined. Since tracking MC provide several other ways to create; materials/mixtures, the materials classes are likely to evolve as the; interfaces to these engines are being developed. Generally in the; process of tracking material properties are not enough and more specific; media properties have to be defined. These highly depend on the MC; performing tracking and sometimes allow the definition of different; media properties (e.g. energy or range cuts) for the same material. ### Radionuclides. A new class **`TGeoElementRN`** was introduced in this version to; provide support for radioactive nuclides and their decays. A database of; 3162 radionuclides can be loaded on demand via the table of elements; (**`TGeoElementTable`** class). One can make then materials/mixtures; based on these radionuclides and use them in a geometry. ``` {.cpp}; root[] TGeoManager *geom = new TGeoManager(""geom"",""radionuclides"");; root[] TGeoElementTable *table = geom->GetElementTable();; root[] TGeoElementRN *c14 = table->GetElementRN(14,6); // A,Z; root[] c14->Print();; 6-C-014 ENDF=60140; A=14; Z=6; Iso=0; Level=0[MeV]; Dmass=3.0199[MeV];; Hlife=1.81e+11[s] J/P=0+; Abund=0; Htox=5.8e-10; Itox=5.8e-10; Stat=0; Decay modes:; BetaMinus Diso: 0 BR: 100.000% Qval: 0.1565; ```. One can make materials or mixtures from radionuclides:. ``` {.cpp}; root[] TGeoMaterial *mat = new TGeo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:17340,perform,performing,17340,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performing']
Performance,"ent expression. .. _pchinternals-ident-table:. Identifier Table Block; ^^^^^^^^^^^^^^^^^^^^^^. The identifier table block contains an on-disk hash table that maps each; identifier mentioned within the AST file to the serialized representation of; the identifier's information (e.g, the ``IdentifierInfo`` structure). The; serialized representation contains:. * The actual identifier string.; * Flags that describe whether this identifier is the name of a built-in, a; poisoned identifier, an extension token, or a macro.; * If the identifier names a macro, the offset of the macro definition within; the :ref:`pchinternals-preprocessor`.; * If the identifier names one or more declarations visible from translation; unit scope, the :ref:`declaration IDs <pchinternals-decls>` of these; declarations. When an AST file is loaded, the AST file reader mechanism introduces itself; into the identifier table as an external lookup source. Thus, when the user; program refers to an identifier that has not yet been seen, Clang will perform; a lookup into the identifier table. If an identifier is found, its contents; (macro definitions, flags, top-level declarations, etc.) will be deserialized,; at which point the corresponding ``IdentifierInfo`` structure will have the; same contents it would have after parsing the headers in the AST file. Within the AST file, the identifiers used to name declarations are represented; with an integral value. A separate table provides a mapping from this integral; value (the identifier ID) to the location within the on-disk hash table where; that identifier is stored. This mapping is used when deserializing the name of; a declaration, the identifier of a token, or any other construct in the AST; file that refers to a name. .. _pchinternals-method-pool:. Method Pool Block; ^^^^^^^^^^^^^^^^^. The method pool block is represented as an on-disk hash table that serves two; purposes: it provides a mapping from the names of Objective-C selectors to the; set of Obj",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:19429,perform,perform,19429,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['perform']
Performance,"ent import requests.; This is the ``ASTImporterLookupTable`` class. This lookup table should be; shared amongst the different ``ASTImporter`` instances if they happen to import; to the very same ""to"" context. This is why we can use the importer specific; lookup only via the ``ASTImporterSharedState`` class. ExternalASTSource; ~~~~~~~~~~~~~~~~~. The ``ExternalASTSource`` is an abstract interface associated with the; ``ASTContext`` class. It provides the ability to read the declarations stored; within a declaration context either for iteration or for name lookup. A; declaration context with an external AST source may load its declarations; on-demand. This means that the list of declarations (represented as a linked; list, the head is ``DeclContext::FirstDecl``) could be empty. However, member; functions like ``DeclContext::lookup()`` may initiate a load. Usually, external sources are associated with precompiled headers. For example,; when we load a class from a PCH then the members are loaded only if we do want; to look up something in the class' context. In case of LLDB, an implementation of the ``ExternalASTSource`` interface is; attached to the AST context which is related to the parsed expression. This; implementation of the ``ExternalASTSource`` interface is realized with the help; of the ``ASTImporter`` class. This way, LLDB can reuse Clang's parsing; machinery while synthesizing the underlying AST from the debug data (e.g. from; DWARF). From the view of the ``ASTImporter`` this means both the ""to"" and the; ""from"" context may have declaration contexts with external lexical storage. If; a ``DeclContext`` in the ""to"" AST context has external lexical storage then we; must take extra attention to work only with the already loaded declarations!; Otherwise, we would end up with an uncontrolled import process. For instance,; if we used the regular ``DeclContext::lookup()`` to find the existing; declarations in the ""to"" context then the ``lookup()`` call itself would; i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:107020,load,load,107020,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,2,['load'],"['load', 'loaded']"
Performance,"ent is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". This intrinsic reverses the order of the first ``evl`` elements in a vector.; The lanes in the result vector disabled by ``mask`` are ``poison``. The; elements past ``evl`` are poison. .. _int_vp_load:. '``llvm.vp.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.vp.load.v4f32.p0(ptr %ptr, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.vp.load.nxv2i16.p0(ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare <8 x float> @llvm.vp.load.v8f32.p1(ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p6(ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``llvm.masked.load``' operands do not have corresponding operands in; '``llvm.vp.load``': the '``passthru``' operand is implicitly ``poison``; the; '``alignment``' operand is taken as the ``align`` parameter attribute, if; provided. The default alignment is taken as the ABI alignment of the return; type as specified by the :ref:`datalayout string<langref_datalayout>`. Exampl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:783632,load,load,783632,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ent of which is the sum, and the second element of which is a; bit specifying if the unsigned summation resulted in a carry. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %carry, label %normal. '``llvm.ssub.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.ssub.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.ssub.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.ssub.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.ssub.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.ssub.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.ssub.with.overflow``' family of intrinsic functions perform; a signed subtraction of the two arguments, and indicate whether an; overflow occurred during the signed subtraction. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo signed; subtraction. Semantics:; """""""""""""""""""". The '``llvm.ssub.with.overflow``' family of intrinsic functions perform; a signed subtraction of the two arguments. They return a structure --- the; first element of which is the subtraction, and the second element of; which is a bit specifying if the signed subtraction resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.ssub.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.usub.with.overflow.*``' Intr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:604680,perform,perform,604680,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"ent of; 1 is always safe. The maximum possible alignment is ``1 << 32``. An alignment; value higher than the size of the loaded type implies memory up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for the target. The optional ``!nontemporal`` metadata must reference a single metadata; name ``<nontemp_node>`` corresponding to a metadata node with one ``i32`` entry; of value 1. The existence of the ``!nontemporal`` metadata on the instruction; tells the optimizer and code generator that this load is not expected to; be reused in the cache. The code generator may select special; instructions to save cache bandwidth, such as the ``MOVNT`` instruction on; x86. The optional ``!invariant.group`` metadata must reference a; single metadata name ``<empty_node>``. See ``invariant.group`` metadata. Semantics:; """""""""""""""""""". The contents of memory are updated to contain ``<value>`` at the; location specified by the ``<pointer>`` operand. If ``<value>`` is; of scalar type then the number of bytes written does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, storing an ``i24`` writes at most three bytes. When writing a; value of a type like ``i20`` with a size that is not an integral number; of bytes, it is unspecified what happens to the extra bits that do not; belong to the type, but they will typically be overwritten.; If ``<value>`` is of aggregate type, padding is filled with; :ref:`undef <undefvalues>`.; If ``<pointer>`` is not a well-defined value, the behavior is undefined. Example:; """""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields pt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:422322,cache,cache,422322,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['cache'],['cache']
Performance,"ent of; 1 is always safe. The maximum possible alignment is ``1 << 32``. An alignment; value higher than the size of the loaded type implies memory up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for the target. The optional ``!nontemporal`` metadata must reference a single; metadata name ``<nontemp_node>`` corresponding to a metadata node with one; ``i32`` entry of value 1. The existence of the ``!nontemporal``; metadata on the instruction tells the optimizer and code generator; that this load is not expected to be reused in the cache. The code; generator may select special instructions to save cache bandwidth, such; as the ``MOVNT`` instruction on x86. The optional ``!invariant.load`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. If a load instruction tagged with the ``!invariant.load``; metadata is executed, the memory location referenced by the load has; to contain the same value at all points in the program where the; memory location is dereferenceable; otherwise, the behavior is; undefined. The optional ``!invariant.group`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a metadata node with no entries.; See ``invariant.group`` metadata :ref:`invariant.group <md_invariant.group>`. The optional ``!nonnull`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. The existence of the ``!nonnull`` metadata on the; instruction tells the optimizer that the value loaded is known to; never be null. If the value is n",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:415236,cache,cache,415236,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['cache'],['cache']
Performance,"ent-handling is done by the TGLEventHandler class. One can; sub-class it now and modify behaviour of a given viewer. For; example, see TEveLegoEventHandler. Support highlighting of physical shapes for providing feedback and; showing selection. Minor changes, fixes and improvements. Improve saving of images from the GL-viewer so that the dialog; boxes and other windows do not result in black areas on the saved; image. The window must still be fully contained within the desktop. Improved camera controls. Three new orthographic cameras have; been added to TGLViewer, looking at the scene from another; side than the one present so far. Improved FTGL font management across rendering contexts and text; rendering support. New class TGLAxisPainter that can render 2D and 3D axes; via GL. For example see tutorials/eve/cms_calo.C. Possible performance issues with ATI drivers (fglrx). In late 2007 ATI switched to a new driver architecture. With these; drivers a significant degradation of GL performance in selection mode,; up to a factor of 50, was observed. Both linux and Windows drivers; were affected. The issue has been resolved in the latest driver; versions. Eve; Major changes. Support for multiple, parallel OpenGL views that can show different; projections of the same event. Provide object selection and feedback highlight across all GL-views and; list-trees. New classes for visualization of calorimeter data,; TEveCaloXYZ, see tutorials/eve/cms_calo.C. Available; representations: 3D-cylindrical view, projected views r-phi and rho-z,; and lego-view (with dedicated event handler allowing detailed; inspection of the data). Support for compound objects in view of selection, highlight and; color managament (see class TEveCompound). Optimize updates of GL-scenes by introducing change-stamping bits; into TEveElement. See methods AddStamp() and; StampXyzz(). Added support for central management of visualization parameters; of objects. Instead of specifying visual attributes individu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html:1200,perform,performance,1200,graf3d/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html,2,['perform'],['performance']
Performance,"entation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6958,cache,cache,6958,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['cache'],['cache']
Performance,"entities --- variables,; functions, Objective-C methods, C++ constructors, destructors, and operators; --- are represented as subclasses of Clang's common ``NamedDecl`` class,; ``DeclarationName`` is designed to efficiently represent any kind of name. Given a ``DeclarationName`` ``N``, ``N.getNameKind()`` will produce a value; that describes what kind of name ``N`` stores. There are 10 options (all of; the names are inside the ``DeclarationName`` class). ``Identifier``. The name is a simple identifier. Use ``N.getAsIdentifierInfo()`` to retrieve; the corresponding ``IdentifierInfo*`` pointing to the actual identifier. ``ObjCZeroArgSelector``, ``ObjCOneArgSelector``, ``ObjCMultiArgSelector``. The name is an Objective-C selector, which can be retrieved as a ``Selector``; instance via ``N.getObjCSelector()``. The three possible name kinds for; Objective-C reflect an optimization within the ``DeclarationName`` class:; both zero- and one-argument selectors are stored as a masked; ``IdentifierInfo`` pointer, and therefore require very little space, since; zero- and one-argument selectors are far more common than multi-argument; selectors (which use a different structure). ``CXXConstructorName``. The name is a C++ constructor name. Use ``N.getCXXNameType()`` to retrieve; the :ref:`type <QualType>` that this constructor is meant to construct. The; type is always the canonical type, since all constructors for a given type; have the same name. ``CXXDestructorName``. The name is a C++ destructor name. Use ``N.getCXXNameType()`` to retrieve; the :ref:`type <QualType>` whose destructor is being named. This type is; always a canonical type. ``CXXConversionFunctionName``. The name is a C++ conversion function. Conversion functions are named; according to the type they convert to, e.g., ""``operator void const *``"".; Use ``N.getCXXNameType()`` to retrieve the type that this conversion function; converts to. This type is always a canonical type. ``CXXOperatorName``. The name is a C++ ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:69779,optimiz,optimization,69779,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['optimiz'],['optimization']
Performance,"ently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:260154,load,load,260154,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===-----------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24456,optimiz,optimized,24456,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"ently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18222,optimiz,optimizations,18222,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,2,"['optimiz', 'perform']","['optimizations', 'performed']"
Performance,"entptr float, float addrspace(1)* %A, i32 %id; %ptrB = getelementptr float, float addrspace(1)* %B, i32 %id; %ptrC = getelementptr float, float addrspace(1)* %C, i32 %id. ; Read A, B; %valA = load float, float addrspace(1)* %ptrA, align 4; %valB = load float, float addrspace(1)* %ptrB, align 4. ; Compute C = pow(A, B); %valC = call float @__nv_powf(float %valA, float %valB). ; Store back to C; store float %valC, float addrspace(1)* %ptrC, align 4. ret void; }. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. To compile this kernel, we perform the following steps:. 1. Link with libdevice; 2. Internalize all but the public kernel function; 3. Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0; 4. Optimize the linked module; 5. Codegen the module. These steps can be performed by the LLVM ``llvm-link``, ``opt``, and ``llc``; tools. In a complete compiler, these steps can also be performed entirely; programmatically by setting up an appropriate pass configuration (see; :ref:`libdevice`). .. code-block:: text. # llvm-link t2.bc libdevice.compute_20.10.bc -o t2.linked.bc; # opt -internalize -internalize-public-api-list=kernel -nvvm-reflect-list=__CUDA_FTZ=0 -nvvm-reflect -O3 t2.linked.bc -o t2.opt.bc; # llc -mcpu=sm_20 t2.opt.bc -o t2.ptx. .. note::. The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any; undefined variables will default to zero. It is shown here for evaluation; purposes. This gives us the following PTX (excerpt):. .. code-block:: text. //; // Generated by LLVM NVPTX Back-End; //. .version 3.1; .target sm_20; .address_size 64. // .globl kernel; // @kernel; .visible .entry kernel(; .param .u64 kernel_param_0,; .param .u64 kernel_param_1,; .param .u64 kernel_param_2; ); {; .reg .pred %p<30>;; .reg .f32 %f<111>;; .reg .s32 %r<21>;; .reg .s64 %rl<8>;. // %bb.0: // %entry; ld.param.u64 %rl2, [kernel_param_0];; mov.u32 %r3, %tid.x;; ld.param.u64 %rl3, [kernel_param",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:25350,perform,performed,25350,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['perform'],['performed']
Performance,"ents that follow; this function. - Input: `in` (a pointer to the calling class, used to determine the loop; dependent variables).; - Output: A scope for iterating over vector observables. - **RooFit::Detail::CodeSquashContext::buildArg()**: helps convert RooFit; objects into arrays or other C++ representations for efficient computation. - Input: `in` (the list to convert to array).; - Output: Name of the array that stores the input list in the squashed code. - **RooFit::Detail::CodeSquashContext::buildCall()**: Creates a string; representation of the function to be called and its arguments. - Input: A function with name `funcname`, passing some arguments.; - Output: A string representation of the function to be called. - **RooFit::Detail::makeValidVarName()**: It helps fetch and save a valid name; from the name of the respective RooFit class. - Input: `in` (the input string).; - Output: A new string that is a valid variable name. - **RooFuncWrapper::buildCode()**: Generates the optimized code for evaluating; the function and its derivatives. - Input: `head` (starting mathematical expression).; - Output: code for evaluating the function. - **RooFuncWrapper::declareAndDiffFunction()**: Declare the function and create; its derivative. - Inputs: `funcName` (name of the function being differentiated), `funcBody`; (actual mathematical formula or equation).; - Output: Function declaration and its derivative. - **RooFuncWrapper::dumpCode()**: Prints the squashed code body to console; (useful for debugging). - Output: Print squashed code body to console. - **RooFuncWrapper::dumpGradient()**: Prints the derivative code body to; console (useful for debugging). - Output: Print derivative code body to console. - **RooFuncWrapper::gradient()**: Calculates the gradient of the function with; respect to its parameters. - Input: `out` (array where the computed gradient values will be stored).; - Output: Populates the `out` array with gradient values. - **RooFuncWrapper::loadParamsAnd",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:37484,optimiz,optimized,37484,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,1,['optimiz'],['optimized']
Performance,"ents to make a JIT, and as replacements; for earlier LLVM JIT APIs (e.g. MCJIT). The LLJIT class uses an IRCompileLayer and RTDyldObjectLinkingLayer to support; compilation of LLVM IR and linking of relocatable object files. All operations; are performed eagerly on symbol lookup (i.e. a symbol's definition is compiled; as soon as you attempt to look up its address). LLJIT is a suitable replacement; for MCJIT in most cases (note: some more advanced features, e.g.; JITEventListeners are not supported yet). The LLLazyJIT extends LLJIT and adds a CompileOnDemandLayer to enable lazy; compilation of LLVM IR. When an LLVM IR module is added via the addLazyIRModule; method, function bodies in that module will not be compiled until they are first; called. LLLazyJIT aims to provide a replacement of LLVM's original (pre-MCJIT); JIT API. LLJIT and LLLazyJIT instances can be created using their respective builder; classes: LLJITBuilder and LLazyJITBuilder. For example, assuming you have a; module ``M`` loaded on a ThreadSafeContext ``Ctx``:. .. code-block:: c++. // Try to detect the host arch and construct an LLJIT instance.; auto JIT = LLJITBuilder().create();. // If we could not construct an instance, return an error.; if (!JIT); return JIT.takeError();. // Add the module.; if (auto Err = JIT->addIRModule(TheadSafeModule(std::move(M), Ctx))); return Err;. // Look up the JIT'd code entry point.; auto EntrySym = JIT->lookup(""entry"");; if (!EntrySym); return EntrySym.takeError();. // Cast the entry point address to a function pointer.; auto *Entry = EntrySym.getAddress().toPtr<void(*)()>();. // Call into JIT'd code.; Entry();. The builder classes provide a number of configuration options that can be; specified before the JIT instance is constructed. For example:. .. code-block:: c++. // Build an LLLazyJIT instance that uses four worker threads for compilation,; // and jumps to a specific error handler (rather than null) on lazy compile; // failures. void handleLazyCompileFailure()",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:4717,load,loaded,4717,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loaded']
Performance,"ents. N: David Blaikie; E: dblaikie@gmail.com; D: General bug fixing/fit & finish, mostly in Clang. N: Neil Booth; E: neil@daikokuya.co.uk; D: APFloat implementation. N: Alex Bradbury; E: asb@igalia.com; D: RISC-V backend. N: Misha Brukman; E: brukman+llvm@uiuc.edu; W: http://misha.brukman.net; D: Portions of X86 and Sparc JIT compilers, PowerPC backend; D: Incremental bitcode loader. N: Cameron Buschardt; E: buschard@uiuc.edu; D: The `mem2reg' pass - promotes values stored in memory to registers. N: Brendon Cahoon; E: bcahoon@codeaurora.org; D: Loop unrolling with run-time trip counts. N: Chandler Carruth; E: chandlerc@gmail.com; E: chandlerc@google.com; D: Hashing algorithms and interfaces; D: Inline cost analysis; D: Machine block placement pass; D: SROA. N: Casey Carter; E: ccarter@uiuc.edu; D: Fixes to the Reassociation pass, various improvement patches. N: Evan Cheng; E: evan.cheng@apple.com; D: ARM and X86 backends; D: Instruction scheduler improvements; D: Register allocator improvements; D: Loop optimizer improvements; D: Target-independent code generator improvements. N: Dan Villiom Podlaski Christiansen; E: danchr@gmail.com; E: danchr@cs.au.dk; W: http://villiom.dk; D: LLVM Makefile improvements; D: Clang diagnostic & driver tweaks; S: Aarhus, Denmark. N: Jeff Cohen; E: jeffc@jolt-lang.org; W: http://jolt-lang.org; D: Native Win32 API portability layer. N: John T. Criswell; E: criswell@uiuc.edu; D: Original Autoconf support, documentation improvements, bug fixes. N: Anshuman Dasgupta; E: adasgupt@codeaurora.org; D: Deterministic finite automaton based infrastructure for VLIW packetization. N: Stefanus Du Toit; E: stefanus.du.toit@intel.com; D: Bug fixes and minor improvements. N: Rafael Avila de Espindola; E: rafael@espindo.la; D: MC and LLD work. N: Dave Estes; E: cestes@codeaurora.org; D: AArch64 machine description for Cortex-A53. N: Alkis Evlogimenos; E: alkis@evlogimenos.com; D: Linear scan register allocator, many codegen improvements, Java frontend.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT:2491,optimiz,optimizer,2491,interpreter/llvm-project/llvm/CREDITS.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT,1,['optimiz'],['optimizer']
Performance,"ents:; """""""""""""""""""". The ``llvm.arithmetic.fence`` intrinsic takes only one argument.; The argument and the return value are floating-point numbers,; or vector floating-point numbers, of the same type. Semantics:; """""""""""""""""""". This intrinsic returns the value of its operand. The optimizer can optimize; the argument, but the optimizer cannot hoist any component of the operand; to the containing context, and the optimizer cannot move the calculation of; any expression in the containing context into the operand. '``llvm.donothing``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.donothing() nounwind memory(none). Overview:; """""""""""""""""". The ``llvm.donothing`` intrinsic doesn't perform any operation. It's one of only; three intrinsics (besides ``llvm.experimental.patchpoint`` and; ``llvm.experimental.gc.statepoint``) that can be called with an invoke; instruction. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". This intrinsic does nothing, and it's removed by optimizers and ignored; by codegen. '``llvm.experimental.deoptimize``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare type @llvm.experimental.deoptimize(...) [ ""deopt""(...) ]. Overview:; """""""""""""""""". This intrinsic, together with :ref:`deoptimization operand bundles; <deopt_opbundles>`, allow frontends to express transfer of control and; frame-local state from the currently executing (typically more specialized,; hence faster) version of a function into another (typically more generic, hence; slower) version. In languages with a fully integrated managed runtime like Java and JavaScript; this intrinsic can be used to implement ""uncommon trap"" or ""side exit"" like; functionality. In unmanaged languages like C and C++, this intrinsic can be; used to represent the slow paths of specialized functions. Arguments:; """""""""""""""""""". The intrinsic takes an arbitrary number of arguments, whose meaning is; decided by the :ref:`lowering strategy<deoptimize_lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:941946,optimiz,optimizers,941946,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"ents:; """""""""""""""""""". The argument to the ``load`` instruction specifies the memory address from which; to load. The type specified must be a :ref:`first class <t_firstclass>` type of; known size (i.e. not containing an :ref:`opaque structural type <t_opaque>`). If; the ``load`` is marked as ``volatile``, then the optimizer is not allowed to; modify the number or order of execution of this ``load`` with other; :ref:`volatile operations <volatile>`. If the ``load`` is marked as ``atomic``, it takes an extra :ref:`ordering; <ordering>` and optional ``syncscope(""<target-scope>"")`` argument. The; ``release`` and ``acq_rel`` orderings are not valid on ``load`` instructions.; Atomic loads produce :ref:`defined <memmodel>` results when they may see; multiple atomic stores. The type of the pointee must be an integer, pointer, or; floating-point type whose bit width is a power of two greater than or equal to; eight and less than or equal to a target-specific size limit. ``align`` must be; explicitly specified on atomic loads. Note: if the alignment is not greater or; equal to the size of the `<value>` type, the atomic operation is likely to; require a lock and have poor performance. ``!nontemporal`` does not have any; defined semantics for atomic loads. The optional constant ``align`` argument specifies the alignment of the; operation (that is, the alignment of the memory address). It is the; responsibility of the code emitter to ensure that the alignment information is; correct. Overestimating the alignment results in undefined behavior.; Underestimating the alignment may produce less efficient code. An alignment of; 1 is always safe. The maximum possible alignment is ``1 << 32``. An alignment; value higher than the size of the loaded type implies memory up to the; alignment value bytes can be safely loaded without trapping in the default; address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:413632,load,loads,413632,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"eoManager::FindNode(x,y,z); ~~~. Note that the current particle position can be set using; SetCurrentPosition(x,y,z) method of the manager class, in which; case FindNode() can be called without arguments. The method; returns a pointer to the ""deepest node"" that geometrically contains *P*; (in our case let us suppose it is `B\_3`). Since a node is just a; positioned volume, we can then get a pointer to the volume, medium or; material objects related to it. ""Deepest"" means that `B\_3` still; contains point `P` (as well as `A\_1` and `TOP\_1`), but none of the; daughters of volume `B` does. After finding out the node containing; the particle, one can check if the geometry state is different compared; to the last located point:. ~~~{.cpp}; Bool_t *TGeoManager::IsSameLocation(); ~~~. The algorithm for finding where a point is located in geometry is; presented in the figure 17-36. It always starts by checking if the last computed modeller state is the; answer. This optimizes the search when continuously tracking a particle.; The main actions performed are:. - moving up and down in the logical node tree while updating the; current node and its global matrix; - converting the global position into the local frame of the current; node/volume; - checking whether the local position lies within the geometrical; shape of the current volume - if this is the case continue the; search downwards for the daughters of the current node, otherwise; search upwards its containers until the top level is reached.; - the number of candidate nodes to be checked at a given level is; minimized by an additional optimization structure: voxels. This is; effective even in case there is only one daughter of the current; volume.; - in case the current node is declared as possibly overlapping, the; method FindInCluster() is invoked. This method checks all different; possibilities within the cluster of overlapping candidates. One of; the candidates is prioritized if one of the following conditions id; fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:117668,optimiz,optimizes,117668,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimizes']
Performance,"eometrically; insulates the structure from the rest (as a normal volume). Physically,; a point that is INSIDE a TGeoShapeAssembly is always inside one of; the components, so a TGeoVolumeAssembly does not need to have a; medium. Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at: assembly.C. Creation of an assembly is very easy: one has just to create a; TGeoVolumeAssembly object and position the components inside as; for any volume:. ~~~{.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ~~~. Note that components cannot be declared as ""overlapping"" and that a; component can be an assembly volume. For existing flat volume; structures, one can define assemblies to force a hierarchical structure; therefore optimizing the performance. Usage of assemblies does NOT imply; penalties in performance, but in some cases, it can be observed that it; is not as performing as bounding the structure in a container volume; with a simple shape. Choosing a normal container is therefore; recommended whenever possible. \image html geometry006.png ""Assemblies of volumes"" width=600px. \anchor GP01c; ### Geometrical Transformations. All geometrical transformations handled by the modeller are provided as; a built-in package. This was designed to minimize memory requirements; and optimize performance of point/vector master-to-local and; local-to-master computation. We need to have in mind that a; transformation in **`TGeo`** has two major use-cases. The first one is; for defining the placement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:50925,optimiz,optimizing,50925,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"eparate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent can be kept coherent with other agents by:; using the MTYPE RW (read-write) or MTYPE CC (cache-coherent) with the PTE; C-bit for memory local to the L2; and using the MTYPE NC (non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE C-bit set, by CPU accesses over; XGMI, and by PCIe requests that are configured to be coherent requests.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter and and the PTE C-bit being set.; * Since all work-groups on the same agent share the same L2, no L2; invalidation or writeback is required for coherence.; * To ensure coherence of local and remote memory writes of work-groups in; different agents a ``buffer_wbl2`` is required. It will writeback dirty L2; cache lines of MTYPE RW (used for local coarse grain memory) and MTYPE NC; ()used for remote coarse grain memory). Note that MTYPE CC (used for local; fine grain memory) causes write through to DRAM, and MTYPE UC (used for; remote fi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:237917,cache,cache,237917,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,3,['cache'],"['cache', 'caches']"
Performance,"ept by flat and scratch instructions in GFX9-GFX11. The generic address space uses the hardware flat address support available in; GFX7-GFX11. This uses two fixed ranges of virtual addresses (the private and; local apertures), that are outside the range of addressible global memory, to; map from a flat address to a private or local address. FLAT instructions can take a flat address and access global, private (scratch); and group (LDS) memory depending on if the address is within one of the; aperture ranges. Flat access to scratch requires hardware aperture setup and; setup in the kernel prologue (see; :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`). Flat access to LDS requires; hardware aperture setup and M0 (GFX7-GFX8) register setup (see; :ref:`amdgpu-amdhsa-kernel-prolog-m0`). To convert between a segment address and a flat address the base address of the; apertures address can be used. For GFX7-GFX8 these are available in the; :ref:`amdgpu-amdhsa-hsa-aql-queue` the address of which can be obtained with; Queue Ptr SGPR (see :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). For; GFX9-GFX11 the aperture base addresses are directly available as inline constant; registers ``SRC_SHARED_BASE/LIMIT`` and ``SRC_PRIVATE_BASE/LIMIT``. In 64 bit; address mode the aperture sizes are 2^32 bytes and the base is aligned to 2^32; which makes it easier to convert from flat to segment or segment to flat. Image and Samplers; ~~~~~~~~~~~~~~~~~~. Image and sample handles created by an HSA compatible runtime (see; :ref:`amdgpu-os`) are 64-bit addresses of a hardware 32-byte V# and 48 byte S#; object respectively. In order to support the HSA ``query_sampler`` operations; two extra dwords are used to store the HSA BRIG enumeration values for the; queries that are not trivially deducible from the S# representation. HSA Signals; ~~~~~~~~~~~. HSA signal handles created by an HSA compatible runtime (see :ref:`amdgpu-os`); are 64-bit addresses of a structure allocated in memory accessib",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:156277,queue,queue,156277,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"eptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hardening | Mitigated Speedup |; | -------------------------------------- | -------: | -------------: | ----------------: |; | Google microbenchmark suite | -74.8% | -36.4% | **2.5x** |; | Large server QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a visualization of the microbenchmark suite results which helps show; the distribution of results that is somewhat lost in the summary. The y-axis is; a log-scale speedup ratio of load hardening relative to `lfence` (up -> faster; -> better). Each box-and-whiskers represents one microbenchmark which may have; many different metrics measured. The red line marks the median, the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:47397,load,load,47397,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"equentially lines up classes, whereas; multiple (virtual) inheritance usually requires thunks.; Thus, when calling base class methods on a derived instance, the latter; requires offset calculations that depend on that instance, whereas the former; has fixed offsets fully determined by the class definitions themselves.; By labeling classes appropriately, single inheritance classes (by far the; most common case) do not incur the overhead in PyPy's JIT-ed traces that is; otherwise unavoidable for multiple virtual inheritance.; As another example, consider that the C++ standard does not allow modifying; a ``std::vector`` while looping over it, whereas Python has no such; restriction, complicating loops.; Thus, cppyy has specialized ``std::vector`` iteration for both PyPy and; CPython, easily outperforming looping over an equivalent numpy array. In CPython, the performance of `non-overloaded` function calls depends; greatly on the Python interpreter's internal specializations; and Python3; has many specializations specific to basic extension modules (C function; pointer calls), gaining a performance boost of more than 30% over Python2.; Only since Python3.8 is there also better support for closure objects (vector; calls) as cppyy uses, to short-cut through the interpreter's own overhead. As a practical consideration, whether a binder performs well on code that you; care about, depends `entirely` on whether it has the relevant specializations; for your most performance-sensitive use cases.; The only way to know for sure is to write a test application and measure, but; a binder that provides more specializations, or makes it easy to add your; own, is more likely to deliver. `Manual v.s. automatic`; -----------------------. Python is, today, one of the most popular programming languages and has a; rich and mature eco-system around it.; But when the project that became cppyy started in the field of High Energy; Physics (HEP), Python usage was non-existent there.; As a Python",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:3690,perform,performance,3690,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,2,['perform'],['performance']
Performance,"equires much less memory. Merging in; one-go (the previous default) can be activated by passing 'H' in the; constructor options.; In ProofBench, add possibility to change the location of the; generated files via the third argument of TProofBench::MakeDataSet.; Several optimizations in the low level PROOF event loop; (TProofPlayer::Process), allowing to reduce dramatically the; overhead introduced by the operations PROOF needs to perform during the; event loop. A measurement of the overhead can be obtained from a very; light computational task, for example, generating one random number and; filling one histogram; executing this task within a PROOF-Lite session; with 1 worker now takes only 1.8 times the time required by a straight; loop in the parent ROOT session; the same number before was about 13. ; In TDrawFeedback::Feedback, call method Draw() of objects not; identified as TH1 derivation. This allows user-defined objects; implementing Draw to be displayed via this utility class.; In TProof::LoadPackageOnClient, do not create a symlink; 'pack_name' to the package dir, but add directly the package dir to the; include path. This solves the longstanding annoying problem of failure; when a directory or file with the name of the package did already exist; in the local working directory. . Fixes; ; Fix merging issue affecting automatic dataset creation when; only one worker is active.; Fix the realtime reported by TProof::GetRealTime() for masters; (it was overwritten with the ones coming from workers).; Fix serious problem with TProof::Load: additional files were; not copied in the master sandbox but left in the cache. A workaround; for backward compatibility has also been implemented.; Fix a problem preventing actions requiring access to worker; nodes (log file retrieval, reset) to work on workers where the username; is different from the one o the master, e.g. PoD on gLite.; Fix issue with the specification of the working directory; template in 'xpd.multiuser'.; . ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html:4807,cache,cache,4807,proof/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html,4,"['Load', 'cache']","['Load', 'LoadPackageOnClient', 'cache']"
Performance,"er (or similar low latency, high accuracy clocks) on those; targets that support it. On X86, it should map to RDTSC. On Alpha, it; should map to RPCC. As the backing counters overflow quickly (on the; order of 9 seconds on alpha), this should only be used for small; timings. Semantics:; """""""""""""""""""". When directly supported, reading the cycle counter should not modify any; memory. Implementations are allowed to either return an application; specific value or a system wide value. On backends without support, this; is lowered to a constant 0. Note that runtime support may be conditional on the privilege-level code is; running at and the host platform. '``llvm.clear_cache``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.clear_cache(ptr, ptr). Overview:; """""""""""""""""". The '``llvm.clear_cache``' intrinsic ensures visibility of modifications; in the specified range to the execution unit of the processor. On; targets with non-unified instruction and data cache, the implementation; flushes the instruction cache. Semantics:; """""""""""""""""""". On platforms with coherent instruction and data caches (e.g. x86), this; intrinsic is a nop. On platforms with non-coherent instruction and data; cache (e.g. ARM, MIPS), the intrinsic is lowered either to appropriate; instructions or a system call, if cache flushing requires special; privileges. The default behavior is to emit a call to ``__clear_cache`` from the run; time library. This intrinsic does *not* empty the instruction pipeline. Modifications; of the current function are outside the scope of the intrinsic. '``llvm.instrprof.increment``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.instrprof.increment(ptr <name>, i64 <hash>,; i32 <num-counters>, i32 <index>). Overview:; """""""""""""""""". The '``llvm.instrprof.increment``' intrinsic can be emitted by a; frontend for use with instrumentation based profiling. These will be; lowered by the ``-instrprof`` pass to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:526182,cache,cache,526182,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['cache'],['cache']
Performance,"er 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompile",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2353,optimiz,optimization,2353,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,3,['optimiz'],"['optimization', 'optimize']"
Performance,"er PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provid",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6655,load,loads,6655,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['load'],['loads']
Performance,"er systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassMa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:4422,optimiz,optimization,4422,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimization']
Performance,"er that every ``load`` and ``store`` to the same pointer operand; can be assumed to load or store the same; value (but see the ``llvm.launder.invariant.group`` intrinsic which affects; when two pointers are considered the same). Pointers returned by bitcast or; getelementptr with only zero indices are considered the same. Examples:. .. code-block:: llvm. @unknownPtr = external global i8; ...; %ptr = alloca i8; store i8 42, ptr %ptr, !invariant.group !0; call void @foo(ptr %ptr). %a = load i8, ptr %ptr, !invariant.group !0 ; Can assume that value under %ptr didn't change; call void @foo(ptr %ptr). %newPtr = call ptr @getPointer(ptr %ptr); %c = load i8, ptr %newPtr, !invariant.group !0 ; Can't assume anything, because we only have information about %ptr. %unknownValue = load i8, ptr @unknownPtr; store i8 %unknownValue, ptr %ptr, !invariant.group !0 ; Can assume that %unknownValue == 42. call void @foo(ptr %ptr); %newPtr2 = call ptr @llvm.launder.invariant.group.p0(ptr %ptr); %d = load i8, ptr %newPtr2, !invariant.group !0 ; Can't step through launder.invariant.group to get value of %ptr. ...; declare void @foo(ptr); declare ptr @getPointer(ptr); declare ptr @llvm.launder.invariant.group.p0(ptr). !0 = !{}. The invariant.group metadata must be dropped when replacing one pointer by; another based on aliasing information. This is because invariant.group is tied; to the SSA value of the pointer operand. .. code-block:: llvm. %v = load i8, ptr %x, !invariant.group !0; ; if %x mustalias %y then we can replace the above instruction with; %v = load i8, ptr %y. Note that this is an experimental feature, which means that its semantics might; change in the future. '``type``' Metadata; ^^^^^^^^^^^^^^^^^^^. See :doc:`TypeMetadata`. '``associated``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^. The ``associated`` metadata may be attached to a global variable definition with; a single argument that references a global object (optionally through an alias). This metadata lowers to the ELF sectio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:317135,load,load,317135,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"er to call them through; generated wrappers.; Thus, any classes, functions, and data that will be used in Python need; to be exported.; This is the default behavior on Mac and Linux, but not on Windows.; On that platform, use ``__declspec(dllexport)`` to explicitly export the; classes and function you expect to call.; CMake has simple `support for exporting all`_ C++ symbols. In tandem with any dictionary, a pre-compiled module (.pcm) file will be; generated.; C++ modules are still on track for inclusion in the C++20 standard and most; modern C++ compilers, ``clang`` among them, already have implementations.; The benefits for cppyy include faster bindings generation, lower memory; footprint, and isolation from preprocessor macros and compiler flags.; The use of modules is transparent, other than the requirement that they; need to be co-located with the compiled dictionary shared library. Optionally, the dictionary generation process also produces a mapping file,; which lists the libraries needed to load C++ classes on request (for details,; see the section on the class loader below). Structurally, you could have a single dictionary for a project as a whole,; but more likely a large project will have a pre-existing functional; decomposition that can be followed, with a dictionary per functional unit. Generation; ^^^^^^^^^^. There are two interfaces onto the same underlying dictionary generator:; ``rootcling`` and ``genreflex``.; The reason for having two is historic and they are not complete duplicates,; so one or the other may suit your preference better.; It is foreseen that both will be replaced once C++ modules become more; mainstream, as that will allow simplification and improved robustness. rootcling; """""""""""""""""". The first interface is called ``rootcling``::. $ rootcling; Usage: rootcling [-v][-v0-4] [-f] [out.cxx] [opts] file1.h[+][-][!] file2.h[+][-][!] ...[Linkdef.h]; For more extensive help type: /usr/local/lib/python2.7/dist-packages/cppyy_backend/bin/root",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:3088,load,load,3088,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,2,['load'],"['load', 'loader']"
Performance,"er to memory, use; ``VirtRegMap::assignVirt2StackSlot(vreg)``. This method will return the stack; slot where ``vreg``'s value will be located. If it is necessary to map another; virtual register to the same stack slot, use; ``VirtRegMap::assignVirt2StackSlot(vreg, stack_location)``. One important point; to consider when using the indirect mapping, is that even if a virtual register; is mapped to memory, it still needs to be mapped to a physical register. This; physical register is the location where the virtual register is supposed to be; found before being stored or after being reloaded. If the indirect strategy is used, after all the virtual registers have been; mapped to physical registers or stack slots, it is necessary to use a spiller; object to place load and store instructions in the code. Every virtual that has; been mapped to a stack slot will be stored to memory after being defined and will; be loaded before being used. The implementation of the spiller tries to recycle; load/store instructions, avoiding unnecessary instructions. For an example of; how to invoke the spiller, see ``RegAllocLinearScan::runOnMachineFunction`` in; ``lib/CodeGen/RegAllocLinearScan.cpp``. Handling two address instructions; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. With very rare exceptions (e.g., function calls), the LLVM machine code; instructions are three address instructions. That is, each instruction is; expected to define at most one register, and to use at most two registers.; However, some architectures use two address instructions. In this case, the; defined register is also one of the used registers. For instance, an instruction; such as ``ADD %EAX, %EBX``, in X86 is actually equivalent to ``%EAX = %EAX +; %EBX``. In order to produce correct code, LLVM must convert three address instructions; that represent two address instructions into true two address instructions. LLVM; provides the pass ``TwoAddressInstructionPass`` for this specific purpose. It; must be run before regis",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:66194,load,load,66194,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"er with a boolean; invariant that is defined to be true. **Syntax**:. .. code-block:: c++. __builtin_assume(bool). **Example of Use**:. .. code-block:: c++. int foo(int x) {; __builtin_assume(x != 0);; // The optimizer may short-circuit this check using the invariant.; if (x == 0); return do_something();; return do_something_else();; }. **Description**:. The boolean argument to this function is defined to be true. The optimizer may; analyze the form of the expression provided as the argument and deduce from; that information used to optimize the program. If the condition is violated; during execution, the behavior is undefined. The argument itself is never; evaluated, so any side effects of the expression will be discarded. Query for this feature with ``__has_builtin(__builtin_assume)``. .. _langext-__builtin_assume_separate_storage:. ``__builtin_assume_separate_storage``; -------------------------------------. ``__builtin_assume_separate_storage`` is used to provide the optimizer with the; knowledge that its two arguments point to separately allocated objects. **Syntax**:. .. code-block:: c++. __builtin_assume_separate_storage(const volatile void *, const volatile void *). **Example of Use**:. .. code-block:: c++. int foo(int *x, int *y) {; __builtin_assume_separate_storage(x, y);; *x = 0;; *y = 1;; // The optimizer may optimize this to return 0 without reloading from *x.; return *x;; }. **Description**:. The arguments to this function are assumed to point into separately allocated; storage (either different variable definitions or different dynamic storage; allocations). The optimizer may use this fact to aid in alias analysis. If the; arguments point into the same storage, the behavior is undefined. Note that the; definition of ""storage"" here refers to the outermost enclosing allocation of any; particular object (so for example, it's never correct to call this function; passing the addresses of fields in the same struct, elements of the same array,; etc.). Query f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:99706,optimiz,optimizer,99706,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizer']
Performance,"er, a store cannot be inserted along a path where it might not execute; otherwise. Take the following example:. .. code-block:: c. /* C code, for readability; run through clang -O2 -S -emit-llvm to get; equivalent IR */; int x;; void f(int* a) {; for (int i = 0; i < 100; i++) {; if (a[i]); x += 1;; }; }. The following is equivalent in non-concurrent situations:. .. code-block:: c. int x;; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; if (a[i]); xtemp += 1;; }; x = xtemp;; }. However, LLVM is not allowed to transform the former to the latter: it could; indirectly introduce undefined behavior if another thread can access ``x`` at; the same time. That thread would read `undef` instead of the value it was; expecting, which can lead to undefined behavior down the line. (This example is; particularly of interest because before the concurrency model was implemented,; LLVM would perform this transformation.). Note that speculative loads are allowed; a load which is part of a race returns; ``undef``, but does not have undefined behavior. Atomic instructions; ===================. For cases where simple loads and stores are not sufficient, LLVM provides; various atomic instructions. The exact guarantees provided depend on the; ordering; see `Atomic orderings`_. ``load atomic`` and ``store atomic`` provide the same basic functionality as; non-atomic loads and stores, but provide additional guarantees in situations; where threads and signals are involved. ``cmpxchg`` and ``atomicrmw`` are essentially like an atomic load followed by an; atomic store (where the store is conditional for ``cmpxchg``), but no other; memory operation can happen on any thread between the load and store. A ``fence`` provides Acquire and/or Release ordering which is not part; of another operation; it is normally used along with Monotonic memory; operations. A Monotonic load followed by an Acquire fence is roughly; equivalent to an Acquire load, and a Monotonic store following a; Rele",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:4037,load,loads,4037,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],"['load', 'loads']"
Performance,"er-location-description-operations:. A.2.5.4.4.4 Register Location Description Operations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. note::. This section replaces DWARF Version 5 section 2.6.1.1.3. There is a register location storage that corresponds to each of the target; architecture registers. The size of each register location storage corresponds; to the size of the corresponding target architecture register. A register location description specifies a register location storage. The bit; offset corresponds to a bit position within the register. Bits accessed using a; register location description access the corresponding target architecture; register starting at the specified bit offset. 1. ``DW_OP_reg0``, ``DW_OP_reg1``, ..., ``DW_OP_reg31``. ``DW_OP_reg<N>`` operations encode the numbers of up to 32 registers,; numbered from 0 through 31, inclusive. The target architecture register; number R corresponds to the N in the operation name. The operation is equivalent to performing ``DW_OP_regx R``. 2. ``DW_OP_regx``. ``DW_OP_regx`` has a single unsigned LEB128 integer operand that represents; a target architecture register number R. If the current call frame is the top call frame, it pushes a location; description L that specifies one register location description SL on the; stack. SL specifies the register location storage that corresponds to R with; a bit offset of 0 for the current thread. If the current call frame is not the top call frame, call frame information; (see :ref:`amdgpu-dwarf-call-frame-information`) is used to determine the; location description that holds the register for the current call frame and; current program location of the current thread. The resulting location; description L is pushed. *Note that if call frame information is used, the resulting location; description may be register, memory, or undefined.*. *An implementation may evaluate the call frame information immediately, or; may defer evaluation until L is accessed b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:118648,perform,performing,118648,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"er. N: John T. Criswell; E: criswell@uiuc.edu; D: Original Autoconf support, documentation improvements, bug fixes. N: Anshuman Dasgupta; E: adasgupt@codeaurora.org; D: Deterministic finite automaton based infrastructure for VLIW packetization. N: Stefanus Du Toit; E: stefanus.du.toit@intel.com; D: Bug fixes and minor improvements. N: Rafael Avila de Espindola; E: rafael@espindo.la; D: MC and LLD work. N: Dave Estes; E: cestes@codeaurora.org; D: AArch64 machine description for Cortex-A53. N: Alkis Evlogimenos; E: alkis@evlogimenos.com; D: Linear scan register allocator, many codegen improvements, Java frontend. N: Hal Finkel; E: hfinkel@anl.gov; D: Basic-block autovectorization, PowerPC backend improvements. N: Eric Fiselier; E: eric@efcs.ca; D: LIT patches and documentation. N: Ryan Flynn; E: pizza@parseerror.com; D: Miscellaneous bug fixes. N: Brian Gaeke; E: gaeke@uiuc.edu; W: http://www.students.uiuc.edu/~gaeke/; D: Portions of X86 static and JIT compilers; initial SparcV8 backend; D: Dynamic trace optimizer; D: FreeBSD/X86 compatibility fixes, the llvm-nm tool. N: Nicolas Geoffray; E: nicolas.geoffray@lip6.fr; W: http://www-src.lip6.fr/homepages/Nicolas.Geoffray/; D: PPC backend fixes for Linux. N: Louis Gerbarg; E: lgg@apple.com; D: Portions of the PowerPC backend. N: Saem Ghani; E: saemghani@gmail.com; D: Callgraph class cleanups. N: Mikhail Glushenkov; E: foldr@codedgers.com; D: Author of llvmc2. N: Dan Gohman; E: llvm@sunfishcode.online; D: Miscellaneous bug fixes; D: WebAssembly Backend. N: Renato Golin; E: rengolin@systemcall.eu; E: rengolin@gmail.com; D: ARM/AArch64 back-end improvements; D: Loop Vectorizer improvements; D: Regression and Test Suite improvements; D: Linux compatibility (GNU, musl, etc); D: Initial Linux kernel / Android support effort; I: rengolin. N: David Goodwin; E: david@goodwinz.net; D: Thumb-2 code generator. N: David Greene; E: greened@obbligato.org; D: Miscellaneous bug fixes; D: Register allocation refactoring. N: Gabor Greif; E",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT:3872,optimiz,optimizer,3872,interpreter/llvm-project/llvm/CREDITS.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT,1,['optimiz'],['optimizer']
Performance,"er.cpp``, which implements the ``AsmPrinter`` class that converts; the LLVM to printable assembly. The implementation must include the following; headers that have declarations for the ``AsmPrinter`` and; ``MachineFunctionPass`` classes. The ``MachineFunctionPass`` is a subclass of; ``FunctionPass``. .. code-block:: c++. #include ""llvm/CodeGen/AsmPrinter.h""; #include ""llvm/CodeGen/MachineFunctionPass.h"". As a ``FunctionPass``, ``AsmPrinter`` first calls ``doInitialization`` to set; up the ``AsmPrinter``. In ``SparcAsmPrinter``, a ``Mangler`` object is; instantiated to process variable names. In ``XXXAsmPrinter.cpp``, the ``runOnMachineFunction`` method (declared in; ``MachineFunctionPass``) must be implemented for ``XXXAsmPrinter``. In; ``MachineFunctionPass``, the ``runOnFunction`` method invokes; ``runOnMachineFunction``. Target-specific implementations of; ``runOnMachineFunction`` differ, but generally do the following to process each; machine function:. * Call ``SetupMachineFunction`` to perform initialization. * Call ``EmitConstantPool`` to print out (to the output stream) constants which; have been spilled to memory. * Call ``EmitJumpTableInfo`` to print out jump tables used by the current; function. * Print out the label for the current function. * Print out the code for the function, including basic block labels and the; assembly for the instruction (using ``printInstruction``). The ``XXXAsmPrinter`` implementation must also include the code generated by; TableGen that is output in the ``XXXGenAsmWriter.inc`` file. The code in; ``XXXGenAsmWriter.inc`` contains an implementation of the ``printInstruction``; method that may call these methods:. * ``printOperand``; * ``printMemOperand``; * ``printCCOperand`` (for conditional statements); * ``printDataDirective``; * ``printDeclare``; * ``printImplicitDef``; * ``printInlineAsm``. The implementations of ``printDeclare``, ``printImplicitDef``,; ``printInlineAsm``, and ``printLabel`` in ``AsmPrinter.cpp`` are general",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:69520,perform,perform,69520,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['perform']
Performance,"er.h"". using namespace llvm;. namespace {; class LLVM_LIBRARY_VISIBILITY MyGC : public GCStrategy {; public:; MyGC() {}; };. GCRegistry::Add<MyGC>; X(""mygc"", ""My bespoke garbage collector."");; }. This boilerplate collector does nothing. More specifically:. * ``llvm.gcread`` calls are replaced with the corresponding ``load``; instruction. * ``llvm.gcwrite`` calls are replaced with the corresponding ``store``; instruction. * No safe points are added to the code. * The stack map is not compiled into the executable. Using the LLVM makefiles, this code; can be compiled as a plugin using a simple makefile:. .. code-block:: make. # lib/MyGC/Makefile. LEVEL := ../..; LIBRARYNAME = MyGC; LOADABLE_MODULE = 1. include $(LEVEL)/Makefile.common. Once the plugin is compiled, code using it may be compiled using ``llc; -load=MyGC.so`` (though MyGC.so may have some other platform-specific; extension):. ::. $ cat sample.ll; define void @f() gc ""mygc"" {; entry:; ret void; }; $ llvm-as < sample.ll | llc -load=MyGC.so. It is also possible to statically link the collector plugin into tools, such as; a language-specific compiler front-end. .. _collector-algos:. Overview of available features; ------------------------------. ``GCStrategy`` provides a range of features through which a plugin may do useful; work. Some of these are callbacks, some are algorithms that can be enabled,; disabled, or customized. This matrix summarizes the supported (and planned); features and correlates them with the collection techniques which typically; require them. .. |v| unicode:: 0x2714; :trim:. .. |x| unicode:: 0x2718; :trim:. +------------+------+--------+----------+-------+---------+-------------+----------+------------+; | Algorithm | Done | Shadow | refcount | mark- | copying | incremental | threaded | concurrent |; | | | stack | | sweep | | | | |; +============+======+========+==========+=======+=========+=============+==========+============+; | stack map | |v| | | | |x| | |x| | |x| | |x| | |x| |; +-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:25591,load,load,25591,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['load']
Performance,"er/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:345447,load,load,345447,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"er2) {; if (condition) {; // ... lots of code ...; leak(*pointer1);; } else {; // ... more code ...; leak(*pointer2);; }; }; ```. This would get transformed into something resembling the following:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; if (condition) {; // Assuming ?: is implemented using branchless logic...; predicate_state = !condition ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; // Harden the pointer so it can't be loaded; pointer1 &= predicate_state;; leak(*pointer1);; } else {; predicate_state = condition ? all_zeros_mask : predicate_state;; // ... more code ...; //; // Alternative: Harden the loaded value; int value2 = *pointer2 & predicate_state;; leak(value2);; }; }; ```. The result should be that if the `if (condition) {` branch is mis-predicted,; there is a *data* dependency on the condition used to zero out any pointers; prior to loading through them or to zero out all of the loaded bits. Even; though this code pattern may still execute speculatively, *invalid* speculative; executions are prevented from leaking secret data from memory (but note that; this data might still be loaded in safe ways, and some regions of memory are; required to not hold secrets, see below for detailed limitations). This; approach only requires the underlying hardware have a way to implement a; branchless and unpredicted conditional update of a register's value. All modern; architectures have support for this, and in fact such support is necessary to; correctly implement constant time cryptographic primitives. Crucial properties of this approach:; * It is not preventing any particular side-channel from working. This is; important as there are an unknown number of potential side channels and we; expect to continue discovering more. Instead, it prevents the observation of; secret data",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:4410,load,loading,4410,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],"['loaded', 'loading']"
Performance,"er:; ```; struct array {; unsigned long length;; unsigned char data[];; };; struct array *arr1 = ...; // small array; struct array *arr2 = ...; // array of size 0x400; unsigned long untrusted_offset_from_caller = ...;; if (untrusted_offset_from_caller < arr1->length) {; unsigned char value = arr1->data[untrusted_offset_from_caller];; unsigned long index2 = ((value&1)*0x100)+0x200;; unsigned char value2 = arr2->data[index2];; }; ```. The key of the attack is to call this with `untrusted_offset_from_caller` that; is far outside of the bounds when the branch predictor will predict that it; will be in-bounds. In that case, the body of the `if` will be executed; speculatively, and may read secret data into `value` and leak it via a; cache-timing side channel when a dependent access is made to populate `value2`. ## High Level Mitigation Approach. While several approaches are being actively pursued to mitigate specific; branches and/or loads inside especially risky software (most notably various OS; kernels), these approaches require manual and/or static analysis aided auditing; of code and explicit source changes to apply the mitigation. They are unlikely; to scale well to large applications. We are proposing a comprehensive; mitigation approach that would apply automatically across an entire program; rather than through manual changes to the code. While this is likely to have a; high performance cost, some applications may be in a good position to take this; performance / security tradeoff. The specific technique we propose is to cause loads to be checked using; branchless code to ensure that they are executing along a valid control flow; path. Consider the following C-pseudo-code representing the core idea of a; predicate guarding potentially invalid loads:; ```; void leak(int data);; void example(int* pointer1, int* pointer2) {; if (condition) {; // ... lots of code ...; leak(*pointer1);; } else {; // ... more code ...; leak(*pointer2);; }; }; ```. This would get transf",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:2450,load,loads,2450,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"erPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and implemented via the; ""swapped"" form. //===----------------------------------------------------------------------===//. There is a utility program called PerfectShuffle that generates a table of the; shortest instruction sequence for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===--------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:8453,load,load,8453,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,2,['load'],['load']
Performance,"er_inv sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc0=1 sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. GFX940, GFX941; - wavefront - generic buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:305906,load,load,305906,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"er_return=0``.; * Use-after-scope (clang flag ``-fsanitize-address-use-after-scope``); * Double-free, invalid free; * Memory leaks (experimental). Typical slowdown introduced by AddressSanitizer is **2x**. How to build; ============. Build LLVM/Clang with `CMake <https://llvm.org/docs/CMake.html>` and enable; the ``compiler-rt`` runtime. An example CMake configuration that will allow; for the use/testing of AddressSanitizer:. .. code-block:: console. $ cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=""clang"" -DLLVM_ENABLE_RUNTIMES=""compiler-rt"" <path to source>/llvm. Usage; =====. Simply compile and link your program with ``-fsanitize=address`` flag. The; AddressSanitizer run-time library should be linked to the final executable, so; make sure to use ``clang`` (not ``ld``) for the final link step. When linking; shared libraries, the AddressSanitizer run-time is not linked, so; ``-Wl,-z,defs`` may cause link errors (don't use it with AddressSanitizer). To; get a reasonable performance add ``-O1`` or higher. To get nicer stack traces; in error messages add ``-fno-omit-frame-pointer``. To get perfect stack traces; you may need to disable inlining (just use ``-O1``) and tail call elimination; (``-fno-optimize-sibling-calls``). .. code-block:: console. % cat example_UseAfterFree.cc; int main(int argc, char **argv) {; int *array = new int[100];; delete [] array;; return array[argc]; // BOOM; }. # Compile and link; % clang++ -O1 -g -fsanitize=address -fno-omit-frame-pointer example_UseAfterFree.cc. or:. .. code-block:: console. # Compile; % clang++ -O1 -g -fsanitize=address -fno-omit-frame-pointer -c example_UseAfterFree.cc; # Link; % clang++ -g -fsanitize=address example_UseAfterFree.o. If a bug is detected, the program will print an error message to stderr and; exit with a non-zero exit code. AddressSanitizer exits on the first detected error.; This is by design:. * This approach allows AddressSanitizer to produce faster and smaller generated code; (both by ~5%).;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst:1607,perform,performance,1607,interpreter/llvm-project/clang/docs/AddressSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst,1,['perform'],['performance']
Performance,"erage Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; The ``-bottleneck-analysis`` command line option enables the analysis of; performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in; backend pressure (caused by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product example on btver2. .. code-block:: none. Cycles with backend pressure increase [ 48.07% ]; Throughput Bottlenecks:; Resource Pressure [ 47.77% ]; - JFPA [ 47.77% ]; - JFPU0 [ 47.77% ]; Data Dependencies: [ 0.30% ]; - Register Dependencies [ 0.30% ]; - Memory Dependencies [ 0.00% ]. Critical sequence based on the simulation:. Instruction Dependency Information; +----< 2. vhaddps %xmm3, %xmm3, %xmm4; |; | < loop carried >; |; | 0. vmulps %xmm0, %xmm1, %xmm2; +----> 1. vhaddps %xmm2, %xmm2, %xmm3 ## RES",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:26468,latency,latency,26468,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['latency'],['latency']
Performance,"erance must be provided. #### `ROOT::Math::GSLMInimizer1D`. This class wraps two different methods from the GSL.; The algorithms which can be chosen at construction time are *GOLDENSECTION*, which is the simplest method; but the slowest and *BRENT* (the default one) which combines the golden section with a parabolic interpolation.; The algorithm can be chosen as a different enumeration in the constructor:; * `ROOT::Math::Minim1D::kBRENT` for the Brent algorithm (default); * `ROOT::Math::Minim1D::kGOLDENSECTION` for the golden section algorithm. ```{.cpp}; // this makes class with the default Brent algorithm; ROOT::Math::GSLMinimizer1D minBrent;; // this make the class with the Golden Section algorithm; ROOT::Math::GSLMinimizer1D minGold(ROOT::Math::Minim1D::kGOLDENSECTION);; ```. The interface to set the function and to minimize is the same as in the case of the `BrentMinimizer1D`. #### Using the TF1 class. It is possible to perform the one-dimensional minimization/maximization of a function by using directly the function class in ROOT, `TF1` of the *Hist* library.; The minmization is implemented in `TF1` using the BrentMInimizer1D and available with the class member functions; * `TF1::GetMinimum`/`TF1::GetMaximum` to find the function minimum/maximum value; * `TF1::GetMinimumX`/`TF1::GetMaximumX` to find the x value corresponding at the function minimum. The interval to search for the minimum (the default is the `TF1` range), tolerance and maximum iterations can be provided as optional parameters of the; `TF1::GetMinimum/Maximum` functions. ### Multi-Dimensional Minimization. All the algorithms for multi-dimensional minimization are implementing the `ROOT::Math::Minimizer`; interface and they can be used in the same way and one can switch between minimizer at run-time.; The minimizer concrete class can be in different ROOT libraries and they can be instantiate using the ROOT; plug-in manager.; More information on multi-dimensional minimization is provided in the Fit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:65846,perform,perform,65846,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['perform']
Performance,"erations in particular), make sure; it doesn't replace an atomic load or store with a non-atomic operation. Some examples of how optimizations interact with various kinds of atomic; operations:. * ``memcpyopt``: An atomic operation cannot be optimized into part of a; memcpy/memset, including unordered loads/stores. It can pull operations; across some atomic operations. * LICM: Unordered loads/stores can be moved out of a loop. It just treats; monotonic operations like a read+write to a memory location, and anything; stricter than that like a nothrow call. * DSE: Unordered stores can be DSE'ed like normal stores. Monotonic stores can; be DSE'ed in some cases, but it's tricky to reason about, and not especially; important. It is possible in some case for DSE to operate across a stronger; atomic operation, but it is fairly tricky. DSE delegates this reasoning to; MemoryDependencyAnalysis (which is also used by other passes like GVN). * Folding a load: Any atomic load from a constant global can be constant-folded,; because it cannot be observed. Similar reasoning allows sroa with; atomic loads and stores. Atomics and Codegen; ===================. Atomic operations are represented in the SelectionDAG with ``ATOMIC_*`` opcodes.; On architectures which use barrier instructions for all atomic ordering (like; ARM), appropriate fences can be emitted by the AtomicExpand Codegen pass if; ``shouldInsertFencesForAtomic()`` returns true. The MachineMemOperand for all atomic operations is currently marked as volatile;; this is not correct in the IR sense of volatile, but CodeGen handles anything; marked volatile very conservatively. This should get fixed at some point. One very important property of the atomic operations is that if your backend; supports any inline lock-free atomic operations of a given size, you should; support *ALL* operations of that size in a lock-free manner. When the target implements atomic ``cmpxchg`` or LL/SC instructions (as most do); this is trivial: all ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:17936,load,load,17936,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['load']
Performance,"ered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - workgroup - generic; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214200,load,load,214200,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"erenceable_or_null`` (if the pointer is non-null) and the; ``sret``, ``byval``, ``byref``, ``inalloca``, ``preallocated`` family of; attributes. Note that not all of these combinations are useful, e.g.; ``byval`` arguments are known to be writable even without this attribute. The ``writable`` attribute cannot be combined with ``readnone``,; ``readonly`` or a ``memory`` attribute that does not contain; ``argmem: write``. ``dead_on_unwind``; At a high level, this attribute indicates that the pointer argument is dead; if the call unwinds, in the sense that the caller will not depend on the; contents of the memory. Stores that would only be visible on the unwind; path can be elided. More precisely, the behavior is as-if any memory written through the; pointer during the execution of the function is overwritten with a poison; value on unwind. This includes memory written by the implicit write implied; by the ``writable`` attribute. The caller is allowed to access the affected; memory, but all loads that are not preceded by a store will return poison. This attribute cannot be applied to return values. .. _gc:. Garbage Collector Strategy Names; --------------------------------. Each function may specify a garbage collector strategy name, which is simply a; string:. .. code-block:: llvm. define void @f() gc ""name"" { ... }. The supported values of *name* includes those :ref:`built in to LLVM; <builtin-gc-strategies>` and any provided by loaded plugins. Specifying a GC; strategy will cause the compiler to alter its output in order to support the; named garbage collection algorithm. Note that LLVM itself does not contain a; garbage collector, this functionality is restricted to generating machine code; which can interoperate with a collector provided externally. .. _prefixdata:. Prefix Data; -----------. Prefix data is data associated with a function which the code; generator will emit immediately before the function's entrypoint.; The purpose of this feature is to allow fronte",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:71622,load,loads,71622,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"erences. The second ``%s`` is the module type and it; determines what the remaining fields are. The following module types are; supported:. * ``elf:%x``. Here ``%x`` encodes an ELF Build ID. The Build ID should refer to a single; linked binary. The Build ID string is the sole way to identify the binary from; which this module was loaded. Example::. {{{module:1:libc.so:elf:83238ab56ba10497}}}. ``{{{mmap:%p:%i:...}}}``. This contextual element is used to give information about a particular region; in memory. ``%p`` is the starting address and ``%i`` gives the size in hex of the; region of memory. The ``...`` part can take different forms to give different; information about the specified region of memory. The allowed forms are the; following:. * ``load:%i:%s:%p``. This subelement informs the filter that a segment was loaded from a module.; The module is identified by its module ID ``%i``. The ``%s`` is one or more of; the letters 'r', 'w', and 'x' (in that order and in either upper or lower; case) to indicate this segment of memory is readable, writable, and/or; executable. The symbolizing filter can use this information to guess whether; an address is a likely code address or a likely data address in the given; module. The remaining ``%p`` gives the module relative address. For ELF files; the module relative address will be the ``p_vaddr`` of the associated program; header. For example if your module's executable segment has; ``p_vaddr=0x1000``, ``p_memsz=0x1234``, and was loaded at ``0x7acba69d5000``; then you need to subtract ``0x7acba69d4000`` from any address between; ``0x7acba69d5000`` and ``0x7acba69d6234`` to get the module relative address.; The starting address will usually have been rounded down to the active page; size, and the size rounded up. Example::. {{{mmap:0x7acba69d5000:0x5a000:load:1:rx:0x1000}}}. .. rubric:: Footnotes. .. [#not_yet_implemented] This markup element is not yet implemented in; :doc:`llvm-symbolizer <CommandGuide/llvm-symbolizer>`.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst:21193,load,loaded,21193,interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SymbolizerMarkupFormat.rst,2,['load'],"['load', 'loaded']"
Performance,"erent CUs. * Atomic read-modify-write instructions implicitly bypass the L1 cache.; Therefore, they do not use the sc0 bit for coherence and instead use it to; indicate if the instruction returns the original value being updated. They; do use sc1 to indicate system or agent scope coherence. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache. * The gfx942 can be configured as a number of smaller agents with each having; a single L2 shared by all CUs on the same agent, or as fewer (possibly one); larger agents with groups of CUs on each agent each sharing separate L2; caches.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:287659,queue,queue,287659,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"erformed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_wbinvl1_vol`` is required as described in; the following item. * A ``buffer_wbinvl1_vol`` is required for coherence between wavefronts; executing in different work-groups as they may be executing on different; CUs. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to mee",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:236555,cache,cache,236555,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"ergo unsigned subtraction. Semantics:; """""""""""""""""""". The minimum value this operation can clamp to is 0, which is the smallest; unsigned value representable by the bit width of the unsigned arguments.; Because this is an unsigned operation, the result will never saturate towards; the largest possible value representable by this bit width. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.usub.sat.i4(i4 2, i4 1) ; %res = 1; %res = call i4 @llvm.usub.sat.i4(i4 2, i4 6) ; %res = 0. '``llvm.sshl.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sshl.sat``; on integers or vectors of integers of any bit width. ::. declare i16 @llvm.sshl.sat.i16(i16 %a, i16 %b); declare i32 @llvm.sshl.sat.i32(i32 %a, i32 %b); declare i64 @llvm.sshl.sat.i64(i64 %a, i64 %b); declare <4 x i32> @llvm.sshl.sat.v4i32(<4 x i32> %a, <4 x i32> %b). Overview; """""""""""""""""". The '``llvm.sshl.sat``' family of intrinsic functions perform signed; saturating left shift on the first argument. Arguments; """""""""""""""""""". The arguments (``%a`` and ``%b``) and the result may be of integer types of any; bit width, but they must have the same bit width. ``%a`` is the value to be; shifted, and ``%b`` is the amount to shift by. If ``b`` is (statically or; dynamically) equal to or larger than the integer bit width of the arguments,; the result is a :ref:`poison value <poisonvalues>`. If the arguments are; vectors, each vector element of ``a`` is shifted by the corresponding shift; amount in ``b``. Semantics:; """""""""""""""""""". The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the arguments. The minimum value is the; smallest signed value representable by this bit width. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.sshl.sat.i4(i4 2, i4 1) ; %res = 4; %res = call i4 @llvm.sshl.sat.i4(i4 2, i4 2) ; %res = 7; %res = call i4 @llvm.sshl.sat.i4(i4 -5, i4 1) ; %res = -8; %res",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:616446,perform,perform,616446,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"erializing an; Objective-C method declaration (or other Objective-C construct) that refers to; the selector. AST Reader Integration Points; -----------------------------. The ""lazy"" deserialization behavior of AST files requires their integration; into several completely different submodules of Clang. For example, lazily; deserializing the declarations during name lookup requires that the name-lookup; routines be able to query the AST file to find entities stored there. For each Clang data structure that requires direct interaction with the AST; reader logic, there is an abstract class that provides the interface between; the two modules. The ``ASTReader`` class, which handles the loading of an AST; file, inherits from all of these abstract classes to provide lazy; deserialization of Clang's data structures. ``ASTReader`` implements the; following abstract classes:. ``ExternalSLocEntrySource``; This abstract interface is associated with the ``SourceManager`` class, and; is used whenever the :ref:`source manager <pchinternals-sourcemgr>` needs to; load the details of a file, buffer, or macro instantiation. ``IdentifierInfoLookup``; This abstract interface is associated with the ``IdentifierTable`` class, and; is used whenever the program source refers to an identifier that has not yet; been seen. In this case, the AST reader searches for this identifier within; its :ref:`identifier table <pchinternals-ident-table>` to load any top-level; declarations or macros associated with that identifier. ``ExternalASTSource``; This abstract interface is associated with the ``ASTContext`` class, and is; used whenever the abstract syntax tree nodes need to loaded from the AST; file. It provides the ability to de-serialize declarations and types; identified by their numeric values, read the bodies of functions when; required, and read the declarations stored within a declaration context; (either for iteration or for name lookup). ``ExternalSemaSource``; This abstract interface is a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:22394,load,load,22394,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['load']
Performance,"eric; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw-with-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; atomicrmw-no-return-value; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:351966,load,load,351966,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"erification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling instrumentation added during compilation by the frontend, i.e. ``clang`` |; +----------------+--------------------------------------------------------------------------------------+; | IR | Profiling instrumentation",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:2182,optimiz,optimization,2182,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,1,['optimiz'],['optimization']
Performance,"eritance tree.; #. The argument to ``classof`` should be a ``const Base *``, where ``Base``; is some ancestor in the inheritance hierarchy. The argument should; *never* be a derived class or the class itself: the template machinery; for ``isa<>`` already handles this case and optimizes it.; #. For each class in the hierarchy that has no children, implement a; ``classof`` that checks only against its ``Kind``.; #. For each class in the hierarchy that has children, implement a; ``classof`` that checks a range of the first child's ``Kind`` and the; last child's ``Kind``. RTTI for Open Class Hierarchies; ===============================. Sometimes it is not possible to know all types in a hierarchy ahead of time.; For example, in the shapes hierarchy described above the authors may have; wanted their code to work for user defined shapes too. To support use cases; that require open hierarchies LLVM provides the ``RTTIRoot`` and; ``RTTIExtends`` utilities. The ``RTTIRoot`` class describes an interface for performing RTTI checks. The; ``RTTIExtends`` class template provides an implementation of this interface; for classes derived from ``RTTIRoot``. ``RTTIExtends`` uses the ""`Curiously; Recurring Template Idiom`_"", taking the class being defined as its first; template argument and the parent class as the second argument. Any class that; uses ``RTTIExtends`` must define a ``static char ID`` member, the address of; which will be used to identify the type. This open-hierarchy RTTI support should only be used if your use case requires; it. Otherwise the standard LLVM RTTI system should be preferred. .. _`Curiously Recurring Template Idiom`:; https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern. E.g. .. code-block:: c++. class Shape : public RTTIExtends<Shape, RTTIRoot> {; public:; static char ID;; virtual double computeArea() = 0;; };. class Square : public RTTIExtends<Square, Shape> {; double SideLength;; public:; static char ID;. Square(double S) : SideLength(S) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst:13279,perform,performing,13279,interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,1,['perform'],['performing']
Performance,"ermed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:224486,load,load,224486,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ermed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:254415,load,load,254415,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"ermost loops. These alternatives may have significant performance impact,; both positive and negative. A cost model is therefore employed to identify the; best alternative, including the alternative of avoiding any transformation; altogether. The Vectorization Plan is an explicit model for describing vectorization; candidates. It serves for both optimizing candidates including estimating their; cost reliably, and for performing their final translation into IR. This; facilitates dealing with multiple vectorization candidates. High-level Design; =================. Vectorization Workflow; ----------------------; VPlan-based vectorization involves three major steps, taking a ""scenario-based; approach"" to vectorization planning:. 1. Legal Step: check if a loop can be legally vectorized; encode constraints and; artifacts if so.; 2. Plan Step:. a. Build initial VPlans following the constraints and decisions taken by; Legal Step 1, and compute their cost.; b. Apply optimizations to the VPlans, possibly forking additional VPlans.; Prune sub-optimal VPlans having relatively high cost.; 3. Execute Step: materialize the best VPlan. Note that this is the only step; that modifies the IR. Design Guidelines; -----------------; In what follows, the term ""input IR"" refers to code that is fed into the; vectorizer whereas the term ""output IR"" refers to code that is generated by the; vectorizer. The output IR contains code that has been vectorized or ""widened""; according to a loop Vectorization Factor (VF), and/or loop unroll-and-jammed; according to an Unroll Factor (UF).; The design of VPlan follows several high-level guidelines:. 1. Analysis-like: building and manipulating VPlans must not modify the input IR.; In particular, if the best option is not to vectorize at all, the; vectorization process terminates before reaching Step 3, and compilation; should proceed as if VPlans had not been built. 2. Align Cost & Execute: each VPlan must support both estimating the cost and; generating ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst:1237,optimiz,optimizations,1237,interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,1,['optimiz'],['optimizations']
Performance,"ernel. This notebook is running `llvm-tblgen`. ```tablegen; %reset; // This is some tablegen; class Foo {}; ```. ------------- Classes -----------------; class Foo {; }; ------------- Defs -----------------. Errors printed to stderr are shown. ```tablegen; %reset; This is not tablegen.; ```. <stdin>:1:1: error: Unexpected token at top level; This is not tablegen.; ^. Add some classes to get some output. ```tablegen; %reset; class Stuff {}; def thing : Stuff {}; ```. ------------- Classes -----------------; class Stuff {; }; ------------- Defs -----------------; def thing {	// Stuff; }. By default cells are connected. Meaning that we cache the code and magic directives from the previously run cells. This means that the next cell still sees the `Stuff` class. ```tablegen; def other_thing : Stuff {}; ```. ------------- Classes -----------------; class Stuff {; }; ------------- Defs -----------------; def other_thing {	// Stuff; }; def thing {	// Stuff; }. You can use the magic `%reset` to clear this cache and start fresh. ```tablegen; %reset; def other_thing : Stuff {}; ```. <stdin>:1:19: error: Couldn't find class 'Stuff'; def other_thing : Stuff {}; ^. You can also configure the default reset behaviour using the `%config` magic. ```tablegen; %config cellreset on; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; // The cache is reset here so this is an error.; def AThing: Thing {}; ```. <stdin>:2:13: error: Couldn't find class 'Thing'; def AThing: Thing {}; ^. The default value is `off`, meaning cells are connected. If you want to override the default for one cell only, use the `%reset` or `%noreset` magic. These always override the default. ```tablegen; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; %noreset; // This works because of the noreset above.; def AThing: Thing {}; ```. ------------- Classes --",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md:1029,cache,cache,1029,interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,1,['cache'],['cache']
Performance,"erns matched. .. option:: -omit-comments. Make -gen-dag-isel omit comments. The default is false. .. option:: -gen-dfa-packetizer. Generate DFA Packetizer for VLIW targets. .. option:: -gen-directive-decl. Generate directive related declaration code (header file). .. option:: -gen-directive-gen. Generate directive related implementation code part. .. option:: -gen-directive-impl. Generate directive related implementation code. .. option:: -gen-disassembler. Generate disassembler. .. option:: -gen-emitter. Generate machine code emitter. .. option:: -gen-exegesis. Generate llvm-exegesis tables. .. option:: -gen-fast-isel. Generate a ""fast"" instruction selector. .. option:: -gen-global-isel. Generate GlobalISel selector. .. option:: -gisel-coverage-file=filename. Specify the file from which to retrieve coverage information. .. option:: -instrument-gisel-coverage. Make -gen-global-isel generate coverage instrumentation. .. option:: -optimize-match-table. Make -gen-global-isel generate an optimized version of the match table. .. option:: -warn-on-skipped-patterns. Make -gen-global-isel explain why a pattern was skipped for inclusion. .. option:: -gen-global-isel-combiner. Generate GlobalISel combiner. .. option:: -combiners=list. Make -gen-global-isel-combiner emit the specified combiners. .. option:: -gicombiner-debug-cxxpreds. Add debug comments to all C++ predicates emitted by -gen-global-isel-combiner. .. option:: -gicombiner-stop-after-parse. Make -gen-global-isel-combiner stop processing after parsing rules and dump state. .. option:: -gen-instr-info. Generate instruction descriptions. .. option:: -gen-instr-docs. Generate instruction documentation. .. option:: -gen-intrinsic-enums. Generate intrinsic enums. .. option:: -intrinsic-prefix=prefix. Make -gen-intrinsic-enums generate intrinsics with this target *prefix*. .. option:: -gen-intrinsic-impl. Generate intrinsic information. .. option:: -gen-opt-parser-defs. Generate options definitions. .. option:: -gen-opt-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/tblgen.rst:10683,optimiz,optimized,10683,interpreter/llvm-project/llvm/docs/CommandGuide/tblgen.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/tblgen.rst,1,['optimiz'],['optimized']
Performance,"erpret the; stack map record given only the ID, offset, and the order of the; locations, records, and functions, which LLVM preserves. Note that this is quite different from the goal of debug information,; which is a best-effort attempt to track the location of named; variables at every instruction. An important motivation for this design is to allow a runtime to; commandeer a stack frame when execution reaches an instruction address; associated with a stack map. The runtime must be able to rebuild a; stack frame and resume program execution using the information; provided by the stack map. For example, execution may resume in an; interpreter or a recompiled version of the same function. This usage restricts LLVM optimization. Clearly, LLVM must not move; stores across a stack map. However, loads must also be handled; conservatively. If the load may trigger an exception, hoisting it; above a stack map could be invalid. For example, the runtime may; determine that a load is safe to execute without a type check given; the current state of the type system. If the type system changes while; some activation of the load's function exists on the stack, the load; becomes unsafe. The runtime can prevent subsequent execution of that; load by immediately patching any stack map location that lies between; the current call site and the load (typically, the runtime would; simply patch all stack map locations to invalidate the function). If; the compiler had hoisted the load above the stack map, then the; program could crash before the runtime could take back control. To enforce these semantics, stackmap and patchpoint intrinsics are; considered to potentially read and write all memory. This may limit; optimization more than some clients desire. This limitation may be; avoided by marking the call site as ""readonly"". In the future we may; also allow meta-data to be added to the intrinsic call to express; aliasing, thereby allowing optimizations to hoist certain loads above; stack ma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:18150,load,load,18150,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['load'],['load']
Performance,"erred down the data-invariant expression graph. We can generalize the previous idea and sink the hardening down the expression; graph across as many data-invariant operations as desirable. This can use very; conservative rules for whether something is data-invariant. The primary goal; should be to handle multiple loads with a single hardening instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-extending the value to the full word size; and then shifting right by at least the number of original bits using the BMI2; `shrx` instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate 32 bits of data.; shrxq %rax, %rdi, %rdi # Shift out all 32 bits loaded.; ```. Because on x86 the zero-extend is free, this can efficiently harden the loaded; value. ##### Hardening the address of the load. When hardening the loaded value is inapplicable, most often because the; instruction directly leaks information (like `cmp` or `jmpq`), we switch to; hardening the _address_ of the load instead of the loaded value. This avoids; increasing register pressure by unfolding the load or paying some other high; cost. To understand how this works in practice, we need to examine the exact; semantics of the x86 addressing modes which, in its fully general form, looks; like `(%base,%index,scale)offset`. Here `%base` and `%",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:26919,load,loaded,26919,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"error with title display/update. ## Changes in 4.1; 1. Introduce object inspector - one could browse object members of any class; 2. Let draw sub-items from TCanvas list of primitives like sub-pad or TLatex; 3. Provide possibility to save drawn SVG canvas as PNG; 4. TGraph drawing optimization - limit number of drawn points; 5. Implement painter for TPolyMarker3D; 6. Improve drawing and update of TMultiGraph; 7. Reorganize 3D drawing of TH2/TH3 histograms, allow to mix 2D and 3D display together; 8. Support overlay of 3D graphic over SVG canvas (used for IE); 9. Fix problems and improve flex(ible) layout. ## Changes in 4.0; 1. New TGeo classes support:; - browsing through volumes hierarchy; - changing visibility flags; - drawing of selected volumes; 2. New 'flex' layout:; - create frames like in Multi Document Interface; - one could move/resize/minimize/maximize such frames; 3. Significant (factor 4) I/O performance improvement:; - use ArrayBuffer class in HTTP requests instead of String; - use native arrays (like Int32Array) for array data members; - highly optimize streamer infos handling; 4. TH2 drawing optimization:; - if there are too many non-empty bins, combine them together; - when zoom-in, all original bins will be displayed separately; - let draw big TH2 histogram faster than in 1 sec; - optimization can be disabled by providing '&optimize=0' in URL; 5. TF1 drawing optimization:; - function 'compiled' only once; 6. Reorganize scripts structure:; - move all math functions to JSRootMath.js; - TH2, TF1, THStack and TMultiGraph painters moved into JSRootPainter.more.js script; - reduce size of scripts required for default functionality; 7. Update all basic libraries:; - d3.js - v3.5.9,; - jquery.js - v2.1.4,; - jquery-ui.js - v1.11.4,; - three.js - r73; 8. Implement ROOT6-like color palettes:; - all palettes in range 51...112 are implemented; - by default palette 57 is used; - one could change default palette with '&palette=111' in URL; - or palette can be spec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:59740,perform,performance,59740,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"ers (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`):. The Flat Scratch Init is the 64-bit address of the base of scratch backing; memory being managed by SPI for the queue executing the kernel dispatch. CP obtains this from the runtime. The kernel prolog must add the value of the wave's Scratch Wavefront Offset; and move the result as a 64-bit value to the FLAT_SCRATCH SGPR register pair; which is SGPRn-6 and SGPRn-5. It is used as the FLAT SCRATCH BASE in flat; memory instructions. The Scratch Wavefront Offset must also be used as an offset with Private; segment address when using the Scratch Segment Buffer (see; :ref:`amdgpu-amdhsa-kernel-prolog-private-segment-buffer`). * If the *Target Properties* column of :ref:`amdgpu-processor-table`; specifies *Architected flat scratch*:. If ENABLE_PRIVATE_SEGMENT is enabled in; :ref:`amdgpu-amdhsa-compute_pgm_rsrc2-gfx6-gfx12-table` then the FLAT_SCRATCH; register pair will be initialized to the 64-bit address of the base of scratch; backing memory being managed by SPI for the queue executing the kernel; dispatch plus the value of the wave's Scratch Wavefront Offset for use as the; flat scratch base in flat memory instructions. .. _amdgpu-amdhsa-kernel-prolog-private-segment-buffer:. Private Segment Buffer; ++++++++++++++++++++++. If the *Target Properties* column of :ref:`amdgpu-processor-table` specifies; *Architected flat scratch* then a Private Segment Buffer is not supported.; Instead the flat SCRATCH instructions are used. Otherwise, Private Segment Buffer SGPR register is used to initialize 4 SGPRs; that are used as a V# to access scratch. CP uses the value provided by the; runtime. It is used, together with Scratch Wavefront Offset as an offset, to; access the private memory space using a segment address. See; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`. The scratch V# is a four-aligned SGPR and always selected for the kernel as; follows:. - If it is known during instruction selection that there i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:197862,queue,queue,197862,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"ers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4982,optimiz,optimization,4982,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,2,['optimiz'],"['optimization', 'optimizeModule']"
Performance,"ersion of; Clang/LLVM, can take a long time, so by default the setup script will use all; cores (x2 if hyperthreading is enabled).; Once built, however, the wheel of ``cppyy-cling`` is reused by pip for all; versions of CPython and for PyPy, thus the long compilation is needed only; once for all different versions of Python on the same machine. See the :ref:`section on repos <building_from_source>` for more; details/options. PyPy; ----. PyPy 5.7 and 5.8 have a built-in module ``cppyy``.; You can still install the cppyy package, but the built-in module takes; precedence.; To use cppyy, first import a compatibility module::. $ pypy; [PyPy 5.8.0 with GCC 5.4.0] on linux2; >>>> import cppyy_compat, cppyy; >>>>. You may have to set ``LD_LIBRARY_PATH`` appropriately if you get an; ``EnvironmentError`` (it will indicate the needed directory). Note that your python interpreter (whether CPython or ``pypy-c``) may not have; been linked by the C++ compiler.; This can lead to problems during loading of C++ libraries and program shutdown.; In that case, re-linking is highly recommended. Very old versions of PyPy (5.6.0 and earlier) have a built-in ``cppyy`` based; on `Reflex`_, which is less feature-rich and no longer supported.; However, both the :doc:`distribution utilities <utilities>` and user-facing; Python codes are very backwards compatible, making migration straightforward. Precompiled header; ------------------. For performance reasons (reduced memory and CPU usage), a precompiled header; (PCH) of the system and compiler header files will be installed or, failing; that, generated on startup.; Obviously, this PCH is not portable and should not be part of any wheel. Some compiler features, such as AVX, OpenMP, fast math, etc. need to be; active during compilation of the PCH, as they depend both on compiler flags; and system headers (for intrinsics, or API calls).; You can control compiler flags through the ``EXTRA_CLING_ARGS`` envar and thus; what is active in the PCH.; In",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/installation.rst:7079,load,loading,7079,bindings/pyroot/cppyy/cppyy/doc/source/installation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/installation.rst,1,['load'],['loading']
Performance,"erstanding of the hierarchy, have a look at; <https://root.cern.ch/doc/master/classTGeoManager.html>. Just close now the `X3D` window and focus at the wire frame picture; drawn in a pad. Activate Options/Event Status. Moving the mouse in the; pad, you will notice that objects are sometimes changing color to red.; Volumes are highlighted in this way whenever the mouse pointer is close; enough to one of its vertices. When this happens, the corresponding; volume is selected and you will see in the bottom right size of the ROOT; canvas its name, shape type and corresponding path in the physical tree.; Right clicking on the screen when a volume is selected will also open; its context menu (picking). Note that there are several actions that can; be performed both at view (no volume selected) and volume level. **`TView`** (mouse not selecting any volume):. - Click-and-drag rotates the view.; - Pressing some keys perform different actions:; - J/K - zoom / unzoom; - H, L, U, I - move the viewpoint; - Right click + `SetParallel` `()/SetPerspective` `()` - switch from; parallel to perspective view.; - Right click + `ShowAxis()` - show coordinate axes.; - Right click + `Centered/Left/Side/Top` - change view direction. **`TGeoVolume`** (mouse selecting a volume):. - Double click will focus the corresponding volume.; - Right click + `CheckOverlaps()` - run overlap checker on current; volume.; - Right click + `Draw` `()` - draw that volume according current; global visualization options; - Right click + `DrawOnly()`***` - `***draw only the selected volume.; - Right click + `InspectShape/Material()` - print info about shape or; material.; - Right click + `Raytrace()` - initiate a ray tracing algorithm on; current view.; - Right click + `RandomPoints/Rays()` - shoot random points or rays; inside the bounding box of the clicked volume and display only those; inside visible volumes.; - Right click + `Weight()` - estimates the weight of a volume within a; given precision. Note that the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:11658,perform,perform,11658,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['perform']
Performance,"ertain math function calls (such as ``log``, ``sqrt``, ``pow``, etc); to be replaced with an approximately equivalent set of instructions; or alternative math function calls. For example, a ``pow(x, 0.25)``; may be replaced with ``sqrt(sqrt(x))``, despite being an inexact result; in cases where ``x`` is ``-0.0`` or ``-inf``.; Defaults to ``-fno-approx-func``. .. option:: -f[no-]signed-zeros. Allow optimizations that ignore the sign of floating point zeros.; Defaults to ``-fsigned-zeros``. .. option:: -f[no-]associative-math. Allow floating point operations to be reassociated.; Defaults to ``-fno-associative-math``. .. option:: -f[no-]reciprocal-math. Allow division operations to be transformed into multiplication by a; reciprocal. This can be significantly faster than an ordinary division; but can also have significantly less precision. Defaults to; ``-fno-reciprocal-math``. .. option:: -f[no-]unsafe-math-optimizations. Allow unsafe floating-point optimizations.; ``-funsafe-math-optimizations`` also implies:. * ``-fapprox-func``; * ``-fassociative-math``; * ``-freciprocal-math``; * ``-fno-signed-zeros``; * ``-fno-trapping-math``; * ``-ffp-contract=fast``. ``-fno-unsafe-math-optimizations`` implies:. * ``-fno-approx-func``; * ``-fno-associative-math``; * ``-fno-reciprocal-math``; * ``-fsigned-zeros``; * ``-ftrapping-math``; * ``-ffp-contract=on``; * ``-fdenormal-fp-math=ieee``. There is ambiguity about how ``-ffp-contract``,; ``-funsafe-math-optimizations``, and ``-fno-unsafe-math-optimizations``; behave when combined. Explanation in :option:`-fno-fast-math` also applies; to these options. Defaults to ``-fno-unsafe-math-optimizations``. .. option:: -f[no-]finite-math-only. Allow floating-point optimizations that assume arguments and results are; not NaNs or +-Inf. ``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * ``-fno-honor-infinities``; * ``-fno-honor-nans``. ``-ffno-inite-math-only`` implies:. * `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:59304,optimiz,optimizations,59304,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"ertically in; a column. The next widget we create as a child of the main frame is the horizontal; frame `hframe`:. ``` {.cpp}; TGHorizontalFrame *hframe=new TGHorizontalFrame(fMain,200,40);; ```. The first parameter of its constructor is again the address of its; parent, `fMain`. The next ones define the frame width and height in; pixels. The name of the class **`TGHorizontalFrame`** gives a hint that; a horizontal layout will apply on its children widgets. The Draw and; Exit buttons will be laid out horizontally. Here are their constructors:. ``` {.cpp}; TGTextButton *draw = new TGTextButton(hframe,""&Draw"");; hframe ->AddFrame(draw, new TGLayoutHints(kLHintsCenterX,5,5,3,4));; TGTextButton *exit = new TGTextButton(hframe,""&Exit"",; ""gApplication->Terminate(0)"");; hframe ->AddFrame(exit,new TGLayoutHints(kLHintsCenterX,5,5,3,4));; ```. They are created as objects of the **`TGTextButton`** class that; represent the command buttons with a text label. When you click on a; command button it performs the action shown on its label. These buttons; are well known as ""push buttons"" or just ""buttons"". The parent address; `hframe` is passed as first parameter. The second one defines the button; label and normally indicates the action to be taken when the button is; clicked. It is possible to define a hot key for the button at that point; using the hot string for its label. A hot string is a string with a; ""hot"" character underlined. This character we call the button hot key.; It shows the assigned keyboard mnemonic for the button choice. Following; our example, this means that you can use `Alt+D` to click on Draw button; and `Alt+E` to click on Exit. There is a possibility to specify a; command string as third parameter of the button constructor. We use it; to assign the command `gApplication->Terminate(0)`. The application will; be terminated when you click on the Exit button. We call again `AddFrame()` to add the buttons to their parent widget; giving layout hints for each of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:11057,perform,performs,11057,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['perform'],['performs']
Performance,"erto unseen magic number. .. _primitives:. Primitives; ----------. A bitstream literally consists of a stream of bits, which are read in order; starting with the least significant bit of each byte. The stream is made up of; a number of primitive values that encode a stream of unsigned integer values.; These integers are encoded in two ways: either as `Fixed Width Integers`_ or as; `Variable Width Integers`_. .. _Fixed Width Integers:; .. _fixed-width value:. Fixed Width Integers; ^^^^^^^^^^^^^^^^^^^^. Fixed-width integer values have their low bits emitted directly to the file.; For example, a 3-bit integer value encodes 1 as 001. Fixed width integers are; used when there are a well-known number of options for a field. For example,; boolean values are usually encoded with a 1-bit wide integer. .. _Variable Width Integers:; .. _Variable Width Integer:; .. _variable-width value:. Variable Width Integers; ^^^^^^^^^^^^^^^^^^^^^^^. Variable-width integer (VBR) values encode values of arbitrary size, optimizing; for the case where the values are small. Given a 4-bit VBR field, any 3-bit; value (0 through 7) is encoded directly, with the high bit set to zero. Values; larger than N-1 bits emit their bits in a series of N-1 bit chunks, where all; but the last set the high bit. For example, the value 30 (0x1E) is encoded as 62 (0b0011'1110) when emitted as; a vbr4 value. The first set of four bits starting from the least significant; indicates the value 6 (110) with a continuation piece (indicated by a high bit; of 1). The next set of four bits indicates a value of 24 (011 << 3) with no; continuation. The sum (6+24) yields the value 30. .. _char6-encoded value:. 6-bit characters; ^^^^^^^^^^^^^^^^. 6-bit characters encode common characters into a fixed 6-bit field. They; represent the following characters with the following 6-bit values:. ::. 'a' .. 'z' --- 0 .. 25; 'A' .. 'Z' --- 26 .. 51; '0' .. '9' --- 52 .. 61; '.' --- 62; '_' --- 63. This encoding is only suitable for enco",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst:3453,optimiz,optimizing,3453,interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,1,['optimiz'],['optimizing']
Performance,"erts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:: clearer. When we see a bitcast from type ``X`` to type ``Y``, what we need to do is to change the in-register representation of the data to be *as if* it had just been loaded by a ``LD1`` of type ``Y``. .. image:: ARM-BE-bitcastsuccess.png; :align: right. Conceptually this is simple - we can insert a ``REV`` undoing the ``LD1`` of type ``X`` (converting the in-register representation to the same as if it had been loaded by ``LDR``) and then insert another ``REV`` to change the representation to be as if it had been loaded by an ``LD1`` of type ``Y``. For the previous example, this would be::. LD1 v0.4s, [x]. REV64 v0.4s, v0.4s // There is no REV128 instruction, so it must be synthesizedcd; EXT v0.16b, v0.16b, v0.16b, #8 // with a REV64 then an EXT to swap the two 64-bit elements. REV64 v0.2d, v0.2d; EXT v0.16b, v0.16b, v0.16b, #8. ST1 v0.2d, [y]. It turns out that these ``REV`` pairs can, in almost all cases, be squashed toget",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:10639,load,loaded,10639,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loaded']
Performance,"erved-module-identifier`` to suppress the warning. How to specify the dependent BMIs; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. There are 3 methods to specify the dependent BMIs:. * (1) ``-fprebuilt-module-path=<path/to/directory>``.; * (2) ``-fmodule-file=<path/to/BMI>`` (Deprecated).; * (3) ``-fmodule-file=<module-name>=<path/to/BMI>``. The option ``-fprebuilt-module-path`` tells the compiler the path where to search for dependent BMIs.; It may be used multiple times just like ``-I`` for specifying paths for header files. The look up rule here is:. * (1) When we import module M. The compiler would look up M.pcm in the directories specified; by ``-fprebuilt-module-path``.; * (2) When we import partition module unit M:P. The compiler would look up M-P.pcm in the; directories specified by ``-fprebuilt-module-path``. The option ``-fmodule-file=<path/to/BMI>`` tells the compiler to load the specified BMI directly.; The option ``-fmodule-file=<module-name>=<path/to/BMI>`` tells the compiler to load the specified BMI; for the module specified by ``<module-name>`` when necessary. The main difference is that; ``-fmodule-file=<path/to/BMI>`` will load the BMI eagerly, whereas; ``-fmodule-file=<module-name>=<path/to/BMI>`` will only load the BMI lazily, which is similar; with ``-fprebuilt-module-path``. The option ``-fmodule-file=<path/to/BMI>`` for named modules is deprecated; and is planning to be removed in future versions. In case all ``-fprebuilt-module-path=<path/to/directory>``, ``-fmodule-file=<path/to/BMI>`` and; ``-fmodule-file=<module-name>=<path/to/BMI>`` exist, the ``-fmodule-file=<path/to/BMI>`` option; takes highest precedence and ``-fmodule-file=<module-name>=<path/to/BMI>`` will take the second; highest precedence. We need to specify all the dependent (directly and indirectly) BMIs.; See https://github.com/llvm/llvm-project/issues/62707 for detail. When we compile a ``module implementation unit``, we must specify the BMI of the corresponding; ``primary module inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:11636,load,load,11636,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['load'],['load']
Performance,"erver could observe this sequence of events; in a way which precludes the operation being performed before the; safepoint. To understand why this 'observable-after' property is required,; consider a null comparison performed on the original copy of a; relocated pointer. Assuming that control flow follows the safepoint,; there is no way to observe externally whether the null comparison is; performed before or after the safepoint. (Remember, the original; Value is unmodified by the safepoint.) The compiler is free to make; either scheduling choice. The actual correctness property implemented is slightly stronger than; this. We require that there be no *static path* on which a; potentially relocated pointer is 'observably-after' it may have been; relocated. This is slightly stronger than is strictly necessary (and; thus may disallow some otherwise valid programs), but greatly; simplifies reasoning about correctness of the compiled code. By construction, this property will be upheld by the optimizer if; correctly established in the source IR. This is a key invariant of; the design. The existing IR Verifier pass has been extended to check most of the; local restrictions on the intrinsics mentioned in their respective; documentation. The current implementation in LLVM does not check the; key relocation invariant, but this is ongoing work on developing such; a verifier. Please ask on llvm-dev if you're interested in; experimenting with the current version. .. _statepoint-utilities:. Utility Passes for Safepoint Insertion; ======================================. .. _RewriteStatepointsForGC:. RewriteStatepointsForGC; ^^^^^^^^^^^^^^^^^^^^^^^^. The pass RewriteStatepointsForGC transforms a function's IR to lower from the; abstract machine model described above to the explicit statepoint model of; relocations. To do this, it replaces all calls or invokes of functions which; might contain a safepoint poll with a ``gc.statepoint`` and associated full; relocation sequence, includin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:24692,optimiz,optimizer,24692,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['optimiz'],['optimizer']
Performance,"erving the same purpose as the Linkdef.h file above (in fact, ``rootcling``; accepts a ""selection.xml"" file in lieu of a ""Linkdef.h"").; For more tags, see the `selection file`_ documentation.; Commonly used are ``namespace``, ``function``, ``enum``, or ``variable``; instead of the ``class`` tag, and ``pattern`` instead of ``name`` with; wildcarding in the value string. Next, use ``genreflex`` to generate the dictionary (here:; ``MyClass_rflx.cxx``) and module files::. $ genreflex MyClass.h --selection=myclass_selection.xml -o MyClass_rflx.cxx. From here, compile and link the generated dictionary file with the project; and/or system specific options and libraries into a shared library, using; ``cling-config`` for the relevant cppyy compiler/linker flags.; (For work on MS Windows, this `helper script`_ may be useful.); To continue the example, assuming Linux::. $ g++ `cling-config --cppflags` -fPIC -O2 -shared MyClass_rflx.cxx -o MyClassDict.so. Instead of loading the header text into ``cling``, you can now load the; dictionary:. .. code-block:: python. >>> import cppyy; >>> cppyy.load_reflection_info('MyClassDict'); >>> cppyy.gbl.MyClass(42); <cppyy.gbl.MyClass object at 0x7ffb9f230950>; >>> print(_.get_int()); 42; >>>. and use the selected C++ entities as if the header was loaded. The dictionary shared library can be relocated, as long as it can be found; by the dynamic loader (e.g. through ``LD_LIBRARY_PATH``) and the header file; is fully embedded or still accessible (e.g. through a path added to; ``cppyy.add_include_path`` at run-time, or with ``-I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Cla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:7125,load,loading,7125,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,2,['load'],"['load', 'loading']"
Performance,"ery unlikely) worst; case the instruction sequence could be completely reversed. In such; circumstances LLVM follows the principle applied to optimizations, that it is; better for the debugger not to display any state than a misleading state.; Thus, whenever instructions are advanced in order of execution, any; corresponding DBG_VALUE is kept in its original position, and if an instruction; is delayed then the variable is given an undefined location for the duration; of the delay. To illustrate, consider this pseudo-MIR:. .. code-block:: text. %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); DBG_VALUE %1, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DBG_VALUE %4, $noreg, !3, !4; %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; DBG_VALUE %7, $noreg, !5, !6. Imagine that the SUB32rr were moved forward to give us the following MIR:. .. code-block:: text. %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, debug-location !5 :: (load 4 from %ir.addr1); DBG_VALUE %1, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DBG_VALUE %4, $noreg, !3, !4; DBG_VALUE %7, $noreg, !5, !6. In this circumstance LLVM would leave the MIR as shown above. Were we to move; the DBG_VALUE of virtual register %7 upwards with the SUB32rr, we would re-order; assignments and introduce a new state of the program. Whereas with the solution; above, the debugger will see one fewer combination of variable values, because; ``!3`` and ``!5`` will change value at the same time. This is preferred over; misrepresenting the original program. In comparison, if one sunk the MOV32rm, LLVM would produce the following:. .. code-block:: text. DBG_VALUE $noreg, $noreg, !1, !2; %4:gr32 = ADD32rr %3, %2, implicit-def dead $eflags; DBG_VALUE %4, $noreg, !3, !4; %7:gr32 = SUB32rr %6, %5, implicit-def dead $eflags; DBG_VALUE %7, $noreg, !5, !6; %1:gr32 = MOV32rm %0, 1, $noreg, 4, $noreg, de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:33954,load,load,33954,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,"erything relevant today; except x86 and SPARC) require some sort of fence to maintain the Acquire; semantics. The precise fences required varies widely by architecture, but for; a simple implementation, most architectures provide a barrier which is strong; enough for everything (``dmb`` on ARM, ``sync`` on PowerPC, etc.). Putting; such a fence after the equivalent Monotonic operation is sufficient to; maintain Acquire semantics for a memory operation. Release; -------. Release is similar to Acquire, but with a barrier of the sort necessary to; release a lock. Relevant standard; This corresponds to the C++/C ``memory_order_release``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Release only provides a semantic guarantee when paired with an Acquire; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move loads from after a Release store or read-modify-write; operation to before it, and move non-Release stores from after a Release; operation to before it. Notes for code generation; See the section on Acquire; a fence before the relevant operation is usually; sufficient for Release. Note that a store-store fence is not sufficient to; implement Release semantics; store-store fences are generally not exposed to; IR because they are extremely difficult to use correctly. AcquireRelease; --------------. AcquireRelease (``acq_rel`` in IR) provides both an Acquire and a Release; barrier (for fences and operations which both read and write memory). Relevant standard; This corresponds to the C++/C ``memory_order_acq_rel``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Note",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:13198,load,loads,13198,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"es File Size = 400 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 2.23 *; *............................................................................*; *Br 19 :fIsValid : Bool_t *; *Entries : 20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a friend.; (Previously it was re-printing the previous value). This required changes in ; When the draw option to TTree::Draw contains ""norm"" the output histogram is normalized to 1.; Improve the selection of the leaf used for size of an array in a leaflist by giving preference; for the leaf inside the same branch and by adding support for explicit full path name. For example the following now works properl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:6193,cache,cache,6193,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,2,['cache'],['cache']
Performance,"es a list of; passes, ensures their :ref:`prerequisites <writing-an-llvm-pass-interaction>`; are set up correctly, and then schedules passes to run efficiently. All of the; LLVM tools that run passes use the PassManager for execution of these passes. The PassManager does two main things to try to reduce the execution time of a; series of passes:. #. **Share analysis results.** The ``PassManager`` attempts to avoid; recomputing analysis results as much as possible. This means keeping track; of which analyses are available already, which analyses get invalidated, and; which analyses are needed to be run for a pass. An important part of work; is that the ``PassManager`` tracks the exact lifetime of all analysis; results, allowing it to :ref:`free memory; <writing-an-llvm-pass-releaseMemory>` allocated to holding analysis results; as soon as they are no longer needed. #. **Pipeline the execution of passes on the program.** The ``PassManager``; attempts to get better cache and memory usage behavior out of a series of; passes by pipelining the passes together. This means that, given a series; of consecutive :ref:`FunctionPass <writing-an-llvm-pass-FunctionPass>`, it; will execute all of the :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` on the first function, then all of the; :ref:`FunctionPasses <writing-an-llvm-pass-FunctionPass>` on the second; function, etc... until the entire program has been run through the passes. This improves the cache behavior of the compiler, because it is only; touching the LLVM program representation for a single function at a time,; instead of traversing the entire program. It reduces the memory consumption; of compiler, because, for example, only one `DominatorSet; <https://llvm.org/doxygen/classllvm_1_1DominatorSet.html>`_ needs to be; calculated at a time. The effectiveness of the ``PassManager`` is influenced directly by how much; information it has about the behaviors of the passes it is scheduling. For; example, the ""preserve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:41823,cache,cache,41823,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['cache'],['cache']
Performance,"es an iteration, and the second index is the; instruction index (i.e., where it appears in the code sequence). Since this; example was generated using 3 iterations: ``-iterations=3``, the iteration; indices range from 0-2 inclusively. Excluding the first and last column, the remaining columns are in cycles.; Cycles are numbered sequentially starting from 0. From the example output above, we know the following:. * Instruction [1,0] was dispatched at cycle 1.; * Instruction [1,0] started executing at cycle 2.; * Instruction [1,0] reached the write back stage at cycle 4.; * Instruction [1,0] was retired at cycle 10. Instruction [1,0] (i.e., vmulps from iteration #1) does not have to wait in the; scheduler's queue for the operands to become available. By the time vmulps is; dispatched, operands are already available, and pipeline JFPU1 is ready to; serve another instruction. So the instruction can be immediately issued on the; JFPU1 pipeline. That is demonstrated by the fact that the instruction only; spent 1cy in the scheduler's queue. There is a gap of 5 cycles between the write-back stage and the retire event.; That is because instructions must retire in program order, so [1,0] has to wait; for [0,2] to be retired first (i.e., it has to wait until cycle 10). In the example, all instructions are in a RAW (Read After Write) dependency; chain. Register %xmm2 written by vmulps is immediately used by the first; vhaddps, and register %xmm3 written by the first vhaddps is used by the second; vhaddps. Long data dependencies negatively impact the ILP (Instruction Level; Parallelism). In the dot-product example, there are anti-dependencies introduced by; instructions from different iterations. However, those dependencies can be; removed at register renaming stage (at the cost of allocating register aliases,; and therefore consuming physical registers). Table *Average Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:24608,queue,queue,24608,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"es and binding requirements, before committing any time to; :ref:`trying it out <starting>`. Run-time v.s. compile-time; --------------------------. What performs better, run-time or compile-time?; The obvious answer is compile-time: see the performance differences between; C++ and Python, for example.; Obvious, but completely wrong, however.; In fact, when it comes to Python, it is even the `wrong question.`. Everything in Python is run-time: modules, classes, functions, etc. are all; run-time constructs.; A Python module that defines a class is a set of instructions to the Python; interpreter that lead to the construction of the desired class object.; A C/C++ extension module that defines a class does the same thing by calling; a succession of Python interpreter Application Programming Interfaces (APIs;; the exact same that Python uses itself internally).; If you use a compile-time binder such as `SWIG`_ or `pybind11`_ to bind a C++; class, then what gets compiled is the series of API calls necessary to; construct a Python-side equivalent at `run-time` (when the module gets; loaded), not the Python class object.; In short, whether a binding is created at ""compile-time"" or at run-time has; no measurable bearing on performance. What does affect performance is the overhead to cross the language barrier.; This consists of unboxing Python objects to extract or convert the underlying; objects or data to something that matches what C++ expects; overload; resolution based on the unboxed arguments; offset calculations; and finally; the actual dispatch.; As a practical matter, overload resolution is the most costly part, followed; by the unboxing and conversion.; Best performance is achieved by specialization of the paths through the; run-time: recognize early the case at hand and select an optimized path.; For that reason, `PyPy`_ is so fast: JIT-ed traces operate on unboxed objects; and resolved overloads are baked into the trace, incurring no further cost.; Similarly, th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:1583,load,loaded,1583,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,1,['load'],['loaded']
Performance,"es are ""tiny"", ""small"", ""kernel"", ""medium"", ""large"".; This may be extended in the future to specify global data layout that; doesn't cleanly fit into a specific code model. By default, global initializers are optimized by assuming that global; variables defined within the module are not modified from their; initial values before the start of the global initializer. This is; true even for variables potentially accessible from outside the; module, including those with external linkage or appearing in; ``@llvm.used`` or dllexported variables. This assumption may be suppressed; by marking the variable with ``externally_initialized``. An explicit alignment may be specified for a global, which must be a; power of 2. If not present, or if the alignment is set to zero, the; alignment of the global is set by the target to whatever it feels; convenient. If an explicit alignment is specified, the global is forced; to have exactly that alignment. Targets and optimizers are not allowed; to over-align the global if the global has an assigned section. In this; case, the extra alignment could be observable: for example, code could; assume that the globals are densely packed in their section and try to; iterate over them as an array, alignment padding would break this; iteration. For TLS variables, the module flag ``MaxTLSAlign``, if present,; limits the alignment to the given value. Optimizers are not allowed to; impose a stronger alignment on these variables. The maximum alignment; is ``1 << 32``. For global variable declarations, as well as definitions that may be; replaced at link time (``linkonce``, ``weak``, ``extern_weak`` and ``common``; linkage types), the allocation size and alignment of the definition it resolves; to must be greater than or equal to that of the declaration or replaceable; definition, otherwise the behavior is undefined. Globals can also have a :ref:`DLL storage class <dllstorageclass>`,; an optional :ref:`runtime preemption specifier <runtime_preemption_m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:34707,optimiz,optimizers,34707,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"es are fast relative to the whole compilation process.; More importantly, the imported code only needs to be processed once in frontend code generation,; as well as the whole middle end and backend.; So we could get a big win for the compilation time in O0. But with optimizations, things are different:. (we omit ``code generation`` part for each end due to the limited space). .. code-block:: none. -------- frontend ------------------------ middle end -------------------------- backend ----;    ; --- parsing ---- sema -------- optimizations --- IPO ---- optimizations------ optimizations -. -----------------------------------------------------------------------------------------------;  ;  source file ;  ; -----------------------------------------------------------------------------------------------; ---------------------------------------;  ;  ;  imported code ;  ;  ; ---------------------------------------. It would be very unfortunate if we end up with worse performance after using modules.; The main concern is that when we compile a source file, the compiler needs to see the function body; of imported module units so that it can perform IPO (InterProcedural Optimization, primarily inlining; in practice) to optimize functions in current source file with the help of the information provided by; the imported module units.; In other words, the imported code would be processed again and again in importee units; by optimizations (including IPO itself).; The optimizations before IPO and the IPO itself are the most time-consuming part in whole compilation process.; So from this perspective, we might not be able to get the improvements described in the theory.; But we could still save the time for optimizations after IPO and the whole backend. Overall, at ``O0`` the implementations of functions defined in a module will not impact module users,; but at higher optimization levels the definitions of such functions are provided to user comp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:42140,perform,performance,42140,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['perform'],['performance']
Performance,"es for updating ``DIAssignID`` Attachments; =============================================. ``DIAssignID`` metadata attachments are used by Assignment Tracking, which is; currently an experimental debug mode. See :doc:`AssignmentTracking` for how to update them and for more info on; Assignment Tracking. How to automatically convert tests into debug info tests; ========================================================. .. _IRDebugify:. Mutation testing for IR-level transformations; ---------------------------------------------. An IR test case for a transformation can, in many cases, be automatically; mutated to test debug info handling within that transformation. This is a; simple way to test for proper debug info handling. The ``debugify`` utility pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``debugify`` testing utility is just a pair of passes: ``debugify`` and; ``check-debugify``. The first applies synthetic debug information to every instruction of the; module, and the second checks that this DI is still available after an; optimization has occurred, reporting any errors/warnings while doing so. The instructions are assigned sequentially increasing line locations, and are; immediately used by debug value intrinsics everywhere possible. For example, here is a module before:. .. code-block:: llvm. define void @f(i32* %x) {; entry:; %x.addr = alloca i32*, align 8; store i32* %x, i32** %x.addr, align 8; %0 = load i32*, i32** %x.addr, align 8; store i32 10, i32* %0, align 4; ret void; }. and after running ``opt -debugify``:. .. code-block:: llvm. define void @f(i32* %x) !dbg !6 {; entry:; %x.addr = alloca i32*, align 8, !dbg !12; call void @llvm.dbg.value(metadata i32** %x.addr, metadata !9, metadata !DIExpression()), !dbg !12; store i32* %x, i32** %x.addr, align 8, !dbg !13; %0 = load i32*, i32** %x.addr, align 8, !dbg !14; call void @llvm.dbg.value(metadata i32* %0, metadata !11, metadata !DIExpression()), !dbg !14; store i32 10, i32* %0, align 4, !dbg !15; ret void, !db",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:9887,optimiz,optimization,9887,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['optimiz'],['optimization']
Performance,"es from the file ``<filename>`` and mark defined non-common; symbols with those names as local in the output. In the file, each line; represents a single symbol, with leading and trailing whitespace ignored, as is; anything following a '#'. Can be specified multiple times to read names from; multiple files. .. option:: --new-symbol-visibility <visibility>. Specify the visibility of the symbols automatically created when using binary; input or :option:`--add-symbol`. Valid options are:. - `default`; - `hidden`; - `internal`; - `protected`. The default is `default`. .. option:: --output-target <format>, -O. Write the output as the specified format. See `SUPPORTED FORMATS`_ for a list; of valid ``<format>`` values. If unspecified, the output format is assumed to; be the same as the value specified for :option:`--input-target` or the input; file's format if that option is also unspecified. .. option:: --pad-to <address>. For binary outputs, pad the output to the load address ``<address>`` using a value; of zero or the value specified by :option:`--gap-fill`. .. option:: --prefix-alloc-sections <prefix>. Add ``<prefix>`` to the front of the names of all allocatable sections in the; output. .. option:: --prefix-symbols <prefix>. Add ``<prefix>`` to the front of every symbol name in the output. .. option:: --preserve-dates, -p. Preserve access and modification timestamps in the output. .. option:: --rename-section <old>=<new>[,<flag>,...]. Rename sections called ``<old>`` to ``<new>`` in the output, and apply any; specified ``<flag>`` values. See :option:`--set-section-flags` for a list of; supported flags. Can be specified multiple times to rename multiple sections. .. option:: --set-section-type <section>=<type>. Set the type of section ``<section>`` to the integer ``<type>``. Can be; specified multiple times to update multiple sections. .. option:: --set-start-addr <addr>. Set the start address of the output to ``<addr>``. Overrides any previously; specified :option:`--",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst:15628,load,load,15628,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,1,['load'],['load']
Performance,"es stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original sta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4074,cache,cache,4074,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['cache'],['cache']
Performance,"es that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - system *none* 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:316185,load,load,316185,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"es that calls to this function should never be merged; during optimization. For example, it will prevent tail merging otherwise; identical code sequences that raise an exception or terminate the program.; Tail merging normally reduces the precision of source location information,; making stack traces less useful for debugging. This attribute gives the; user control over the tradeoff between code size and debug information; precision.; ``nonlazybind``; This attribute suppresses lazy symbol binding for the function. This; may make calls to the function faster, at the cost of extra program; startup time if the function is not called during program startup.; ``noprofile``; This function attribute prevents instrumentation based profiling, used for; coverage or profile based optimization, from being added to a function. It; also blocks inlining if the caller and callee have different values of this; attribute.; ``skipprofile``; This function attribute prevents instrumentation based profiling, used for; coverage or profile based optimization, from being added to a function. This; attribute does not restrict inlining, so instrumented instruction could end; up in this function.; ``noredzone``; This attribute indicates that the code generator should not use a; red zone, even if the target-specific ABI normally permits it.; ``indirect-tls-seg-refs``; This attribute indicates that the code generator should not use; direct TLS access through segment registers, even if the; target-specific ABI normally permits it.; ``noreturn``; This function attribute indicates that the function never returns; normally, hence through a return instruction. This produces undefined; behavior at runtime if the function ever does dynamically return. Annotated; functions may still raise an exception, i.a., ``nounwind`` is not implied.; ``norecurse``; This function attribute indicates that the function does not call itself; either directly or indirectly down any possible call path. This produces; undef",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:91965,optimiz,optimization,91965,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"es the operations that push location descriptions on the; stack. .. _amdgpu-dwarf-general-location-description-operations:. A.2.5.4.4.1 General Location Description Operations; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. note::. This section replaces part of DWARF Version 5 section 2.5.1.3. 1. ``DW_OP_LLVM_offset`` *New*. ``DW_OP_LLVM_offset`` pops two stack entries. The first must be an integral; type value that represents a byte displacement B. The second must be a; location description L. It adds the value of B scaled by 8 (the byte size) to the bit offset of each; single location description SL of L, and pushes the updated L. It is an evaluation error if the updated bit offset of any SL is less than 0; or greater than or equal to the size of the location storage specified by; SL. 2. ``DW_OP_LLVM_offset_uconst`` *New*. ``DW_OP_LLVM_offset_uconst`` has a single unsigned LEB128 integer operand; that represents a byte displacement B. The operation is equivalent to performing ``DW_OP_constu B;; DW_OP_LLVM_offset``. *This operation is supplied specifically to be able to encode more field; displacements in two bytes than can be done with* ``DW_OP_lit*;; DW_OP_LLVM_offset``\ *.*. .. note::. Should this be named ``DW_OP_LLVM_offset_uconst`` to match; ``DW_OP_plus_uconst``, or ``DW_OP_LLVM_offset_constu`` to match; ``DW_OP_constu``?. 3. ``DW_OP_LLVM_bit_offset`` *New*. ``DW_OP_LLVM_bit_offset`` pops two stack entries. The first must be an; integral type value that represents a bit displacement B. The second must be; a location description L. It adds the value of B to the bit offset of each single location description; SL of L, and pushes the updated L. It is an evaluation error if the updated bit offset of any SL is less than 0; or greater than or equal to the size of the location storage specified by; SL. 4. ``DW_OP_push_object_address``. ``DW_OP_push_object_address`` pushes the location description L of the; current object. *This object may correspond to an ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:98991,perform,performing,98991,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"es,; **bugpoint** may crash. **--enable-valgrind**. Use valgrind to find faults in the optimization phase. This will allow; bugpoint to find otherwise asymptomatic problems caused by memory; mis-management. **-find-bugs**. Continually randomize the specified passes and run them on the test program; until a bug is found or the user kills **bugpoint**. **-help**. Print a summary of command line options. **--input** *filename*. Open *filename* and redirect the standard input of the test program, whenever; it runs, to come from that file. **--load** *plugin*. Load the dynamic object *plugin* into **bugpoint** itself. This object should; register new optimization passes. Once loaded, the object will add new command; line options to enable various optimizations. To see the new complete list of; optimizations, use the **-help** and **--load** options together; for example:. .. code-block:: bash. bugpoint --load myNewPass.so -help. **--mlimit** *megabytes*. Specifies an upper limit on memory usage of the optimization and codegen. Set; to zero to disable the limit. **--output** *filename*. Whenever the test program produces output on its standard output stream, it; should match the contents of *filename* (the ""reference output""). If you; do not use this option, **bugpoint** will attempt to generate a reference output; by compiling the program with the ""safe"" backend and running it. **--run-{int,jit,llc,custom}**. Whenever the test program is compiled, **bugpoint** should generate code for it; using the specified code generator. These options allow you to choose the; interpreter, the JIT compiler, the static native code compiler, or a; custom command (see **--exec-command**) respectively. **--safe-{llc,custom}**. When debugging a code generator, **bugpoint** should use the specified code; generator as the ""safe"" code generator. This is a known-good code generator; used to generate the ""reference output"" if it has not been provided, and to; compile portions of the program that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst:3763,optimiz,optimization,3763,interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/bugpoint.rst,1,['optimiz'],['optimization']
Performance,"es. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space data; may change between kernel dispatch executions. See; :ref:`amdgpu-amdhsa-memory-spaces`. The one exception is if scalar writes are used to spill SGPR registers. In this; case the AMDGPU backend ensures the memory location used to spill is never; accessed by vector memory operations at the same time. If scalar writes are used; then a ``s_dcache_wb`` is inserted before the ``s_endpgm`` and before a function; return since the locations may be used for vector memory instructions by a; future wavefront that uses the same scratch area, or a function call that; creates a frame at the same address, respectively. There is no need for a; ``s_dcache_inv`` as all scalar writes are write-before-read in the same thread. For kernarg backing memory:. * CP invalidates the L1 cache at the start of each kernel dispatch.; * On dGPU the kernarg backing memory is allocated in host memory accessed as; MTYPE UC (uncached) to avoid needing to invalidate the L2 cache. This also; causes it to be treated as non-volatile and so is not invalidated by; ``*_vol``.; * On APU the kernarg backing memory it is accessed as MTYPE CC (cache coherent); and so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX6-GFX9 are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table`. .. table:: AMDHSA Memory Model Code Sequences GFX6-GFX9; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table. =========",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:210710,cache,cache,210710,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"es.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic. - Must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:229968,load,load,229968,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"es.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:277626,load,load,277626,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"es=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12661,load,loads,12661,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['loads']
Performance,"es=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===-----------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37959,load,load,37959,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"es@zafena.se; D: Cmake dependency chain and various bug fixes. N: Alex Rosenberg; E: alexr@leftfield.org; I: arosenberg; D: ARM calling conventions rewrite, hard float support. N: Chad Rosier; E: mcrosier@codeaurora.org; I: mcrosier; D: AArch64 fast instruction selection pass; D: Fixes and improvements to the ARM fast-isel pass; D: Fixes and improvements to the AArch64 backend. N: Nadav Rotem; E: nadav.rotem@me.com; D: X86 code generation improvements, Loop Vectorizer, SLP Vectorizer. N: Roman Samoilov; E: roman@codedgers.com; D: MSIL backend. N: Duncan Sands; E: baldrick@free.fr; I: baldrick; D: Ada support in llvm-gcc; D: Dragonegg plugin; D: Exception handling improvements; D: Type legalizer rewrite. N: Ruchira Sasanka; E: sasanka@uiuc.edu; D: Graph coloring register allocator for the Sparc64 backend. N: Alina Sbirlea; E: alina.sbirlea@gmail.com; D: MemorySSA, BatchAA, misc loop and new pass manager work. N: Arnold Schwaighofer; E: arnold.schwaighofer@gmail.com; D: Tail call optimization for the x86 backend. N: Shantonu Sen; E: ssen@apple.com; D: Miscellaneous bug fixes. N: Anand Shukla; E: ashukla@cs.uiuc.edu; D: The `paths' pass. N: Michael J. Spencer; E: bigcheesegs@gmail.com; D: Shepherding Windows COFF support into MC.; D: Lots of Windows stuff. N: Reid Spencer; E: rspencer@reidspencer.com; W: http://reidspencer.com/; D: Lots of stuff, see: http://wiki.llvm.org/index.php/User:Reid. N: Abhina Sreeskantharajan; E: Abhina.Sreeskantharajan@ibm.com; D: z/OS support. N: Alp Toker; E: alp@nuanti.com; W: http://atoker.com/; D: C++ frontend next generation standards implementation. N: Craig Topper; E: craig.topper@gmail.com; D: X86 codegen and disassembler improvements. AVX2 support. N: Edwin Torok; E: edwintorok@gmail.com; D: Miscellaneous bug fixes. N: Adam Treat; E: manyoso@yahoo.com; D: C++ bugs filed, and C++ front-end bug fixes. N: Andrew Trick; E: atrick@apple.com; D: Instruction Scheduling, ... N: Lauro Ramos Venancio; E: lauro.venancio@indt.org.br; D: ARM bac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT:11750,optimiz,optimization,11750,interpreter/llvm-project/llvm/CREDITS.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CREDITS.TXT,1,['optimiz'],['optimization']
Performance,"escribed in the :doc:`AdvancedBuilds` document. General Distribution Guidance; =============================. When building a distribution of a compiler it is generally advised to perform a; bootstrap build of the compiler. That means building a ""stage 1"" compiler with; your host toolchain, then building the ""stage 2"" compiler using the ""stage 1""; compiler. This is done so that the compiler you distribute benefits from all the; bug fixes, performance optimizations and general improvements provided by the; new compiler. In deciding how to build your distribution there are a few trade-offs that you; will need to evaluate. The big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving symbols; at process launch time, which can be very slow for C++ code. .. _shared_libs:. .. warning::; One very important note: Distributions should never be built using the; *BUILD_SHARED_LIBS* CMake option. That option exists for optimizing developer; workflow only. Due to design and implementation decisions, LLVM relies on; global data which can end up being duplicated across shared libraries; resulting in bugs. As such this is not a safe way to distribute LLVM or; LLVM-based tools. The simplest example of building a distribution with reasonable performance is; captured in the DistributionExample C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:1677,optimiz,optimize,1677,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['optimiz'],['optimize']
Performance,"esponding to the object being called (i.e., the ""receiver"" in Objective-C; parlance), which ExprEngine uses to decide whether or not the call should be; inlined. Inlining Dynamic Calls; ^^^^^^^^^^^^^^^^^^^^^^. The -analyzer-config ipa option has five different modes: none, basic-inlining,; inlining, dynamic, and dynamic-bifurcate. Under -analyzer-config ipa=dynamic,; all dynamic calls are inlined, whether we are certain or not that this will; actually be the definition used at runtime. Under -analyzer-config ipa=inlining,; only ""near-perfect"" devirtualized calls are inlined*, and other dynamic calls; are evaluated conservatively (as if no definition were available). * Currently, no Objective-C messages are not inlined under; -analyzer-config ipa=inlining, even if we are reasonably confident of the type; of the receiver. We plan to enable this once we have tested our heuristics; more thoroughly. The last option, -analyzer-config ipa=dynamic-bifurcate, behaves similarly to; ""dynamic"", but performs a conservative invalidation in the general virtual case; in *addition* to inlining. The details of this are discussed below. As stated above, -analyzer-config ipa=basic-inlining does not inline any C++; member functions or Objective-C method calls, even if they are non-virtual or; can be safely devirtualized. Bifurcation; ^^^^^^^^^^^. ExprEngine::BifurcateCall implements the ``-analyzer-config ipa=dynamic-bifurcate``; mode. When a call is made on an object with imprecise dynamic type information; (RuntimeDefinition::mayHaveOtherDefinitions() evaluates to TRUE), ExprEngine; bifurcates the path and marks the object's region (retrieved from the; RuntimeDefinition object) with a path-sensitive ""mode"" in the ProgramState. Currently, there are 2 modes:. * ``DynamicDispatchModeInlined`` - Models the case where the dynamic type information; of the receiver (MemoryRegion) is assumed to be perfectly constrained so; that a given definition of a method is expected to be the code actually",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/IPA.rst:12587,perform,performs,12587,interpreter/llvm-project/clang/docs/analyzer/developer-docs/IPA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/IPA.rst,1,['perform'],['performs']
Performance,"esponds to a developers intuitions, allowing them to make changes in; their code, and to see the result of these changes without interrupting the; running program. Interactive programming gives programmers the freedom to; explore different scenarios while developing software, writing one expression; at a time, figuring out what to do next at each step, and enabling them to; quickly identify and fix bugs whenever they arise. As an example, the; High-Energy Physics community includes professionals with a variety of; backgrounds, including physicists, nuclear engineers, and software; engineers. Cling allows for interactive data analysis in `ROOT; <https://root.cern/>`_ by giving researchers a way to prototype their C++ code,; allowing them to tailor it to the particular scope of the analysis they want to; pursue on a particular set of data before being added to the main framework. **Interpreted language** is a way to achieve interactive programming. In; statically compiled language, all source code is converted into native machine; code and then executed by the processor before being run. An interpreted; language instead runs through source programs line by line, taking an; executable segment of source code, turning it into machine code, and then; executing it. With this approach, when a change is made by the programmer, the; interpreter will convey it without the need for the entire source code to be; manually compiled. Interpreted languages are flexible, and offer features like; dynamic typing and smaller program size. **Cling** is not an interpreter, it is a Just-In-Time (JIT) compiler that feels; like an interpreter, and allows C++, a language designed to be compiled, to be; interpreted. When using Cling, the programmer benefits from both the power of; C++ language, such as high-performance, robustness, fastness, efficiency,; versatility, and the capability of an interpreter, which allows for interactive; exploration and on-the-fly inspection of the source-code.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/interactivity.rst:2047,perform,performance,2047,interpreter/cling/docs/chapters/interactivity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/interactivity.rst,1,['perform'],['performance']
Performance,"ess of an aggregate argument that is being passed by; value through memory. Primarily, this feature is required for; compatibility with the Microsoft C++ ABI. Under that ABI, class; instances that are passed by value are constructed directly into; argument stack memory. Prior to the addition of inalloca, calls in LLVM; were indivisible instructions. There was no way to perform intermediate; work, such as object construction, between the first stack adjustment; and the final control transfer. With inalloca, all arguments passed in; memory are modelled as a single alloca, which can be stored to prior to; the call. Unfortunately, this complicated feature comes with a large; set of restrictions designed to bound the lifetime of the argument; memory around the call. For now, it is recommended that frontends and optimizers avoid producing; this construct, primarily because it forces the use of a base pointer.; This feature may grow in the future to allow general mid-level; optimization, but for now, it should be regarded as less efficient than; passing by value with a copy. Intended Usage; ==============. The example below is the intended LLVM IR lowering for some C++ code; that passes two default-constructed ``Foo`` objects to ``g`` in the; 32-bit Microsoft C++ ABI. .. code-block:: c++. // Foo is non-trivial.; struct Foo { int a, b; Foo(); ~Foo(); Foo(const Foo &); };; void g(Foo a, Foo b);; void f() {; g(Foo(), Foo());; }. .. code-block:: text. %struct.Foo = type { i32, i32 }; declare void @Foo_ctor(%struct.Foo* %this); declare void @Foo_dtor(%struct.Foo* %this); declare void @g(<{ %struct.Foo, %struct.Foo }>* inalloca %memargs). define void @f() {; entry:; %base = call i8* @llvm.stacksave(); %memargs = alloca <{ %struct.Foo, %struct.Foo }>; %b = getelementptr <{ %struct.Foo, %struct.Foo }>* %memargs, i32 1; call void @Foo_ctor(%struct.Foo* %b). ; If a's ctor throws, we must destruct b.; %a = getelementptr <{ %struct.Foo, %struct.Foo }>* %memargs, i32 0; invoke void @Fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst:1225,optimiz,optimization,1225,interpreter/llvm-project/llvm/docs/InAlloca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst,1,['optimiz'],['optimization']
Performance,"ess space pointer; to an OpenCL device enqueue; queue is passed in the; kernarg. ""hidden_global_offset_x""; The OpenCL grid dispatch; global offset for the X; dimension is passed in the; kernarg. ""hidden_global_offset_y""; The OpenCL grid dispatch; global offset for the Y; dimension is passed in the; kernarg. ""hidden_global_offset_z""; The OpenCL grid dispatch; global offset for the Z; dimension is passed in the; kernarg. ""hidden_none""; An argument that is not used; by the kernel. Space needs to; be left for it, but it does; not need to be set up. ""hidden_printf_buffer""; A global address space pointer; to the runtime printf buffer; is passed in kernarg. Mutually; exclusive with; ""hidden_hostcall_buffer""; before Code Object V5. ""hidden_hostcall_buffer""; A global address space pointer; to the runtime hostcall buffer; is passed in kernarg. Mutually; exclusive with; ""hidden_printf_buffer""; before Code Object V5. ""hidden_default_queue""; A global address space pointer; to the OpenCL device enqueue; queue that should be used by; the kernel by default is; passed in the kernarg. ""hidden_completion_action""; A global address space pointer; to help link enqueued kernels into; the ancestor tree for determining; when the parent kernel has finished. ""hidden_multigrid_sync_arg""; A global address space pointer for; multi-grid synchronization is; passed in the kernarg. "".value_type"" string Unused and deprecated. This should no longer; be emitted, but is accepted for compatibility. "".pointee_align"" integer Alignment in bytes of pointee; type for pointer type kernel; argument. Must be a power; of 2. Only present if; "".value_kind"" is; ""dynamic_shared_pointer"".; "".address_space"" string Kernel argument address space; qualifier. Only present if; "".value_kind"" is ""global_buffer"" or; ""dynamic_shared_pointer"". Values; are:. - ""private""; - ""global""; - ""constant""; - ""local""; - ""generic""; - ""region"". .. TODO::. Is ""global_buffer"" only ""global""; or ""constant""? Is; ""dynamic_shared_pointer"" always; ""lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:138448,queue,queue,138448,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"ess than or equal to ``op2``.; - ""``one``"": yields ``true`` if both operands are not a NAN and ``op1``; is not equal to ``op2``.; - ""``ord``"": yields ``true`` if both operands are not a NAN.; - ""``ueq``"": yields ``true`` if either operand is a NAN or ``op1`` is; equal to ``op2``.; - ""``ugt``"": yields ``true`` if either operand is a NAN or ``op1`` is; greater than ``op2``.; - ""``uge``"": yields ``true`` if either operand is a NAN or ``op1`` is; greater than or equal to ``op2``.; - ""``ult``"": yields ``true`` if either operand is a NAN or ``op1`` is; less than ``op2``.; - ""``ule``"": yields ``true`` if either operand is a NAN or ``op1`` is; less than or equal to ``op2``.; - ""``une``"": yields ``true`` if either operand is a NAN or ``op1`` is; not equal to ``op2``.; - ""``uno``"": yields ``true`` if either operand is a NAN. The quiet comparison operation performed by; '``llvm.experimental.constrained.fcmp``' will only raise an exception; if either operand is a SNAN. The signaling comparison operation; performed by '``llvm.experimental.constrained.fcmps``' will raise an; exception if either operand is a NAN (QNAN or SNAN). Such an exception; does not preclude a result being produced (e.g. exception might only; set a flag), therefore the distinction between ordered and unordered; comparisons is also relevant for the; '``llvm.experimental.constrained.fcmps``' intrinsic. '``llvm.experimental.constrained.fmuladd``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.experimental.constrained.fmuladd(<type> <op1>, <type> <op2>,; <type> <op3>,; metadata <rounding mode>,; metadata <exception behavior>). Overview:; """""""""""""""""". The '``llvm.experimental.constrained.fmuladd``' intrinsic represents; multiply-add expressions that can be fused if the code generator determines; that (a) the target instruction set has support for a fused operation,; and (b) that the fused operation is more efficient than the equivalent,; separate pair o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:887116,perform,performed,887116,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"ess>, i32 <rw>, i32 <locality>, i32 <cache type>). Overview:; """""""""""""""""". The '``llvm.prefetch``' intrinsic is a hint to the code generator to; insert a prefetch instruction if supported; otherwise, it is a noop.; Prefetches have no effect on the behavior of the program but can change; its performance characteristics. Arguments:; """""""""""""""""""". ``address`` is the address to be prefetched, ``rw`` is the specifier; determining if the fetch should be for a read (0) or write (1), and; ``locality`` is a temporal locality specifier ranging from (0) - no; locality, to (3) - extremely local keep in cache. The ``cache type``; specifies whether the prefetch is performed on the data (1) or; instruction (0) cache. The ``rw``, ``locality`` and ``cache type``; arguments must be constant integers. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. In; particular, prefetches cannot trap and do not produce a value. On; targets that support this intrinsic, the prefetch can provide hints to; the processor cache for better performance. '``llvm.pcmarker``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.pcmarker(i32 <id>). Overview:; """""""""""""""""". The '``llvm.pcmarker``' intrinsic is a method to export a Program; Counter (PC) in a region of code to simulators and other tools. The; method is target specific, but it is expected that the marker will use; exported symbols to transmit the PC of the marker. The marker makes no; guarantees that it will remain with any specific instruction after; optimizations. It is possible that the presence of a marker will inhibit; optimizations. The intended use is to be inserted after optimizations to; allow correlations of simulation runs. Arguments:; """""""""""""""""""". ``id`` is a numerical id identifying the marker. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. Backends; that do not support this intrinsic may ignore it. '``llvm.readcyclecounter``' Intrinsic; ^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:524012,cache,cache,524012,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"essing. ### Implicit Multi-Threading; When `ROOT::EnableImplicitMT()` is used, RNTuple uses ROOT's task arena to compress and decompress pages.; That requires writes to be buffered and reads uses the cluster pool resp.; The RNTuple data source for RDataFrame lets RDataFrame full control of the thread pool.; That means that RDataFrame uses a separate data source for every thread, each of the data sources runs in sequential mode. ### Concurrent Readers; Multiple readers can read the same RNTuple concurrently as long as access to every individual reader is sequential. ### Parallel REntry Preparation; Multiple `REntry` object can be concurrently prepared by multiple threads.; I.e., construction and binding of the objects can happen in parallel.; The actual reading and writing of entries (`RNTupleReader::LoadEntry()`, `RNTupleWriter::Fill()`) needs to be protected by a mutex.; This is considered ""mild scalability parallelization"" in RNTuple. ### RNTupleParallelWriter; The parallel writer offers the most scalable parallel writing interface.; Multiple _fill contexts_ can concurrently serialize and compress data.; Every fill context prepares a set of entire clusters in the final on-disk layout.; When a fill context flushes data,; a brief serialization point handles the RNTuple meta-data updates and the reservation of disk space to write into. Low precision float types; --------------------------; RNTuple supports encoding floating point types with a lower precision when writing them to disk. This encoding is specified by the; user per field and it is independent on the in-memory type used for that field (meaning both a `RField<double>` or `RField<float>` can; be mapped to e.g. a low-precision 16 bit float). RNTuple supports the following encodings (all mutually exclusive):. - **Real16**/**SplitReal16**: IEEE-754 half precision float. Set by calling `RField::SetHalfPrecision()`;; - **Real32Trunc**: floating point with less than 32 bits of precision (truncated mantissa).; Set",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:25484,scalab,scalable,25484,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['scalab'],['scalable']
Performance,estBatchNormalizationCuda TestBatchNormalizationCuda.cxx ); TARGET_LINK_LIBRARIES(testBatchNormalizationCuda ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cuda COMMAND testBatchNormalizationCuda). # DNN - Minimization Cuda; add_executable(testMinimizationCuda TestMinimizationCuda.cxx); TARGET_LINK_LIBRARIES(testMinimizationCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-MinimizationCuda COMMAND testMinimizationCuda). # DNN - Arithmetic Cuda; add_executable(testArithmeticCuda TestMatrixArithmeticCuda.cxx); TARGET_LINK_LIBRARIES(testArithmeticCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-ArithmeticCuda COMMAND testArithmeticCuda). # DNN - DataLoader Cuda; add_executable(testDataLoaderCuda TestDataLoaderCuda.cxx); TARGET_LINK_LIBRARIES(testDataLoaderCuda ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-DataLoaderCuda COMMAND testDataLoaderCuda). # DNN - Optimization GPU. add_executable(testOptimizationCuda TestOptimizationCuda.cxx); TARGET_LINK_LIBRARIES(testOptimizationCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cuda COMMAND testOptimizationCuda). #Cuda tests using CUDNN; if (tmva-cudnn). # DNN - Batch normalization Cudnn; add_executable(testBatchNormalizationCudnn TestBatchNormalizationCudnn.cxx ); TARGET_LINK_LIBRARIES(testBatchNormalizationCudnn ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cudnn COMMAND testBatchNormalizationCudnn). # DNN Optimization GPU Cudnn. add_executable(testOptimizationCudnn TestOptimizationCudnn.cxx); TARGET_LINK_LIBRARIES(testOptimizationCudnn ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cudnn COMMAND testOptimizationCudnn). # DNN - TensorDataLoader Cudnn; #add_executable(testTensorDataLoaderCudnn TestTensorDataLoaderCudnn.cxx); #TARGET_LINK_LIBRARIES(testTensorDataLoaderCudnn ${Libraries} ${DNN_CUDA_LIBRARIES}); #ROOT_ADD_TEST(TMVA-DNN-TensorDataLoaderCudnn COMMAND testTensorDataLoaderCudnn). endif(). endif (). #--- CPU tests. ----------------------------; #; # always run the Cpu tests. If tmva-cpu is off (no Bla,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:2950,Optimiz,Optimization-Cuda,2950,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cuda']
Performance,"ested on a G4; PowerPC. The improvement becomes clearly visible around sizes of (50x50); were the execution speed improvement of the Altivec processor becomes; more significant than the overhead of filling its pipe. 2. $A^{-1}$ Here, the time is measured for an in-place matrix inversion. Except for `ROOT v3.10`, the algorithms are all based on an; `LU `factorization followed by forward/back-substitution. `ROOT v3.10`; is using the slower Gaussian elimination method. The numerical accuracy; of the `CLHEP` routine is poor:. - up to 6x6 the numerical imprecise Cramer multiplication is hard-coded.; For instance, calculating `U=H*H-1`, where `H` is a (5x5) Hilbert; matrix, results in off-diagonal elements of $10^{-7}$ instead of the $10^{-13}$; using an `LU `according to `Crout`. - scaling protection is non-existent and limits are hard-coded, as a; consequence inversion of a Hilbert matrix for `sizes>(12x12)` fails. In; order to gain speed the `CLHEP` algorithm stores its permutation info of; the pivots points in a static array, making multi-threading not; possible. `GSL` uses LU decomposition without the implicit scaling of `Crout`.; Therefore, its accuracy is not as good. For instance a (10x10) Hilbert; matrix has errors 10 times larger than the `LU Crout` result. In; `ROOT v4.0`, the user can choose between the `Invert()` and; `IvertFast()` routines, where the latter is using the Cramer algorithm; for `sizes<7x7`. The speed graph shows the result for `InvertFast()`. 3. `A*x=b` the execution time is measured for solving the linear; equation `A*x=b`. The same factorizations are used as in the matrix; inversion. However, only 1 forward/back-substitution has to be used; instead of msize as in the inversion of (msize x msize) matrix. As a; consequence the same differences are observed but less amplified. CLHEP; shows the same numerical issues as in step the matrix inversion. Since; ROOT3.10 has no dedicated equation solver, the solution is calculated; through `x=A-1*b`. Thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md:51403,multi-thread,multi-threading,51403,documentation/users-guide/LinearAlgebra.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/LinearAlgebra.md,1,['multi-thread'],['multi-threading']
Performance,"ests in the browser. It can be easily done with [CORS Everywhere plugin](https://addons.mozilla.org/de/firefox/addon/cors-everywhere/) for the Firefox browser or [Allow CORS plugin](https://chrome.google.com/webstore/detail/allow-control-allow-origi/nlfbmbojpeacfghkpbjhddihlkkiljbi?hl=en) for the Chrome browser. Next solution - install JSROOT on the server hosting ROOT files. In such configuration JSROOT does not issue CORS requests, therefore server and browsers can be used with their default settings. A simplified variant of such solution - copy only the top index.htm file from JSROOT package and specify the full path to `modules/gui.mjs` script like:. ```javascript; <script type=""module"">; import { openFile, draw } from 'https://root.cern/js/latest/modules/gui.mjs';; // ...; </script>; ```. In the main `<div>` element one can specify many custom parameters like one do it in URL string:. ```html; <div id=""simpleGUI"" path=""files/path"" files=""userfile1.root;subdir/usefile2.root"">; loading scripts ...; </div>; ```. ## Reading local ROOT files. JSROOT can read files from local file system using HTML5 FileReader functionality.; Main limitation here - user should interactively select files for reading.; There is button __""...""__ on the main JSROOT page, which starts file selection dialog.; If valid ROOT file is selected, JSROOT will be able to normally read content of such file. ## JSROOT with THttpServer. THttpServer provides http access to objects from running ROOT application.; JSROOT is used to implement the user interface in the web browsers. The layout of the main page coming from THttpServer is very similar to normal JSROOT page.; One could browse existing items and display them. A snapshot of running; server can be seen on the [demo page](https://root.cern/js/latest/httpserver.C/). One could also specify similar URL parameters to configure the displayed items and drawing options. It is also possible to display one single item from the THttpServer server like:. <h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:29342,load,loading,29342,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['loading']
Performance,"esults; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you choose to make your own walker, you can find the clobber for a; ``MemoryAccess`` by walking every ``MemoryDef`` that dominates said; ``MemoryAccess``. The structure of ``MemoryDef``\ s makes this relatively simple;; they ultimately form a linked list of every clobber that dominates the; ``MemoryAccess`` that you're trying to optimize. In other words, the; ``definingAccess`` of a ``MemoryDef`` is always the nearest dominating; ``MemoryDef`` or ``MemoryPhi`` of said ``MemoryDef``. Use and Def optimization; ------------------------. ``MemoryUse``\ s keep a single operand, which is their defining or optimized; access.; Traditionally ``MemorySSA`` optimized ``MemoryUse``\ s at build-time, up to a; given threshold.; Specifically, the operand of every ``MemoryUse`` was optimized to point to the; actual clobber of said ``MemoryUse``. This can be seen in the above example; the; second ``MemoryUse`` in ``if.end`` has an operand of ``1``, which is a; ``MemoryDef`` from the entry block. This is done to make walking,; value numbering, etc, faster and easier.; As of `this revision <https://reviews.llvm.org/D121381>`_, the default was; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:11071,optimiz,optimized,11071,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimized']
Performance,es} ); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-DLCuda COMMAND testBackpropagationDLCuda). # DNN - Batch normalization Cuda; add_executable(testBatchNormalizationCuda TestBatchNormalizationCuda.cxx ); TARGET_LINK_LIBRARIES(testBatchNormalizationCuda ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cuda COMMAND testBatchNormalizationCuda). # DNN - Minimization Cuda; add_executable(testMinimizationCuda TestMinimizationCuda.cxx); TARGET_LINK_LIBRARIES(testMinimizationCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-MinimizationCuda COMMAND testMinimizationCuda). # DNN - Arithmetic Cuda; add_executable(testArithmeticCuda TestMatrixArithmeticCuda.cxx); TARGET_LINK_LIBRARIES(testArithmeticCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-ArithmeticCuda COMMAND testArithmeticCuda). # DNN - DataLoader Cuda; add_executable(testDataLoaderCuda TestDataLoaderCuda.cxx); TARGET_LINK_LIBRARIES(testDataLoaderCuda ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-DataLoaderCuda COMMAND testDataLoaderCuda). # DNN - Optimization GPU. add_executable(testOptimizationCuda TestOptimizationCuda.cxx); TARGET_LINK_LIBRARIES(testOptimizationCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cuda COMMAND testOptimizationCuda). #Cuda tests using CUDNN; if (tmva-cudnn). # DNN - Batch normalization Cudnn; add_executable(testBatchNormalizationCudnn TestBatchNormalizationCudnn.cxx ); TARGET_LINK_LIBRARIES(testBatchNormalizationCudnn ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cudnn COMMAND testBatchNormalizationCudnn). # DNN Optimization GPU Cudnn. add_executable(testOptimizationCudnn TestOptimizationCudnn.cxx); TARGET_LINK_LIBRARIES(testOptimizationCudnn ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cudnn COMMAND testOptimizationCudnn). # DNN - TensorDataLoader Cudnn; #add_executable(testTensorDataLoaderCudnn TestTensorDataLoaderCudnn.cxx); #TARGET_LINK_LIBRARIES(testTensorDataLoaderCudnn ${Libraries} ${DNN_CUDA_LIBRARIES}); #ROOT_ADD_TEST(TMVA-DNN-TensorDataLoaderCudnn COMMAND testTensorD,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:2787,Optimiz,Optimization,2787,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization']
Performance,"et-specific calling conventions are known to backend:. * **x86_StdCall** --- stdcall calling convention seen on Microsoft Windows; platform (CC ID = 64). * **x86_FastCall** --- fastcall calling convention seen on Microsoft Windows; platform (CC ID = 65). * **x86_ThisCall** --- Similar to X86_StdCall. Passes first argument in ECX,; others via stack. Callee is responsible for stack cleaning. This convention is; used by MSVC by default for methods in its ABI (CC ID = 70). .. _X86 addressing mode:. Representing X86 addressing modes in MachineInstrs; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The x86 has a very flexible way of accessing memory. It is capable of forming; memory addresses of the following expression directly in integer instructions; (which use ModR/M addressing):. ::. SegmentReg: Base + [1,2,4,8] * IndexReg + Disp32. In order to represent this, LLVM tracks no less than 5 operands for each memory; operand of this form. This means that the ""load"" form of '``mov``' has the; following ``MachineOperand``\s in this order:. ::. Index: 0 | 1 2 3 4 5; Meaning: DestReg, | BaseReg, Scale, IndexReg, Displacement Segment; OperandTy: VirtReg, | VirtReg, UnsImm, VirtReg, SignExtImm PhysReg. Stores, and all other instructions, treat the four memory operands in the same; way and in the same order. If the segment register is unspecified (regno = 0),; then no segment override is generated. ""Lea"" operations do not have a segment; register specified, so they only have 4 operands for their memory reference. X86 address spaces supported; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. x86 has a feature which provides the ability to perform loads and stores to; different address spaces via the x86 segment registers. A segment override; prefix byte on an instruction causes the instruction's memory access to go to; the specified segment. LLVM address space 0 is the default address space, which; includes the stack, and any unqualified memory accesses in a program. Address; spaces 1-255 are cu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:91111,load,load,91111,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"et. Defaults to OFF. Targets; for building each unit test are generated in any case. You can build a; specific unit test using the targets defined under *unittests*, such as; ADTTests, IRTests, SupportTests, etc. (Search for ``add_llvm_unittest`` in; the subdirectories of *unittests* for a complete list of unit tests.) It is; possible to build all unit tests with the target *UnitTests*. **LLVM_BUILD_TOOLS**:BOOL; Build LLVM tools. Defaults to ON. Targets for building each tool are generated; in any case. You can build a tool separately by invoking its target. For; example, you can build *llvm-as* with a Makefile-based system by executing *make; llvm-as* at the root of your build directory. **LLVM_CCACHE_BUILD**:BOOL; If enabled and the ``ccache`` program is available, then LLVM will be; built using ``ccache`` to speed up rebuilds of LLVM and its components.; Defaults to OFF. The size and location of the cache maintained; by ``ccache`` can be adjusted via the LLVM_CCACHE_MAXSIZE and LLVM_CCACHE_DIR; options, which are passed to the CCACHE_MAXSIZE and CCACHE_DIR environment; variables, respectively. **LLVM_CREATE_XCODE_TOOLCHAIN**:BOOL; macOS Only: If enabled CMake will generate a target named; 'install-xcode-toolchain'. This target will create a directory at; $CMAKE_INSTALL_PREFIX/Toolchains containing an xctoolchain directory which can; be used to override the default system tools. **LLVM_<target>_LINKER_FLAGS**:STRING; Defines the set of linker flags that should be applied to a <target>. **LLVM_DEFAULT_TARGET_TRIPLE**:STRING; LLVM target to use for code generation when no target is explicitly specified.; It defaults to ""host"", meaning that it shall pick the architecture; of the machine where LLVM is being built. If you are building a cross-compiler,; set it to the target triple of your desired architecture. **LLVM_DOXYGEN_QCH_FILENAME**:STRING; The filename of the Qt Compressed Help file that will be generated when; ``-DLLVM_ENABLE_DOXYGEN=ON`` and; ``-DLLVM_ENABLE_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:17647,cache,cache,17647,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,"et_label(i + j + k);. assert(ijk_label & i_label); // ijk_label has i_label; assert(ijk_label & j_label); // ijk_label has j_label; assert(ijk_label & k_label); // ijk_label has k_label; assert(ijk_label == 7); // Verifies all of the above. // Or, equivalently:; assert(dfsan_has_label(ijk_label, i_label));; assert(dfsan_has_label(ijk_label, j_label));; assert(dfsan_has_label(ijk_label, k_label));. return 0;; }. Origin Tracking; ===============. DataFlowSanitizer can track origins of labeled values. This feature is enabled by; ``-mllvm -dfsan-track-origins=1``. For example,. .. code-block:: console. % cat test.cc; #include <sanitizer/dfsan_interface.h>; #include <stdio.h>. int main(int argc, char** argv) {; int i = 0;; dfsan_set_label(i_label, &i, sizeof(i));; int j = i + 1;; dfsan_print_origin_trace(&j, ""A flow from i to j"");; return 0;; }. % clang++ -fsanitize=dataflow -mllvm -dfsan-track-origins=1 -fno-omit-frame-pointer -g -O2 test.cc; % ./a.out; Taint value 0x1 (at 0x7ffd42bf415c) origin tracking (A flow from i to j); Origin value: 0x13900001, Taint value was stored to memory at; #0 0x55676db85a62 in main test.cc:7:7; #1 0x7f0083611bbc in __libc_start_main libc-start.c:285. Origin value: 0x9e00001, Taint value was created at; #0 0x55676db85a08 in main test.cc:6:3; #1 0x7f0083611bbc in __libc_start_main libc-start.c:285. By ``-mllvm -dfsan-track-origins=1`` DataFlowSanitizer collects only; intermediate stores a labeled value went through. Origin tracking slows down; program execution by a factor of 2x on top of the usual DataFlowSanitizer; slowdown and increases memory overhead by 1x. By ``-mllvm -dfsan-track-origins=2``; DataFlowSanitizer also collects intermediate loads a labeled value went through.; This mode slows down program execution by a factor of 4x. Current status; ==============. DataFlowSanitizer is a work in progress, currently under development for; x86\_64 Linux. Design; ======. Please refer to the :doc:`design document<DataFlowSanitizerDesign>`.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizer.rst:13512,load,loads,13512,interpreter/llvm-project/clang/docs/DataFlowSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DataFlowSanitizer.rst,1,['load'],['loads']
Performance,"etadata; ^^^^^^^^^^^^^^^^^^^^^. ``fpmath`` metadata may be attached to any instruction of floating-point; type. It can be used to express the maximum acceptable error in the; result of that instruction, in ULPs, thus potentially allowing the; compiler to use a more efficient but less accurate method of computing; it. ULP is defined as follows:. If ``x`` is a real number that lies between two finite consecutive; floating-point numbers ``a`` and ``b``, without being equal to one; of them, then ``ulp(x) = |b - a|``, otherwise ``ulp(x)`` is the; distance between the two non-equal finite floating-point numbers; nearest ``x``. Moreover, ``ulp(NaN)`` is ``NaN``. The metadata node shall consist of a single positive float type number; representing the maximum relative error, for example:. .. code-block:: llvm. !0 = !{ float 2.5 } ; maximum acceptable inaccuracy is 2.5 ULPs. .. _range-metadata:. '``range``' Metadata; ^^^^^^^^^^^^^^^^^^^^. ``range`` metadata may be attached only to ``load``, ``call`` and ``invoke`` of; integer or vector of integer types. It expresses the possible ranges the loaded; value or the value returned by the called function at this call site is in. If; the loaded or returned value is not in the specified range, a poison value is; returned instead. The ranges are represented with a flattened list of integers.; The loaded value or the value returned is known to be in the union of the ranges; defined by each consecutive pair. Each pair has the following properties:. - The type must match the scalar type of the instruction.; - The pair ``a,b`` represents the range ``[a,b)``.; - Both ``a`` and ``b`` are constants.; - The range is allowed to wrap.; - The range should not represent the full or empty set. That is,; ``a!=b``. In addition, the pairs must be in signed order of the lower bound and; they must be non-contiguous. For vector-typed instructions, the range is applied element-wise. Examples:. .. code-block:: llvm. %a = load i8, ptr %x, align 1, !range !0 ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:284823,load,load,284823,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"eted before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comme",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:303019,load,load,303019,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"eters are used. * On ppc32/64 GOT/PIC only module-local calls (visibility = hidden or protected); are supported. WebAssembly constraints:. * No variable argument lists are used. * The 'tail-call' target attribute is enabled. * The caller and callee's return types must match. The caller cannot; be void unless the callee is, too. AArch64 constraints:. * No variable argument lists are used. Example:. Call as ``llc -tailcallopt test.ll``. .. code-block:: llvm. declare fastcc i32 @tailcallee(i32 inreg %a1, i32 inreg %a2, i32 %a3, i32 %a4). define fastcc i32 @tailcaller(i32 %in1, i32 %in2) {; %l1 = add i32 %in1, %in2; %tmp = tail call fastcc i32 @tailcallee(i32 inreg %in1, i32 inreg %in2, i32 %in1, i32 %l1); ret i32 %tmp; }. Implications of ``-tailcallopt``:. To support tail call optimization in situations where the callee has more; arguments than the caller a 'callee pops arguments' convention is used. This; currently causes each ``fastcc`` call that is not tail call optimized (because; one or more of above constraints are not met) to be followed by a readjustment; of the stack. So performance might be worse in such cases. Sibling call optimization; -------------------------. Sibling call optimization is a restricted form of tail call optimization.; Unlike tail call optimization described in the previous section, it can be; performed automatically on any tail calls when ``-tailcallopt`` option is not; specified. Sibling call optimization is currently performed on x86/x86-64 when the; following constraints are met:. * Caller and callee have the same calling convention. It can be either ``c`` or; ``fastcc``. * The call is a tail call - in tail position (ret immediately follows call and; ret uses value of call or is void). * Caller and callee have matching return type or the callee result is not used. * If any of the callee arguments are being passed in stack, they must be; available in caller's own incoming argument stack and the frame offsets must; be the same. Example:. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:88073,optimiz,optimized,88073,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimized']
Performance,"ethod 2. Another method allows to create the evolution of a given radioactive; material/mixture at a given moment in time:. ~~~{.cpp}; TGeoMaterial::DecayMaterial(Double_t time, Double_t precision=0.001); ~~~. The method will create the mixture that result from the decay of a; initial material/mixture at time, while all resulting elements having a; fractional weight less than precision are excluded. A demo macro for radioactive material features is; `$ROOTSYS/tutorials/geom/RadioNuclides.C` It demonstrates also the decay; of a mixture made of radionuclides. \image html geometry004.png width=600px. \anchor GM00c; ### Tracking Media. The class TGeoMedium describes tracking media properties. This has; a pointer to a material and the additional data members representing the; properties related to tracking. ~~~{.cpp}; TGeoMedium(const char *name,Int_t numed,TGeoMaterial *mat,; Double_t *params=0);; ~~~. - `name:` name assigned to the medium; - `mat:` pointer to a material; - `params:` array of additional parameters. Another constructor allows effectively defining tracking parameters in; GEANT3 style:. ~~~{.cpp}; TGeoMedium(const char *name,Int_t numed,Int_t imat,Int_t ifield,; Double_t fieldm,Double_t tmaxfd,Double_t stemax,; Double_t deemax,Double_t epsil,Double_t stmin);; ~~~. This constructor is reserved for creating tracking media from the VMC; interface [...]:. - `numed:` user-defined medium index; - `imat:` unique ID of the material; - `others:` see G3 documentation. Looking at our simple world example, one can see that for creating; volumes one needs to create tracking media before. The way to proceed; for those not interested in performing tracking with external MC's is to; define and use only one `dummy tracking medium` as in the example (or a; `NULL` pointer). \anchor GM00d; ### User Interface for Handling Materials and Media. The TGeoManager class contains the API for accessing and handling; defined materials:. ~~~{.cpp}; TGeoManager::GetMaterial(name);; ~~~. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/materials.md:10191,perform,performing,10191,geom/geom/doc/materials.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/materials.md,1,['perform'],['performing']
Performance,"ethod. * ``ProtectionFlags`` are represented as a sys::Memory::ProtectionFlags enum,; and accessible via the ``getProtectionFlags`` method. These flags describe; whether the section is readable, writable, executable, or some combination; of these. The most common combinations are ``RW-`` for writable data,; ``R--`` for constant data, and ``R-X`` for code. * ``SectionOrdinal``, accessible via ``getOrdinal``, is a number used to order; the section relative to others. It is usually used to preserve section; order within a segment (a set of sections with the same memory protections); when laying out memory. For the graph-theorists: The ``LinkGraph`` is bipartite, with one set of; ``Symbol`` nodes and one set of ``Addressable`` nodes. Each ``Symbol`` node has; one (implicit) edge to its target ``Addressable``. Each ``Block`` has a set of; edges (possibly empty, represented as ``Edge`` instances) back to elements of; the ``Symbol`` set. For convenience and performance of common algorithms,; symbols and blocks are further grouped into ``Sections``. The ``LinkGraph`` itself provides operations for constructing, removing, and; iterating over sections, symbols, and blocks. It also provides metadata; and utilities relevant to the linking process:. * Graph element operations. * ``sections`` returns an iterator over all sections in the graph. * ``findSectionByName`` returns a pointer to the section with the given; name (as a ``Section*``) if it exists, otherwise returns a nullptr. * ``blocks`` returns an iterator over all blocks in the graph (across all; sections). * ``defined_symbols`` returns an iterator over all defined symbols in the; graph (across all sections). * ``external_symbols`` returns an iterator over all external symbols in the; graph. * ``absolute_symbols`` returns an iterator over all absolute symbols in the; graph. * ``createSection`` creates a section with a given name and protection flags. * ``createContentBlock`` creates a block with the given initial content,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:13910,perform,performance,13910,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['perform'],['performance']
Performance,"etry, or it may simply happen that they really represent a; single object, too complex to be described by a primitive shape. Usually handling structures like these can be easily done by positioning; all components in the same container volume, then positioning the; container itself. However, there are many practical cases when defining; such a container is not straightforward or even possible without; generating overlaps with the rest of the geometry. There are few ways; out of this:. - Defining the container for the structure as ""overlapping"" (see also; ""Overlapping Volumes""); - Representing the container as a composite shape - the Boolean union; of all components (see also ""Composite Shapes""); - Using an assembly volume - this will be described in the following. The first two approaches have the disadvantage of penalizing the; navigation performance with a factor increasing more than linear of the; number of components in the structure. The best solution is the third; one because it uses all volume-related navigation optimizations. The; class TGeoVolumeAssembly represents an assembly volume. Its shape; is represented by TGeoShapeAssembly class that is the union of all; components. It uses volume voxelization to perform navigation tasks. An assembly volume creates a hierarchical level and it geometrically; insulates the structure from the rest (as a normal volume). Physically,; a point that is INSIDE a TGeoShapeAssembly is always inside one of; the components, so a TGeoVolumeAssembly does not need to have a; medium. Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at: assembly.C. Creation of an assembly is very easy: one has just to create a; TGeoVolumeAssembly object and position the components inside as; for any volume:. ~~~{.cpp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:49603,optimiz,optimizations,49603,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimizations']
Performance,"ets and spectators are now checked if they are; constant. (The execution of TMVA is stopped for variables and; targets, a warning is given for spectators.). On Regression:; ; The analysis type is no longer defined by calling a dedicated; TestAllMethods-member-function of the Factory, but with the; option ""AnalysisType"" in the Factory. The default value is; ""Auto"" where TMVA tries to determine the most suitable analysis; type from the targets and classes the user has defined. Other; values are ""regression"", ""classification"" and ""multiclass"" for; the forthcoming multiclass classification.; Missing regression evaluation plots for training sample were; added. On Cut method:. Removed obsolete option ""FVerySmart"" from Cuts method. On MLP method:; ; Display of convergence information in the progress bar for MLP during training. Creation of animated gifs for MLP convergence monitoring (please; contact authors if you want to do this). On Datasets: . Checks are performed if events are unvoluntarily cut by using a; non-filled array entry (e.g. ""arr[4]"" is used, when the array; has not always at least 5 entries). A warning is given in that; case.; Bug fixes. Spectators and Targets could not be used with by-hand assignment of events.; Corrected types (training/testing) for assigning single events.; Changed message from FATAL to WARNING when the user requests more events for ; training or testing than available.; Fixed bug which caused TMVA to crash if the number of input variables exceeded ; the allowed maximum for generating scatter plots.; Prevent TMVA from crashing when running with an empty TTree or TChain.; A variable expression like ""Alt$(arr[3],0)"" can now be used; to give a default value for a variable if for some events the; array don't contain enough elements (e.g. in two jet events,; sometimes only one jet is found and thus, the array jetPt[] has; only one entry in that cases).; Plot ranges for scatter-plots showing the transformed events are now correct.; User define",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html:2661,perform,performed,2661,tmva/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html,2,['perform'],['performed']
Performance,"eturn adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8325,optimiz,optimization,8325,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimization']
Performance,"eturn with; that error. (Exception: when we import the members of a class, we collect the; individual errors with each member and we concatenate them in one Error; object.) We cache these errors in cases of declarations. During the next import; call if there is an existing error we just return with that. So, clients of the; library receive an Error object, which they must check. During import of a specific declaration, it may happen that some AST nodes had; already been created before we recognize an error. In this case, we signal back; the error to the caller, but the ""to"" context remains polluted with those nodes; which had been created. Ideally, those nodes should not had been created, but; that time we did not know about the error, the error happened later. Since the; AST is immutable (most of the cases we can't remove existing nodes) we choose; to mark these nodes as erroneous. We cache the errors associated with declarations in the ""from"" context in; ``ASTImporter::ImportDeclErrors`` and the ones which are associated with the; ""to"" context in ``ASTImporterSharedState::ImportErrors``. Note that, there may; be several ASTImporter objects which import into the same ""to"" context but from; different ""from"" contexts; in this case, they have to share the associated; errors of the ""to"" context. When an error happens, that propagates through the call stack, through all the; dependant nodes. However, in case of dependency cycles, this is not enough,; because we strive to mark the erroneous nodes so clients can act upon. In those; cases, we have to keep track of the errors for those nodes which are; intermediate nodes of a cycle. An **import path** is the list of the AST nodes which we visit during an Import; call. If node ``A`` depends on node ``B`` then the path contains an ``A->B``; edge. From the call stack of the import functions, we can read the very same; path. Now imagine the following AST, where the ``->`` represents dependency in terms; of the import (all nodes ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:101380,cache,cache,101380,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['cache'],['cache']
Performance,"eturns an already constructed object. If the slot is being used for the; first time, it calls the default constructor otherwise it returns the object as; is (unless a string is passed as the 2nd argument to the function in which case,; it also calls Clear(second_argument) on the object).; This allows to replace code like:. for (int i = 0; i < ev->Ntracks; i++) {; new(a[i]) TTrack(x,y,z,...);; ...; ...; }; ...; a.Delete(); // or a.Clear(""C""). with the simpler and more efficient:. for (int i = 0; i < ev->Ntracks; i++) {; TTrack *track = (TTrack*)a.ConstructedAt(i);; track->Set(x,y,z,....);; ...; ...; }; ...; a.Clear();. even in case where the TTrack class allocates memory. TClonesArray: update ExpandCreateFast to also reset the non-used slots; so that calling Clear (which does too much) is no longer necessary; when using ExpandCreateFast. New Thread Pool class. A first version of TThreadPool class has been introduced.; This class implements a Thread Pool pattern.; So far it supports only one type of queue - FIFO. Thread library. Reduces risk of internal dead lock by using a private internal lock to protect the internals of TThread, rather than using TThread::Lock. New header TThreadSlots.h to centralize and formalize the use of the TThread local memory slots amongst the ROOT packages. Global Variables. The global values gPad, gVirtualX, gInterpreter, gDirectory and gFile; are now all accessed via a static function of their respective class. The; access is made transparent via a CPP macro.; The access is now also made transparent from the CINT and python prompt.; gPad, gVirtualX and gInterpreter are now accessible even when their value; is zero and they now properly support tab completion.; See the important note in the I/O section on gDirectory and gFile which; are now thread local. Meta. The new interface TDictionary::GetDictionary(const char*) offers a; single entry point to query the type based on its name, conveniently combining; TDataType and TClass queries. It do",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v532/index.html:2597,queue,queue,2597,core/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v532/index.html,2,['queue'],['queue']
Performance,"eudo data: a signal shape over a background trend. This plot; is another example of how making a plot ""self-explanatory"" can help you; better displaying your results. \label{f61}][f61]. ## Toy Monte Carlo Experiments ##. Let us look at a simple example of a toy experiment comparing two; methods to fit a function to a histogram, the $\chi^{2}$. method and a method called ""binned log-likelihood fit"", both available in ROOT. As a very simple yet powerful quantity to check the quality of the fit; results, we construct for each pseudo-data set the so-called ""pull"", the; difference of the estimated and the true value of a parameter,; normalised to the estimated error on the parameter,; $\frac{(p_{estim} - p_{true})}{\sigma_{p}}$. If everything is OK, the; distribution of the pull values is a standard normal distribution, i.e.; a Gaussian distribution centred around zero with a standard deviation of one. The macro performs a rather big number of toy experiments, where a; histogram is repeatedly filled with Gaussian distributed numbers,; representing the pseudo-data in this example. Each time, a fit is; performed according to the selected method, and the pull is calculated; and filled into a histogram. Here is the code:. ``` {.cpp .numberLines}; @ROOT_INCLUDE_FILE macros/macro9.C; ```. Your present knowledge of ROOT should be enough to understand all the; technicalities behind the macro. Note that the variable `pull` in line; *61* is different from the definition above: instead of the parameter; error on `mean`, the fitted standard deviation of the distribution; divided by the square root of the number of entries,; `sig/sqrt(n_tot_entries)`, is used. - What method exhibits the better performance with the default; parameters ?. - What happens if you increase the number of entries per histogram by; a factor of ten ? Why ?. The answers to these questions are well beyond the scope of this guide.; Basically all books about statistical methods provide a complete; treatment of the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md:4748,perform,performs,4748,documentation/primer/functions_and_parameter_estimation.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/functions_and_parameter_estimation.md,1,['perform'],['performs']
Performance,"eveloper, however, you better stick to the; official coding rules!. ### Configure ROOT at start-up ###. The behaviour of a ROOT session can be tailored with the options in the; `.rootrc` file. Examples of the tunable parameters are the ones related; to the operating and window system, to the fonts to be used, to the; location of start-up files. At start-up, ROOT looks for a `.rootrc` file; in the following order:. - `./.rootrc //local directory`. - `$HOME/.rootrc //user directory`. - `$ROOTSYS/etc/system.rootrc //global ROOT directory`. If more than one `.rootrc` files are found in the search paths above,; the options are merged, with precedence local, user, global. The parsing; and interpretation of this file is handled by the ROOT class `TEnv`.; Have a look to its documentation if you need such rather advanced; features. The file `.rootrc` defines the location of two rather; important files inspected at start-up: `rootalias.C` and `rootlogon.C`.; They can contain code that needs to be loaded and executed at ROOT; startup. `rootalias.C` is only loaded and best used to define some often; used functions. `rootlogon.C` contains code that will be executed at; startup: this file is extremely useful for example to pre-load a custom; style for the plots created with ROOT. This is done most easily by; creating a new `TStyle` object with your preferred settings, as; described in the class reference guide, and then use the command; `gROOT->SetStyle(""MyStyleName"");` to make this new style definition the; default one. As an example, have a look in the file `rootlogon.C` coming; with this tutorial. Another relevant file is `rootlogoff.C` that it; called when the session is finished. ### ROOT command history ###. Every command typed at the ROOT prompt is stored in a file `.root_hist`; in your home directory. ROOT uses this file to allow for navigation in; the command history with the up-arrow and down-arrow keys. It is also; convenient to extract successful ROOT commands with th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:18808,load,loaded,18808,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['load'],['loaded']
Performance,"event a ``grep freebsd`` from finding this test. Better to use:; ``target={{.+-freebsd.*}} || target={{.+-netbsd.*}}``. Substitutions; -------------. Besides replacing LLVM tool names the following substitutions are performed in; RUN lines:. ``%%``; Replaced by a single ``%``. This allows escaping other substitutions. ``%s``; File path to the test case's source. This is suitable for passing on the; command line as the input to an LLVM tool. Example: ``/home/user/llvm/test/MC/ELF/foo_test.s``. ``%S``; Directory path to the test case's source. Example: ``/home/user/llvm/test/MC/ELF``. ``%t``; File path to a temporary file name that could be used for this test case.; The file name won't conflict with other test cases. You can append to it; if you need multiple temporaries. This is useful as the destination of; some redirected output. Example: ``/home/user/llvm.build/test/MC/ELF/Output/foo_test.s.tmp``. ``%T``; Directory of ``%t``. Deprecated. Shouldn't be used, because it can be easily; misused and cause race conditions between tests. Use ``rm -rf %t && mkdir %t`` instead if a temporary directory is necessary. Example: ``/home/user/llvm.build/test/MC/ELF/Output``. ``%{pathsep}``. Expands to the path separator, i.e. ``:`` (or ``;`` on Windows). ``${fs-src-root}``; Expands to the root component of file system paths for the source directory,; i.e. ``/`` on Unix systems or ``C:\`` (or another drive) on Windows. ``${fs-tmp-root}``; Expands to the root component of file system paths for the test's temporary; directory, i.e. ``/`` on Unix systems or ``C:\`` (or another drive) on; Windows. ``${fs-sep}``; Expands to the file system separator, i.e. ``/`` or ``\`` on Windows. ``%/s, %/S, %/t, %/T``. Act like the corresponding substitution above but replace any ``\``; character with a ``/``. This is useful to normalize path separators. Example: ``%s: C:\Desktop Files/foo_test.s.tmp``. Example: ``%/s: C:/Desktop Files/foo_test.s.tmp``. ``%{s:real}, %{S:real}, %{t:real}, %{T:real}``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:24589,race condition,race conditions,24589,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,1,['race condition'],['race conditions']
Performance,"ever, under certain circumstances, ARC is permitted to re-order and; eliminate operations in a manner which may alter the overall; computation history beyond what is permitted by the general ""as if""; rule of C/C++ and the :ref:`restrictions <arc.objects.retains>` on; the implementation of ``retain`` and ``release``. .. admonition:: Rationale. Specifically, ARC is sometimes permitted to optimize ``release``; operations in ways which might cause an object to be deallocated; before it would otherwise be. Without this, it would be almost; impossible to eliminate any ``retain``/``release`` pairs. For; example, consider the following code:. .. code-block:: objc. id x = _ivar;; [x foo];. If we were not permitted in any event to shorten the lifetime of the; object in ``x``, then we would not be able to eliminate this retain; and release unless we could prove that the message send could not; modify ``_ivar`` (or deallocate ``self``). Since message sends are; opaque to the optimizer, this is not possible, and so ARC's hands; would be almost completely tied. ARC makes no guarantees about the execution of a computation history; which contains undefined behavior. In particular, ARC makes no; guarantees in the presence of race conditions. ARC may assume that any retainable object pointers it receives or; generates are instantaneously valid from that point until a point; which, by the concurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:77231,optimiz,optimizer,77231,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizer']
Performance,"evices for the trap handler ABI (see :ref:`amdgpu-amdhsa-trap-handler-abi`). ====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Progr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149680,queue,queue,149680,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"evious; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic sc1=1; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:324624,cache,caches,324624,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"ew algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of; objects are identified: those that change with every event (i.e. the pdf) and those that change; only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated; for every event, whereas integrals are only evaluated when needed. The exception to the first type are pdfs; that only depend on constant parameters (or no parameters) - these are identified at the beginning, and precalculated once ; to avoid recalculating an expression with the same outcome for every iteration of the likelihood calculation. For composite pdfs a further optimization has been included: for a M(x,a,b) = f*F(x,a)+(1-f)G(x,b) ; it is e.g. not needed to recalculate G(x,b) if only parameter a has changed w.r.t to the previous likelihood; calculation. This optimization is now implemented by extending the value caching originally designed; for constant terms to be usable for non-constant terms, with a check executed at the beginning of each; likelihood evaluation if selected columns need to be updated because parameters have changed. The speed gain; of this optimization depends much on the structure of the pdf: in models with many free parameters most of the; likelihood evaluations are executed when MINUIT calculates numerical likelihood derivatives which vary ; one parameter at a time and the speedup is potentially larger. In models with few free parameters the; effect will be smaller. The new per-component caching strategy is enabled by default for all pdfs that are a component of; a RooAddPdf or a RooRealSumPdf, unless that component is a RooProdPdf or a RooProduct, in that; case the components of the product are cached instead of the product itself. You can disable this; new optimization by adding Optimize(1) to the RooAbsPdf::fitTo() command line (0 = no caching,; 1 = cache constant terms only, 2 = cache also variable terms according to above mentioned strategy (DEFAUL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:2543,optimiz,optimization,2543,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimization']
Performance,"ex`` type operation and its paired; ``strex``. In practice this is only usually a risk when the extra store is on; the same cache line as the variable being modified and Clang will only insert; stack stores on its own, so it is best not to use these operations on variables; with automatic storage duration. Also, loads and stores may be implicit in code written between the ``ldrex`` and; ``strex``. Clang will not necessarily mitigate the effects of these either, so; care should be exercised. For these reasons the higher level atomic primitives should be preferred where; possible. Non-temporal load/store builtins; --------------------------------. Clang provides overloaded builtins allowing generation of non-temporal memory; accesses. .. code-block:: c. T __builtin_nontemporal_load(T *addr);; void __builtin_nontemporal_store(T value, T *addr);. The types ``T`` currently supported are:. * Integer types.; * Floating-point types.; * Vector types. Note that the compiler does not guarantee that non-temporal loads or stores; will be used. C++ Coroutines support builtins; --------------------------------. .. warning::; This is a work in progress. Compatibility across Clang/LLVM releases is not; guaranteed. Clang provides experimental builtins to support C++ Coroutines as defined by; https://wg21.link/P0057. The following four are intended to be used by the; standard library to implement the ``std::coroutine_handle`` type. **Syntax**:. .. code-block:: c. void __builtin_coro_resume(void *addr);; void __builtin_coro_destroy(void *addr);; bool __builtin_coro_done(void *addr);; void *__builtin_coro_promise(void *addr, int alignment, bool from_promise). **Example of use**:. .. code-block:: c++. template <> struct coroutine_handle<void> {; void resume() const { __builtin_coro_resume(ptr); }; void destroy() const { __builtin_coro_destroy(ptr); }; bool done() const { return __builtin_coro_done(ptr); }; // ...; protected:; void *ptr;; };. template <typename Promise> struct coroutine_ha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:145161,load,loads,145161,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['loads']
Performance,"example`` GC strategy uses to distinguish references from; non references. This is controlled via GCStrategy::isGCManagedPointer. The; ``statepoint-example`` and ``coreclr`` strategies (the only two default; strategies that support statepoints) both use addrspace(1) to determine which; pointers are references, however custom strategies don't have to follow this; convention. This pass can be used an utility function by a language frontend that doesn't; want to manually reason about liveness, base pointers, or relocation when; constructing IR. As currently implemented, RewriteStatepointsForGC must be; run after SSA construction (i.e. mem2ref). RewriteStatepointsForGC will ensure that appropriate base pointers are listed; for every relocation created. It will do so by duplicating code as needed to; propagate the base pointer associated with each pointer being relocated to; the appropriate safepoints. The implementation assumes that the following; IR constructs produce base pointers: loads from the heap, addresses of global; variables, function arguments, function return values. Constant pointers (such; as null) are also assumed to be base pointers. In practice, this constraint; can be relaxed to producing interior derived pointers provided the target; collector can find the associated allocation from an arbitrary interior; derived pointer. By default RewriteStatepointsForGC passes in ``0xABCDEF00`` as the statepoint; ID and ``0`` as the number of patchable bytes to the newly constructed; ``gc.statepoint``. These values can be configured on a per-callsite; basis using the attributes ``""statepoint-id""`` and; ``""statepoint-num-patch-bytes""``. If a call site is marked with a; ``""statepoint-id""`` function attribute and its value is a positive; integer (represented as a string), then that value is used as the ID; of the newly constructed ``gc.statepoint``. If a call site is marked; with a ``""statepoint-num-patch-bytes""`` function attribute and its; value is a positive integer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:27702,load,loads,27702,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['load'],['loads']
Performance,"examples in the PowerPC and X86 backend; to follow. Adding a new SelectionDAG node; ==============================. As with intrinsics, adding a new SelectionDAG node to LLVM is much easier than; adding a new instruction. New nodes are often added to help represent; instructions common to many targets. These nodes often map to an LLVM; instruction (add, sub) or intrinsic (byteswap, population count). In other; cases, new nodes have been added to allow many targets to perform a common task; (converting between floating point and integer representation) or capture more; complicated behavior in a single node (rotate). #. ``include/llvm/CodeGen/ISDOpcodes.h``:. Add an enum value for the new SelectionDAG node. #. ``lib/CodeGen/SelectionDAG/SelectionDAG.cpp``:. Add code to print the node to ``getOperationName``. If your new node can be; evaluated at compile time when given constant arguments (such as an add of a; constant with another constant), find the ``getNode`` method that takes the; appropriate number of arguments, and add a case for your node to the switch; statement that performs constant folding for nodes that take the same number; of arguments as your new node. #. ``lib/CodeGen/SelectionDAG/LegalizeDAG.cpp``:. Add code to `legalize, promote, and expand; <CodeGenerator.html#selectiondag_legalize>`_ the node as necessary. At a; minimum, you will need to add a case statement for your node in; ``LegalizeOp`` which calls LegalizeOp on the node's operands, and returns a; new node if any of the operands changed as a result of being legalized. It; is likely that not all targets supported by the SelectionDAG framework will; natively support the new node. In this case, you must also add code in your; node's case statement in ``LegalizeOp`` to Expand your node into simpler,; legal operations. The case for ``ISD::UREM`` for expanding a remainder into; a divide, multiply, and a subtract is a good example. #. ``lib/CodeGen/SelectionDAG/LegalizeDAG.cpp``:. If targets may suppo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst:4393,perform,performs,4393,interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,1,['perform'],['performs']
Performance,"executable to be used. Not set by default and not; required unless running the Fortran Test Suite. - `CMAKE_BUILD_TYPE`. Select a build type like `OPTIMIZE` or `DEBUG` selecting a set of predefined; compiler flags. These flags are applied regardless of the `CMAKE_C_FLAGS`; option and may be changed by modifying `CMAKE_C_FLAGS_OPTIMIZE` etc. See; [https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html](https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html). - `TEST_SUITE_FORTRAN`. Activate that Fortran tests. This is a work in progress. More information can be; found in the [Flang documentation](https://flang.llvm.org/docs/FortranLLVMTestSuite.html). - `TEST_SUITE_RUN_UNDER`. Prefix test invocations with the given tool. This is typically used to run; cross-compiled tests within a simulator tool. - `TEST_SUITE_BENCHMARKING_ONLY`. Disable tests that are unsuitable for performance measurements. The disabled; tests either run for a very short time or are dominated by I/O performance; making them unsuitable as compiler performance tests. - `TEST_SUITE_SUBDIRS`. Semicolon-separated list of directories to include. This can be used to only; build parts of the test-suite or to include external suites. This option; does not work reliably with deeper subdirectories as it skips intermediate; `CMakeLists.txt` files which may be required. - `TEST_SUITE_COLLECT_STATS`. Collect internal LLVM statistics. Appends `-save-stats=obj` when invoking the; compiler and makes the lit runner collect and merge the statistic files. - `TEST_SUITE_RUN_BENCHMARKS`. If this is set to `OFF` then lit will not actually run the tests but just; collect build statistics like compile time and code size. - `TEST_SUITE_USE_PERF`. Use the `perf` tool for time measurement instead of the `timeit` tool that; comes with the test-suite. The `perf` is usually available on linux systems. - `TEST_SUITE_SPEC2000_ROOT`, `TEST_SUITE_SPEC2006_ROOT`, `TEST_SUITE_SPEC2017_ROOT`, ... Specify install",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:5746,perform,performance,5746,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,2,['perform'],['performance']
Performance,"executed.; * \- : Instruction executed, waiting to be retired. Below is the timeline view for a subset of the dot-product example located in; ``test/tools/llvm-mca/X86/BtVer2/dot-product.s`` and processed by; :program:`llvm-mca` using the following command:. .. code-block:: bash. $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=3 -timeline dot-product.s. .. code-block:: none. Timeline view:; 012345; Index 0123456789. [0,0] DeeER. . . vmulps	%xmm0, %xmm1, %xmm2; [0,1] D==eeeER . . vhaddps	%xmm2, %xmm2, %xmm3; [0,2] .D====eeeER . vhaddps	%xmm3, %xmm3, %xmm4; [1,0] .DeeE-----R . vmulps	%xmm0, %xmm1, %xmm2; [1,1] . D=eeeE---R . vhaddps	%xmm2, %xmm2, %xmm3; [1,2] . D====eeeER . vhaddps	%xmm3, %xmm3, %xmm4; [2,0] . DeeE-----R . vmulps	%xmm0, %xmm1, %xmm2; [2,1] . D====eeeER . vhaddps	%xmm2, %xmm2, %xmm3; [2,2] . D======eeeER vhaddps	%xmm3, %xmm3, %xmm4. Average Wait times (based on the timeline view):; [0]: Executions; [1]: Average time spent waiting in a scheduler's queue; [2]: Average time spent waiting in a scheduler's queue while ready; [3]: Average time elapsed from WB until retire stage. [0] [1] [2] [3]; 0. 3 1.0 1.0 3.3 vmulps	%xmm0, %xmm1, %xmm2; 1. 3 3.3 0.7 1.0 vhaddps	%xmm2, %xmm2, %xmm3; 2. 3 5.7 0.0 0.0 vhaddps	%xmm3, %xmm3, %xmm4; 3 3.3 0.5 1.4 <total>. The timeline view is interesting because it shows instruction state changes; during execution. It also gives an idea of how the tool processes instructions; executed on the target, and how their timing information might be calculated. The timeline view is structured in two tables. The first table shows; instructions changing state over time (measured in cycles); the second table; (named *Average Wait times*) reports useful timing statistics, which should; help diagnose performance bottlenecks caused by long data dependencies and; sub-optimal usage of hardware resources. An instruction in the timeline view is identified by a pair of indices, where; the first index identifies an iteration, and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:22580,queue,queue,22580,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,['queue'],['queue']
Performance,"execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:300229,load,load,300229,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"expect_with_probability``; -------------------------------------. ``__builtin_expect_with_probability`` is similar to ``__builtin_expect`` but it; takes a probability as third argument. **Syntax**:. .. code-block:: c++. long __builtin_expect_with_probability(long expr, long val, double p). **Example of use**:. .. code-block:: c++. if (__builtin_expect_with_probability(x, 0, .3)) {; bar();; }. **Description**:. The ``__builtin_expect_with_probability()`` builtin is typically used with; control flow conditions such as in ``if`` and ``switch`` statements to help; branch prediction. It means that its first argument ``expr`` is expected to take; the value of its second argument ``val`` with probability ``p``. ``p`` must be; within ``[0.0 ; 1.0]`` bounds. This builtin always returns the value of ``expr``. Query for this feature with ``__has_builtin(__builtin_expect_with_probability)``. ``__builtin_prefetch``; ----------------------. ``__builtin_prefetch`` is used to communicate with the cache handler to bring; data into the cache before it gets used. **Syntax**:. .. code-block:: c++. void __builtin_prefetch(const void *addr, int rw=0, int locality=3). **Example of use**:. .. code-block:: c++. __builtin_prefetch(a + i);. **Description**:. The ``__builtin_prefetch(addr, rw, locality)`` builtin is expected to be used to; avoid cache misses when the developer has a good understanding of which data; are going to be used next. ``addr`` is the address that needs to be brought into; the cache. ``rw`` indicates the expected access mode: ``0`` for *read* and ``1``; for *write*. In case of *read write* access, ``1`` is to be used. ``locality``; indicates the expected persistence of data in cache, from ``0`` which means that; data can be discarded from cache after its next use to ``3`` which means that; data is going to be reused a lot once in cache. ``1`` and ``2`` provide; intermediate behavior between these two extremes. Query for this feature with ``__has_builtin(__builtin_prefet",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:115563,cache,cache,115563,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['cache'],['cache']
Performance,"expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1186,perform,perform,1186,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,1,['perform'],['perform']
Performance,"expression. This applies to source languages that are implemented for a target; architecture using a SIMT execution model. These implementations map source; language threads of execution to lanes of the target architecture threads. It is required for operations that are related to SIMT lanes. *For example, the* ``DW_OP_LLVM_push_lane`` *operation and*; ``DW_OP_LLVM_form_aspace_address`` *operation when given an address space that; is SIMT lane specific.*. If specified, it must be consistent with the value of the ``DW_AT_LLVM_lanes``; attribute of the subprogram corresponding to context's frame and program; location. It is consistent if the value is greater than or equal to 0 and less; than the, possibly default, value of the ``DW_AT_LLVM_lanes`` attribute.; Otherwise the result is undefined. *A current iteration*. The 0 based source language iteration instance to be used in evaluating a user; presented expression. This applies to target architectures that support; optimizations that result in executing multiple source language loop iterations; concurrently. *For example, software pipelining and SIMD vectorization.*. It is required for operations that are related to source language loop; iterations. *For example, the* ``DW_OP_LLVM_push_iteration`` *operation.*. If specified, it must be consistent with the value of the; ``DW_AT_LLVM_iterations`` attribute of the subprogram corresponding to; context's frame and program location. It is consistent if the value is greater; than or equal to 0 and less than the, possibly default, value of the; ``DW_AT_LLVM_iterations`` attribute. Otherwise the result is undefined. *A current call frame*. The target architecture call frame identifier. It identifies a call frame that; corresponds to an active invocation of a subprogram in the current thread. It; is identified by its address on the call stack. The address is referred to as; the Canonical Frame Address (CFA). The call frame information is used to; determine the CFA for the call",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:47427,optimiz,optimizations,47427,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,2,"['concurren', 'optimiz']","['concurrently', 'optimizations']"
Performance,"ext. In the source-centric view,; all redeclarations will be present, in the order they occurred in the source; code, making this view suitable for clients that wish to see the structure of; the source code. In the semantics-centric view, only the most recent ""``f``""; will be found by the lookup, since it effectively replaces the first; declaration of ""``f``"". (Note that because ``f`` can be redeclared at block scope, or in a friend; declaration, etc. it is possible that the declaration of ``f`` found by name; lookup will not be the most recent one.). In the semantics-centric view, overloading of functions is represented; explicitly. For example, given two declarations of a function ""``g``"" that are; overloaded, e.g.,. .. code-block:: c++. void g();; void g(int);. the ``DeclContext::lookup`` operation will return a; ``DeclContext::lookup_result`` that contains a range of iterators over; declarations of ""``g``"". Clients that perform semantic analysis on a program; that is not concerned with the actual source code will primarily use this; semantics-centric view. .. _LexicalAndSemanticContexts:. Lexical and Semantic Contexts; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Each declaration has two potentially different declaration contexts: a; *lexical* context, which corresponds to the source-centric view of the; declaration context, and a *semantic* context, which corresponds to the; semantics-centric view. The lexical context is accessible via; ``Decl::getLexicalDeclContext`` while the semantic context is accessible via; ``Decl::getDeclContext``, both of which return ``DeclContext`` pointers. For; most declarations, the two contexts are identical. For example:. .. code-block:: c++. class X {; public:; void f(int x);; };. Here, the semantic and lexical contexts of ``X::f`` are the ``DeclContext``; associated with the class ``X`` (itself stored as a ``RecordDecl`` AST node).; However, we can now define ``X::f`` out-of-line:. .. code-block:: c++. void X::f(int x = 17) { /* ... */ }. Th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:77152,perform,perform,77152,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['perform'],['perform']
Performance,"ext. std; std1; std.foo; __test; // and so on ... If you still want to use the reserved module names for any reason, use; ``-Wno-reserved-module-identifier`` to suppress the warning. How to specify the dependent BMIs; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. There are 3 methods to specify the dependent BMIs:. * (1) ``-fprebuilt-module-path=<path/to/directory>``.; * (2) ``-fmodule-file=<path/to/BMI>`` (Deprecated).; * (3) ``-fmodule-file=<module-name>=<path/to/BMI>``. The option ``-fprebuilt-module-path`` tells the compiler the path where to search for dependent BMIs.; It may be used multiple times just like ``-I`` for specifying paths for header files. The look up rule here is:. * (1) When we import module M. The compiler would look up M.pcm in the directories specified; by ``-fprebuilt-module-path``.; * (2) When we import partition module unit M:P. The compiler would look up M-P.pcm in the; directories specified by ``-fprebuilt-module-path``. The option ``-fmodule-file=<path/to/BMI>`` tells the compiler to load the specified BMI directly.; The option ``-fmodule-file=<module-name>=<path/to/BMI>`` tells the compiler to load the specified BMI; for the module specified by ``<module-name>`` when necessary. The main difference is that; ``-fmodule-file=<path/to/BMI>`` will load the BMI eagerly, whereas; ``-fmodule-file=<module-name>=<path/to/BMI>`` will only load the BMI lazily, which is similar; with ``-fprebuilt-module-path``. The option ``-fmodule-file=<path/to/BMI>`` for named modules is deprecated; and is planning to be removed in future versions. In case all ``-fprebuilt-module-path=<path/to/directory>``, ``-fmodule-file=<path/to/BMI>`` and; ``-fmodule-file=<module-name>=<path/to/BMI>`` exist, the ``-fmodule-file=<path/to/BMI>`` option; takes highest precedence and ``-fmodule-file=<module-name>=<path/to/BMI>`` will take the second; highest precedence. We need to specify all the dependent (directly and indirectly) BMIs.; See https://github.com/llvm/llvm-project/issues/6270",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:11523,load,load,11523,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['load'],['load']
Performance,"external functions,; function attributes, metadata, debug info. Instead some MIR data references; the IR constructs. You can often remove them if the test doesn't depend on; them. - Alias Analysis is performed on IR values. These are referenced by memory; operands in MIR. Example: `:: (load 8 from %ir.foobar, !alias.scope !9)`.; If the test doesn't depend on (good) alias analysis the references can be; dropped: `:: (load 8)`. - MIR blocks can reference IR blocks for debug printing, profile information; or debug locations. Example: `bb.42.myblock` in MIR references the IR block; `myblock`. It is usually possible to drop the `.myblock` reference and simply; use `bb.42`. - If there are no memory operands or blocks referencing the IR then the; IR function can be replaced by a parameterless dummy function like; `define @func() { ret void }`. - It is possible to drop the whole IR section of the MIR file if it only; contains dummy functions (see above). The .mir loader will create the; IR functions automatically in this case. .. _limitations:. Limitations; -----------. Currently the MIR format has several limitations in terms of which state it; can serialize:. - The target-specific state in the target-specific ``MachineFunctionInfo``; subclasses isn't serialized at the moment. - The target-specific ``MachineConstantPoolValue`` subclasses (in the ARM and; SystemZ backends) aren't serialized at the moment. - The ``MCSymbol`` machine operands don't support temporary or local symbols. - A lot of the state in ``MachineModuleInfo`` isn't serialized - only the CFI; instructions and the variable debug information from MMI is serialized right; now. These limitations impose restrictions on what you can test with the MIR format.; For now, tests that would like to test some behaviour that depends on the state; of temporary or local ``MCSymbol`` operands or the exception handling state in; MMI, can't use the MIR format. As well as that, tests that test some behaviour; that depends on t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst:5191,load,loader,5191,interpreter/llvm-project/llvm/docs/MIRLangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MIRLangRef.rst,1,['load'],['loader']
Performance,extra/clang-tidy/cert/PostfixOperatorCheck.cpp; clang-tools-extra/clang-tidy/cert/PostfixOperatorCheck.h; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.cpp; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.h; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.cpp; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.h; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.cpp; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.h; clang-tools-extra/clang-tidy/cert/StrToNumCheck.cpp; clang-tools-extra/clang-tidy/cert/StrToNumCheck.h; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.cpp; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.h; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.cpp; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.h; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.cpp; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.h; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.cpp; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/CppCoreGuidelinesTidyModule.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/NarrowingConversionsC,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:51323,concurren,concurrency,51323,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['concurren'],['concurrency']
Performance,"ey that was used to generate `value`, the behavior is; target-specific. #### '`llvm.ptrauth.resign`'. ##### Syntax:. ```llvm; declare i64 @llvm.ptrauth.resign(i64 <value>,; i32 <old key>, i64 <old discriminator>,; i32 <new key>, i64 <new discriminator>); ```. ##### Overview:. The '`llvm.ptrauth.resign`' intrinsic re-signs a signed pointer using; a different key and diversity data. ##### Arguments:. The `value` argument is the signed pointer value to be authenticated.; The `old key` argument is the identifier of the key that was used to generate; the signed value.; The `old discriminator` argument is the additional diversity data to be used; as a discriminator in the auth operation.; The `new key` argument is the identifier of the key to use to generate the; resigned value.; The `new discriminator` argument is the additional diversity data to be used; as a discriminator in the sign operation. ##### Semantics:. The '`llvm.ptrauth.resign`' intrinsic performs a combined `auth`_ and `sign`_; operation, without exposing the intermediate raw pointer.; It returns a signed pointer value.; If `value` does not have a correct signature for `old key` and; `old discriminator`, the intrinsic traps in a target-specific way. #### '`llvm.ptrauth.sign_generic`'. ##### Syntax:. ```llvm; declare i64 @llvm.ptrauth.sign_generic(i64 <value>, i64 <discriminator>); ```. ##### Overview:. The '`llvm.ptrauth.sign_generic`' intrinsic computes a generic signature of; arbitrary data. ##### Arguments:. The `value` argument is the arbitrary data value to be signed.; The `discriminator` argument is the additional diversity data to be used as a; discriminator. ##### Semantics:. The '`llvm.ptrauth.sign_generic`' intrinsic computes the signature of a given; combination of value and additional diversity data. It returns a full signature value (as opposed to a signed pointer value, with; an embedded partial signature). As opposed to [`llvm.ptrauth.sign`](#llvm-ptrauth-sign), it does not interpret; `value` ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PointerAuth.md:4632,perform,performs,4632,interpreter/llvm-project/llvm/docs/PointerAuth.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/PointerAuth.md,1,['perform'],['performs']
Performance,"f C/C++ and the :ref:`restrictions <arc.objects.retains>` on; the implementation of ``retain`` and ``release``. .. admonition:: Rationale. Specifically, ARC is sometimes permitted to optimize ``release``; operations in ways which might cause an object to be deallocated; before it would otherwise be. Without this, it would be almost; impossible to eliminate any ``retain``/``release`` pairs. For; example, consider the following code:. .. code-block:: objc. id x = _ivar;; [x foo];. If we were not permitted in any event to shorten the lifetime of the; object in ``x``, then we would not be able to eliminate this retain; and release unless we could prove that the message send could not; modify ``_ivar`` (or deallocate ``self``). Since message sends are; opaque to the optimizer, this is not possible, and so ARC's hands; would be almost completely tied. ARC makes no guarantees about the execution of a computation history; which contains undefined behavior. In particular, ARC makes no; guarantees in the presence of race conditions. ARC may assume that any retainable object pointers it receives or; generates are instantaneously valid from that point until a point; which, by the concurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior and does not rely on exact or minimum retain count values; except as guaranteed by ``__strong`` object invariants or +1 transfer; conventions. For example, if an object is provably double-retained; and dou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:77481,race condition,race conditions,77481,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['race condition'],['race conditions']
Performance,"f Ordering <LangRef.html#ordering>`_.). .. _NotAtomic:. NotAtomic; ---------. NotAtomic is the obvious, a load or store which is not atomic. (This isn't; really a level of atomicity, but is listed here for comparison.) This is; essentially a regular load or store. If there is a race on a given memory; location, loads from that location return undef. Relevant standard; This is intended to match shared variables in C/C++, and to be used in any; other context where memory access is necessary, and a race is impossible. (The; precise definition is in `LangRef Memory Model <LangRef.html#memmodel>`_.). Notes for frontends; The rule is essentially that all memory accessed with basic loads and stores; by multiple threads should be protected by a lock or other synchronization;; otherwise, you are likely to run into undefined behavior. If your frontend is; for a ""safe"" language like Java, use Unordered to load and store any shared; variable. Note that NotAtomic volatile loads and stores are not properly; atomic; do not try to use them as a substitute. (Per the C/C++ standards,; volatile does provide some limited guarantees around asynchronous signals, but; atomics are generally a better solution.). Notes for optimizers; Introducing loads to shared variables along a codepath where they would not; otherwise exist is allowed; introducing stores to shared variables is not. See; `Optimization outside atomic`_. Notes for code generation; The one interesting restriction here is that it is not allowed to write to; bytes outside of the bytes relevant to a store. This is mostly relevant to; unaligned stores: it is not allowed in general to convert an unaligned store; into two aligned stores of the same width as the unaligned store. Backends are; also expected to generate an i8 store as an i8 store, and not an instruction; which writes to surrounding bytes. (If you are writing a backend for an; architecture which cannot satisfy these restrictions and cares about; concurrency, please send",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:6874,load,loads,6874,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"f ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation exactly as if the object had been sent the ``retain`` message. Always returns ``value``. .. _arc.runtime.objc_retainAutorelease:. ``id objc_retainAutorelease(id value);``; ----------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation followed by an autorelease operation. Equivalent to the following; code:. .. code-block:: objc. id objc_retainAutorelease(id value) {; return objc_autorelease(objc_retain(value));; }. Always returns ``value``. .. _arc.runtime.objc_retainAutoreleaseReturnValue:. ``id objc_retainAutoreleaseReturnValue(id value);``; ---------------------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it performs a retain; operation followed by the operation described in; :ref:`objc_autoreleaseReturnValue <arc.runtime.objc_autoreleaseReturnValue>`.; Equivalent to the following code:. .. code-block:: objc. id objc_retainAutoreleaseReturnValue(id value) {; return objc_autoreleaseReturnValue(objc_retain(value));; }. Always returns ``value``. .. _arc.runtime.objc_retainAutoreleasedReturnValue:. ``id objc_retainAutoreleasedReturnValue(id value);``; ----------------------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it attempts to; accept a hand off of a retain count from a call to; :ref:`objc_autoreleaseReturnValue <arc.runtime.objc_autoreleaseReturnValue>` on; ``value`` in a recently-called function or something it tail-calls. If that; fails, it performs a retain operation exactly like :ref:`objc_retain; <arc.runtime.objc_retain>`. Always returns ``value``. .. _arc.runtime.objc_retainBlock:. ``id objc_retainBlock(id value);``; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:114508,perform,performs,114508,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance,"f a :ref:`br <i_br>` instruction.; - The callee operand of a :ref:`call <i_call>` or :ref:`invoke <i_invoke>`; instruction.; - The parameter operand of a :ref:`call <i_call>` or :ref:`invoke <i_invoke>`; instruction, when the function or invoking call site has a ``noundef``; attribute in the corresponding position.; - The operand of a :ref:`ret <i_ret>` instruction if the function or invoking; call site has a `noundef` attribute in the return value position. Here are some examples:. .. code-block:: llvm. entry:; %poison = sub nuw i32 0, 1 ; Results in a poison value.; %poison2 = sub i32 poison, 1 ; Also results in a poison value.; %still_poison = and i32 %poison, 0 ; 0, but also poison.; %poison_yet_again = getelementptr i32, ptr @h, i32 %still_poison; store i32 0, ptr %poison_yet_again ; Undefined behavior due to; ; store to poison. store i32 %poison, ptr @g ; Poison value stored to memory.; %poison3 = load i32, ptr @g ; Poison value loaded back from memory. %poison4 = load i16, ptr @g ; Returns a poison value.; %poison5 = load i64, ptr @g ; Returns a poison value. %cmp = icmp slt i32 %poison, 0 ; Returns a poison value.; br i1 %cmp, label %end, label %end ; undefined behavior. end:. .. _welldefinedvalues:. Well-Defined Values; -------------------. Given a program execution, a value is *well defined* if the value does not; have an undef bit and is not poison in the execution.; An aggregate value or vector is well defined if its elements are well defined.; The padding of an aggregate isn't considered, since it isn't visible; without storing it into memory and loading it with a different type. A constant of a :ref:`single value <t_single_value>`, non-vector type is well; defined if it is neither '``undef``' constant nor '``poison``' constant.; The result of :ref:`freeze instruction <i_freeze>` is well defined regardless; of its operand. .. _blockaddress:. Addresses of Basic Blocks; -------------------------. ``blockaddress(@function, %block)``. The '``blockaddress``'",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:198935,load,load,198935,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"f a variable is declared in the condition of an Objective-C fast enumeration; loop, and the variable has no explicit ownership qualifier, then it is; implicitly :ref:`externally-retained <arc.misc.externally_retained>` so that; objects encountered during the enumeration are not actually retained and; released. .. admonition:: Rationale. This is an optimization made possible because fast enumeration loops promise; to keep the objects retained during enumeration, and the collection itself; cannot be synchronously modified. It can be overridden by explicitly; qualifying the variable with ``__strong``, which will make the variable; mutable again and cause the loop to retain the objects it encounters. .. _arc.misc.blocks:. Blocks; ------. The implicit ``const`` capture variables created when evaluating a block; literal expression have the same ownership semantics as the local variables; they capture. The capture is performed by reading from the captured variable; and initializing the capture variable with that value; the capture variable is; destroyed when the block literal is, i.e. at the end of the enclosing scope. The :ref:`inference <arc.ownership.inference>` rules apply equally to; ``__block`` variables, which is a shift in semantics from non-ARC, where; ``__block`` variables did not implicitly retain during capture. ``__block`` variables of retainable object owner type are moved off the stack; by initializing the heap copy with the result of moving from the stack copy. With the exception of retains done as part of initializing a ``__strong``; parameter variable or reading a ``__weak`` variable, whenever these semantics; call for retaining a value of block-pointer type, it has the effect of a; ``Block_copy``. The optimizer may remove such copies when it sees that the; result is used only as an argument to a call. When a block pointer type is converted to a non-block pointer type (such as; ``id``), ``Block_copy`` is called. This is necessary because a block allocated",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:96373,perform,performed,96373,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performed']
Performance,"f all; callees to exactly match the prototype of the function definition.; ""``coldcc``"" - The cold calling convention; This calling convention attempts to make code in the caller as; efficient as possible under the assumption that the call is not; commonly executed. As such, these calls often preserve all registers; so that the call does not break any live ranges in the caller side.; This calling convention does not support varargs and requires the; prototype of all callees to exactly match the prototype of the; function definition. Furthermore the inliner doesn't consider such function; calls for inlining.; ""``ghccc``"" - GHC convention; This calling convention has been implemented specifically for use by; the `Glasgow Haskell Compiler (GHC) <http://www.haskell.org/ghc>`_.; It passes everything in registers, going to extremes to achieve this; by disabling callee save registers. This calling convention should; not be used lightly but only for specific situations such as an; alternative to the *register pinning* performance technique often; used when implementing functional programming languages. At the; moment only X86, AArch64, and RISCV support this convention. The ; following limitations exist:. - On *X86-32* only up to 4 bit type parameters are supported. No; floating-point types are supported.; - On *X86-64* only up to 10 bit type parameters and 6; floating-point parameters are supported.; - On *AArch64* only up to 4 32-bit floating-point parameters,; 4 64-bit floating-point parameters, and 10 bit type parameters; are supported.; - *RISCV64* only supports up to 11 bit type parameters, 4; 32-bit floating-point parameters, and 4 64-bit floating-point; parameters. This calling convention supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; both the caller and callee are using it.; ""``cc 11``"" - The HiPE calling convention; This calling convention has been implemented specifically for use by; the `High-Performance Erlang; (HiP",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:14027,perform,performance,14027,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performance']
Performance,"f an :ref:`aggregate <t_aggregate>` data structure. It performs; address calculation only and does not access memory. The instruction can also; be used to calculate a vector of such addresses. Arguments:; """""""""""""""""""". The first argument is always a type used as the basis for the calculations.; The second argument is always a pointer or a vector of pointers, and is the; base address to start from. The remaining arguments are indices; that indicate which of the elements of the aggregate object are indexed.; The interpretation of each index is dependent on the type being indexed; into. The first index always indexes the pointer value given as the; second argument, the second index indexes a value of the type pointed to; (not necessarily the value directly pointed to, since the first index; can be non-zero), etc. The first type indexed into must be a pointer; value, subsequent types can be arrays, vectors, and structs. Note that; subsequent types being indexed into can never be pointers, since that; would require loading the pointer before continuing calculation. The type of each index argument depends on the type it is indexing into.; When indexing into a (optionally packed) structure, only ``i32`` integer; **constants** are allowed (when using a vector of indices they must all; be the **same** ``i32`` integer constant). When indexing into an array,; pointer or vector, integers of any width are allowed, and they are not; required to be constant. These integers are treated as signed values; where relevant. For example, let's consider a C code fragment and how it gets compiled; to LLVM:. .. code-block:: c. struct RT {; char A;; int B[10][20];; char C;; };; struct ST {; int X;; double Y;; struct RT Z;; };. int *foo(struct ST *s) {; return &s[1].Z.B[5][13];; }. The LLVM code generated by Clang is approximately:. .. code-block:: llvm. %struct.RT = type { i8, [10 x [20 x i32]], i8 }; %struct.ST = type { i32, double, %struct.RT }. define ptr @foo(ptr %s) {; entry:; %arrayidx = ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:433823,load,loading,433823,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"f and only if the function has; returned a non-zero value.; Note that for non-retained out parameters conditionals do not matter, as the; caller has no obligations regardless of whether an object is written into or; not. Custom Assertion Handlers. The analyzer exploits code assertions by pruning off paths where the; assertion condition is false. The idea is capture any program invariants; specified in the assertion that the developer may know but is not immediately; apparent in the code itself. In this way assertions make implicit assumptions; explicit in the code, which not only makes the analyzer more accurate when; finding bugs, but can help others better able to understand your code as well.; It can also help remove certain kinds of analyzer false positives by pruning off; false paths.; In order to exploit assertions, however, the analyzer must understand when it; encounters an ""assertion handler."" Typically assertions are; implemented with a macro, with the macro performing a check for the assertion; condition and, when the check fails, calling an assertion handler. For example, consider the following code; fragment:. void foo(int *p) {; assert(p != NULL);; }. When this code is preprocessed on Mac OS X it expands to the following:. void foo(int *p) {; (__builtin_expect(!(p != NULL), 0) ? __assert_rtn(__func__, ""t.c"", 4, ""p != NULL"") : (void)0);; }. In this example, the assertion handler is __assert_rtn. When called,; most assertion handlers typically print an error and terminate the program. The; analyzer can exploit such semantics by ending the analysis of a path once it; hits a call to an assertion handler.; The trick, however, is that the analyzer needs to know that a called function; is an assertion handler; otherwise the analyzer might assume the function call; returns and it will continue analyzing the path where the assertion condition; failed. This can lead to false positives, as the assertion condition usually; implies a safety condition (e.g., a pointe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html:20352,perform,performing,20352,interpreter/llvm-project/clang/www/analyzer/annotations.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/annotations.html,2,['perform'],['performing']
Performance,"f bytes it read. This; implies that some bytes within the value may be ``undef`` **without**; the entire value being ``undef``. Note that this only defines the; semantics of the operation; it doesn't mean that targets will emit more; than one instruction to read the series of bytes. Note that in cases where none of the atomic intrinsics are used, this; model places only one restriction on IR transformations on top of what; is required for single-threaded execution: introducing a store to a byte; which might not otherwise be stored is not allowed in general.; (Specifically, in the case where another thread might write to and read; from an address, introducing a store can change a load that may see; exactly one write into a load that may see multiple writes.). .. _ordering:. Atomic Memory Ordering Constraints; ----------------------------------. Atomic instructions (:ref:`cmpxchg <i_cmpxchg>`,; :ref:`atomicrmw <i_atomicrmw>`, :ref:`fence <i_fence>`,; :ref:`atomic load <i_load>`, and :ref:`atomic store <i_store>`) take; ordering parameters that determine which other atomic instructions on; the same address they *synchronize with*. These semantics implement; the Java or C++ memory models; if these descriptions aren't precise; enough, check those specs (see spec references in the; :doc:`atomics guide <Atomics>`). :ref:`fence <i_fence>` instructions; treat these orderings somewhat differently since they don't take an; address. See that instruction's documentation for details. For a simpler introduction to the ordering constraints, see the; :doc:`Atomics`. ``unordered``; The set of values that can be read is governed by the happens-before; partial order. A value cannot be read unless some operation wrote; it. This is intended to provide a guarantee strong enough to model; Java's non-volatile shared variables. This ordering cannot be; specified for read-modify-write operations; it is not strong enough; to make them atomic in any interesting way.; ``monotonic``; In addition ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:152400,load,load,152400,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"f cases, memory accesses are prefixed with a call to; an outlined instruction sequence that verifies the tags. The code size; and performance overhead of the call is reduced by using a custom calling; convention that. * preserves most registers, and; * is specialized to the register containing the address, and the type and; size of the memory access. Currently, the following sequence is used:. .. code-block:: none. // int foo(int *a) { return *a; }; // clang -O2 --target=aarch64-linux-android30 -fsanitize=hwaddress -S -o - load.c; [...]; foo:; stp x30, x20, [sp, #-16]!; adrp x20, :got:__hwasan_shadow // load shadow address from GOT into x20; ldr x20, [x20, :got_lo12:__hwasan_shadow]; bl __hwasan_check_x0_2_short_v2 // call outlined tag check; // (arguments: x0 = address, x20 = shadow base;; // ""2"" encodes the access type and size); ldr w0, [x0] // inline load; ldp x30, x20, [sp], #16; ret. [...]; __hwasan_check_x0_2_short_v2:; sbfx x16, x0, #4, #52 // shadow offset; ldrb w16, [x20, x16] // load shadow tag; cmp x16, x0, lsr #56 // extract address tag, compare with shadow tag; b.ne .Ltmp0 // jump to short tag handler on mismatch; .Ltmp1:; ret; .Ltmp0:; cmp w16, #15 // is this a short tag?; b.hi .Ltmp2 // if not, error; and x17, x0, #0xf // find the address's position in the short granule; add x17, x17, #3 // adjust to the position of the last byte loaded; cmp w16, w17 // check that position is in bounds; b.ls .Ltmp2 // if not, error; orr x16, x0, #0xf // compute address of last byte of granule; ldrb w16, [x16] // load tag from it; cmp x16, x0, lsr #56 // compare with pointer tag; b.eq .Ltmp1 // if matches, continue; .Ltmp2:; stp x0, x1, [sp, #-256]! // save original x0, x1 on stack (they will be overwritten); stp x29, x30, [sp, #232] // create frame record; mov x1, #2 // set x1 to a constant indicating the type of failure; adrp x16, :got:__hwasan_tag_mismatch_v2 // call runtime function to save remaining registers and report error; ldr x16, [x16, :got_lo12:__hwasan_tag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst:4247,load,load,4247,interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HardwareAssistedAddressSanitizerDesign.rst,1,['load'],['load']
Performance,"f command line options. .. option:: -o <filename>. Specify the output filename. .. option:: -S. Write output in LLVM intermediate language (instead of bitcode). .. option:: -{passname}. :program:`opt` provides the ability to run any of LLVM's optimization or; analysis passes in any order. The :option:`-help` option lists all the passes; available. The order in which the options occur on the command line are the; order in which they are executed (within pass constraints). .. option:: -strip-debug. This option causes opt to strip debug information from the module before; applying other optimizations. It is essentially the same as `-strip`; but it ensures that stripping of debug information is done first. .. option:: -verify-each. This option causes opt to add a verify pass after every pass otherwise; specified on the command line (including `-verify`). This is useful; for cases where it is suspected that a pass is creating an invalid module but; it is not clear which pass is doing it. .. option:: -stats. Print statistics. .. option:: -time-passes. Record the amount of time needed for each pass and print it to standard; error. .. option:: -debug. If this is a debug build, this option will enable debug printouts from passes; which use the ``LLVM_DEBUG()`` macro. See the `LLVM Programmer's Manual; <../ProgrammersManual.html>`_, section ``#DEBUG`` for more information. .. option:: -load=<plugin>. Load the dynamic object ``plugin``. This object should register new; optimization or analysis passes. Once loaded, the object will add new command; line options to enable various optimizations or analyses. To see the new; complete list of optimizations, use the :option:`-help` and :option:`-load`; options together. For example:. .. code-block:: sh. opt -load=plugin.so -help. .. option:: -print-passes. Print all available passes and exit. EXIT STATUS; -----------. If :program:`opt` succeeds, it will exit with 0. Otherwise, if an error; occurs, it will exit with a non-zero value.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst:2659,load,load,2659,interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,7,"['load', 'optimiz']","['load', 'loaded', 'optimization', 'optimizations']"
Performance,"f complexity,; higher compile times may be encountered. ## Appendix B - Where does AD Logic Implementation reside?. Following classes provide several Helper Functions to translate existing logic; into AD-supported logic. a - RooFit::Detail::CodeSquashContext. b - RooFuncWrapper. ### a. RooFit::Detail::CodeSquashContext. > [roofit/roofitcore/inc/RooFit/Detail/CodeSquashContext.h](https://github.com/root-project/root/blob/master/roofit/roofitcore/inc/RooFit/Detail/CodeSquashContext.h). It handles how to create a C++ function out of the compute graph (which is; created with different RooFit classes). This C++ function will be independent; of these RooFit classes. RooFit::Detail::CodeSquashContext helps traverse the compute graph received from RooFit and; then it translates that into a single piece of code (a C++ function), that can; then be differentiated using Clad. It also helps evaluate the model. In RooFit, evaluation is done using the 'evaluate()' function. It also; performs a lot of book-keeping, caching, etc. that is required for RooFit (but; not necessarily for AD). A new `translate()` function is added to RooFit classes that includes a call; to this `evaluate()` function. `translate()` helps implement the Code; Squashing logic. All RooFit classes that should support AD need to use this; function. It creates a string of code, which is then just-in-time compiled; using Cling (C++ interpreter for ROOT). For each of the `translate()`; functions, it is important to call `addResult()` since this is what enables; the squashing to happen. #### Helper Functions. - **RooFit::Detail::CodeSquashContext**: this class maintains the context for squashing of; RooFit models into code. It keeps track of the results of various; expressions to avoid redundant calculations. - **Loop Scopes()**: `beginloop()` and `endloop()` are used to create a scope; for iterating over vector observables (collections of data). This is; especially useful when dealing with data that comes in sets or",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:30544,perform,performs,30544,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,1,['perform'],['performs']
Performance,"f the algorithm and check the result. The MIR and; FileCheck directives can be embedded using strings so you still have access to; the convenience available in llvm-lit. Debugging; ---------. One debugging technique that's proven particularly valuable is to use the; BlockExtractor to extract basic blocks into new functions. This can be used; to track down correctness bugs and can also be used to track down performance; regressions. It can also be coupled with function attributes to disable; GlobalISel for one or more of the extracted functions. .. image:: block-extract.png. The command to do the extraction is:. .. code-block:: shell. ./bin/llvm-extract-o--S -bfoo:bb1;bb4<input>>extracted.ll. This particular example extracts two basic blocks from a function named ``foo``.; The new LLVM-IR can then be modified to add the ``failedISel`` attribute to the; extracted function containing bb4 to make that function use SelectionDAG. This can prevent some optimizations as GlobalISel is generally able to work on a; single function at a time. This technique can be repeated for different; combinations of basic blocks until you have identified the critical blocks; involved in a bug. Once the critical blocks have been identified, you can further increase the; resolution to the critical instructions by splitting the blocks like from:. .. code-block:: none. bb1:; ... instructions group 1 ...; ... instructions group 2 ... into:. .. code-block:: none. bb1:; ... instructions group 1 ...; br %bb2. bb2:; ... instructions group 2 ... and then repeating the process for the new blocks. It's also possible to use this technique in a mode where the main function; is compiled with GlobalISel and the extracted basic blocks are compiled with; SelectionDAG (or the other way around) to leverage the existing quality of; another code generator to track down bugs. This technique can also be used to; improve the similarity between fast and slow code when tracking down performance; regressions a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst:5884,optimiz,optimizations,5884,interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Pipeline.rst,1,['optimiz'],['optimizations']
Performance,"f the countPath call. We keep track of the; number of iterations and the number of paths. We only run this; version 30 or 40 times. Find the BBs that total 90% or more of execution, and aggregate them; together to form our trace. But we do not allow more than 5 paths; if; we have more than 5 we take the ones that are executed the most. We; verify our assumption that we picked a hot back-edge in first-level; instrumentation, by making sure that the number of times we took an; exit edge from the hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3116,queue,queue,3116,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['queue'],['queue']
Performance,"f the enclosing module. .. parsed-literal::. *config-macros-declaration*:; ``config_macros`` *attributes*:sub:`opt` *config-macro-list*:sub:`opt`. *config-macro-list*:; *identifier* (',' *identifier*)*. Each *identifier* in the *config-macro-list* specifies the name of a macro. The compiler is required to maintain different variants of the given module for differing definitions of any of the named macros. A *config-macros-declaration* shall only be present on a top-level module, i.e., a module that is not nested within an enclosing module. The ``exhaustive`` attribute specifies that the list of macros in the *config-macros-declaration* is exhaustive, meaning that no other macro definition is intended to have an effect on the API of that module. .. note::. The ``exhaustive`` attribute implies that any macro definitions; for macros not listed as configuration macros should be ignored; completely when building the module. As an optimization, the; compiler could reduce the number of unique module variants by not; considering these non-configuration macros. This optimization is not; yet implemented in Clang. A translation unit shall not import the same module under different definitions of the configuration macros. .. note::. Clang implements a weak form of this requirement: the definitions; used for configuration macros are fixed based on the definitions; provided by the command line. If an import occurs and the definition; of any configuration macro has changed, the compiler will produce a; warning (under the control of ``-Wconfig-macros``). **Example:** A logging library might provide different API (e.g., in the form of different definitions for a logging macro) based on the ``NDEBUG`` macro setting:. .. parsed-literal::. module MyLogger {; umbrella header ""MyLogger.h""; config_macros [exhaustive] NDEBUG; }. Conflict declarations; ~~~~~~~~~~~~~~~~~~~~~; A *conflict-declaration* describes a case where the presence of two different modules in the same translation unit is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:47304,optimiz,optimization,47304,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['optimiz'],['optimization']
Performance,"f the required module is already available, will load that representation directly. Thus, a module's headers will only be parsed once per language configuration, rather than once per translation unit that uses the module. Modules maintain references to each of the headers that were part of the module build. If any of those headers changes, or if any of the modules on which a module depends change, then the module will be (automatically) recompiled. The process should never require any user intervention. Command-line parameters; -----------------------; ``-fmodules``; Enable the modules feature. ``-fbuiltin-module-map``; Load the Clang builtins module map file. (Equivalent to ``-fmodule-map-file=<resource dir>/include/module.modulemap``). ``-fimplicit-module-maps``; Enable implicit search for module map files named ``module.modulemap`` and similar. This option is implied by ``-fmodules``. If this is disabled with ``-fno-implicit-module-maps``, module map files will only be loaded if they are explicitly specified via ``-fmodule-map-file`` or transitively used by another module map file. ``-fmodules-cache-path=<directory>``; Specify the path to the modules cache. If not provided, Clang will select a system-appropriate default. ``-fno-autolink``; Disable automatic linking against the libraries associated with imported modules. ``-fmodules-ignore-macro=macroname``; Instruct modules to ignore the named macro when selecting an appropriate module variant. Use this for macros defined on the command line that don't affect how modules are built, to improve sharing of compiled module files. ``-fmodules-prune-interval=seconds``; Specify the minimum delay (in seconds) between attempts to prune the module cache. Module cache pruning attempts to clear out old, unused module files so that the module cache itself does not grow without bound. The default delay is large (604,800 seconds, or 7 days) because this is an expensive operation. Set this value to 0 to turn off pruning. ``-fmodu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:14804,load,loaded,14804,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loaded']
Performance,"f the successors, we forward the edge; from the predecessor to the successor by duplicating the contents of this; block. An example of when this can occur is code like this:. .. code-block:: c++. if () { ...; X = 4;; }; if (X < 3) {. In this case, the unconditional branch at the end of the first if can be; revectored to the false side of the second if. .. _passes-lcssa:. ``lcssa``: Loop-Closed SSA Form Pass; ------------------------------------. This pass transforms loops by placing phi nodes at the end of the loops for all; values that are live across the loop boundary. For example, it turns the left; into the right code:. .. code-block:: c++. for (...) for (...); if (c) if (c); X1 = ... X1 = ...; else else; X2 = ... X2 = ...; X3 = phi(X1, X2) X3 = phi(X1, X2); ... = X3 + 4 X4 = phi(X3); ... = X4 + 4. This is still valid LLVM; the extra phi nodes are purely redundant, and will be; trivially eliminated by ``InstCombine``. The major benefit of this; transformation is that it makes many other loop optimizations, such as; ``LoopUnswitch``\ ing, simpler. You can read more in the; :ref:`loop terminology section for the LCSSA form <loop-terminology-lcssa>`. .. _passes-licm:. ``licm``: Loop Invariant Code Motion; ------------------------------------. This pass performs loop invariant code motion, attempting to remove as much; code from the body of a loop as possible. It does this by either hoisting code; into the preheader block, or by sinking code to the exit blocks if it is safe.; This pass also promotes must-aliased memory locations in the loop to live in; registers, thus hoisting and sinking ""invariant"" loads and stores. Hoisting operations out of loops is a canonicalization transform. It enables; and simplifies subsequent optimizations in the middle-end. Rematerialization; of hoisted instructions to reduce register pressure is the responsibility of; the back-end, which has more accurate information about register pressure and; also handles other optimizations than LICM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:23238,optimiz,optimizations,23238,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimizations']
Performance,"f the vector-of-pointers-typed operand; of the ``getelementptr``.; - The result value of a ``bitcast`` is *based* on the operand of the; ``bitcast``.; - A pointer value formed by an ``inttoptr`` is *based* on all pointer; values that contribute (directly or indirectly) to the computation of; the pointer's value.; - The ""*based* on"" relationship is transitive. Note that this definition of *""based""* is intentionally similar to the; definition of *""based""* in C99, though it is slightly weaker. LLVM IR does not associate types with memory. The result type of a; ``load`` merely indicates the size and alignment of the memory from; which to load, as well as the interpretation of the value. The first; operand type of a ``store`` similarly only indicates the size and; alignment of the store. Consequently, type-based alias analysis, aka TBAA, aka; ``-fstrict-aliasing``, is not applicable to general unadorned LLVM IR.; :ref:`Metadata <metadata>` may be used to encode additional information; which specialized optimization passes may use to implement type-based; alias analysis. .. _pointercapture:. Pointer Capture; ---------------. Given a function call and a pointer that is passed as an argument or stored in; the memory before the call, a pointer is *captured* by the call if it makes a; copy of any part of the pointer that outlives the call.; To be precise, a pointer is captured if one or more of the following conditions; hold:. 1. The call stores any bit of the pointer carrying information into a place,; and the stored bits can be read from the place by the caller after this call; exits. .. code-block:: llvm. @glb = global ptr null; @glb2 = global ptr null; @glb3 = global ptr null; @glbi = global i32 0. define ptr @f(ptr %a, ptr %b, ptr %c, ptr %d, ptr %e) {; store ptr %a, ptr @glb ; %a is captured by this call. store ptr %b, ptr @glb2 ; %b isn't captured because the stored value is overwritten by the store below; store ptr null, ptr @glb2. store ptr %c, ptr @glb3; call void @g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:143770,optimiz,optimization,143770,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"f the; '``value``' operand as specified by the :ref:`datalayout; string<langref_datalayout>` is used instead. Examples:; """""""""""""""""". .. code-block:: text. call void @llvm.vp.store.v8i8.p0(<8 x i8> %val, ptr align 4 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, the call above is lane-wise equivalent to the call below. call void @llvm.masked.store.v8i8.p0(<8 x i8> %val, ptr %ptr, i32 4, <8 x i1> %mask). .. _int_experimental_vp_strided_load:. '``llvm.experimental.vp.strided.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.experimental.vp.strided.load.v4f32.i64(ptr %ptr, i64 %stride, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.experimental.vp.strided.load.nxv2i16.i64(ptr %ptr, i64 %stride, <vscale x 2 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, scalar values from; memory locations evenly spaced apart by '``stride``' number of bytes, starting from '``ptr``'. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the stride; value expressed in bytes. The third operand is a vector of boolean values; with the same number of elements as the return type. The fourth is the explicit; vector length of the operation. The base pointer underlying type matches the type of the scalar; elements of the return operand. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.experimental.vp.strided.load``' intrinsic loads, into a vector, multiple scalar; values from memory in the same way as the :ref:`llvm.vp.gather <int_vp_gather>` intrinsic,; where the vector of pointers is in the form:. ``%ptrs = <%ptr, %ptr + %stride, %ptr + 2 * %stride, ... >``,. with '``ptr``' previously casted to a pointer '``i8``', '``stride``' always interpreted as a signed; integer and all ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:787500,load,load,787500,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],"['load', 'loads']"
Performance,"f their constructors, for example; ```c++; auto myHisto = myTdf.Histo1D({""histName"", ""histTitle"", 64, 0, 128}, ""myColumn"");; ```; or; ```c++; auto myHistoCustomBinning = myTdf.Histo1D({""histName"", ""histTitle"", 64, binEdges}, ""myColumn"");; ```; Models can be created as stand alone objects:; ```c++; TDF::TH1DModel myModel {""histName"", ""histTitle"", 64, binEdges};; auto myHistoCustomBinning = myTdf.Histo1D(myModel, ""myColumn"");; ```; - pyROOT users can now easily specify parameters for the TDF histograms and profiles thanks to the newly introduced tuple-initialization; ```python; myHisto = myTdf.Histo1D(('histName', 'histTitle', 64, 0, 128), 'myColumn'); ```; - Add support for friend trees and chains. Just add the friends before passing the tree/chain to TDataFrame's constructor and refer to friend branches as usual. #### Fixes; - Fixed race condition: concurrent deletion of TTreeReader/TTreeReaderValue; - Fixed reading of c-style arrays from jitted transformations and actions; - Fixed writing of c-style arrays with `Snapshot`; - Improved checks for column name validity (throw if column does not exist and if `Define`d column overrides an already existing column). #### Other changes; - Improved documentation; - TDF now avoids performing virtual calls for parts of the analysis that are not jitted; - Removed ""custom column"" nodes from the internal functional graph therewith optimising its traversal; - Improvements in Cling drastically enhanced scaling and performance of TDF jitted code; - Test coverage has been increased with the introduction of google tests; - Interface change: users must now use TDF::TArrayBranch rather than std::array\_view to specify that the column being read is a c-style array TTree branch; - Interface change: `Min` and `Max` now return results as the same type specified as template parameter, or double if no template parameter was specified. ## Histogram Libraries; - Histogram-based fits are implicitly parallelized.; - Added new options to the histog",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:14728,race condition,race condition,14728,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,2,"['concurren', 'race condition']","['concurrent', 'race condition']"
Performance,"f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1990,load,load,1990,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,2,['load'],['load']
Performance,"f32(%0, %c,; metadata <rounding mode>,; metadata <exception behavior>). except that it is unspecified whether rounding will be performed between the; multiplication and addition steps. Fusion is not guaranteed, even if the target; platform supports it.; If a fused multiply-add is required, the corresponding; :ref:`llvm.experimental.constrained.fma <int_fma>` intrinsic function should be; used instead.; This never sets errno, just as '``llvm.experimental.constrained.fma.*``'. Constrained libm-equivalent Intrinsics; --------------------------------------. In addition to the basic floating-point operations for which constrained; intrinsics are described above, there are constrained versions of various; operations which provide equivalent behavior to a corresponding libm function.; These intrinsics allow the precise behavior of these operations with respect to; rounding mode and exception behavior to be controlled. As with the basic constrained floating-point intrinsics, the rounding mode; and exception behavior arguments only control the behavior of the optimizer.; They do not change the runtime floating-point environment. '``llvm.experimental.constrained.sqrt``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.experimental.constrained.sqrt(<type> <op1>,; metadata <rounding mode>,; metadata <exception behavior>). Overview:; """""""""""""""""". The '``llvm.experimental.constrained.sqrt``' intrinsic returns the square root; of the specified value, returning the same value as the libm '``sqrt``'; functions would, but without setting ``errno``. Arguments:; """""""""""""""""""". The first argument and the return type are floating-point numbers of the same; type. The second and third arguments specify the rounding mode and exception; behavior as described above. Semantics:; """""""""""""""""""". This function returns the nonnegative square root of the specified value.; If the value is less than negative zero, a floating-point exception occurs; and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:889913,optimiz,optimizer,889913,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"f32(<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, metadata <condition code>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i1> @llvm.vp.fcmp.v256f64(<256 x double> <left_op>, <256 x double> <right_op>, metadata <condition code>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.fcmp``' intrinsic returns a vector of boolean values based on; the comparison of its operands. The operation has a mask and an explicit vector; length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.fcmp``' intrinsic takes the two values to compare as its first; and second operands. These two values must be vectors of :ref:`floating-point; <t_floating>` types.; The return type is the result of the comparison. The return type must be a; vector of :ref:`i1 <t_integer>` type. The fourth operand is the vector mask.; The return type, the values to compare, and the vector mask have the same; number of elements. The third operand is the condition code indicating the kind; of comparison to perform. It must be a metadata string with :ref:`one of the; supported floating-point condition code values <fcmp_md_cc>`. The fifth operand; is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fcmp``' compares its first two operands according to the; condition code given as the third operand. The operands are compared element by; element on each enabled lane, where the semantics of the comparison are; defined :ref:`according to the condition code <fcmp_md_cc_sem>`. Masked-off; lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i1> @llvm.vp.fcmp.v4f32(<4 x float> %a, <4 x float> %b, metadata !""oeq"", <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fcmp oeq <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x i1> %t, <4 x i1> poison. .. _int_vp_icmp:. '``llvm.vp.icmp.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:818219,perform,perform,818219,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"f64.nxv2f64(<vscale x 4 x float> %vec, <vscale x 2 x float> %subvec, i64 <idx>). ; Insert fixed type into fixed type; declare <4 x double> @llvm.vector.insert.v4f64.v2f64(<4 x double> %vec, <2 x double> %subvec, i64 <idx>). Overview:; """""""""""""""""". The '``llvm.vector.insert.*``' intrinsics insert a vector into another vector; starting from a given index. The return type matches the type of the vector we; insert into. Conceptually, this can be used to build a scalable vector out of; non-scalable vectors, however this intrinsic can also be used on purely fixed; types. Scalable vectors can only be inserted into other scalable vectors. Arguments:; """""""""""""""""""". The ``vec`` is the vector which ``subvec`` will be inserted into.; The ``subvec`` is the vector that will be inserted. ``idx`` represents the starting element number at which ``subvec`` will be; inserted. ``idx`` must be a constant multiple of ``subvec``'s known minimum; vector length. If ``subvec`` is a scalable vector, ``idx`` is first scaled by; the runtime scaling factor of ``subvec``. The elements of ``vec`` starting at; ``idx`` are overwritten with ``subvec``. Elements ``idx`` through (``idx`` +; num_elements(``subvec``) - 1) must be valid ``vec`` indices. If this condition; cannot be determined statically but is false at runtime, then the result vector; is a :ref:`poison value <poisonvalues>`. '``llvm.vector.extract``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. ; Extract fixed type from scalable type; declare <4 x float> @llvm.vector.extract.v4f32.nxv4f32(<vscale x 4 x float> %vec, i64 <idx>); declare <2 x double> @llvm.vector.extract.v2f64.nxv2f64(<vscale x 2 x double> %vec, i64 <idx>). ; Extract scalable type from scalable type; declare <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> %vec, i64 <idx>). ; Extract fixed type from fixed type; declare <2 x double> @llvm.vector.extract.v2f64.v4f64(<4 x double> %vec, i64 <idx>",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:663730,scalab,scalable,663730,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,f:`amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table`. .. table:: AMDHSA Memory Model Code Sequences GFX90A; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX90A; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monoton,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:242935,load,load,242935,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"f:`amdgpu-amdhsa-sgpr-register-set-up-order-table`. .. table:: SGPR Register Set Up Order; :name: amdgpu-amdhsa-sgpr-register-set-up-order-table. ========== ========================== ====== ==============================; SGPR Order Name Number Description; (kernel descriptor enable of; field) SGPRs; ========== ========================== ====== ==============================; First Private Segment Buffer 4 See; (enable_sgpr_private :ref:`amdgpu-amdhsa-kernel-prolog-private-segment-buffer`.; _segment_buffer); then Dispatch Ptr 2 64-bit address of AQL dispatch; (enable_sgpr_dispatch_ptr) packet for kernel dispatch; actually executing.; then Queue Ptr 2 64-bit address of amd_queue_t; (enable_sgpr_queue_ptr) object for AQL queue on which; the dispatch packet was; queued.; then Kernarg Segment Ptr 2 64-bit address of Kernarg; (enable_sgpr_kernarg segment. This is directly; _segment_ptr) copied from the; kernarg_address in the kernel; dispatch packet. Having CP load it once avoids; loading it at the beginning of; every wavefront.; then Dispatch Id 2 64-bit Dispatch ID of the; (enable_sgpr_dispatch_id) dispatch packet being; executed.; then Flat Scratch Init 2 See; (enable_sgpr_flat_scratch :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`.; _init); then Preloaded Kernargs N/A See; (kernarg_preload_spec :ref:`amdgpu-amdhsa-kernarg-preload`.; _length); then Private Segment Size 1 The 32-bit byte size of a; (enable_sgpr_private single work-item's memory; _segment_size) allocation. This is the; value from the kernel; dispatch packet Private; Segment Byte Size rounded up; by CP to a multiple of; DWORD. Having CP load it once avoids; loading it at the beginning of; every wavefront. This is not used for; GFX7-GFX8 since it is the same; value as the second SGPR of; Flat Scratch Init. However, it; may be needed for GFX9-GFX11 which; changes the meaning of the; Flat Scratch Init value.; then Work-Group Id X 1 32-bit work-group id in X; (enable_sgpr_workgroup_id dimension of grid for",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:184855,load,load,184855,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],"['load', 'loading']"
Performance,"f:`code generation plugins <plugin>` - to; generate code and data structures which conforms to the *binary interface*; specified by the *runtime library*. This is similar to the relationship between; LLVM and DWARF debugging info, for example. The difference primarily lies in; the lack of an established standard in the domain of garbage collection --- thus; the need for a flexible extension mechanism. The aspects of the binary interface with which LLVM's GC support is; concerned are:. * Creation of GC safepoints within code where collection is allowed to execute; safely. * Computation of the stack map. For each safe point in the code, object; references within the stack frame must be identified so that the collector may; traverse and perhaps update them. * Write barriers when storing object references to the heap. These are commonly; used to optimize incremental scans in generational collectors. * Emission of read barriers when loading object references. These are useful; for interoperating with concurrent collectors. There are additional areas that LLVM does not directly address:. * Registration of global roots with the runtime. * Registration of stack map entries with the runtime. * The functions used by the program to allocate memory, trigger a collection,; etc. * Computation or compilation of type maps, or registration of them with the; runtime. These are used to crawl the heap for object references. In general, LLVM's support for GC does not include features which can be; adequately addressed with other features of the IR and does not specify a; particular binary interface. On the plus side, this means that you should be; able to integrate LLVM with an existing runtime. On the other hand, it can; have the effect of leaving a lot of work for the developer of a novel; language. We try to mitigate this by providing built in collector strategy; descriptions that can work with many common collector designs and easy; extension points. If you don't already have a speci",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:7051,concurren,concurrent,7051,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['concurren'],['concurrent']
Performance,"f; optimizations are disabled (``-O0``), if stack objects already exist (for; locals, etc.), or if there are any function calls. - Otherwise, four high numbered SGPRs beginning at a four-aligned SGPR index; are reserved for the tentative scratch V#. These will be used if it is; determined that spilling is needed. - If no use is made of the tentative scratch V#, then it is unreserved,; and the register count is determined ignoring it.; - If use is made of the tentative scratch V#, then its register numbers; are shifted to the first four-aligned SGPR index after the highest one; allocated by the register allocator, and all uses are updated. The; register count includes them in the shifted location.; - In either case, if the processor has the SGPR allocation bug, the; tentative allocation is not shifted or unreserved in order to ensure; the register count is higher to workaround the bug. .. note::. This approach of using a tentative scratch V# and shifting the register; numbers if used avoids having to perform register allocation a second; time if the tentative V# is eliminated. This is more efficient and; avoids the problem that the second register allocation may perform; spilling which will fail as there is no longer a scratch V#. When the kernel prolog code is being emitted it is known whether the scratch V#; described above is actually used. If it is, the prolog code must set it up by; copying the Private Segment Buffer to the scratch V# registers and then adding; the Private Segment Wavefront Offset to the queue base address in the V#. The; result is a V# with a base address pointing to the beginning of the wavefront; scratch backing memory. The Private Segment Buffer is always requested, but the Private Segment; Wavefront Offset is only requested if it is used (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). .. _amdgpu-amdhsa-memory-model:. Memory Model; ~~~~~~~~~~~~. This section describes the mapping of the LLVM memory model onto AMDGPU machine; code",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:199918,perform,perform,199918,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['perform']
Performance,"fault, only; loops marked with the pragma are considered. .. code-block:: c++. #pragma clang loop distribute(enable); for (i = 0; i < N; ++i) {; S1: A[i + 1] = A[i] + B[i];; S2: C[i] = D[i] * E[i];; }. This loop will be split into two loops between statements S1 and S2. The; second loop containing S2 will be vectorized. Loop Distribution is currently not enabled by default in the optimizer because; it can hurt performance in some cases. For example, instruction-level; parallelism could be reduced by sequentializing the execution of the; statements S1 and S2 above. If Loop Distribution is turned on globally with; ``-mllvm -enable-loop-distribution``, specifying ``distribute(disable)`` can; be used the disable it on a per-loop basis. Additional Information; ----------------------. For convenience multiple loop hints can be specified on a single line. .. code-block:: c++. #pragma clang loop vectorize_width(4) interleave_count(8); for(...) {; ...; }. If an optimization cannot be applied any hints that apply to it will be ignored.; For example, the hint ``vectorize_width(4)`` is ignored if the loop is not; proven safe to vectorize. To identify and diagnose optimization issues use; `-Rpass`, `-Rpass-missed`, and `-Rpass-analysis` command line options. See the; user guide for details. Extensions to specify floating-point flags; ====================================================. The ``#pragma clang fp`` pragma allows floating-point options to be specified; for a section of the source code. This pragma can only appear at file scope or; at the start of a compound statement (excluding comments). When using within a; compound statement, the pragma is active within the scope of the compound; statement. Currently, the following settings can be controlled with this pragma:. ``#pragma clang fp reassociate`` allows control over the reassociation; of floating point expressions. When enabled, this pragma allows the expression; ``x + (y + z)`` to be reassociated as ``(x + y) + z``.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:169216,optimiz,optimization,169216,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"fe variables. Transform Passes; ================. This section describes the LLVM Transform Passes. ``adce``: Aggressive Dead Code Elimination; ------------------------------------------. ADCE aggressively tries to eliminate code. This pass is similar to :ref:`DCE; <passes-dce>` but it assumes that values are dead until proven otherwise. This; is similar to :ref:`SCCP <passes-sccp>`, except applied to the liveness of; values. ``always-inline``: Inliner for ``always_inline`` functions; ----------------------------------------------------------. A custom inliner that handles only functions that are marked as ""always; inline"". ``argpromotion``: Promote 'by reference' arguments to scalars; -------------------------------------------------------------. This pass promotes ""by reference"" arguments to be ""by value"" arguments. In; practice, this means looking for internal functions that have pointer; arguments. If it can prove, through the use of alias analysis, that an; argument is *only* loaded, then it can pass the value into the function instead; of the address of the value. This can cause recursive simplification of code; and lead to the elimination of allocas (especially in C++ template code like; the STL). This pass also handles aggregate arguments that are passed into a function,; scalarizing them if the elements of the aggregate are only loaded. Note that; it refuses to scalarize aggregates which would require passing in more than; three operands to the function, because passing thousands of operands for a; large array or structure is unprofitable!. Note that this transformation could also be done for arguments that are only; stored to (returning the value instead), but does not currently. This case; would be best handled when and if LLVM starts supporting multiple return values; from functions. ``block-placement``: Profile Guided Basic Block Placement; ---------------------------------------------------------. This pass is a very simple profile guided basic block pl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:12689,load,loaded,12689,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loaded']
Performance,"feModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:. .. code-block:: c++. // Maximize concurrency opportunities by loading every module on a; // separate context.; for (const auto &IRPath : IRPaths) {; auto Ctx = std::make_unique<LLVMContext>();; auto M = std::make_unique<LLVMContext>(""M"", *Ctx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(M), std::move(Ctx)));; }. Clients who plan to run single-threaded may choose to save memory by loading; all modules on the same context:. .. code-block:: c++. // Save memory by using one context for all Modules:; ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());; for (const auto &IRPath : IRPaths) {; ThreadSafeModule TSM(parsePath(IRPath, *TSCtx.getContext()), TSCtx);; CompileLayer.add(MainJD, ThreadSafeModule(std::move(TSM));; }. .. _ProcessAndLibrarySymbols:. How to Add Process and Library Symbols to JITDylibs; ===================================================. JIT'd code may need to access symbols in the host program or in supporting; libraries. The best way to enable this is to reflect these symbols into your; JITDylibs so that they appear the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:30818,concurren,concurrency,30818,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,2,"['concurren', 'load']","['concurrency', 'loading']"
Performance,"features; - Add `Alias`, a facility to specify an alternative name for a given column: `auto histo = mytdf.Alias(""myAlias"", ""myColumn"").Histo1D(""myAlias"");`. Especially useful for pyROOT users to deal with column names that are not valid C++ identifiers (e.g. `Filter(""1branch > 0"") --> Alias(""1branch"", ""branch1"").Filter(""branch1 > 0"")`.; - Add `Cache`, a facility to cache `TDataFrame`s in memory. All or some columns can be cached. Two versions of the method are proposed: one which allows to explicitly list the types of the columns and another one allowing to let the system infer them (the same mechanism of the `Snapshot` method). Only columns containing instances of classes which have a copy constructor can be cached.; - Add `DefineSlot`, a `Define` transformation that is aware of the multi-threading slot where the workload is executed; - Add `DefineSlotEntry`, a `Define` transformation that is aware of the multi-threading slot and of the current entry number; - Add `GetColumnsNames`: users can now get the names of the available columns coming from trees, data sources or `Define`d columns; - Add `OnPartialResult` and `OnPartialResultSlot`: users can now register one or more functions to be executed on partial results of TDF actions during the event loop.; This mechanism is meant to be used to inspect partial results of the analysis or print useful debug information.; For example, both in single- and multi-thread event loops, one can draw a result histogram and update the canvas every 100 entries like this:; ```c++; auto h = tdf.Histo1D(""x"");; TCanvas c(""c"",""x hist"");; h.OnPartialResult(100, [&c](TH1D &h_) { c.cd(); h_.Draw(); c.Update(); });; ```; See the tutorials for more examples.; - Add `Sum`, an action that sums all values of a column for the processed entries; - The new TDataSource interface allows developers to pipe any kind of columnar data format into TDataFrame. Two example data sources have been provided: the TRootDS and the TTrivialDS. The former allows t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:11405,multi-thread,multi-threading,11405,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,2,['multi-thread'],['multi-threading']
Performance,"fect of any preceding dbg.values for (any; overlapping fragments of) the specified variable.; * The dbg.value's position in the IR defines where in the instruction stream; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a state that would never have existed; in the execution of the unoptimized program, given the same input. Doing so; risks misleading the developer by reporting a state that does not exist,; damaging their understanding of the optimized program and undermining their; trust in the debugger. Sometimes perfectly preserving variable locations is not possible, often when a; redundant calculation is optimized out. In such cases, a ``llvm.dbg.value``; with operand ``poison`` should be used, to terminate earlier variable locations; and let the debugger present ``optimized out`` to the developer. Withholding; these potentially stale variable values from the developer diminishes the; amount of available debug information, but increases the reliability of the; remaining information. To illustrate some potential issues, consider the following example:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); br i1 %cond, label %truebr, label %falsebr; truebr:; %tval = add i32 %bar, 1; call @llvm.dbg.value(metadata i32 %tval, metadata !1, metadata !2); %g1 = call i32 @gazonk(); br label %exit; falsebr:; %fval = add i32 %bar, 2; call @llvm.dbg.value(metadata i32 %fval, metadata !1, metadata !2); %g2 = call i32 @gazonk(); br label %exit; exit:; %merge = phi [ %tval, %tr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:19464,optimiz,optimized,19464,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"fective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:10162,optimiz,optimized,10162,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"fer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:270775,load,load,270775,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"fer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:370175,load,load,370175,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ference between reference guide `libHist` and; `libHistPainter` is that the former needs to be explicitly linked and; the latter will be loaded automatically at runtime when ROOT needs it,; by means of the Plugin Manager. plugin manager. In the Figure 1-2, the libraries represented by green boxes outside of; the core are loaded via the plugin manager plugin manager or; equivalent techniques, while the white ones are not. Of course, if one; wants to access a plugin library directly, it has to be explicitly; linked. An example of a plugin library is `libMinuit`. To create and; fill histograms you need to link `libHist.so`. If the code has a call; to fit the histogram, the ""fitter"" will dynamically load libMinuit if; it is not yet loaded. #### Plugins: Runtime Library Dependencies for Linking. plugin manager The Plugin Manager **`TPluginManager`** allows; postponing library dependencies to runtime: a plugin library will only; be loaded when it is needed. Non-plugins will need to be linked, and; are thus loaded at start-up. Plugins are defined by a base class (e.g.; **`TFile`**) that will be implemented in a plugin, a tag used to; identify the plugin (e.g. `^rfio:` as part of the protocol string),; the plugin class of which an object will be created; (e.g. **`TRFIOFile`**), the library to be loaded (in short; `libRFIO.so` to RFIO), and the constructor to be called (e.g.; ""`TRFIOFile()`""). This can be specified in the `.rootrc` which already; contains many plugin definitions, or by calls to; `gROOT->GetPluginManager()->AddHandler()`. #### Library AutoLoading. When using a class in Cling, e.g. in an interpreted source file, ROOT; will automatically load the library that defines this class. On; start-up, ROOT parses all files ending on `.rootmap` rootmap that are; in one of the `$LD_LIBRARY_PATH` (or `$DYLD_LIBRARY_PATH` for `MacOS`,; or `$PATH` for `Windows`). They contain class names and the library; names that the class depends on. After reading them, ROOT knows which; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:19188,load,loaded,19188,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,1,['load'],['loaded']
Performance,"ferent SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_inv sc0`` is required which will invalidate; the L1 cache. * A ``buffer_inv sc0`` is required to invalidate the L1 cac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:285632,perform,performed,285632,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"ferent SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_wbinvl1_vol`` is required as described in; the following item. * A ``buffer_wbinvl1_vol`` is required for coherence be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:235484,perform,performed,235484,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"ferent work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire, release and sequential consistency.; * The L1 caches use an L2 cache shared by all SAs on the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each L1 quadrant of a single SA accesses a different L2 channel. Each L1; quadrant has a separate request queue per L2 channel. Therefore, the vector; and scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different SAs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0) &",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:338185,perform,performed,338185,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"fetime.start``' call marks the object as alive, but it; does not change the address of the object. If ``ptr`` is a non-stack-allocated object, it does not point to the first; byte of the object or it is a stack object that is already alive, it simply; fills all bytes of the object with ``poison``. .. _int_lifeend:. '``llvm.lifetime.end``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.lifetime.end(i64 <size>, ptr nocapture <ptr>). Overview:; """""""""""""""""". The '``llvm.lifetime.end``' intrinsic specifies the end of a memory object's; lifetime. Arguments:; """""""""""""""""""". The first argument is a constant integer representing the size of the; object, or -1 if it is variable sized. The second argument is a pointer; to the object. Semantics:; """""""""""""""""""". If ``ptr`` is a stack-allocated object and it points to the first byte of the; object, the object is dead.; ``ptr`` is conservatively considered as a non-stack-allocated object if; the stack coloring algorithm that is used in the optimization pipeline cannot; conclude that ``ptr`` is a stack-allocated object. Calling ``llvm.lifetime.end`` on an already dead alloca is no-op. If ``ptr`` is a non-stack-allocated object or it does not point to the first; byte of the object, it is equivalent to simply filling all bytes of the object; with ``poison``. '``llvm.invariant.start``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The memory object can belong to any address space. ::. declare ptr @llvm.invariant.start.p0(i64 <size>, ptr nocapture <ptr>). Overview:; """""""""""""""""". The '``llvm.invariant.start``' intrinsic specifies that the contents of; a memory object will not change. Arguments:; """""""""""""""""""". The first argument is a constant integer representing the size of the; object, or -1 if it is variable sized. The second argument is a pointer; to the object. Semantics:; """""""""""""""""""". This intrinsic indicates that until an ``llvm.invariant.end`` that u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:863273,optimiz,optimization,863273,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"fidence belt and confidence intervals. The ConfidenceBelt class; is still under development, but the current version works fine for; producing ConfidenceIntervals. We are also working to make this class; work with parallelization approaches, which is not yet complete.; The FeldmanCousins class is a separate concrete implementation of the; IntervalCalculator interface. It uses the NeymanConstruction internally,; andenforcesspecific choices of the test statistic and ordering; principle to realize the Unified intervals described by Feldman and; Cousins in their paperPhys.Rev.D57:3873-3889,1998. In an extension to the technique discussed in Feldman and Cousins paper,; the FeldmanCousins class also performs a ""profile construction"" if their are nuisance parameters.; In this case, the parameters of interest are scanned in a regular grid. For each point in the grid; the calculator finds the best fit value of the nuisance parameters (given the data). The construction; is then only performed in this subspace of the parameters. As a result, the number of points in the; construction only scales in the number of parameters of interest, not in the number of nuisance parameters. Markov Chain Monte Carlo Interval; A flexible framework for Markov Chain Monte Carlo was added in this; release. The MCMCCalculator is a concrete implementation of the; IntervalCalculator interface. To use it one needs to specify the ProposalFunction.; There is a base class for ProposalFunctions and one concrete implementation: UniformProposal.; Support for other proposal functions will be added in the next release.; The MCMCCalculator scans the space of the parameters of interest and nuisance parameters and; produces a Bayesian posterior. In this version, the prior must be added to the model initially,; otherwise a flat prior is assumed. The MCMCCalculator returns an MCMCInterval, which produces; the smallest interval by taking a contour of the posterior. This first version only supports; 1,2, and 3",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:29253,perform,performed,29253,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,2,['perform'],['performed']
Performance,"file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59595,perform,performs,59595,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performs']
Performance,"file. MultiTargetRegression No False  Do regression with multiple targets. Nmin No 100  Number of events in cell required to split cell. MaxDepth No 0  Maximum depth of cell tree (0=unlimited). FillFoamWithOrigWeights No False  Fill foam with original or boost weights. UseYesNoCell No False  Return -1 or 1 for bkg or signal like events. DTLogic No None None, GiniIndex, MisClassificationError, CrossEntropy, GiniIndexWithLaplace, SdivSqrtSplusB Use decision tree algorithm to split cells. Kernel No None None, Gauss, LinNeighbors Kernel type used. TargetSelection No Mean Mean, Mpv Target selection method. Configuration options for MVA method :. Configuration options reference for MVA method: TMlpANN. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). NCycles No 200  Number of training cycles. HiddenLayers No N,N-1  Specification of hidden layer architecture (N stands for number of variables; any integers may also be used). ValidationFraction No 0.5  Fraction of events in training tree used for cross validation. LearningMethod No Stochastic Stochastic, Batch, SteepestDescent, RibierePolak, FletcherReeves, BFGS Learning method. Configuration options for setup an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:28154,perform,performed,28154,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"filing tests are very limited, and generating the profile takes a; significant amount of time, but it can result in a significant improvement in; the performance of the generated binaries. In addition to PGO profiling we also have limited support in-tree for generating; linker order files. These files provide the linker with a suggested ordering for; functions in the final binary layout. This can measurably speed up clang by; physically grouping functions that are called temporally close to each other.; The current tooling is only available on Darwin systems with ``dtrace(1)``. It; is worth noting that dtrace is non-deterministic, and so the order file; generation using dtrace is also non-deterministic. Options for Reducing Size; =========================. .. warning::; Any steps taken to reduce the binary size will come at a cost of runtime; performance in the generated binaries. The simplest and least significant way to reduce binary size is to set the; *CMAKE_BUILD_TYPE* variable to ``MinSizeRel``, which will set the compiler; optimization level to ``-Os`` which optimizes for binary size. This will have; both the least benefit to size and the least impact on performance. The most impactful way to reduce binary size is to dynamically link LLVM into; all the tools. This reduces code size by decreasing duplication of common code; between the LLVM-based tools. This can be done by setting the following two; CMake options to ``On``: *LLVM_BUILD_LLVM_DYLIB* and *LLVM_LINK_LLVM_DYLIB*. .. warning::; Distributions should never be built using the *BUILD_SHARED_LIBS* CMake; option. (:ref:`See the warning above for more explanation <shared_libs>`.). Relevant CMake Options; ======================. This section provides documentation of the CMake options that are intended to; help construct distributions. This is not an exhaustive list, and many; additional options are documented in the :doc:`CMake` page. Some key options; that are already documented include: *LLVM_TARGETS_TO_B",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:10179,optimiz,optimization,10179,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,2,['optimiz'],"['optimization', 'optimizes']"
Performance,"final stage, ROOT might be able to integrate the Name.pcm with the; shared library itself.; * Improved correctness in number of cases -- in a few cases ROOT is more; correct. In particular, when resolving global variables and function; declarations which are not part of the ROOT PCH.; * Enhanced symbol resolution mechanisms, bloom filters -- standard ROOT relies; on information in ROOTMAP files to react when the llvm JIT issues an; unresolved symbol callback. C++ Modules-aware ROOT relies on a behavior much; closer to the standard linker behavior. In particular, we start searching on; the LD_LIBRARY_PATH descending to the system libraries. The algorithm is very; efficient because it uses bloom filters[[5]]. This in turn allows ROOT symbol; to be extended to system libraries. ### Module Registration Approaches. The C++ modules system supports /*preloading*/ of all modules at startup time.; The current implementation of loading of C++ modules in clang has an overhead; and is between 40-60 MB depending on the ROOT configuration while there might; be 2x slowdown depending on the workflow. These issues are very likely to be; addressed by the LLVM community in midterm. Preloading of all C++ modules is semantically the closest to C++ behavior.; However, in order to achieve performance ROOT loads them on demand using; a global module index file. It has sufficient information to map a looked up; identifier to the module which contains the corresponding definition. Switching; back to preloading of all C++ modules is done by setting the `ROOT_USE_GMI`; environment variable to false.; ; ### Supported Platforms. We support all platforms with glibc++ versions: 5.2 onward. We support OSX from XCode 10 onward. ## Changes required by the users; * Self-contained header files -- every header file should be able to compile; on its own. For instance, `cat header.h header.h | gcc -fsyntax-only -xc++ -`.; This command concatenates `header.h` twice before compiling it to make sure; it has p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:13845,load,loading,13845,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loading']
Performance,"final, the resume and destroy branches should lead to the same; basic blocks. Example (normal suspend point):; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". .. code-block:: llvm. %0 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %0, label %suspend [i8 0, label %resume; i8 1, label %cleanup]. Example (final suspend point):; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""". .. code-block:: llvm. while.end:; %s.final = call i8 @llvm.coro.suspend(token none, i1 true); switch i8 %s.final, label %suspend [i8 0, label %trap; i8 1, label %cleanup]; trap:; call void @llvm.trap(); unreachable. Semantics:; """""""""""""""""""". If a coroutine that was suspended at the suspend point marked by this intrinsic; is resumed via `coro.resume`_ the control will transfer to the basic block; of the 0-case. If it is resumed via `coro.destroy`_, it will proceed to the; basic block indicated by the 1-case. To suspend, coroutine proceed to the; default label. If suspend intrinsic is marked as final, it can consider the `true` branch; unreachable and can perform optimizations that can take advantage of that fact. .. _coro.save:. 'llvm.coro.save' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; ::. declare token @llvm.coro.save(ptr <handle>). Overview:; """""""""""""""""". The '``llvm.coro.save``' marks the point where a coroutine need to update its; state to prepare for resumption to be considered suspended (and thus eligible; for resumption). It is illegal to merge two '``llvm.coro.save``' calls unless their; '``llvm.coro.suspend``' users are also merged. So '``llvm.coro.save``' is currently; tagged with the `no_merge` function attribute. Arguments:; """""""""""""""""""". The first argument points to a coroutine handle of the enclosing coroutine. Semantics:; """""""""""""""""""". Whatever coroutine state changes are required to enable resumption of; the coroutine from the corresponding suspend point should be done at the point; of `coro.save` intrinsic. Example:; """""""""""""""". Separate save and suspend points are necessary when a coroutine is us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:51583,perform,perform,51583,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"finition should start at. Note that a single memory definition can be; mapped multiple times. Using this annotation requires the subprocess; execution mode.; * `LLVM-EXEGESIS-SNIPPET-ADDRESS <address>` - This annotation allows for; setting the address where the beginning of the snippet to be executed will; be mapped in at. The address is given in hexadecimal. Note that the snippet; also includes setup code, so the instruction exactly at the specified; address will not be the first instruction in the snippet. Using this; annotation requires the subprocess execution mode. This is useful in; cases where the memory accessed by the snippet depends on the location; of the snippet, like RIP-relative addressing. EXAMPLE 1: benchmarking instructions; ------------------------------------. Assume you have an X86-64 machine. To measure the latency of a single; instruction, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-name=ADD64rr. Measuring the uop decomposition or inverse throughput of an instruction works similarly:. .. code-block:: bash. $ llvm-exegesis --mode=uops --opcode-name=ADD64rr; $ llvm-exegesis --mode=inverse_throughput --opcode-name=ADD64rr. The output is a YAML document (the default is to write to stdout, but you can; redirect the output to a file using `--benchmarks-file`):. .. code-block:: none. ---; key:; opcode_name: ADD64rr; mode: latency; config: ''; cpu_name: haswell; llvm_triple: x86_64-unknown-linux-gnu; num_repetitions: 10000; measurements:; - { key: latency, value: 1.0058, debug_string: '' }; error: ''; info: 'explicit self cycles, selecting one aliasing configuration.; Snippet:; ADD64rr R8, R8, R10; '; ... To measure the latency of all instructions for the host architecture, run:. .. code-block:: bash. $ llvm-exegesis --mode=latency --opcode-index=-1. EXAMPLE 2: benchmarking a custom code snippet; ---------------------------------------------. To measure the latency/uops of a custom piece of code, you can specify the; `snippets-f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:5000,throughput,throughput,5000,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['throughput'],['throughput']
Performance,"flist (exclusive of the decompression time).; Introduce TVirtualPerfStats::FileUnzipEvent to be able to keep track of the cost of unzipping and use this in TTreePerfStats and TBasket ... This give a good picture of where the time in unzip or in unstreaming; Add more clusters to the TTreeCache buffer until fBufferMinSize is hit to avoid severely underfilled buffer when; a low number of branches is selected/used.; When reading backwards, make sure to load a full (new) cluster and several other fixes to TTreeCache.; Reduce the memory used by a TTree in half. Refactor the code reading and writing the TBasket data.; A single transient buffer holding the compressed data is now managed by TTree (and could be made thread local); rather than having one per TBranch.; In TTree::Fill, call FlushBasket before calling OptimizeBaskets so that we have a correct; and accurate value of fTotBytes to use as the requested memory.; In TTree::OptimizeBasket enforces hard minimun for the basket size (no lower than the; estimate size of one entry in the branch and no lower than 8 bytes). TTree::Process. Add support for the flag TSelector::kAbortFile. TTree::Draw. The line width setting was missing in a few places.; Namely support the option 'a' for TGraphs in TTree::Draw (delegate the axis management to the TGraph object). TTreeSQL. Allow TTreeSQL to see temporary tables.; Avoid creating the unnecessary array fEntryOffset ... which when its content is always set to zero actually prevent reading text field with TTreeSQL.; Properly find the column even if they were not created by TTreeSQL itself. Fix the loading of data for the last column. Other. Update the branch split mechanism to no longer split a base class; that can not be split (i.e. respect the information returned; by TStreamerElement::CannotSplit (and thus TClass::CanSplit).; In TChain::ls, print the name of the chain and indent the list of files (this fixes #79909).; When setting fBranch in the loaded basket, make sure to set it als",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html:2483,Optimiz,OptimizeBasket,2483,tree/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html,1,['Optimiz'],['OptimizeBasket']
Performance,"floating-point operation must treat any input denormal value as; zero. In some situations, if an instruction does not respect this; mode, the input may need to be converted to 0 as if by; ``@llvm.canonicalize`` during lowering for correctness. ``""denormal-fp-math-f32""``; Same as ``""denormal-fp-math""``, but only controls the behavior of; the 32-bit float type (or vectors of 32-bit floats). If both are; are present, this overrides ``""denormal-fp-math""``. Not all targets; support separately setting the denormal mode per type, and no; attempt is made to diagnose unsupported uses. Currently this; attribute is respected by the AMDGPU and NVPTX backends. ``""thunk""``; This attribute indicates that the function will delegate to some other; function with a tail call. The prototype of a thunk should not be used for; optimization purposes. The caller is expected to cast the thunk prototype to; match the thunk target prototype. ``""tls-load-hoist""``; This attribute indicates that the function will try to reduce redundant; tls address calculation by hoisting tls variable. ``uwtable[(sync|async)]``; This attribute indicates that the ABI being targeted requires that; an unwind table entry be produced for this function even if we can; show that no exceptions passes by it. This is normally the case for; the ELF x86-64 abi, but it can be disabled for some compilation; units. The optional parameter describes what kind of unwind tables; to generate: ``sync`` for normal unwind tables, ``async`` for asynchronous; (instruction precise) unwind tables. Without the parameter, the attribute; ``uwtable`` is equivalent to ``uwtable(async)``.; ``nocf_check``; This attribute indicates that no control-flow check will be performed on; the attributed entity. It disables -fcf-protection=<> for a specific; entity to fine grain the HW control flow protection mechanism. The flag; is target independent and currently appertains to a function or function; pointer.; ``shadowcallstack``; This attribute indicate",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:108479,load,load-hoist,108479,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load-hoist']
Performance,"fo. .. option:: -feliminate-unused-debug-types. By default, Clang does not emit type information for types that are defined; but not used in a program. To retain the debug info for these unused types,; the negation **-fno-eliminate-unused-debug-types** can be used. Controlling Macro Debug Info Generation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Debug info for C preprocessor macros increases the size of debug information in; the binary. Macro debug info generated by Clang can be controlled by the flags; listed below. .. option:: -fdebug-macro. Generate debug info for preprocessor macros. This flag is discarded when; **-g0** is enabled. .. option:: -fno-debug-macro. Do not generate debug info for preprocessor macros (default). Controlling Debugger ""Tuning""; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. While Clang generally emits standard DWARF debug info (http://dwarfstd.org),; different debuggers may know how to take advantage of different specific DWARF; features. You can ""tune"" the debug info for one of several different debuggers. .. option:: -ggdb, -glldb, -gsce, -gdbx. Tune the debug info for the ``gdb``, ``lldb``, Sony PlayStation\ |reg|; debugger, or ``dbx``, respectively. Each of these options implies **-g**.; (Therefore, if you want both **-gline-tables-only** and debugger tuning, the; tuning option must come first.). Controlling LLVM IR Output; --------------------------. Controlling Value Names in LLVM IR; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Emitting value names in LLVM IR increases the size and verbosity of the IR.; By default, value names are only emitted in assertion-enabled builds of Clang.; However, when reading IR it can be useful to re-enable the emission of value; names to improve readability. .. option:: -fdiscard-value-names. Discard value names when generating LLVM IR. .. option:: -fno-discard-value-names. Do not discard value names when generating LLVM IR. This option can be used; to re-enable names for release builds of Clang. Comment Parsing Options; ----",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:127258,tune,tune,127258,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['tune'],['tune']
Performance,"fo.h`` (see :ref:`TargetInstrInfo`).; These functions return ``0`` or a Boolean or they assert, unless overridden.; Here's a list of functions that are overridden for the SPARC implementation in; ``SparcInstrInfo.cpp``:. * ``isLoadFromStackSlot`` --- If the specified machine instruction is a direct; load from a stack slot, return the register number of the destination and the; ``FrameIndex`` of the stack slot. * ``isStoreToStackSlot`` --- If the specified machine instruction is a direct; store to a stack slot, return the register number of the destination and the; ``FrameIndex`` of the stack slot. * ``copyPhysReg`` --- Copy values between a pair of physical registers. * ``storeRegToStackSlot`` --- Store a register value to a stack slot. * ``loadRegFromStackSlot`` --- Load a register value from a stack slot. * ``storeRegToAddr`` --- Store a register value to memory. * ``loadRegFromAddr`` --- Load a register value from memory. * ``foldMemoryOperand`` --- Attempt to combine instructions of any load or; store instruction for the specified operand(s). Branch Folding and If Conversion; --------------------------------. Performance can be improved by combining instructions or by eliminating; instructions that are never reached. The ``analyzeBranch`` method in; ``XXXInstrInfo`` may be implemented to examine conditional instructions and; remove unnecessary instructions. ``analyzeBranch`` looks at the end of a; machine basic block (MBB) for opportunities for improvement, such as branch; folding and if conversion. The ``BranchFolder`` and ``IfConverter`` machine; function passes (see the source files ``BranchFolding.cpp`` and; ``IfConversion.cpp`` in the ``lib/CodeGen`` directory) call ``analyzeBranch``; to improve the control flow graph that represents the instructions. Several implementations of ``analyzeBranch`` (for ARM, Alpha, and X86) can be; examined as models for your own ``analyzeBranch`` implementation. Since SPARC; does not implement a useful ``analyzeBranch``, the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:46808,load,load,46808,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"foo.lst is the generated optimization report. .. code-block::. < foo.c; 1 | void bar();; 2 | void foo() { bar(); }; 3 |; 4 | void Test(int *res, int *c, int *d, int *p, int n) {; 5 | int i;; 6 |; 7 | #pragma clang loop vectorize(assume_safety); 8 V4,1 | for (i = 0; i < 1600; i++) {; 9 | res[i] = (p[i] == 0) ? res[i] : res[i] + d[i];; 10 | }; 11 |; 12 U16 | for (i = 0; i < 16; i++) {; 13 | res[i] = (p[i] == 0) ? res[i] : res[i] + d[i];; 14 | }; 15 |; 16 I | foo();; 17 |; 18 | foo(); bar(); foo();; I | ^; I | ^; 19 | }; 20 |. Symbols printed on the left side of the program indicate what kind of optimization was performed.; The meanings of the symbols are as follows:. - I: The function is inlined.; - U: The loop is unrolled. The following number indicates the unroll factor.; - V: The loop is vectorized. The following numbers indicate the vector length and the interleave factor. .. note:: . If a specific line of code is output twice, it means that the same optimization pass was applied to that ; line of code twice, and the pass was able to further optimize the code on the second iteration. OPTIONS; -------. If ``input`` is ""``-``"" or omitted, :program:`llvm-opt-report` reads from standard; input. Otherwise, it will read from the specified filename. If the :option:`-o` option is omitted, then :program:`llvm-opt-report` will send its output; to standard output. If the :option:`-o` option specifies ""``-``"", then the output will also; be sent to standard output. .. option:: --help. Display available options. .. option:: --version. Display the version of this program. .. option:: --format=<string>. The format of the optimization record file.; The Argument is one of the following:. - yaml; - yaml-strtab; - bitstream. .. option:: --no-demangle. Do not demangle function names. .. option:: -o=<string>. Output file. .. option:: -r=<string>. Root for relative input paths. .. option:: -s. Do not include vectorization factors, etc. EXIT STATUS; -----------. :program:`llvm-opt-report`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst:1815,optimiz,optimization,1815,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst,2,['optimiz'],"['optimization', 'optimize']"
Performance,"for each vector lane, and is used to prevent memory accesses to the masked-off lanes. Arguments:; """""""""""""""""""". The first operand is the vector value to be written to memory. The second operand is the base pointer for the store, it has the same underlying type as the value operand. The third operand is the alignment of the destination location. It must be a power of two constant integer value. The fourth operand, mask, is a vector of boolean values. The types of the mask and the value operand must have the same number of vector elements. Semantics:; """""""""""""""""""". The '``llvm.masked.store``' intrinsics is designed for conditional writing of selected vector elements in a single IR operation. It is useful for targets that support vector masked store and allows vectorizing predicated basic blocks on these targets. Other targets may support this intrinsic differently, for example by lowering it into a sequence of branches that guard scalar store operations.; The result of this operation is equivalent to a load-modify-store sequence. However, using this intrinsic prevents exceptions and data races on memory access to masked-off lanes. ::. call void @llvm.masked.store.v16f32.p0(<16 x float> %value, ptr %ptr, i32 4, <16 x i1> %mask). ;; The result of the following instructions is identical aside from potential data races and memory access exceptions; %oldval = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %value, <16 x float> %oldval; store <16 x float> %res, ptr %ptr, align 4. Masked Vector Gather and Scatter Intrinsics; -------------------------------------------. LLVM provides intrinsics for vector gather and scatter operations. They are similar to :ref:`Masked Vector Load and Store <int_mload_mstore>`, except they are designed for arbitrary memory accesses, rather than sequential memory accesses. Gather and scatter also employ a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:847283,load,load-modify-store,847283,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load-modify-store']
Performance,"for entering functions. ##### Use `lfence` to guard function transitions. An `lfence` instruction can be used to prevent subsequent loads from; speculatively executing until all prior mispredicted predicates have resolved.; We can use this broader barrier to speculative loads executing between; functions. We emit it in the entry block to handle calls, and prior to each; return. This approach also has the advantage of providing the strongest degree; of mitigation when mixed with unmitigated code by halting all misspeculation; entering a function which is mitigated, regardless of what occurred in the; caller. However, such a mixture is inherently more risky. Whether this kind of; mixture is a sufficient mitigation requires careful analysis. Unfortunately, experimental results indicate that the performance overhead of; this approach is very high for certain patterns of code. A classic example is; any form of recursive evaluation engine. The hot, rapid call and return; sequences exhibit dramatic performance loss when mitigated with `lfence`. This; component alone can regress performance by 2x or more, making it an unpleasant; tradeoff even when only used in a mixture of code. ##### Use an internal TLS location to pass predicate state. We can define a special thread-local value to hold the predicate state between; functions. This avoids direct ABI implications by using a side channel between; callers and callees to communicate the predicate state. It also allows implicit; zero-initialization of the state, which allows non-checked code to be the first; code executed. However, this requires a load from TLS in the entry block, a store to TLS; before every call and every ret, and a load from TLS after every call. As a; consequence it is expected to be substantially more expensive even than using; `%rsp` and potentially `lfence` within the function entry block. ##### Define a new ABI and/or calling convention. We could define a new ABI and/or calling convention to explicitly ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:42208,perform,performance,42208,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['perform'],['performance']
Performance,"for example in the case of a Lorentz rotation,; from (`0,0`) thru (`3,3`). ``` {.cpp}; TMatrixD(4,4) m;; LorentzRotation r(m); //create Lorentz r; ```. #### Connection to Other Vector Classes. The 3D and 4D vectors of the `GenVector` package can be constructed and; assigned from any vector which satisfies the following requisites:. - for 3D vectors implementing the `x()`, `y()` and `z()` methods. - for Lorentz vectors implementing the `x()`, `y()`, `z()` and `t()`; methods. ``` {.cpp}; CLHEP::Hep3Vector hv;; XYZVector v1(hv); //create 3D vector from; //CLHEP 3D Vector; HepGeom::Point3D hp;; XYZPoint p1(hp); //create a 3D p; ```. ## Linear Algebra: SMatrix Package. The ROOT Linear algebra package is documented in a separate chapter (see; ""Linear Algebra in ROOT""). `SMatrix` is a C++ package, for high; performance vector and matrix computations. It has been introduced in; ROOT v5.08. It is optimized for describing small matrices and vectors; and It can be used only in problems when the size of the matrices is; known at compile time, like in the tracking reconstruction of physics; experiments. It is based on a C++ technique, called expression; templates, to achieve an high level optimization. The C++ templates can; be used to implement vector and matrix expressions such that these; expressions can be transformed at compile time to code which is; equivalent to hand optimized code in a low-level language like FORTRAN; or C (see for example T. Veldhuizen, Expression Templates, C++ Report,; 1995). The `SMatrix` has been developed initially by T. Glebe in; Max-Planck-Institut, Heidelberg, as part of the `HeraB` analysis; framework. A subset of the original package has been now incorporated in; the ROOT distribution, with the aim to provide a stand-alone and high; performance matrix package. The API of the current package differs from; the original one, in order to be compliant to the ROOT coding; conventions. `SMatrix` contains the generic **`ROOT::Math::SMatrix`** and; **`R",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:99154,optimiz,optimized,99154,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimized']
Performance,"for specialized cases, where the outcome; of a condition is severely skewed. As a result, the optimizer can be extremely; aggressive, which can result in performance degradation if the outcome is less; predictable than the annotation suggests. Even when the annotation is correct; 90% of the time, it may be beneficial to either remove the annotation or to use; a different intrinsic that can communicate the probability more directly. Because this may be too strict, MisExpect diagnostics are not enabled by; default, and support an additional flag to tolerate some deviation from the; exact thresholds. The ``-fdiagnostic-misexpect-tolerance=N`` accepts; deviations when comparing branch weights within ``N%`` of the expected values.; So passing ``-fdiagnostic-misexpect-tolerance=5`` will not report diagnostic messages; if the branch weight from the profile is within 5% of the weight added by; the ``llvm.expect`` intrinsic. MisExpect diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -Rpass=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -Wmisexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. .. option:: -fdiagnostic-misexpect-tolerance=N. Relaxes misexpect checking to tolerate profiling values within N% of the; expected branch weight. e.g., a value of ``N=5`` allows misexpect to check against; ``0.95 * Threshold``. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+======================================================================================+; | Frontend | Profiling",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst:2175,optimiz,optimization,2175,interpreter/llvm-project/clang/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst,1,['optimiz'],['optimization']
Performance,"for; the top volume. These are explained in details in the section; ""Visualization Settings and Attributes"". At this point, you will; probably like to see how this geometry looks like. You just need to run; the example and you will get the following picture that you can rotate; using the mouse; or you can zoom / move it around (see what the Help; menu of the GL window displays). ``` {.cpp}; % root rootgeom.C; ```. ![](pictures/020001B1.jpg). Now let us browse the hierarchy that was just created. Start a browser; and double-click on the item simple1 representing the; ***`gGeoManager`*** object. Note that right click opens the context menu; of the manager class where several global methods are available. ``` {.cpp}; root[] new TBrowser;; ```. ![](pictures/020001B2.jpg). The folders `Materials`, `Media` and `Local transformations` are in fact; the containers where the geometry manager stores the corresponding; objects. The `Illegal overlaps` folder is empty but can be filled after; performing a geometry validity check (see section: ""Checking the; Geometry""). If tracking is performed using **`TGeo`**, the folder; `Tracks` might contain user-defined tracks that can be; visualized/animated in the geometry context (see section: ""Creating and; Visualizing Tracks""). Since for the time being we are interested more in; the geometrical hierarchy, we will focus on the last two displayed items; `TOP `and `TOP_1`. These are the top volume and the corresponding top; node in the hierarchy. Double clicking on the `TOP` volume will unfold all different volumes; contained by the top volume. In the right panel, we will see all the; volumes contained by `TOP` (if the same is positioned 4 times we will; get 4 identical items). This rule will apply to any clicked volume in; the hierarchy. Note that right clicking a volume item activates the; volume context menu containing several specific methods. We will call; the volume hierarchy developed in this way as the; `logical geometry graph`. Th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:9264,perform,performing,9264,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performing']
Performance,"fore any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes.; **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:257511,load,load,257511,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"fore invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc0=1 sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. GFX940, GFX941; - wavefront - generic buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. store atomic release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_store; store atomic release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc0=1; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcn",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:307177,load,load,307177,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"fore, this might take some time as it needs to build all the modules. Note that this option doesn't apply in module builds, to avoid the recursion. ``-fno-implicit-modules``; All modules used by the build must be specified with ``-fmodule-file``. ``-fmodule-file=[<name>=]<file>``; Specify the mapping of module names to precompiled module files. If the; name is omitted, then the module file is loaded whether actually required; or not. If the name is specified, then the mapping is treated as another; prebuilt module search mechanism (in addition to ``-fprebuilt-module-path``); and the module is only loaded if required. Note that in this case the; specified file also overrides this module's paths that might be embedded; in other precompiled module files. ``-fprebuilt-module-path=<directory>``; Specify the path to the prebuilt modules. If specified, we will look for modules in this directory for a given top-level module name. We don't need a module map for loading prebuilt modules in this directory and the compiler will not try to rebuild these modules. This can be specified multiple times. ``-fprebuilt-implicit-modules``; Enable prebuilt implicit modules. If a prebuilt module is not found in the; prebuilt modules paths (specified via ``-fprebuilt-module-path``), we will; look for a matching implicit module in the prebuilt modules paths. -cc1 Options; ~~~~~~~~~~~~. ``-fmodules-strict-context-hash``; Enables hashing of all compiler options that could impact the semantics of a; module in an implicit build. This includes things such as header search paths; and diagnostics. Using this option may lead to an excessive number of modules; being built if the command line arguments are not homogeneous across your; build. Using Prebuilt Modules; ----------------------. Below are a few examples illustrating uses of prebuilt modules via the different options. First, let's set up files for our examples. .. code-block:: c. /* A.h */; #ifdef ENABLE_A; void a() {}; #endif. .. code-block",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:17828,load,loading,17828,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loading']
Performance,"fore; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; glo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:252129,cache,caches,252129,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"fore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; follo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:327621,load,load,327621,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"form the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21916,optimiz,optimize,21916,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimize']
Performance,"form, internal libraries built as part of the software itself to provide structure, and third-party libraries. For each library, one needs to access both its interface (API) and its implementation. In the C family of languages, the interface to a library is accessed by including the appropriate header files(s):. .. code-block:: c. #include <SomeLib.h>. The implementation is handled separately by linking against the appropriate library. For example, by passing ``-lSomeLib`` to the linker. Modules provide an alternative, simpler way to use software libraries that provides better compile-time scalability and eliminates many of the problems inherent to using the C preprocessor to access the API of a library. Problems with the current model; -------------------------------; The ``#include`` mechanism provided by the C preprocessor is a very poor way to access the API of a library, for a number of reasons:. * **Compile-time scalability**: Each time a header is included, the; compiler must preprocess and parse the text in that header and every; header it includes, transitively. This process must be repeated for; every translation unit in the application, which involves a huge; amount of redundant work. In a project with *N* translation units; and *M* headers included in each translation unit, the compiler is; performing *M x N* work even though most of the *M* headers are; shared among multiple translation units. C++ is particularly bad,; because the compilation model for templates forces a huge amount of; code into headers. * **Fragility**: ``#include`` directives are treated as textual; inclusion by the preprocessor, and are therefore subject to any; active macro definitions at the time of inclusion. If any of the; active macro definitions happens to collide with a name in the; library, it can break the library API or cause compilation failures; in the library header itself. For an extreme example,; ``#define std ""The C++ Standard""`` and then include a standard; library h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:1112,scalab,scalability,1112,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['scalab'],['scalability']
Performance,"forward. One additional option is ``std::vector<bool>``: we discourage its use for two; reasons 1) the implementation in many common compilers (e.g. commonly; available versions of GCC) is extremely inefficient and 2) the C++ standards; committee is likely to deprecate this container and/or change it significantly; somehow. In any case, please don't use it. .. _dss_bitvector:. BitVector; ^^^^^^^^^. The BitVector container provides a dynamic size set of bits for manipulation.; It supports individual bit setting/testing, as well as set operations. The set; operations take time O(size of bitvector), but operations are performed one word; at a time, instead of one bit at a time. This makes the BitVector very fast for; set operations compared to other containers. Use the BitVector when you expect; the number of set bits to be high (i.e. a dense set). .. _dss_smallbitvector:. SmallBitVector; ^^^^^^^^^^^^^^. The SmallBitVector container provides the same interface as BitVector, but it is; optimized for the case where only a small number of bits, less than 25 or so,; are needed. It also transparently supports larger bit counts, but slightly less; efficiently than a plain BitVector, so SmallBitVector should only be used when; larger counts are rare. At this time, SmallBitVector does not support set operations (and, or, xor), and; its operator[] does not provide an assignable lvalue. .. _dss_sparsebitvector:. SparseBitVector; ^^^^^^^^^^^^^^^. The SparseBitVector container is much like BitVector, with one major difference:; Only the bits that are set, are stored. This makes the SparseBitVector much; more space efficient than BitVector when the set is sparse, as well as making; set operations O(number of set bits) instead of O(size of universe). The; downside to the SparseBitVector is that setting and testing of random bits is; O(N), and on large SparseBitVectors, this can be slower than BitVector. In our; implementation, setting or testing bits in sorted order (either forwards ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:97524,optimiz,optimized,97524,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['optimiz'],['optimized']
Performance,"fpclass>`.; The third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.is.fpclass``' intrinsic performs llvm.is.fpclass (:ref:`llvm.is.fpclass <llvm.is.fpclass>`). Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <2 x i1> @llvm.vp.is.fpclass.v2f16(<2 x half> %x, i32 3, <2 x i1> %m, i32 %evl); %t = call <vscale x 2 x i1> @llvm.vp.is.fpclass.nxv2f16(<vscale x 2 x half> %x, i32 3, <vscale x 2 x i1> %m, i32 %evl). .. _int_mload_mstore:. Masked Vector Load and Store Intrinsics; ---------------------------------------. LLVM provides intrinsics for predicated vector load and store operations. The predicate is specified by a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits of the mask are on, the intrinsic is identical to a regular vector load or store. When all bits are off, no memory is accessed. .. _int_mload:. '``llvm.masked.load.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data is a vector of any integer, floating-point or pointer data type. ::. declare <16 x float> @llvm.masked.load.v16f32.p0(ptr <ptr>, i32 <alignment>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x double> @llvm.masked.load.v2f64.p0(ptr <ptr>, i32 <alignment>, <2 x i1> <mask>, <2 x double> <passthru>); ;; The data is a vector of pointers; declare <8 x ptr> @llvm.masked.load.v8p0.p0(ptr <ptr>, i32 <alignment>, <8 x i1> <mask>, <8 x ptr> <passthru>). Overview:; """""""""""""""""". Reads a vector from memory according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``'",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:843053,load,load,843053,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"from B and spread the elements into array A.; double *A, B; int *C;; for (int i = 0; i < size; ++i) {; if (C[i] != 0); A[i] = B[j++];; }. .. code-block:: llvm. ; Load several elements from array B and expand them in a vector.; ; The number of loaded elements is equal to the number of '1' elements in the Mask.; %Tmp = call <8 x double> @llvm.masked.expandload.v8f64(ptr %Bptr, <8 x i1> %Mask, <8 x double> poison); ; Store the result in A; call void @llvm.masked.store.v8f64.p0(<8 x double> %Tmp, ptr %Aptr, i32 8, <8 x i1> %Mask). ; %Bptr should be increased on each iteration according to the number of '1' elements in the Mask.; %MaskI = bitcast <8 x i1> %Mask to i8; %MaskIPopcnt = call i8 @llvm.ctpop.i8(i8 %MaskI); %MaskI64 = zext i8 %MaskIPopcnt to i64; %BNextInd = add i64 %BInd, %MaskI64. Other targets may support this intrinsic differently, for example, by lowering it into a sequence of conditional scalar load operations and shuffles.; If all mask elements are '1', the intrinsic behavior is equivalent to the regular unmasked vector load. .. _int_compressstore:. '``llvm.masked.compressstore.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. A number of scalar values of integer, floating point or pointer data type are collected from an input vector and stored into adjacent memory addresses. A mask defines which elements to collect from the vector. ::. declare void @llvm.masked.compressstore.v8i32 (<8 x i32> <value>, ptr <ptr>, <8 x i1> <mask>); declare void @llvm.masked.compressstore.v16f32 (<16 x float> <value>, ptr <ptr>, <16 x i1> <mask>). Overview:; """""""""""""""""". Selects elements from input vector '``value``' according to the '``mask``'. All selected elements are written into adjacent memory addresses starting at address '`ptr`', from lower to higher. The mask holds a bit for each vector lane, and is used to select elements to be stored. The number of elements to be stored is equal to the number of active ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:858151,load,load,858151,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"front Offset to get the; wavefront's byte scratch backing memory offset from; ``SH_HIDDEN_PRIVATE_BASE_VIMID``. The Scratch Wavefront Offset must also be used as an offset with Private; segment address when using the Scratch Segment Buffer. Since FLAT_SCRATCH_LO is in units of 256 bytes, the offset must be right; shifted by 8 before moving into FLAT_SCRATCH_HI. FLAT_SCRATCH_HI corresponds to SGPRn-4 on GFX7, and SGPRn-6 on GFX8 (where; SGPRn is the highest numbered SGPR allocated to the wavefront).; FLAT_SCRATCH_HI is multiplied by 256 (as it is in units of 256 bytes) and; added to ``SH_HIDDEN_PRIVATE_BASE_VIMID`` to calculate the per wavefront; FLAT SCRATCH BASE in flat memory instructions that access the scratch; aperture.; 2. The second word of Flat Scratch Init is 32-bit byte size of a single; work-items scratch memory usage. CP obtains this from the runtime, and it is always a multiple of DWORD. CP; checks that the value in the kernel dispatch packet Private Segment Byte; Size is not larger and requests the runtime to increase the queue's scratch; size if necessary. CP directly loads from the kernel dispatch packet Private Segment Byte Size; field and rounds up to a multiple of DWORD. Having CP load it once avoids; loading it at the beginning of every wavefront. The kernel prolog code must move it to FLAT_SCRATCH_LO which is SGPRn-3 on; GFX7 and SGPRn-5 on GFX8. FLAT_SCRATCH_LO is used as the FLAT SCRATCH SIZE; in flat memory instructions. * If the *Target Properties* column of :ref:`amdgpu-processor-table`; specifies *Absolute flat scratch*:. If the kernel or any function it calls may use flat operations to access; scratch memory, the prolog code must set up the FLAT_SCRATCH register pair; (FLAT_SCRATCH_LO/FLAT_SCRATCH_HI which are in SGPRn-4/SGPRn-3). Initialization; uses Flat Scratch Init and Scratch Wavefront Offset SGPR registers (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`):. The Flat Scratch Init is the 64-bit address of the base of scratch ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:196000,queue,queue,196000,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"fset`` 32-bit integer ``reg offset`` is the dword offset into the GFXIP register space of; a GRBM register (i.e., driver accessible GPU register number, not; shader GPR register number). The driver is required to program each; specified register to the corresponding specified value when; executing this pipeline. Typically, the ``reg offsets`` are the; ``uint16_t`` offsets to each register as defined by the hardware; chip headers. The register is set to the provided value. However, a; ``reg offset`` that specifies a user data register (e.g.,; COMPUTE_USER_DATA_0) needs special treatment. See; :ref:`amdgpu-amdpal-code-object-user-data-section` section for more; information.; ========================== ============== ====================================================================. .. _amdgpu-amdpal-code-object-user-data-section:. User Data; +++++++++. Each hardware stage has a set of 32-bit physical SPI *user data registers*; (either 16 or 32 based on graphics IP and the stage) which can be; written from a command buffer and then loaded into SGPRs when waves are; launched via a subsequent dispatch or draw operation. This is the way; most arguments are passed from the application/runtime to a hardware; shader. PAL abstracts this functionality by exposing a set of 128 *user data; entries* per pipeline a client can use to pass arguments from a command; buffer to one or more shaders in that pipeline. The ELF code object must; specify a mapping from virtualized *user data entries* to physical *user; data registers*, and PAL is responsible for implementing that mapping,; including spilling overflow *user data entries* to memory if needed. Since the *user data registers* are GRBM-accessible SPI registers, this; mapping is actually embedded in the ``.registers`` metadata entry. For; most registers, the value in that map is a literal 32-bit value that; should be written to the register by the driver. However, when the; register is a *user data register* (any USER_DATA regi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:413548,load,loaded,413548,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"fsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35999,load,load,35999,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"fstrict-vtable-pointers. Enable optimizations based on the strict rules for overwriting polymorphic; C++ objects, i.e. the vptr is invariant during an object's lifetime.; This enables better devirtualization. Turned off by default, because it is; still experimental. .. option:: -fwhole-program-vtables. Enable whole-program vtable optimizations, such as single-implementation; devirtualization and virtual constant propagation, for classes with; :doc:`hidden LTO visibility <LTOVisibility>`. Requires ``-flto``. .. option:: -f[no]split-lto-unit. Controls splitting the :doc:`LTO unit <LTOVisibility>` into regular LTO and; :doc:`ThinLTO` portions, when compiling with -flto=thin. Defaults to false; unless ``-fsanitize=cfi`` or ``-fwhole-program-vtables`` are specified, in; which case it defaults to true. Splitting is required with ``fsanitize=cfi``,; and it is an error to disable via ``-fno-split-lto-unit``. Splitting is; optional with ``-fwhole-program-vtables``, however, it enables more; aggressive whole program vtable optimizations (specifically virtual constant; propagation). When enabled, vtable definitions and select virtual functions are placed; in the split regular LTO module, enabling more aggressive whole program; vtable optimizations required for CFI and virtual constant propagation.; However, this can increase the LTO link time and memory requirements over; pure ThinLTO, as all split regular LTO modules are merged and LTO linked; with regular LTO. .. option:: -fforce-emit-vtables. In order to improve devirtualization, forces emitting of vtables even in; modules where it isn't necessary. It causes more inline virtual functions; to be emitted. .. option:: -fno-assume-sane-operator-new. Don't assume that the C++'s new operator is sane. This option tells the compiler to do not assume that C++'s global; new operator will always return a pointer that does not alias any; other pointer when the function returns. .. option:: -fassume-nothrow-exception-dtor. Assume that an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:81246,optimiz,optimizations,81246,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"ft out if you had the; configuration script pick up them up from a default location. ### Using PyROOT. Since it is an extension module, the usage of `PyROOT` probably comes; naturally if you're used to Python. In general, `PyROOT` attempts to; allow working in both Python and ROOT style, and although it is; succeeding, it isn't perfect: there are edges. The following sections; explain in some detail what you can expect, and what you need to watch; out for. #### Access to ROOT Classes. Before a ROOT class can be used from Python, its dictionary needs to be; loaded into the current process. Starting with ROOT version 4.00/06,; this happens automatically for all classes that are declared to the; auto-loading mechanism through so-called `rootmap` files. Effectively,; this means that all classes in the ROOT distributions are directly; available for import. For example:. ``` {.cpp}; from ROOT import TCanvas # available at startup; c = TCanvas(). from ROOT import TLorentzVector # triggers auto-load of libPhysics; l = TLorentzVector(); ```. Although it is not recommended, a simple way of working with `PyROOT` is; doing a global import:. ``` {.cpp}; from ROOT import *. c = TCanvas(); l = TLorentzVector(); ```. Keeping the ROOT namespace (""`import ROOT`""), or only importing from; ROOT those classes that you will actually use (see above), however, will; always be cleaner and clearer:. ``` {.cpp}; import ROOT. c = ROOT.TCanvas(); l = ROOT.TLorentzVector(); ```. Since it is foreseen that most people will use the simple approach; anyway, the request to copy all from module ROOT will not actually; result in copying all ROOT classes into the current namespace. Instead,; classes will still be bound (and possibly loaded) on an as-needed basis.; Note carefully how this is different from other Python (extension); modules, and what to expect if you use the normal inspection tools (such; as e.g. '`dir()`'). This feature prevents the inspection tools from; being swamped by an enormous amou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:9903,load,load,9903,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['load']
Performance,"ften require access to the *object pointer* rather than the *derived; pointer* (which is a pointer to the field within the object). Accordingly,; these intrinsics take both pointers as separate arguments for completeness. In; this snippet, ``%object`` is the object pointer, and ``%derived`` is the derived; pointer:. .. code-block:: llvm. ;; An array type.; %class.Array = type { %class.Object, i32, [0 x %class.Object*] }; ... ;; Load the object pointer from a gcroot.; %object = load %class.Array** %object_addr. ;; Compute the derived pointer.; %derived = getelementptr %object, i32 0, i32 2, i32 %n. LLVM does not enforce this relationship between the object and derived pointer; (although a particular :ref:`collector strategy <plugin>` might). However, it; would be an unusual collector that violated it. The use of these intrinsics is naturally optional if the target GC does not; require the corresponding barrier. The GC strategy used with such a collector; should replace the intrinsic calls with the corresponding ``load`` or; ``store`` instruction if they are used. One known deficiency with the current design is that the barrier intrinsics do; not include the size or alignment of the underlying operation performed. It is; currently assumed that the operation is of pointer size and the alignment is; assumed to be the target machine's default alignment. Write barrier: ``llvm.gcwrite``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.gcwrite(i8* %value, i8* %object, i8** %derived). For write barriers, LLVM provides the ``llvm.gcwrite`` intrinsic function. It; has exactly the same semantics as a non-volatile ``store`` to the derived; pointer (the third argument). The exact code generated is specified by the; Function's selected :ref:`GC strategy <plugin>`. Many important algorithms require write barriers, including generational and; concurrent collectors. Additionally, write barriers could be used to implement; reference counting. Read barrier: ``llvm.gcre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:14290,load,load,14290,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['load']
Performance,"fter; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:220240,perform,performing,220240,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"ftware and method performance; validation has been implemented.; . Methods. BDT Automatic parameter optimisation for building the; tree architecture: The optimisation procedure uses the; performance of the trained classifier on the ""test sample"" for; finding the set of optimal parameters. Two different methods to; traverse the parameter space are available (scanning, genetic; algorithm). Currently parameter optimization is implemented only; for these three parameters that influence the tree architectur:; the maximum depth of a tree, MaxDepth, the minimum; number of events in each node, NodeMinEvents, and; the number of tress, NTrees. Optimization can; is invoked by calling; factory->OptimizeAllMethods(); prior to the call; factory->TrainAllMethods();. Automated and configurable parameter optimization is soon to; be enabled for all methods (for those parameters where; optimization is applicable).; . BDT node splitting: While Decision Trees; typically have only univariate splits, in TMVA one can now; also opt for multivariate splits that use a ""Fisher; Discriminant"" (option: UseFisherCuts), built from all; observables that show correlations larger than some threshold; (MinLinCorrForFisher). The training will then test at each; split a cut on this fisher discriminant in addition to all; univariate cuts on the variables (or only on those variables; that have not been used in the Fisher discriminant, option; UseExcusiveVars). No obvious improvement betwen very simple; decision trees after boosting has been observed so far, but; only a limited number of studies has been performed concerning; potiential benenfit of these simple multivariate splits.; . Bug fixes. A problem in the BDTG has been fixed, leading to a much; improved regression performance.; A problem in the TMVA::Reader has been fixed.; With the new test framework and the coverity checks of ROOT; a number of bugs were discovered and fixed. They mainly concerned; memory leaks, and did not affect the performance. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html:3072,perform,performed,3072,tmva/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html,6,['perform'],"['performance', 'performed']"
Performance,"function as a `kernel` function, we make use of special LLVM metadata. The; NVPTX back-end will look for a named metadata node called; ``nvvm.annotations``. This named metadata must contain a list of metadata that; describe the IR. For our purposes, we need to declare a metadata node that; assigns the ""kernel"" attribute to the LLVM IR function that should be emitted; as a PTX `kernel` function. These metadata nodes take the form:. .. code-block:: text. !{<function ref>, metadata !""kernel"", i32 1}. For the previous example, we have:. .. code-block:: llvm. !nvvm.annotations = !{!0}; !0 = !{void (float addrspace(1)*,; float addrspace(1)*,; float addrspace(1)*)* @kernel, !""kernel"", i32 1}. Here, we have a single metadata declaration in ``nvvm.annotations``. This; metadata annotates our ``@kernel`` function with the ``kernel`` attribute. Running the Kernel; ------------------. Generating PTX from LLVM IR is all well and good, but how do we execute it on; a real GPU device? The CUDA Driver API provides a convenient mechanism for; loading and JIT compiling PTX to a native GPU device, and launching a kernel.; The API is similar to OpenCL. A simple example showing how to load and; execute our vector addition code is shown below. Note that for brevity this; code does not perform much error checking!. .. note::. You can also use the ``ptxas`` tool provided by the CUDA Toolkit to offline; compile PTX to machine code (SASS) for a specific GPU architecture. Such; binaries can be loaded by the CUDA Driver API in the same way as PTX. This; can be useful for reducing startup time by precompiling the PTX kernels. .. code-block:: c++. #include <iostream>; #include <fstream>; #include <cassert>; #include ""cuda.h"". void checkCudaErrors(CUresult err) {; assert(err == CUDA_SUCCESS);; }. /// main - Program entry point; int main(int argc, char **argv) {; CUdevice device;; CUmodule cudaModule;; CUcontext context;; CUfunction function;; CUlinkState linker;; int devCount;. // CUDA initializatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:18970,load,loading,18970,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['load'],['loading']
Performance,"function attribute and its value is a positive; integer (represented as a string), then that value is used as the ID; of the newly constructed ``gc.statepoint``. If a call site is marked; with a ``""statepoint-num-patch-bytes""`` function attribute and its; value is a positive integer, then that value is used as the 'num patch; bytes' parameter of the newly constructed ``gc.statepoint``. The; ``""statepoint-id""`` and ``""statepoint-num-patch-bytes""`` attributes; are not propagated to the ``gc.statepoint`` call or invoke if they; could be successfully parsed. In practice, RewriteStatepointsForGC should be run much later in the pass; pipeline, after most optimization is already done. This helps to improve; the quality of the generated code when compiled with garbage collection support. .. _RewriteStatepointsForGC_intrinsic_lowering:. RewriteStatepointsForGC intrinsic lowering; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. As a part of lowering to the explicit model of relocations; RewriteStatepointsForGC performs GC specific lowering for the following; intrinsics:. * ``gc.get.pointer.base``; * ``gc.get.pointer.offset``; * ``llvm.memcpy.element.unordered.atomic.*``; * ``llvm.memmove.element.unordered.atomic.*``. There are two possible lowerings for the memcpy and memmove operations:; GC leaf lowering and GC parseable lowering. If a call is explicitly marked with; ""gc-leaf-function"" attribute the call is lowered to a GC leaf call to; '``__llvm_memcpy_element_unordered_atomic_*``' or; '``__llvm_memmove_element_unordered_atomic_*``' symbol. Such a call can not; take a safepoint. Otherwise, the call is made GC parseable by wrapping the; call into a statepoint. This makes it possible to take a safepoint during; copy operation. Note that a GC parseable copy operation is not required to; take a safepoint. For example, a short copy operation may be performed without; taking a safepoint. GC parseable calls to '``llvm.memcpy.element.unordered.atomic.*``',; '``llvm.memmove.element.unor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:29437,perform,performs,29437,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performs']
Performance,"function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified mechanism) pointer associated with the type; metadata. 2. If the function has a non-void return type, a pointer to a function that; returns an unspecified value without causing side effects. If the function's return value's second element is false, the value of the; first element is undefined. .. _type.checked.load.relative:. '``llvm.type.checked.load.relative``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load.relative(ptr %ptr, i32 %offset, metadata %type) argmemonly nounwind readonly. Overview:; """""""""""""""""". The ``llvm.type.checked.load.relative`` intrinsic loads a relative pointer to a; function from a virtual table pointer using metadata. Otherwise, its semantic is; identical to the ``llvm.type.checked.load`` intrinsic. A relative pointer is a pointer to an offset to the pointed to value. The; address of the underlying pointer of the relative pointer is obtained by adding; the offset to the address of the offset value. '``llvm.arithmetic.fence``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.arithmetic.fence(<type> <op>). Overview:; """""""""""""""""". The purpose of the ``llvm.arithmetic.fence`` intrinsic; is to prevent the optimizer from performing fast-math optimizations,; particularly reassociation,; between the argument and the expression that contains the argument.; It can be used to preserve the parentheses in the source language. Arguments:; """""""""""""""""""". The ``llvm.arithmetic.fence`` intrinsic takes only one argument.; The argument and the return value are floating-point numbers,; or vector floating-point numbers, of the s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:940096,load,loads,940096,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"functions support temporary ranges, for example:. .. code-block:: c++. for (auto [Letter, Count] : zip(SmallVector<char>{'a', 'b', 'c'}, Counts)); errs() << Letter << "": "" << Count << ""\n"";. The difference between the functions in the ``zip`` family is how they behave; when the supplied ranges have different lengths:. * ``zip_equal`` -- requires all input ranges have the same length.; * ``zip`` -- iteration stops when the end of the shortest range is reached.; * ``zip_first`` -- requires the first range is the shortest one.; * ``zip_longest`` -- iteration continues until the end of the longest range is; reached. The non-existent elements of shorter ranges are replaced with; ``std::nullopt``. The length requirements are checked with ``assert``\ s. As a rule of thumb, prefer to use ``zip_equal`` when you expect all; ranges to have the same lengths, and consider alternative ``zip`` functions only; when this is not the case. This is because ``zip_equal`` clearly communicates; this same-length assumption and has the best (release-mode) runtime performance. .. _uf_enumerate:. ``enumerate``; ^^^^^^^^^^^^^. The ``enumerate`` functions allows to iterate over one or more ranges while; keeping track of the index of the current loop iteration. For example:. .. code-block:: c++. for (auto [Idx, BB, Value] : enumerate(Phi->blocks(),; Phi->incoming_values())); errs() << ""#"" << Idx << "" "" << BB->getName() << "": "" << *Value << ""\n"";. The current element index is provided as the first structured bindings element.; Alternatively, the index and the element value can be obtained with the; ``index()`` and ``value()`` member functions:. .. code-block:: c++. char Letters[26] = ...;; for (auto En : enumerate(Letters)); errs() << ""#"" << En.index() << "" "" << En.value() << ""\n"";. Note that ``enumerate`` has ``zip_equal`` semantics and provides elements; through a 'reference wrapper' proxy, which makes them modifiable when accessed; through structured bindings or the ``value()`` member function.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:101921,perform,performance,101921,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['performance']
Performance,"g Size` column shows the size in bytes of instructions. The; `Encodings` column shows the actual instruction encodings (byte sequences in; hex). The third section is the *Resource pressure view*. This view reports; the average number of resource cycles consumed every iteration by instructions; for every processor resource unit available on the target. Information is; structured in two tables. The first table reports the number of resource cycles; spent on average every iteration. The second table correlates the resource; cycles to the machine instruction in the sequence. For example, every iteration; of the instruction vmulps always executes on resource unit [6]; (JFPU1 - floating point pipeline #1), consuming an average of 1 resource cycle; per iteration. Note that on AMD Jaguar, vector floating-point multiply can; only be issued to pipeline JFPU1, while horizontal floating-point additions can; only be issued to pipeline JFPU0. The resource pressure view helps with identifying bottlenecks caused by high; usage of specific hardware resources. Situations with resource pressure mainly; concentrated on a few resources should, in general, be avoided. Ideally,; pressure should be uniformly distributed between multiple resources. Timeline View; ^^^^^^^^^^^^^; The timeline view produces a detailed report of each instruction's state; transitions through an instruction pipeline. This view is enabled by the; command line option ``-timeline``. As instructions transition through the; various stages of the pipeline, their states are depicted in the view report.; These states are represented by the following characters:. * D : Instruction dispatched.; * e : Instruction executing.; * E : Instruction executed.; * R : Instruction retired.; * = : Instruction already dispatched, waiting to be executed.; * \- : Instruction executed, waiting to be retired. Below is the timeline view for a subset of the dot-product example located in; ``test/tools/llvm-mca/X86/BtVer2/dot-product.s`` and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:20773,bottleneck,bottlenecks,20773,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['bottleneck'],['bottlenecks']
Performance,"g `TThreadedObject::fgMaxSlots` is deprecated: TThreadedObject now increases the number of slots; on-demand rather than running out and throwing an exception. ## Core Libraries. - ROOT comes with C++ Modules enabled. More details about the technology found [here](../../README.CXXMODULES.md).; - The `ACLiC` can be configured to pass options to the `rootcling` invocation by enabling in the `.rootrc` the `ACLiC.ExtraRootclingFlags [-opts]` line.; - A call to `ROOT::EnableThreadSafety` is not required before using `TThreadExecutor` or `TTreeProcessorMT` anymore; - `TTreeProcessorMT` does not silently activate implicit multi-threading features anymore. An explicit call to; `ROOT::EnableImplicitMT` is required instead; - `TTreeProcessorMT` now has a constructor argument to set the number of threads for its thread-pool. ## I/O Libraries. ## TTree Libraries. - A new status bit was added to `TTree`: `kEntriesReshuffled`, which indicates a `TTree` that is the output of the; processing of another tree during which its entry order has been changed (this can happen, for instance, when; processing a tree in a multi-thread application). To avoid silent entry number mismatches, trees with this bit set; cannot add friend trees nor can be added as friends, unless the friend `TTree` has an appropriate `TTreeIndex`. ## Histogram Libraries. ## Math Libraries. ## RooFit Libraries. ### RooWorkspace::Import() for Python; `RooWorkspace.import()` cannot be used in Python, since it is a reserved keyword. Users therefore had to resort; to; getattr(workspace, 'import')(...); Now,; workspace.Import(...); has been defined for the new PyROOT, which makes calling the function easier. ### Modernised category classes; RooFit's categories were modernised. Previously, the class RooCatType was used to store category states. It stores; two members, an integer for the category index, and up to 256 characters for a category name. Now, such states are; stored only using an integer, and category names can hav",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v622/index.md:2354,multi-thread,multi-thread,2354,README/ReleaseNotes/v622/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v622/index.md,1,['multi-thread'],['multi-thread']
Performance,"g a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require independent; protection *and* which require hardening the address of the load. However, this; is unlikely to be profitable in practice. The latency hit of the hardening; would need to exceed that of an `lfence` when *correctly* speculatively; executed. But in that case, the `lfence` cost is a complete loss of speculative; execution (at a minimum). So far, the evidence we have of the performance cost; of using `lfence` indicates few if any hot code patterns where this trade off; would make sense. ###### Tempting optimizations that break the security model. Several optimizations were considered which didn't pan out due to failure to; uphold the security model. One in particular is worth discussing as many others; will reduce to it. We wondered whether only the *first* load in a basic block could be checked. If; the chec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:36193,load,load-heavy,36193,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load-heavy']
Performance,"g a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:282214,load,load,282214,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"g all; input into a single module, which is not scalable; in time or memory, and also prevents fast incremental compiles. In ThinLTO mode, as with regular LTO, clang emits LLVM bitcode after the; compile phase. The ThinLTO bitcode is augmented with a compact summary; of the module. During the link step, only the summaries are read and; merged into a combined summary index, which includes an index of function; locations for later cross-module function importing. Fast and efficient; whole-program analysis is then performed on the combined summary index. However, all transformations, including function importing, occur; later when the modules are optimized in fully parallel backends.; By default, linkers_ that support ThinLTO are set up to launch; the ThinLTO backends in threads. So the usage model is not affected; as the distinction between the fast serial thin link step and the backends; is transparent to the user. For more information on the ThinLTO design and current performance,; see the LLVM blog post `ThinLTO: Scalable and Incremental LTO; <http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html>`_.; While tuning is still in progress, results in the blog post show that; ThinLTO already performs well compared to LTO, in many cases matching; the performance improvement. Current Status; ==============. Clang/LLVM; ----------; .. _compiler:. The 3.9 release of clang includes ThinLTO support. However, ThinLTO; is under active development, and new features, improvements and bugfixes; are being added for the next release. For the latest ThinLTO support,; `build a recent version of clang and LLVM; <https://llvm.org/docs/CMake.html>`_. Linkers; -------; .. _linkers:; .. _linker:. ThinLTO is currently supported for the following linkers:. - **gold (via the gold-plugin)**:; Similar to monolithic LTO, this requires using; a `gold linker configured with plugins enabled; <https://llvm.org/docs/GoldPlugin.html>`_.; - **ld64**:; Starting with `Xcode 8 <https://de",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:1325,perform,performance,1325,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['perform'],['performance']
Performance,"g flags; -------------. * ``-Wthread-safety``: Umbrella flag which turns on the following:. + ``-Wthread-safety-attributes``: Semantic checks for thread safety attributes.; + ``-Wthread-safety-analysis``: The core analysis.; + ``-Wthread-safety-precise``: Requires that mutex expressions match precisely.; This warning can be disabled for code which has a lot of aliases.; + ``-Wthread-safety-reference``: Checks when guarded members are passed by reference. :ref:`negative` are an experimental feature, which are enabled with:. * ``-Wthread-safety-negative``: Negative capabilities. Off by default. When new features and checks are added to the analysis, they can often introduce; additional warnings. Those warnings are initially released as *beta* warnings; for a period of time, after which they are migrated into the standard analysis. * ``-Wthread-safety-beta``: New features. Off by default. .. _negative:. Negative Capabilities; =====================. Thread Safety Analysis is designed to prevent both race conditions and; deadlock. The GUARDED_BY and REQUIRES attributes prevent race conditions, by; ensuring that a capability is held before reading or writing to guarded data,; and the EXCLUDES attribute prevents deadlock, by making sure that a mutex is; *not* held. However, EXCLUDES is an optional attribute, and does not provide the same; safety guarantee as REQUIRES. In particular:. * A function which acquires a capability does not have to exclude it.; * A function which calls a function that excludes a capability does not; have transitively exclude that capability. As a result, EXCLUDES can easily produce false negatives:. .. code-block:: c++. class Foo {; Mutex mu;. void foo() {; mu.Lock();; bar(); // No warning.; baz(); // No warning.; mu.Unlock();; }. void bar() { // No warning. (Should have EXCLUDES(mu)).; mu.Lock();; // ...; mu.Unlock();; }. void baz() {; bif(); // No warning. (Should have EXCLUDES(mu)).; }. void bif() EXCLUDES(mu);; };. Negative requirements are an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst:16758,race condition,race conditions,16758,interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,1,['race condition'],['race conditions']
Performance,"g global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If OpenCL omit.; - Ensures that; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348545,load,load,348545,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"g information does not prevent optimizations from; happening (for example inlining, basic block reordering/merging/cleanup,; tail duplication, etc). * LLVM debug information is automatically optimized along with the rest of; the program, using existing facilities. For example, duplicate; information is automatically merged by the linker, and unused information; is automatically removed. Basically, the debug information allows you to compile a program with; ""``-O0 -g``"" and get full debug information, allowing you to arbitrarily modify; the program as it executes from a debugger. Compiling a program with; ""``-O3 -g``"" gives you full debug information that is always available and; accurate for reading (e.g., you get accurate stack traces despite tail call; elimination and inlining), but you might lose the ability to modify the program; and call functions which were optimized out of the program, or inlined away; completely. The :doc:`LLVM test-suite <TestSuiteMakefileGuide>` provides a framework to; test the optimizer's handling of debugging information. It can be run like; this:. .. code-block:: bash. % cd llvm/projects/test-suite/MultiSource/Benchmarks # or some other level; % make TEST=dbgopt. This will test impact of debugging information on optimization passes. If; debugging information influences optimization passes then it will be reported; as a failure. See :doc:`TestingGuide` for more information on LLVM test; infrastructure and how to run various tests. .. _format:. Debugging information format; ============================. LLVM debugging information has been carefully designed to make it possible for; the optimizer to optimize the program and debugging information without; necessarily having to know anything about debugging information. In; particular, the use of metadata avoids duplicated debugging information from; the beginning, and the global dead code elimination pass automatically deletes; debugging information for a function if it decides to delete th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:5764,optimiz,optimizer,5764,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizer']
Performance,"g input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic also reports the percentage of the time the annotation was; correct during profiling to help developers reason about how to proceed. The diagnostics are also available in the form of optimization remarks,; which can be serialized and processed through the ``opt-viewer.py``; scripts in LLVM. .. option:: -pass-remarks=misexpect. Enables optimization remarks for misexpect when profiling data conflicts with; use of ``llvm.expect`` intrinsics. .. option:: -pgo-warn-misexpect. Enables misexpect warnings when profiling data conflicts with use of; ``llvm.expect`` intrinsics. LLVM supports 4 types of profile formats: Frontend, IR, CS-IR, and; Sampling. MisExpect Diagnostics are compatible with all Profiling formats. +----------------+--------------------------------------------------------------------------------------+; | Profile Type | Description |; +================+===========================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:1959,perform,performance,1959,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,1,['perform'],['performance']
Performance,"g llvm about the control transfer???). - ``{register-name}``: Requires exactly the named physical register. Other constraints are target-specific:. AArch64:. - ``z``: An immediate integer 0. Outputs ``WZR`` or ``XZR``, as appropriate.; - ``I``: An immediate integer valid for an ``ADD`` or ``SUB`` instruction,; i.e. 0 to 4095 with optional shift by 12.; - ``J``: An immediate integer that, when negated, is valid for an ``ADD`` or; ``SUB`` instruction, i.e. -1 to -4095 with optional left shift by 12.; - ``K``: An immediate integer that is valid for the 'bitmask immediate 32' of a; logical instruction like ``AND``, ``EOR``, or ``ORR`` with a 32-bit register.; - ``L``: An immediate integer that is valid for the 'bitmask immediate 64' of a; logical instruction like ``AND``, ``EOR``, or ``ORR`` with a 64-bit register.; - ``M``: An immediate integer for use with the ``MOV`` assembly alias on a; 32-bit register. This is a superset of ``K``: in addition to the bitmask; immediate, also allows immediate integers which can be loaded with a single; ``MOVZ`` or ``MOVL`` instruction.; - ``N``: An immediate integer for use with the ``MOV`` assembly alias on a; 64-bit register. This is a superset of ``L``.; - ``Q``: Memory address operand must be in a single register (no; offsets). (However, LLVM currently does this for the ``m`` constraint as; well.); - ``r``: A 32 or 64-bit integer register (W* or X*).; - ``Uci``: Like r, but restricted to registers 8 to 11 inclusive.; - ``Ucj``: Like r, but restricted to registers 12 to 15 inclusive.; - ``w``: A 32, 64, or 128-bit floating-point, SIMD or SVE vector register.; - ``x``: Like w, but restricted to registers 0 to 15 inclusive.; - ``y``: Like w, but restricted to SVE vector registers Z0 to Z7 inclusive.; - ``Uph``: One of the upper eight SVE predicate registers (P8 to P15); - ``Upl``: One of the lower eight SVE predicate registers (P0 to P7); - ``Upa``: Any of the SVE predicate registers (P0 to P15). AMDGPU:. - ``r``: A 32 or 64-bit int",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:222702,load,loaded,222702,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"g mechanism through so-called `rootmap` files. Effectively,; this means that all classes in the ROOT distributions are directly; available for import. For example:. ``` {.cpp}; from ROOT import TCanvas # available at startup; c = TCanvas(). from ROOT import TLorentzVector # triggers auto-load of libPhysics; l = TLorentzVector(); ```. Although it is not recommended, a simple way of working with `PyROOT` is; doing a global import:. ``` {.cpp}; from ROOT import *. c = TCanvas(); l = TLorentzVector(); ```. Keeping the ROOT namespace (""`import ROOT`""), or only importing from; ROOT those classes that you will actually use (see above), however, will; always be cleaner and clearer:. ``` {.cpp}; import ROOT. c = ROOT.TCanvas(); l = ROOT.TLorentzVector(); ```. Since it is foreseen that most people will use the simple approach; anyway, the request to copy all from module ROOT will not actually; result in copying all ROOT classes into the current namespace. Instead,; classes will still be bound (and possibly loaded) on an as-needed basis.; Note carefully how this is different from other Python (extension); modules, and what to expect if you use the normal inspection tools (such; as e.g. '`dir()`'). This feature prevents the inspection tools from; being swamped by an enormous amount of classes, but they can no longer; be used to explore unknown parts of the system (e.g. to find out which; classes are available). Furthermore, because of this approach,; \<`tab`\>-completion will usually not be available until after the first; use (and hence creation) of a class. Access to class static functions, public data members, enums, etc. is as; expected. Many more example uses of ROOT classes from Python can be; found in the tutorials directory in the ROOT distribution. The recipes; section contains a description on working with your own classes (see; ""Using Your Own Classes""). #### Access to STL Classes. The STL classes live in the ROOT.std namespace (or, if you prefer to get; them from the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:10626,load,loaded,10626,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loaded']
Performance,"g of a loop can be prevented by specifying ``unroll(disable)``. Loop unroll parameters can be controlled by options; `-mllvm -unroll-count=n` and `-mllvm -pragma-unroll-threshold=n`. Loop Distribution; -----------------. Loop Distribution allows splitting a loop into multiple loops. This is; beneficial for example when the entire loop cannot be vectorized but some of the; resulting loops can. If ``distribute(enable))`` is specified and the loop has memory dependencies; that inhibit vectorization, the compiler will attempt to isolate the offending; operations into a new loop. This optimization is not enabled by default, only; loops marked with the pragma are considered. .. code-block:: c++. #pragma clang loop distribute(enable); for (i = 0; i < N; ++i) {; S1: A[i + 1] = A[i] + B[i];; S2: C[i] = D[i] * E[i];; }. This loop will be split into two loops between statements S1 and S2. The; second loop containing S2 will be vectorized. Loop Distribution is currently not enabled by default in the optimizer because; it can hurt performance in some cases. For example, instruction-level; parallelism could be reduced by sequentializing the execution of the; statements S1 and S2 above. If Loop Distribution is turned on globally with; ``-mllvm -enable-loop-distribution``, specifying ``distribute(disable)`` can; be used the disable it on a per-loop basis. Additional Information; ----------------------. For convenience multiple loop hints can be specified on a single line. .. code-block:: c++. #pragma clang loop vectorize_width(4) interleave_count(8); for(...) {; ...; }. If an optimization cannot be applied any hints that apply to it will be ignored.; For example, the hint ``vectorize_width(4)`` is ignored if the loop is not; proven safe to vectorize. To identify and diagnose optimization issues use; `-Rpass`, `-Rpass-missed`, and `-Rpass-analysis` command line options. See the; user guide for details. Extensions to specify floating-point flags; ======================================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:168632,optimiz,optimizer,168632,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,"['optimiz', 'perform']","['optimizer', 'performance']"
Performance,"g outline speed up (factor 10) canvas with many small sub-pads; 2. Let configure user click and double-click handlers, extend tooltip.htm example; 3. Implement workaround for standard THREE.SVGRenderer - no need for patched version; 4. When producing 3D graphical images in batch, use normal THREE.CanvasRenderer; 5. Use WebGL renderer in Chrome headless mode for 3D images generation; 6. Provide possibility to create SVG files for canvas or frame (#172); 7. Support text drawing with TH1 bar option; 8. Fix - when drawing text, reserve extra y range to show it correctly; 9. Migrate to Node.js 8, do not support older versions. ## Changes in 5.5.2; 1. Fix - draw TH2Poly bins outline when no content specified; 2. Fix - always set axis interactive handlers (#170); 3. Fix - take into account zaxis properties when drawing color palette (#171). ## Changes in 5.5.1; 1. Fix - adjust v7 part to new class naming convention, started with R; 2. Fix - show RCanvas title; 3. New - implement 'nocache' option for JSROOT scripts loading. When specified in URL with; JSRootCore.js script, tries to avoid scripts caching problem by adding stamp parameter to all URLs; 4. New - provide simple drawing for TObjString (#164). ## Changes in 5.5.0; 1. Introduce JSROOT.StoreJSON() function. It creates JSON code for the; TCanvas with all drawn objects inside. Allows to store current canvas state; 2. Support ""item=img:file.png"" parameter to insert images in existing layout (#151); 3. Support TTree drawing into TGraph (#153), thanks @cozzyd; 4. Let configure ""&toolbar=right"" in URL to change position of tool buttons; 5. Let configure ""&divsize=500x400"" in URL of size of main div element (default - full browser); 6. Implement ""optstat1001"" and ""optfit101"" draw options for histograms; 7. Remove ""autocol"" options - standard ""plc"" should be used instead; 8. Provide drawing of artificial ""$legend"" item - it creates TLegend for all primitives in pad; Can be used when several histograms or several graphs super",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:33250,load,loading,33250,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"g part; needing manual intervention.; This is broadly true, but realize that that 5% contains the most difficult; cases and is where 20-30% of the effort would have gone in case the bindings; were done fully manually.; It is therefore important to consider what manual tools an automatic binder; offers and to make sure they fit your work style and needs, because you are; going to spend a significant amount of time with them. `LLVM dependency`; -----------------. cppyy depends on `LLVM`_, through Cling.; LLVM is properly internalized, so that it doesn't conflict with other uses;; and in particular it is fine to mix `Numba`_ and cppyy code.; It does mean a download cost of about 20MB for the binary wheel (exact size; differs per platform) on installation, and additional `primarily initial`; memory overheads at run-time.; Whether this is onerous depends strongly not only on the application, but; also on the rest of the software stack. The initial cost of loading cppyy, and thus starting the Cling interpreter,; is about 45MB (platform dependent).; Initial uses of standard (e.g. STL) C++ results in deserialization of the; precompiled header at another eventual total cost of about 25MB (again,; platform dependent).; The actual bindings of course also carry overheads.; As a rule of thumb, you should budget for ~100MB all-in for the overhead; caused by the bindings. Other binders do not have this initial memory overhead, but do of course; occur an overhead per module, class, function, etc.; At scale, however, cppyy has some advantages: all binding is lazy (including; the option of automatic loading), standard classes are never duplicated, and; there is no additional ""per-module"" overhead.; Thus, eventually (depending on the number of classes bound, across how many; modules, what use fraction, etc.), this initial cost is recouped when; compared to other binders.; As a rule of thumb, if about 10% of classes are used, it takes several; hundreds of bound classes before the cppyy-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:8371,load,loading,8371,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,1,['load'],['loading']
Performance,"g s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:329763,load,load,329763,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'load']","['cache', 'load']"
Performance,"g table defines an alternative syntax which is supported; for compatibility with SP3 assembler:. =============================== =========================================================; Syntax Description; =============================== =========================================================; dim:SQ_RSRC_IMG_1D One-dimensional image.; dim:SQ_RSRC_IMG_2D Two-dimensional image.; dim:SQ_RSRC_IMG_3D Three-dimensional image.; dim:SQ_RSRC_IMG_CUBE Cubemap array.; dim:SQ_RSRC_IMG_1D_ARRAY One-dimensional image array.; dim:SQ_RSRC_IMG_2D_ARRAY Two-dimensional image array.; dim:SQ_RSRC_IMG_2D_MSAA Two-dimensional multi-sample auto-aliasing image.; dim:SQ_RSRC_IMG_2D_MSAA_ARRAY Two-dimensional multi-sample auto-aliasing image array.; =============================== =========================================================. dlc; ~~~. See a description :ref:`here<amdgpu_synid_dlc>`. Miscellaneous Modifiers; -----------------------. .. _amdgpu_synid_dlc:. dlc; ~~~. Controls device level cache policy for memory operations. Used for synchronization.; When specified, forces operation to bypass device level cache, making the operation device; level coherent. By default, instructions use device level cache. ======================================== ================================================; Syntax Description; ======================================== ================================================; dlc Bypass device level cache.; ======================================== ================================================. .. _amdgpu_synid_glc:. glc; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`slc<amdgpu_synid_slc>`; to specify cache policy. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== =============================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:17057,cache,cache,17057,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance,"g the target assembler. Assembler; This stage runs the target assembler to translate the output of the; compiler into a target object file. The output of this stage is typically; called a "".o"" file or ""object"" file. Linker; This stage runs the target linker to merge multiple object files into an; executable or dynamic library. The output of this stage is typically called; an ""a.out"", "".dylib"" or "".so"" file. :program:`Clang Static Analyzer`. The Clang Static Analyzer is a tool that scans source code to try to find bugs; through code analysis. This tool uses many parts of Clang and is built into; the same driver. Please see <https://clang-analyzer.llvm.org> for more details; on how to use the static analyzer. OPTIONS; -------. Stage Selection Options; ~~~~~~~~~~~~~~~~~~~~~~~. .. option:: -E. Run the preprocessor stage. .. option:: -fsyntax-only. Run the preprocessor, parser and semantic analysis stages. .. option:: -S. Run the previous stages as well as LLVM generation and optimization stages; and target-specific code generation, producing an assembly file. .. option:: -c. Run all of the above, plus the assembler, generating a target "".o"" object file. .. option:: no stage selection option. If no stage selection option is specified, all stages above are run, and the; linker is run to combine the results into an executable or shared library. Language Selection and Mode Options; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. .. option:: -x <language>. Treat subsequent input files as having type language. .. option:: -std=<standard>. Specify the language standard to compile for. Supported values for the C language are:. | ``c89``; | ``c90``; | ``iso9899:1990``. ISO C 1990. | ``iso9899:199409``. ISO C 1990 with amendment 1. | ``gnu89``; | ``gnu90``. ISO C 1990 with GNU extensions. | ``c99``; | ``iso9899:1999``. ISO C 1999. | ``gnu99``. ISO C 1999 with GNU extensions. | ``c11``; | ``iso9899:2011``. ISO C 2011. | ``gnu11``. ISO C 2011 with GNU extensions. | ``c17``; | ``iso9899:2017``.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:3084,optimiz,optimization,3084,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,1,['optimiz'],['optimization']
Performance,"g the; following scheme:. ``<base>.opt.<format>``. where ``<base>`` is based on the output file of the compilation (whether; it's explicitly specified through `-o` or not) when used with `-c` or `-S`.; For example:. * ``clang -fsave-optimization-record -c in.c -o out.o`` will generate; ``out.opt.yaml``. * ``clang -fsave-optimization-record -c in.c`` will generate; ``in.opt.yaml``. When targeting (Thin)LTO, the base is derived from the output filename, and; the extension is not dropped. When targeting ThinLTO, the following scheme is used:. ``<base>.opt.<format>.thin.<num>.<format>``. Darwin-only: when used for generating a linked binary from a source file; (through an intermediate object file), the driver will invoke `cc1` to; generate a temporary object file. The temporary remark file will be emitted; next to the object file, which will then be picked up by `dsymutil` and; emitted in the .dSYM bundle. This is available for all formats except YAML. For example:. ``clang -fsave-optimization-record=bitstream in.c -o out`` will generate. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.o``. * ``/var/folders/43/9y164hh52tv_2nrdxrj31nyw0000gn/T/a-9be59b.opt.bitstream``. * ``out``. * ``out.dSYM/Contents/Resources/Remarks/out``. Darwin-only: compiling for multiple architectures will use the following; scheme:. ``<base>-<arch>.opt.<format>``. Note that this is incompatible with passing the; :option:`-foptimization-record-file` option. .. option:: -foptimization-record-file. Control the file to which optimization reports are written. This implies; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`. On Darwin platforms, this is incompatible with passing multiple; ``-arch <arch>`` options. .. option:: -foptimization-record-passes. Only include passes which match a specified regular expression. When optimization reports are being output (see; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>`), this; option controls the passes that will",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:12603,optimiz,optimization-record,12603,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization-record']
Performance,"g to a node with no entries. The existence of; ``!noundef`` metadata on the instruction tells the optimizer that the value; loaded is known to be :ref:`well defined <welldefinedvalues>`.; If the value isn't well defined, the behavior is undefined. If the ``!noundef``; metadata is combined with poison-generating metadata like ``!nonnull``,; violation of that metadata constraint will also result in undefined behavior. Semantics:; """""""""""""""""""". The location of memory pointed to is loaded. If the value being loaded; is of scalar type then the number of bytes read does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, loading an ``i24`` reads at most three bytes. When loading a; value of a type like ``i20`` with a size that is not an integral number; of bytes, the result is undefined if the value was not originally; written using a store of the same type.; If the value being loaded is of aggregate type, the bytes that correspond to; padding may be accessed but are ignored, because it is impossible to observe; padding from the loaded aggregate value.; If ``<pointer>`` is not a well-defined value, the behavior is undefined. Examples:; """""""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; store i32 3, ptr %ptr ; yields void; %val = load i32, ptr %ptr ; yields i32:val = i32 3. .. _i_store:. '``store``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. store [volatile] <ty> <value>, ptr <pointer>[, align <alignment>][, !nontemporal !<nontemp_node>][, !invariant.group !<empty_node>] ; yields void; store atomic [volatile] <ty> <value>, ptr <pointer> [syncscope(""<target-scope>"")] <ordering>, align <alignment> [, !invariant.group !<empty_node>] ; yields void; !<nontemp_node> = !{ i32 1 }; !<empty_node> = !{}. Overview:; """""""""""""""""". The '``store``' instruction is used to write to memory. Arguments:; """""""""""""""""""". There are two arguments to the ``store`` instruction: a value to store and an; address at which to store it. The ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:418593,load,loaded,418593,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],['loaded']
Performance,"g x, long long y, long long *diff);; bool __builtin_smul_overflow (int x, int y, int *prod);; bool __builtin_smull_overflow (long x, long y, long *prod);; bool __builtin_smulll_overflow(long long x, long long y, long long *prod);. Each builtin performs the specified mathematical operation on the; first two arguments and stores the result in the third argument. If; possible, the result will be equal to mathematically-correct result; and the builtin will return 0. Otherwise, the builtin will return; 1 and the result will be equal to the unique value that is equivalent; to the mathematically-correct result modulo two raised to the *k*; power, where *k* is the number of bits in the result type. The; behavior of these builtins is well-defined for all argument values. The first three builtins work generically for operands of any integer type,; including boolean types. The operands need not have the same type as each; other, or as the result. The other builtins may implicitly promote or; convert their operands before performing the operation. Query for this feature with ``__has_builtin(__builtin_add_overflow)``, etc. Floating point builtins; ---------------------------------------. ``__builtin_isfpclass``; -----------------------. ``__builtin_isfpclass`` is used to test if the specified floating-point values; fall into one of the specified floating-point classes. **Syntax**:. .. code-block:: c++. int __builtin_isfpclass(fp_type expr, int mask); int_vector __builtin_isfpclass(fp_vector expr, int mask). **Example of use**:. .. code-block:: c++. if (__builtin_isfpclass(x, 448)) {; // `x` is positive finite value; 	 ...; }. **Description**:. The ``__builtin_isfpclass()`` builtin is a generalization of functions ``isnan``,; ``isinf``, ``isfinite`` and some others defined by the C standard. It tests if; the floating-point value, specified by the first argument, falls into any of data; classes, specified by the second argument. The latter is an integer constant; bitmask expressio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:129154,perform,performing,129154,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performing']
Performance,"g); bool __atomic_compare_exchange_N(iN *ptr, iN *expected, iN desired, int success_order, int failure_order). Finally there are some read-modify-write functions, which are only available in; the size-specific variants (any other sizes use a ``__atomic_compare_exchange``; loop)::. iN __atomic_fetch_add_N(iN *ptr, iN val, int ordering); iN __atomic_fetch_sub_N(iN *ptr, iN val, int ordering); iN __atomic_fetch_and_N(iN *ptr, iN val, int ordering); iN __atomic_fetch_or_N(iN *ptr, iN val, int ordering); iN __atomic_fetch_xor_N(iN *ptr, iN val, int ordering); iN __atomic_fetch_nand_N(iN *ptr, iN val, int ordering). This set of library functions have some interesting implementation requirements; to take note of:. - They support all sizes and alignments -- including those which cannot be; implemented natively on any existing hardware. Therefore, they will certainly; use mutexes in for some sizes/alignments. - As a consequence, they cannot be shipped in a statically linked; compiler-support library, as they have state which must be shared amongst all; DSOs loaded in the program. They must be provided in a shared library used by; all objects. - The set of atomic sizes supported lock-free must be a superset of the sizes; any compiler can emit. That is: if a new compiler introduces support for; inline-lock-free atomics of size N, the ``__atomic_*`` functions must also have a; lock-free implementation for size N. This is a requirement so that code; produced by an old compiler (which will have called the ``__atomic_*`` function); interoperates with code produced by the new compiler (which will use native; the atomic instruction). Note that it's possible to write an entirely target-independent implementation; of these library functions by using the compiler atomic builtins themselves to; implement the operations on naturally-aligned pointers of supported sizes, and a; generic mutex implementation otherwise. Libcalls: __sync_*; ==================. Some targets or OS/target combina",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:25439,load,loaded,25439,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loaded']
Performance,"g, as Clang consists of a set of; reusable C++ libraries for building powerful source-level tools. The static; analysis engine used by the Clang Static Analyzer is a Clang library, and has; the capability to be reused in different contexts and by different clients.; Important Points to Consider; While we believe that the static analyzer is already very useful for finding; bugs, we ask you to bear in mind a few points when using it.; Work-in-Progress; The analyzer is a continuous work-in-progress. There are many planned; enhancements to improve both the precision and scope of its analysis algorithms; as well as the kinds of bugs it will find. While there are fundamental; limitations to what static analysis can do, we have a long way to go before; hitting that wall.; Slower than Compilation; Operationally, using static analysis to; automatically find deep program bugs is about trading CPU time for the hardening; of code. Because of the deep analysis performed by state-of-the-art static; analysis tools, static analysis can be much slower than compilation.; While the Clang Static Analyzer is being designed to be as fast and; light-weight as possible, please do not expect it to be as fast as compiling a; program (even with optimizations enabled). Some of the algorithms needed to find; bugs require in the worst case exponential time.; The Clang Static Analyzer runs in a reasonable amount of time by both; bounding the amount of checking work it will do as well as using clever; algorithms to reduce the amount of work it must do to find bugs.; False Positives; Static analysis is not perfect. It can falsely flag bugs in a program where; the code behaves correctly. Because some code checks require more analysis; precision than others, the frequency of false positives can vary widely between; different checks. Our long-term goal is to have the analyzer have a low false; positive rate for most code on all checks.; Please help us in this endeavor by reporting false; positives. Fa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/index.html:2869,perform,performed,2869,interpreter/llvm-project/clang/www/analyzer/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/index.html,2,['perform'],['performed']
Performance,g-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.cpp; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.h; clang-tools-extra/clang-tidy/objc/PropertyDeclarationCheck.h; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.cpp; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; cla,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65061,perform,performance,65061,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"g. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_inv sc0`` is required which will invalidate; the L1 cache. * A ``buffer_inv sc0`` is required to invalidate the L1 cache for coherence; between wavefronts executing in different work-groups as they may be; executing on different CUs. * Atomic read-modify-write instructions implicitly bypass the L1 cache.; Therefore, they do not use the sc0 bit for coherence and instead use it to; indicate if the instruction returns the original value being updated. They; do use sc1 to indicate system or agent scope coherence. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:286052,cache,cache,286052,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"g. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_wbinvl1_vol`` is required as described in; the following item. * A ``buffer_wbinvl1_vol`` is required for coherence between wavefronts; executing in different work-groups as they may be executing on different; CUs. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:235904,cache,cache,235904,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"g.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-copyweak-id-dest-id-src>`_. '``llvm.objc.destroyWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.destroyWeak(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_destroyWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-destroyweak-id-object>`_. '``llvm.objc.initWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.initWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_initWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-initweak>`_. '``llvm.objc.loadWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeak(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweak>`_. '``llvm.objc.loadWeakRetained``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeakRetained(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeakRetained <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweakretained>`_. '``llvm.objc.moveWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.moveWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_moveWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-moveweak-id-dest-id-src>`_. '``llvm.objc.release``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.release(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_release <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-release-id-value>`_. '``llvm.objc.retain``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.retain(ptr). Lowering:; """""""""""""""""". Lowers to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:968895,load,loadWeakRetained,968895,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loadWeakRetained']
Performance,"g; ``while.cond``, the reaching definition for it is either ``1`` or ``4``. This; ``MemoryPhi`` is referred to in the textual IR by the number ``6``.; - ``2 = MemoryDef(6)`` notes that ``store i8 0, ptr %p1`` is a definition,; and its reaching definition before it is ``6``, or the ``MemoryPhi`` after; ``while.cond``. (See the `Use and Def optimization`_ and `Precision`_; sections below for why this ``MemoryDef`` isn't linked to a separate,; disambiguated ``MemoryPhi``.); - ``3 = MemoryDef(6)`` notes that ``store i8 0, ptr %p2`` is a definition; its; reaching definition is also ``6``.; - ``5 = MemoryPhi({if.then,2},{if.else,3})`` notes that the clobber before; this block could either be ``2`` or ``3``.; - ``MemoryUse(5)`` notes that ``load i8, ptr %p1`` is a use of memory, and that; it's clobbered by ``5``.; - ``4 = MemoryDef(5)`` notes that ``store i8 2, ptr %p2`` is a definition; its; reaching definition is ``5``.; - ``MemoryUse(1)`` notes that ``load i8, ptr %p3`` is just a user of memory,; and the last thing that could clobber this use is above ``while.cond`` (e.g.; the store to ``%p3``). In memory versioning parlance, it really only depends on; the memory version 1, and is unaffected by the new memory versions generated since; then. As an aside, ``MemoryAccess`` is a ``Value`` mostly for convenience; it's not; meant to interact with LLVM IR. Design of MemorySSA; ===================. ``MemorySSA`` is an analysis that can be built for any arbitrary function. When; it's built, it does a pass over the function's IR in order to build up its; mapping of ``MemoryAccess``\ es. You can then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:7827,load,load,7827,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['load'],['load']
Performance,"g; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_inv sc0=1 sc1=1. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. GFX940, GFX941; - wavefront - generic buffer/global/flat_store; sc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:305683,load,load,305683,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"g; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/at",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:269704,perform,performing,269704,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"g; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:226006,cache,cache,226006,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"g; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:297008,load,load,297008,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"gAnLLVMNewPMPass`; Information on how to write LLVM transformations under the new pass; manager. :doc:`Passes`; A list of optimizations and analyses implemented in LLVM. :doc:`StackSafetyAnalysis`; This document describes the design of the stack safety analysis of local; variables. :doc:`MergeFunctions`; Describes functions merging optimization. :doc:`AliasAnalysis`; Information on how to write a new alias analysis implementation or how to; use existing analyses. :doc:`MemorySSA`; Information about the MemorySSA utility in LLVM, as well as how to use it. :doc:`LoopTerminology`; A document describing Loops and associated terms as used in LLVM. :doc:`CycleTerminology`; A document describing cycles as a generalization of loops. :doc:`Vectorizers`; This document describes the current status of vectorization in LLVM. :doc:`LinkTimeOptimization`; This document describes the interface between LLVM intermodular optimizer; and the linker and its design. :doc:`GoldPlugin`; How to build your programs with link-time optimization on Linux. :doc:`Remarks`; A reference on the implementation of remarks in LLVM. :doc:`Source Level Debugging with LLVM <SourceLevelDebugging>`; This document describes the design and philosophy behind the LLVM; source-level debugger. :doc:`How to Update Debug Info <HowToUpdateDebugInfo>`; This document specifies how to correctly update debug info in various kinds; of code transformations. :doc:`InstrRefDebugInfo`; This document explains how LLVM uses value tracking, or instruction; referencing, to determine variable locations for debug info in the final; stages of compilation. :doc:`RemoveDIsDebugInfo`; This is a migration guide describing how to move from debug info using; intrinsics such as dbg.value to using the non-instruction DPValue object. :doc:`InstrProfileFormat`; This document explains two binary formats of instrumentation-based profiles. Code Generation; ---------------. :doc:`WritingAnLLVMBackend`; Information on how to write LLVM backends f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst:3751,optimiz,optimization,3751,interpreter/llvm-project/llvm/docs/UserGuides.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst,1,['optimiz'],['optimization']
Performance,"gRef, ..., StringRef; Matches when at least one of the supplied string equals to the; Selector.getAsString(). matcher = objCMessageExpr(hasSelector(""methodA:"", ""methodB:""));; matches both of the expressions below:; [myObj methodA:argA];; [myObj methodB:argB];. Matcher<ObjCMessageExpr>hasKeywordSelector; Matches when the selector is a keyword selector. objCMessageExpr(hasKeywordSelector()) matches the generated setFrame; message expression in. UIWebView *webView = ...;; CGRect bodyFrame = webView.frame;; bodyFrame.size.height = self.bodyContentHeight;; webView.frame = bodyFrame;; // ^---- matches here. Matcher<ObjCMessageExpr>hasNullSelector; Matches when the selector is the empty selector. Matches only when the selector of the objCMessageExpr is NULL. This may; represent an error condition in the tree!. Matcher<ObjCMessageExpr>hasSelectorstd::string BaseName; Matches when BaseName == Selector.getAsString(). matcher = objCMessageExpr(hasSelector(""loadHTMLString:baseURL:""));; matches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>hasUnarySelector; Matches when the selector is a Unary Selector. matcher = objCMessageExpr(matchesSelector(hasUnarySelector());; matches self.bodyView in the code below, but NOT the outer message; invocation of ""loadHTMLString:baseURL:"".; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>isClassMessage; Returns true when the Objective-C message is sent to a class. Example; matcher = objcMessageExpr(isClassMessage()); matches; [NSString stringWithFormat:@""format""];; but not; NSString *x = @""hello"";; [x containsString:@""h""];. Matcher<ObjCMessageExpr>isInstanceMessage; Returns true when the Objective-C message is sent to an instance. Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:106735,load,loadHTMLString,106735,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,['load'],['loadHTMLString']
Performance,gation-DL-Cpu COMMAND testBackpropagationDLCpu). # DNN - Batch normalization; ROOT_EXECUTABLE(testBatchNormalizationCpu TestBatchNormalizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cpu COMMAND testBatchNormalizationCpu). # DNN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBR,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:6071,Optimiz,Optimization,6071,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization']
Performance,"ge and preprocessor options that particular module variant was built with. ``-fmodules-decluse``; Enable checking of module ``use`` declarations. ``-fmodule-name=module-id``; Consider a source file as a part of the given module. ``-fmodule-map-file=<file>``; Load the given module map file if a header from its directory or one of its subdirectories is loaded. ``-fmodules-search-all``; If a symbol is not found, search modules referenced in the current module maps but not imported for symbols, so the error message can reference the module by name. Note that if the global module index has not been built before, this might take some time as it needs to build all the modules. Note that this option doesn't apply in module builds, to avoid the recursion. ``-fno-implicit-modules``; All modules used by the build must be specified with ``-fmodule-file``. ``-fmodule-file=[<name>=]<file>``; Specify the mapping of module names to precompiled module files. If the; name is omitted, then the module file is loaded whether actually required; or not. If the name is specified, then the mapping is treated as another; prebuilt module search mechanism (in addition to ``-fprebuilt-module-path``); and the module is only loaded if required. Note that in this case the; specified file also overrides this module's paths that might be embedded; in other precompiled module files. ``-fprebuilt-module-path=<directory>``; Specify the path to the prebuilt modules. If specified, we will look for modules in this directory for a given top-level module name. We don't need a module map for loading prebuilt modules in this directory and the compiler will not try to rebuild these modules. This can be specified multiple times. ``-fprebuilt-implicit-modules``; Enable prebuilt implicit modules. If a prebuilt module is not found in the; prebuilt modules paths (specified via ``-fprebuilt-module-path``), we will; look for a matching implicit module in the prebuilt modules paths. -cc1 Options; ~~~~~~~~~~~~. ``-fmodu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:17257,load,loaded,17257,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['load'],['loaded']
Performance,"ge as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1820,load,loaded,1820,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,1,['load'],['loaded']
Performance,"ge, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates through the relocations for; that sections. For each relocation, it calls the format-specific; processRelocationRef method, which will examine the relocation and store; it in one of two data structures, a section-based relocation list map and; an external symbol relocation map. .. image:: MCJIT-load-object.png. When RuntimeDyldImpl::loadObject returns, all of the code and data; sections for the object will have been loaded into memory allocated by the; memory manager and relocation information will have been prepared, but the; relocations have not yet been applied and the generated code is still not; ready to be executed. [Currently (as of August 2013) the MCJIT engine will immediately apply; relocations when loadObject completes. However, this shouldn't be; happening. Because the code may have been generated for a remote target,; the client should be given a chance to re-map the section addresses before; relocations are applied. It is possible to apply relocations multiple; times, but in the case where addresses are to be re-mapped, this first; application is wasted effort.]. Address Remapping; =================. At any time after initial code has been generated and before; finalizeObject is called, the client can remap the address of sections in; the object. Typically this",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:5122,load,load-object,5122,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['load-object']
Performance,"generating functions (only if arc4random; function is available):; drand48; erand48; jrand48; lcong48; lrand48; mrand48; nrand48; random; rand_r. void test() {; random(); // warn; }. security.insecureAPI.strcpy; (C); Warn on uses of the strcpy and strcat functions. void test() {; char x[4];; char *y = ""abcd"";. strcpy(x, y); // warn; }. security.insecureAPI.vfork; (C); Warn on uses of the vfork function. void test() {; vfork(); // warn; }. security.insecureAPI.decodeValueOfObjCType; (ObjC); Warn on uses of the -[NSCoder decodeValueOfObjCType:at:] method.; The safe alternative is -[NSCoder decodeValueOfObjCType:at:size:]. void test(NSCoder *decoder) {; // This would be a vulnerability on 64-bit platforms; // but not on 32-bit platforms.; NSUInteger x;; [decoder decodeValueOfObjCType:""I"" at:&x]; // warn; }. Unix Checkers. Name, DescriptionExample. unix.API; (C); Check calls to various UNIX/POSIX functions:; open; pthread_once; calloc; malloc; realloc; alloca. // Currently the check is performed for apple targets only.; void test(const char *path) {; int fd = open(path, O_CREAT);; // warn: call to 'open' requires a third argument when the; // 'O_CREAT' flag is set; }. void f();. void test() {; pthread_once_t pred = {0x30B1BCBA, {0}};; pthread_once(&pred, f);; // warn: call to 'pthread_once' uses the local variable; }. void test() {; void *p = malloc(0); // warn: allocation size of 0 bytes; }. void test() {; void *p = calloc(0, 42); // warn: allocation size of 0 bytes; }. void test() {; void *p = malloc(1);; p = realloc(p, 0); // warn: allocation size of 0 bytes; }. void test() {; void *p = alloca(0); // warn: allocation size of 0 bytes; }. void test() {; void *p = valloc(0); // warn: allocation size of 0 bytes; }. unix.Malloc; (C); Check for memory leaks, double free, and use-after-free and offset problems; involving malloc. void test() {; int *p = malloc(1);; free(p);; free(p); // warn: attempt to free released memory; }. void test() {; int *p = malloc(sizeof(int));; f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html:23425,perform,performed,23425,interpreter/llvm-project/clang/www/analyzer/available_checks.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html,2,['perform'],['performed']
Performance,"generator; supported by your development environment. If you want an alternative generator,; you must tell this to CMake with the ``-G`` option. .. todo::. Explain variables and cache. Move explanation here from #options section. .. _Options and variables:. Options and variables; =====================. Variables customize how the build will be generated. Options are boolean; variables, with possible values ON/OFF. Options and variables are defined on the; CMake command line like this:. .. code-block:: console. $ cmake -DVARIABLE=value path/to/llvm/source. You can set a variable after the initial CMake invocation to change its; value. You can also undefine a variable:. .. code-block:: console. $ cmake -UVARIABLE path/to/llvm/source. Variables are stored in the CMake cache. This is a file named ``CMakeCache.txt``; stored at the root of your build directory that is generated by ``cmake``.; Editing it yourself is not recommended. Variables are listed in the CMake cache and later in this document with; the variable name and type separated by a colon. You can also specify the; variable and type on the CMake command line:. .. code-block:: console. $ cmake -DVARIABLE:TYPE=value path/to/llvm/source. Frequently-used CMake variables; -------------------------------. Here are some of the CMake variables that are used often, along with a; brief explanation. For full documentation, consult the CMake manual,; or execute ``cmake --help-variable VARIABLE_NAME``. See `Frequently; Used LLVM-related Variables`_ below for information about commonly; used variables that control features of LLVM and enabled subprojects. .. _cmake_build_type:. **CMAKE_BUILD_TYPE**:STRING; This configures the optimization level for ``make`` or ``ninja`` builds. Possible values:. =========================== ============= ========== ========== ==========================; Build Type Optimizations Debug Info Assertions Best suited for; =========================== ============= ========== ========== ============",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:6291,cache,cache,6291,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,"generic class creates Instruments that provide; no extra information, and InstrumentManager never overrides the default; schedule class for a given instruction. EXIT STATUS; -----------. :program:`llvm-mca` returns 0 on success. Otherwise, an error message is printed; to standard error, and the tool returns 1. USING MARKERS TO ANALYZE SPECIFIC CODE BLOCKS; ---------------------------------------------; :program:`llvm-mca` allows for the optional usage of special code comments to; mark regions of the assembly code to be analyzed. A comment starting with; substring ``LLVM-MCA-BEGIN`` marks the beginning of an analysis region. A; comment starting with substring ``LLVM-MCA-END`` marks the end of a region.; For example:. .. code-block:: none. # LLVM-MCA-BEGIN; ...; # LLVM-MCA-END. If no user-defined region is specified, then :program:`llvm-mca` assumes a; default region which contains every instruction in the input file. Every region; is analyzed in isolation, and the final performance report is the union of all; the reports generated for every analysis region. Analysis regions can have names. For example:. .. code-block:: none. # LLVM-MCA-BEGIN A simple example; add %eax, %eax; # LLVM-MCA-END. The code from the example above defines a region named ""A simple example"" with a; single instruction in it. Note how the region name doesn't have to be repeated; in the ``LLVM-MCA-END`` directive. In the absence of overlapping regions,; an anonymous ``LLVM-MCA-END`` directive always ends the currently active user; defined region. Example of nesting regions:. .. code-block:: none. # LLVM-MCA-BEGIN foo; add %eax, %edx; # LLVM-MCA-BEGIN bar; sub %eax, %edx; # LLVM-MCA-END bar; # LLVM-MCA-END foo. Example of overlapping regions:. .. code-block:: none. # LLVM-MCA-BEGIN foo; add %eax, %edx; # LLVM-MCA-BEGIN bar; sub %eax, %edx; # LLVM-MCA-END foo; add %eax, %edx; # LLVM-MCA-END bar. Note that multiple anonymous regions cannot overlap. Also, overlapping regions; cannot have the same name.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:8755,perform,performance,8755,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,"generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Coul",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:281024,load,load,281024,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ger types (e.g.,; i8, i16, i32, ..., in Rust). ``-fsanitize-cfi-icall-experimental-normalize-integers`` is compatible with; ``-fsanitize-cfi-icall-generalize-pointers``. This option is currently experimental. .. _cfi-canonical-jump-tables:. ``-fsanitize-cfi-canonical-jump-tables``; ----------------------------------------. The default behavior of Clang's indirect function call checker will replace; the address of each CFI-checked function in the output file's symbol table; with the address of a jump table entry which will pass CFI checks. We refer; to this as making the jump table `canonical`. This property allows code that; was not compiled with ``-fsanitize=cfi-icall`` to take a CFI-valid address; of a function, but it comes with a couple of caveats that are especially; relevant for users of cross-DSO CFI:. - There is a performance and code size overhead associated with each; exported function, because each such function must have an associated; jump table entry, which must be emitted even in the common case where the; function is never address-taken anywhere in the program, and must be used; even for direct calls between DSOs, in addition to the PLT overhead. - There is no good way to take a CFI-valid address of a function written in; assembly or a language not supported by Clang. The reason is that the code; generator would need to insert a jump table in order to form a CFI-valid; address for assembly functions, but there is no way in general for the; code generator to determine the language of the function. This may be; possible with LTO in the intra-DSO case, but in the cross-DSO case the only; information available is the function declaration. One possible solution; is to add a C wrapper for each assembly function, but these wrappers can; present a significant maintenance burden for heavy users of assembly in; addition to adding runtime overhead. For these reasons, we provide the option of making the jump table non-canonical; with the flag ``-fno-sanitize-cfi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:12204,perform,performance,12204,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,1,['perform'],['performance']
Performance,"ges in the source IR can have a large effect on the; generated code. Beyond the specific items on the list below, it's worth noting that the most; mature frontend for LLVM is Clang. As a result, the further your IR gets from; what Clang might emit, the less likely it is to be effectively optimized. It; can often be useful to write a quick C program with the semantics you're trying; to model and see what decisions Clang's IRGen makes about what IR to emit.; Studying Clang's CodeGen directory can also be a good source of ideas. Note; that Clang and LLVM are explicitly version locked so you'll need to make sure; you're using a Clang built from the same git revision or release as the LLVM; library you're using. As always, it's *strongly* recommended that you track; tip of tree development, particularly during bring up of a new project. The Basics; ^^^^^^^^^^^. #. Make sure that your Modules contain both a data layout specification and; target triple. Without these pieces, non of the target specific optimization; will be enabled. This can have a major effect on the generated code quality. #. For each function or global emitted, use the most private linkage type; possible (private, internal or linkonce_odr preferably). Doing so will; make LLVM's inter-procedural optimizations much more effective. #. Avoid high in-degree basic blocks (e.g. basic blocks with dozens or hundreds; of predecessors). Among other issues, the register allocator is known to; perform badly with confronted with such structures. The only exception to; this guidance is that a unified return block with high in-degree is fine. Use of allocas; ^^^^^^^^^^^^^^. An alloca instruction can be used to represent a function scoped stack slot,; but can also represent dynamic frame expansion. When representing function; scoped variables or locations, placing alloca instructions at the beginning of; the entry block should be preferred. In particular, place them before any; call instructions. Call instructions might ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:1506,optimiz,optimization,1506,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimization']
Performance,"get as a dependency. After configuration, building the stage2-instrumented-generate-profdata target; will automatically build the stage1 compiler, build the instrumented compiler; with the stage1 compiler, and then run the instrumented compiler against the; perf training data:. .. code-block:: console. $ ninja stage2-instrumented-generate-profdata. If you let that run for a few hours or so, it will place a profdata file in your; build directory. This takes a really long time because it builds clang twice,; and you *must* have compiler-rt in your build tree. This process uses any source files under the perf-training directory as training; data as long as the source files are marked up with LIT-style RUN lines. After it finishes you can use :code:`find . -name clang.profdata` to find it, but it; should be at a path something like:. .. code-block:: console. <build dir>/tools/clang/stage2-instrumented-bins/utils/perf-training/clang.profdata. You can feed that file into the LLVM_PROFDATA_FILE option when you build your; optimized compiler. It may be necessary to build additional targets before running perf training, such as; builtins and runtime libraries. You can use the :code:`CLANG_PGO_TRAINING_DEPS` CMake; variable for that purpose:. .. code-block:: cmake. set(CLANG_PGO_TRAINING_DEPS builtins runtimes CACHE STRING """"). The PGO cache has a slightly different stage naming scheme than other; multi-stage builds. It generates three stages: stage1, stage2-instrumented, and; stage2. Both of the stage2 builds are built using the stage1 compiler. The PGO cache generates the following additional targets:. **stage2-instrumented**; Builds a stage1 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. **stage2-instrumented-generate-profdata**; Depends on stage2-instrumented and will use the instrumented compiler to; generate profdata based on the training files in clang/utils/perf-training. **stage2**;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:8310,optimiz,optimized,8310,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimized']
Performance,"get; encoded in the bitcode file. See the output of **llc -help** for a list of; valid architectures. By default this is inferred from the target triple or; autodetected to the current architecture. .. option:: -mcpu=cpuname. Specify a specific chip in the current architecture to generate code for.; By default this is inferred from the target triple and autodetected to; the current architecture. For a list of available CPUs, use:; **llvm-as < /dev/null | llc -march=xyz -mcpu=help**. .. option:: -mattr=a1,+a2,-a3,... Override or control specific attributes of the target, such as whether SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:; **llvm-as < /dev/null | llc -march=xyz -mattr=help**. FLOATING POINT OPTIONS; ----------------------. .. option:: -disable-excess-fp-precision. Disable optimizations that may increase floating point precision. .. option:: -enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: -enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: -enable-unsafe-fp-math. Causes :program:`lli` to enable optimizations that may decrease floating point; precision. .. option:: -soft-float. Causes :program:`lli` to generate software floating point library calls instead of; equivalent hardware instructions. CODE GENERATION OPTIONS; -----------------------. .. option:: -code-model=model. Choose the code model from:. .. code-block:: text. default: Target default code model; tiny: Tiny code model; small: Small code model; kernel: Kernel code model; medium: Medium code model; large: Large code model. .. option:: -disable-post-RA-scheduler. Disable scheduling after register allocation. .. option:: -disable-spill-fusing. Disable fusing of spill code into instructions. .. option:: -jit-enable-eh. Exception handling should be enabled in the just-in-time compiler. .. option:: -join-liveintervals. Coalesce copies (default=tru",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst:3111,optimiz,optimizations,3111,interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lli.rst,1,['optimiz'],['optimizations']
Performance,"gfx942-table`. .. table:: AMDHSA Memory Model Code Sequences GFX940, GFX941, GFX942; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx940-gfx941-gfx942-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX940, GFX941, GFX942; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; nt=1. - volatile. 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. GFX940, GFX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:293028,load,load,293028,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"gger``. The ``AliasSetTracker`` class (which is used by ``LICM``) makes a; non-deterministic number of alias queries. This can cause debugging techniques; involving pausing execution after a predetermined number of queries to be; unreliable. Many alias queries can be reformulated in terms of other alias queries. When; multiple ``AliasAnalysis`` queries are chained together, it would make sense to; start those queries from the beginning of the chain, with care taken to avoid; infinite looping, however currently an implementation which wants to do this can; only start such queries from itself. Using alias analysis results; ============================. There are several different ways to use alias analysis results. In order of; preference, these are:. Using the ``MemoryDependenceAnalysis`` Pass; -------------------------------------------. The ``memdep`` pass uses alias analysis to provide high-level dependence; information about memory-using instructions. This will tell you which store; feeds into a load, for example. It uses caching and other techniques to be; efficient, and is used by Dead Store Elimination, GVN, and memcpy optimizations. .. _AliasSetTracker:. Using the ``AliasSetTracker`` class; -----------------------------------. Many transformations need information about alias **sets** that are active in; some scope, rather than information about pairwise aliasing. The; `AliasSetTracker <https://llvm.org/doxygen/classllvm_1_1AliasSetTracker.html>`__; class is used to efficiently build these Alias Sets from the pairwise alias; analysis information provided by the ``AliasAnalysis`` interface. First you initialize the AliasSetTracker by using the ""``add``"" methods to add; information about various potentially aliasing instructions in the scope you are; interested in. Once all of the alias sets are completed, your pass should; simply iterate through the constructed alias sets, using the ``AliasSetTracker``; ``begin()``/``end()`` methods. The ``AliasSet``\s formed ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:19673,load,load,19673,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['load'],['load']
Performance,"gging; contiguous function argument sequences starting with argument zero, which will; be the ""this"" pointer for member function invocations. For example, we don't; support logging the first and third argument. A reader of the memory format must maintain a state machine. The format makes no; attempt to pad for alignment, and it is not seekable. Function Records; ----------------. Function Records have an 8 byte layout. This layout encodes information to; reconstruct a call stack of instrumented function and their durations. +---------------+--------------+-----------------------------------------------+; | Field | Size (bits) | Description |; +===============+==============+===============================================+; | discriminant | ``1`` | Indicates whether a reader should read a |; | | | Function or Metadata record. Set to ``0`` for |; | | | Function records. |; +---------------+--------------+-----------------------------------------------+; | action | ``3`` | Specifies whether the function is being |; | | | entered, exited, or is a non-standard entry |; | | | or exit produced by optimizations. |; +---------------+--------------+-----------------------------------------------+; | function_id | ``28`` | A numeric ID for the function. Resolved to a |; | | | name via the xray instrumentation map. The |; | | | instrumentation map is built by xray at |; | | | compile time into an object file and pairs |; | | | the function ids to addresses. It is used for |; | | | patching and as a lookup into the binary's |; | | | symbols to obtain names. |; +---------------+--------------+-----------------------------------------------+; | tsc_delta | ``32`` | The number of ticks of the timestamp counter |; | | | since a previous record recorded a delta or |; | | | other TSC resetting event. |; +---------------+--------------+-----------------------------------------------+. On little-endian machines, the bitfields are ordered from least significant bit; bit to most significan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst:5618,optimiz,optimizations,5618,interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRayFDRFormat.rst,1,['optimiz'],['optimizations']
Performance,"gh the C API is like the following:. .. code-block:: c. LLVMRemarkParserRef Parser = LLVMRemarkParserCreateYAML(Buf, Size);; LLVMRemarkEntryRef Remark = NULL;; while ((Remark = LLVMRemarkParserGetNext(Parser))) {; // use Remark; LLVMRemarkEntryDispose(Remark); // Release memory.; }; bool HasError = LLVMRemarkParserHasError(Parser);; LLVMRemarkParserDispose(Parser);. Remark streamers; ================. The ``RemarkStreamer`` interface is used to unify the serialization; capabilities of remarks across all the components that can generate remarks. All remark serialization should go through the main remark streamer, the; ``llvm::remarks::RemarkStreamer`` set up in the ``LLVMContext``. The interface; takes remark objects converted to ``llvm::remarks::Remark``, and takes care of; serializing it to the requested format, using the requested type of metadata,; etc. Typically, a specialized remark streamer will hold a reference to the one set; up in the ``LLVMContext``, and will operate on its own type of diagnostics. For example, LLVM IR passes will emit ``llvm::DiagnosticInfoOptimization*``; that get converted to ``llvm::remarks::Remark`` objects. Then, clang could set; up its own specialized remark streamer that takes ``clang::Diagnostic``; objects. This can allow various components of the frontend to emit remarks; using the same techniques as the LLVM remarks. This gives us the following advantages:. * Composition: during the compilation pipeline, multiple components can set up; their specialized remark streamers that all emit remarks through the same; main streamer.; * Re-using the remark infrastructure in ``lib/Remarks``.; * Using the same file and format for the remark emitters created throughout the; compilation. at the cost of an extra layer of abstraction. .. FIXME: add documentation for llvm-opt-report.; .. FIXME: add documentation for Passes supporting optimization remarks; .. FIXME: add documentation for IR Passes; .. FIXME: add documentation for CodeGen Passes; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:18433,optimiz,optimization,18433,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,"gh the vtable.; This is not the same as the linkage of the vtable, because call sites could be; using a pointer of a more widely-visible base class. For example, consider this; code:. .. code-block:: c++. __attribute__((visibility(""default""))); struct A {; virtual void f();; };. __attribute__((visibility(""hidden""))); struct B : A {; virtual void f();; };. With LTO, we know that all code which can see the declaration of ``B`` is; visible to us. However, a pointer to a ``B`` could be cast to ``A*`` and passed; to another linkage unit, which could then call ``f`` on it. This call would; load from the vtable for ``B`` (using the object pointer), and then call; ``B::f``. This means we can't remove the function pointer from ``B``'s vtable,; or the implementation of ``B::f``. However, if we can see all code which knows; about any dynamic base class (which would be the case if ``B`` only inherited; from classes with hidden visibility), then this optimisation would be valid. This concept is represented in IR by the ``!vcall_visibility`` metadata; attached to vtable objects, with the following values:. .. list-table::; :header-rows: 1; :widths: 10 90. * - Value; - Behavior. * - 0 (or omitted); - **Public**; Virtual function calls using this vtable could be made from external; code. * - 1; - **Linkage Unit**; All virtual function calls which might use this vtable are in the; current LTO unit, meaning they will be in the current module once; LTO linking has been performed. * - 2; - **Translation Unit**; All virtual function calls which might use this vtable are in the; current module. In addition, all function pointer loads from a vtable marked with the; ``!vcall_visibility`` metadata (with a non-zero value) must be done using the; :ref:`llvm.type.checked.load <type.checked.load>` intrinsic, so that virtual; calls sites can be correlated with the vtables which they might load from.; Other parts of the vtable (RTTI, offset-to-top, ...) can still be accessed with; normal loads.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TypeMetadata.rst:10176,perform,performed,10176,interpreter/llvm-project/llvm/docs/TypeMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TypeMetadata.rst,6,"['load', 'perform']","['load', 'loads', 'performed']"
Performance,"gical and Physical Objects. Some viewers can support two types of object placement:. - Add object as a single independent entity in the world reference; frame - e.g. a sphere, radius `r`, at `x`, `y`, `z`. - Repeated placement (copying) in world frame of this locally unique; piece of geometry (described in local reference frame) e.g. define a; sphere `S` (radius `r`), place copy at `x1`, `y1`, `z1`, another; copy at `x2`, `y2`, `z2` etc. The second case is very typical in geometry packages, e.g. ROOT's; **`TGeo`** package, GEANT4 etc, where we have very large number repeated; placements of relatively few unique ""shapes"". Some viewers (GL Viewer only at present) are able to take advantage of; this by identifying unique logical shapes from the `fID` logical ID; member of **`TBuffer3D`**. If repeated addition of the same `fID` is; found, the shape is cached already - and the costly tessellation does; not need to be sent again. The viewer can also perform internal GL; specific caching (display lists) with considerable performance gains in; these cases. For this to work correctly the logical object in must be; described in **`TBuffer3D`** in the local reference frame, complete with; the local`/`master translation. In some cases you will not have a real; object you can reasonably set **`TBuffer3D::fID` to, or the object is; recycled or temporary. To suppress internal caching in the GL Viewer in; these cases, set `TBuffer3D::fID` to 0 (null).**. The viewer indicates it can support local frame objects through the; **`TVirtualViewer3D`** interface method: `PreferLocalFrame()`. If this; returns `kTRUE` you can make repeated calls to `AddObject()`, with; **`TBuffer3D`** containing the same `fID`, and different `fLocalMaster`; placements. For viewers supporting logical/physical objects, the TBuffer3D content; refers to the properties of the logical object, with the exception of:. - `fLocalMaster` transform. - `fColor `. - `fTransparency`. attributes, which can be varied for **ea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:135152,perform,perform,135152,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,2,['perform'],"['perform', 'performance']"
Performance,"ginal size of the data sample (used whenever bagging is used). Boost_MethodWeightType No ByError ByError, Average, ByROC, ByOverlap, LastMethod How to set the final weight of the boosted classifiers. Boost_RecalculateMVACut No True  Recalculate the classifier MVA Signallike cut at every boost iteration. Boost_AdaBoostBeta No 1  The ADA boost parameter that sets the effect of every boost step on the events' weights. Boost_Transform No step step, linear, log, gauss Type of transform applied to every boosted method linear, log, step. Boost_RandomSeed No 0  Seed for random number generator used for bagging. Configuration options for MVA method :. Configuration options reference for MVA method: RuleFit. Option Array Default value Predefined values Description. V No False  Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None  List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False  Print method-specific help message. CreateMVAPdfs No False  Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False  Events with negative weights are ignored in the training (but are included for testing and performance evaluation). GDTau No -1  Gradient-directed (GD) path: default fit cut-off. GDTauPrec No 0.01  GD path: precision of tau. GDStep No 0.01  GD path: step size. GDNSteps No 10000  GD path: number of steps. GDErrScale No 1.1  Stop scan when error > scale*errmin. LinQuantile No 0.025  Quantile of linear terms (removes outliers). GDPathEveFrac No 0.5  Fraction of events used for the path search. GDValidEveFrac No 0.5  Fraction ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:18000,perform,performed,18000,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"gisters <gcroot>`. Conservative garbage collection is attractive because it does not require any; special compiler support, but it does have problems. In particular, because the; conservative garbage collector cannot *know* that a particular word in the; machine is a pointer, it cannot move live objects in the heap (preventing the; use of compacting and generational GC algorithms) and it can occasionally suffer; from memory leaks due to integer values that happen to point to objects in the; program. In addition, some aggressive compiler transformations can break; conservative garbage collectors (though these seem rare in practice). Accurate garbage collectors do not suffer from any of these problems, but they; can suffer from degraded scalar optimization of the program. In particular,; because the runtime must be able to identify and update all pointers active in; the program, some optimizations are less effective. In practice, however, the; locality and performance benefits of using aggressive garbage collection; techniques dominates any low-level losses. This document describes the mechanisms and interfaces provided by LLVM to; support accurate garbage collection. Goals and non-goals; -------------------. LLVM's intermediate representation provides :ref:`garbage collection intrinsics; <gc_intrinsics>` that offer support for a broad class of collector models. For; instance, the intrinsics permit:. * semi-space collectors. * mark-sweep collectors. * generational collectors. * incremental collectors. * concurrent collectors. * cooperative collectors. * reference counting. We hope that the support built into the LLVM IR is sufficient to support a; broad class of garbage collected languages including Scheme, ML, Java, C#,; Perl, Python, Lua, Ruby, other scripting languages, and more. Note that LLVM **does not itself provide a garbage collector** --- this should; be part of your language's runtime library. LLVM provides a framework for; describing the garbage collectors",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:4776,perform,performance,4776,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['performance']
Performance,"glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:345364,load,load,345364,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"gle volume in our geometry, but since any volume; needs to have an associated medium, we will create a dummy one. You can; safely ignore the following lines for the time being, since materials; and media will be explained in detail later on. ~~~{.cpp}; root[] TGeoMaterial *mat = new TGeoMaterial(""Vacuum"",0,0,0);; root[] TGeoMedium *med = new TGeoMedium(""Vacuum"",1,mat);; ~~~. We can finally make our volume having a box shape. Note that the world; volume does not need to be a box - it can be any other shape. Generally,; boxes and tubes are the most recommendable shapes for this purpose due; to their fast navigation algorithms. ~~~{.cpp}; root[] TGeoVolume *top=gGeoManager->MakeBox(""Top"",med,10.,10.,10.);; ~~~. The default units are in centimeters. Now we want to make this volume; our world. We have to do this operation **before** closing the geometry. ~~~{.cpp}; root[] gGeoManager->SetTopVolume(top);; ~~~. This should be enough, but it is not since always after defining some; geometry hierarchy, TGeo needs to build some optimization; structures and perform some checks. Note the messages posted after the; statement is executed. We will describe the corresponding operations; later. ~~~{.cpp}; root[] gGeoManager->CloseGeometry();; ~~~. Now we are really done with geometry building stage, but we would like; to see our simple world:. ~~~{.cpp}; root[] top->SetLineColor(kMagenta);; root[] gGeoManager->SetTopVisible(); // the TOP is invisible; root[] top->Draw();; ~~~. \anchor GP00b; ### Example 2: A Geometrical Hierarchy Look and Feel. Before going further, let us get a look and feel of interacting with the; modeller. For this, we will use one of the examples illustrating the; geometry package. To get an idea on the geometry structure created in; this example, just look at rootgeom.C. You will; notice that this is a bit more complex that just creating the ""world""; since several other volumes are created and put together in a hierarchy.; The purpose here is just to learn how",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:6544,optimiz,optimization,6544,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic sc1=1; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load sc0=1; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:295549,load,load,295549,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify '2d', 'io' (default) or '3d'.; 13. Use new AssertPrerequisites functionality to load only required functionality.; 14. When displaying single element, one could specify draw options and monitor property like:; <http://localhost:8080/Files/job1.root/hpxpy/draw.htm?opt=col&monitor=2000>; Such link is best possibility to integrate display into different HTML pages,; using `<iframe/>` tag like:; `<iframe src=""http://localhost:8080/Files/job1.root/hpx/draw.htm""`; `style=""width: 800px; height:600px""></iframe>`; 15. Remove 'JSROOTIO.' prefix from _typename. Now real class name is used.; 16. Use in all scripts JSROOT as central 'namespace'; 17. Introduce context menu in 3D, use it for switch between 2D/3D modes; 18. Use own code to generate hierarchical structure in HTML, replace dtree.js which is; extremely slow for complex hierarchies. Dramatically improve performance for; structures with large (~1000) number of items.; 19. Deliver to the server title of the objects, display it as hint in the browser.; 20. Better handling of special characters in the hierarchies -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:75345,load,load,75345,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['load']
Performance,"global/ds/flat_store; - wavefront - local; - generic; store atomic release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to local have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw release - workgroup - global 1. s_waitcnt lgkmcnt(0); - generic; - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_wait",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:220072,load,load,220072,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If not TgSplit execution; mode, omit glc=1. load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic glc=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; -----------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:244064,load,load,244064,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cs",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:281551,load,load,281551,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen bef",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:326810,load,loads,326810,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"gnize ""``COM``"", ""``RUN``"", or any user-defined comment; prefix as a comment directive if it's combined with one of the usual check; directive suffixes, such as ""``-NEXT:``"" or ""``-NOT:``"", discussed below.; FileCheck treats such a combination as plain text instead. If it needs to act; as a comment directive for your test environment, define it as such with; :option:`--comment-prefixes`. The ""CHECK-NEXT:"" directive; ~~~~~~~~~~~~~~~~~~~~~~~~~~~. Sometimes you want to match lines and would like to verify that matches; happen on exactly consecutive lines with no other lines in between them. In; this case, you can use ""``CHECK:``"" and ""``CHECK-NEXT:``"" directives to specify; this. If you specified a custom check prefix, just use ""``<PREFIX>-NEXT:``"".; For example, something like this works as you'd expect:. .. code-block:: llvm. define void @t2(<2 x double>* %r, <2 x double>* %A, double %B) {; 	%tmp3 = load <2 x double>* %A, align 16; 	%tmp7 = insertelement <2 x double> undef, double %B, i32 0; 	%tmp9 = shufflevector <2 x double> %tmp3,; <2 x double> %tmp7,; <2 x i32> < i32 0, i32 2 >; 	store <2 x double> %tmp9, <2 x double>* %r, align 16; 	ret void. ; CHECK: t2:; ; CHECK: 	 movl	8(%esp), %eax; ; CHECK-NEXT: 	movapd	(%eax), %xmm0; ; CHECK-NEXT: 	movhpd	12(%esp), %xmm0; ; CHECK-NEXT: 	movl	4(%esp), %eax; ; CHECK-NEXT: 	movapd	%xmm0, (%eax); ; CHECK-NEXT: 	ret; }. ""``CHECK-NEXT:``"" directives reject the input unless there is exactly one; newline between it and the previous directive. A ""``CHECK-NEXT:``"" cannot be; the first directive in a file. The ""CHECK-SAME:"" directive; ~~~~~~~~~~~~~~~~~~~~~~~~~~~. Sometimes you want to match lines and would like to verify that matches happen; on the same line as the previous match. In this case, you can use ""``CHECK:``""; and ""``CHECK-SAME:``"" directives to specify this. If you specified a custom; check prefix, just use ""``<PREFIX>-SAME:``"". ""``CHECK-SAME:``"" is particularly powerful in conjunction with ""``CHECK-NOT:``""; (described bel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:13469,load,load,13469,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['load'],['load']
Performance,"gnment information provided by the frontend for a non-integral pointer; (typically using attributes or metadata) must be valid for every possible ; representation of the pointer. .. _globalvars:. Global Variables; ----------------. Global variables define regions of memory allocated at compilation time; instead of run-time. Global variable definitions must be initialized. Global variables in other translation units can also be declared, in which; case they don't have an initializer. Global variables can optionally specify a :ref:`linkage type <linkage>`. Either global variable definitions or declarations may have an explicit section; to be placed in and may have an optional explicit alignment specified. If there; is a mismatch between the explicit or inferred section information for the; variable declaration and its definition the resulting behavior is undefined. A variable may be defined as a global ``constant``, which indicates that; the contents of the variable will **never** be modified (enabling better; optimization, allowing the global data to be placed in the read-only; section of an executable, etc). Note that variables that need runtime; initialization cannot be marked ``constant`` as there is a store to the; variable. LLVM explicitly allows *declarations* of global variables to be marked; constant, even if the final definition of the global is not. This; capability can be used to enable slightly better optimization of the; program, but requires the language definition to guarantee that; optimizations based on the 'constantness' are valid for the translation; units that do not include the definition. As SSA values, global variables define pointer values that are in scope; (i.e. they dominate) all basic blocks in the program. Global variables; always define a pointer to their ""content"" type because they describe a; region of memory, and all memory objects in LLVM are accessed through; pointers. Global variables can be marked with ``unnamed_addr`` which indic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:31179,optimiz,optimization,31179,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"gnored, as is; anything following a '#'. Can be specified multiple times to read names from; multiple files. .. option:: --regex. If specified, symbol and section names specified by other switches are treated; as extended POSIX regular expression patterns. .. option:: --remove-section <section>, -R. Remove the specified section from the output. Can be specified multiple times; to remove multiple sections simultaneously. For MachO objects, ``<section>`` must be formatted as; ``<segment name>,<section name>``. .. option:: --set-section-alignment <section>=<align>. Set the alignment of section ``<section>`` to ``<align>``. Can be specified; multiple times to update multiple sections. .. option:: --set-section-flags <section>=<flag>[,<flag>,...]. Set section properties in the output of section ``<section>`` based on the; specified ``<flag>`` values. Can be specified multiple times to update multiple; sections. Supported flag names are `alloc`, `load`, `noload`, `readonly`, `exclude`,; `debug`, `code`, `data`, `rom`, `share`, `contents`, `merge`, `strings`, and; `large`. Not all flags are meaningful for all object file formats or target; architectures. For ELF objects, the flags have the following effects:. - `alloc` = add the `SHF_ALLOC` flag.; - `load` = if the section has `SHT_NOBITS` type, mark it as a `SHT_PROGBITS`; section.; - `readonly` = if this flag is not specified, add the `SHF_WRITE` flag.; - `exclude` = add the `SHF_EXCLUDE` flag.; - `code` = add the `SHF_EXECINSTR` flag.; - `merge` = add the `SHF_MERGE` flag.; - `strings` = add the `SHF_STRINGS` flag.; - `contents` = if the section has `SHT_NOBITS` type, mark it as a `SHT_PROGBITS`; section.; - `large` = add the `SHF_X86_64_LARGE` on x86_64; rejected if the target; architecture is not x86_64. For COFF objects, the flags have the following effects:. - `alloc` = add the `IMAGE_SCN_CNT_UNINITIALIZED_DATA` and `IMAGE_SCN_MEM_READ`; flags, unless the `load` flag is specified.; - `noload` = add the `IMAGE_SCN_LNK",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst:4872,load,load,4872,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-objcopy.rst,1,['load'],['load']
Performance,"gnose Performance Issues; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; The ``-all-stats`` command line option enables extra statistics and performance; counters for the dispatch logic, the reorder buffer, the retire control unit,; and the register file. Below is an example of ``-all-stats`` output generated by :program:`llvm-mca`; for 300 iterations of the dot-product example discussed in the previous; sections. .. code-block:: none. Dynamic Dispatch Stall Cycles:; RAT - Register unavailable: 0; RCU - Retire tokens unavailable: 0; SCHEDQ - Scheduler full: 272 (44.6%); LQ - Load queue full: 0; SQ - Store queue full: 0; GROUP - Static restrictions on the dispatch group: 0. Dispatch Logic - number of cycles where we saw N micro opcodes dispatched:; [# dispatched], [# cycles]; 0, 24 (3.9%); 1, 272 (44.6%); 2, 314 (51.5%). Schedulers - number of cycles where we saw N micro opcodes issued:; [# issued], [# cycles]; 0, 7 (1.1%); 1, 306 (50.2%); 2, 297 (48.7%). Scheduler's queue usage:; [1] Resource name.; [2] Average number of used buffer entries.; [3] Maximum number of used buffer entries.; [4] Total number of buffer entries. [1] [2] [3] [4]; JALU01 0 0 20; JFPU01 17 18 18; JLSAGU 0 0 12. Retire Control Unit - number of cycles where we saw N instructions retired:; [# retired], [# cycles]; 0, 109 (17.9%); 1, 102 (16.7%); 2, 399 (65.4%). Total ROB Entries: 64; Max Used ROB Entries: 35 ( 54.7% ); Average Used ROB Entries per cy: 32 ( 50.0% ). Register File statistics:; Total number of mappings created: 900; Max number of mappings used: 35. * Register File #1 -- JFpuPRF:; Number of physical registers: 72; Total number of mappings created: 900; Max number of mappings used: 35. * Register File #2 -- JIntegerPRF:; Number of physical registers: 64; Total number of mappings created: 0; Max number of mappings used: 0. If we look at the *Dynamic Dispatch Stall Cycles* table, we see the counter for; SCHEDQ reports 272 cycles. This counter is incremented every time the dispa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:29563,queue,queue,29563,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"gnostics are enabled, you can also configure CFI to continue program; execution instead of aborting by using the :ref:`-fsanitize-recover=; <controlling-code-generation>` flag. Forward-Edge CFI for Virtual Calls; ==================================. This scheme checks that virtual calls take place using a vptr of the correct; dynamic type; that is, the dynamic type of the called object must be a; derived class of the static type of the object used to make the call.; This CFI scheme can be enabled on its own using ``-fsanitize=cfi-vcall``. For this scheme to work, all translation units containing the definition; of a virtual member function (whether inline or not), other than members; of :ref:`ignored <cfi-ignorelist>` types or types with public :doc:`LTO; visibility <LTOVisibility>`, must be compiled with ``-flto`` or ``-flto=thin``; enabled and be statically linked into the program. Performance; -----------. A performance overhead of less than 1% has been measured by running the; Dromaeo benchmark suite against an instrumented version of the Chromium; web browser. Another good performance benchmark for this mechanism is the; virtual-call-heavy SPEC 2006 xalancbmk. Note that this scheme has not yet been optimized for binary size; an increase; of up to 15% has been observed for Chromium. Bad Cast Checking; =================. This scheme checks that pointer casts are made to an object of the correct; dynamic type; that is, the dynamic type of the object must be a derived class; of the pointee type of the cast. The checks are currently only introduced; where the class being casted to is a polymorphic class. Bad casts are not in themselves control flow integrity violations, but they; can also create security vulnerabilities, and the implementation uses many; of the same mechanisms. There are two types of bad cast that may be forbidden: bad casts; from a base class to a derived class (which can be checked with; ``-fsanitize=cfi-derived-cast``), and bad casts from a pointer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:4798,perform,performance,4798,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,1,['perform'],['performance']
Performance,"gnostics-show-hotness:. .. option:: -f[no-]diagnostics-show-hotness. Enable profile hotness information in diagnostic line. This option controls whether Clang prints the profile hotness associated; with diagnostics in the presence of profile-guided optimization information.; This is currently supported with optimization remarks (see; :ref:`Options to Emit Optimization Reports <rpass>`). The hotness information; allows users to focus on the hot optimization remarks that are likely to be; more relevant for run-time performance. For example, in this output, the block containing the callsite of `foo` was; executed 3000 times according to the profile data:. ::. s.c:7:10: remark: foo inlined into bar (hotness: 3000) [-Rpass-analysis=inline]; sum += foo(x, x - 2);; ^. This option is implied when; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>` is used.; Otherwise, it defaults to off. .. option:: -fdiagnostics-hotness-threshold. Prevent optimization remarks from being output if they do not have at least; this hotness value. This option, which defaults to zero, controls the minimum hotness an; optimization remark would need in order to be output by Clang. This is; currently supported with optimization remarks (see :ref:`Options to Emit; Optimization Reports <rpass>`) when profile hotness information in; diagnostics is enabled (see; :ref:`-fdiagnostics-show-hotness <opt_fdiagnostics-show-hotness>`). .. _opt_fdiagnostics-fixit-info:. .. option:: -f[no-]diagnostics-fixit-info. Enable ""FixIt"" information in the diagnostics output. This option, which defaults to on, controls whether or not Clang; prints the information on how to fix a specific diagnostic; underneath it when it knows. For example, in this output:. ::. test.c:28:8: warning: extra tokens at end of #endif directive [-Wextra-tokens]; #endif bad; ^; //. Passing **-fno-diagnostics-fixit-info** will prevent Clang from; printing the ""//"" line at the end of the message. This information; is useful for user",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:14704,optimiz,optimization,14704,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimization']
Performance,"gpu-no-workitem-id-z"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workitem.id.z intrinsic. ""amdgpu-no-workgroup-id-x"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.x intrinsic. ""amdgpu-no-workgroup-id-y"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.y intrinsic. ""amdgpu-no-workgroup-id-z"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.z intrinsic. ""amdgpu-no-dispatch-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.ptr intrinsic. ""amdgpu-no-implicitarg-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.implicitarg.ptr intrinsic. ""amdgpu-no-dispatch-id"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.id intrinsic. ""amdgpu-no-queue-ptr"" Similar to amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.queue.ptr intrinsic. Note that unlike the other ABI hint; attributes, the queue pointer may be required in situations where the; intrinsic call does not directly appear in the program. Some subtargets; require the queue pointer for to handle some addrspacecasts, as well; as the llvm.amdgcn.is.shared, llvm.amdgcn.is.private, llvm.trap, and; llvm.debug intrinsics. ""amdgpu-no-hostcall-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to the hostcall buffer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-heap-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to an initialized memory buffer; that conforms to the requirements of the malloc/free device library V1; version implementation. If this attribute is absent, then the; amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-multigrid-sync-arg"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the multigrid synchroniza",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:49747,queue,queue,49747,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"gpu-symbols:. Symbols; -------. Symbols include the following:. .. table:: AMDGPU ELF Symbols; :name: amdgpu-elf-symbols-table. ===================== ================== ================ ==================; Name Type Section Description; ===================== ================== ================ ==================; *link-name* ``STT_OBJECT`` - ``.data`` Global variable; - ``.rodata``; - ``.bss``; *link-name*\ ``.kd`` ``STT_OBJECT`` - ``.rodata`` Kernel descriptor; *link-name* ``STT_FUNC`` - ``.text`` Kernel entry point; *link-name* ``STT_OBJECT`` - SHN_AMDGPU_LDS Global variable in LDS; ===================== ================== ================ ==================. Global variable; Global variables both used and defined by the compilation unit. If the symbol is defined in the compilation unit then it is allocated in the; appropriate section according to if it has initialized data or is readonly. If the symbol is external then its section is ``STN_UNDEF`` and the loader; will resolve relocations using the definition provided by another code object; or explicitly defined by the runtime. If the symbol resides in local/group memory (LDS) then its section is the; special processor specific section name ``SHN_AMDGPU_LDS``, and the; ``st_value`` field describes alignment requirements as it does for common; symbols. .. TODO::. Add description of linked shared object symbols. Seems undefined symbols; are marked as STT_NOTYPE. Kernel descriptor; Every HSA kernel has an associated kernel descriptor. It is the address of the; kernel descriptor that is used in the AQL dispatch packet used to invoke the; kernel, not the kernel entry point. The layout of the HSA kernel descriptor is; defined in :ref:`amdgpu-amdhsa-kernel-descriptor`. Kernel entry point; Every HSA kernel also has a symbol for its machine code entry point. .. _amdgpu-relocation-records:. Relocation Records; ------------------. The AMDGPU backend generates ``Elf64_Rela`` relocation records for; AMDHSA or ``Elf64_Rel`` rel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:78466,load,loader,78466,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"gram location for the region at the end of the; loop. Any lanes active will be in the loop, and any lanes not active must have; exited the loop. An ``IF/THEN/ELSEIF/ELSEIF/...`` region can be treated as a nest of; ``IF/THEN/ELSE`` regions. The DWARF procedures can use the active lane artificial variable described in; :ref:`amdgpu-dwarf-amdgpu-dw-at-llvm-active-lane` rather than the actual; ``EXEC`` mask in order to support whole or quad wavefront mode. .. _amdgpu-dwarf-amdgpu-dw-at-llvm-active-lane:. ``DW_AT_LLVM_active_lane``; ~~~~~~~~~~~~~~~~~~~~~~~~~~. The ``DW_AT_LLVM_active_lane`` attribute on a subprogram debugger information; entry is used to specify the lanes that are conceptually active for a SIMT; thread. The execution mask may be modified to implement whole or quad wavefront mode; operations. For example, all lanes may need to temporarily be made active to; execute a whole wavefront operation. Such regions would save the ``EXEC`` mask,; update it to enable the necessary lanes, perform the operations, and then; restore the ``EXEC`` mask from the saved value. While executing the whole; wavefront region, the conceptual execution mask is the saved value, not the; ``EXEC`` value. This is handled by defining an artificial variable for the active lane mask. The; active lane mask artificial variable would be the actual ``EXEC`` mask for; normal regions, and the saved execution mask for regions where the mask is; temporarily updated. The location list expression created for this artificial; variable is used to define the value of the ``DW_AT_LLVM_active_lane``; attribute. ``DW_AT_LLVM_augmentation``; ~~~~~~~~~~~~~~~~~~~~~~~~~~~. For AMDGPU, the ``DW_AT_LLVM_augmentation`` attribute of a compilation unit; debugger information entry has the following value for the augmentation string:. ::. [amdgpu:v0.0]. The ""vX.Y"" specifies the major X and minor Y version number of the AMDGPU; extensions used in the DWARF of the compilation unit. The version number; conforms to [SEM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:109738,perform,perform,109738,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['perform']
Performance,"gram:`llvm-opt-report`. It provides information on the execution time, memory usage, and other details of each optimization pass. .. code-block:: console. $ clang -c foo.c -o foo.o -O3 -fsave-optimization-record. Then, you create a report using the :program:`llvm-opt-report` command with the YAML optimization record file :file:`foo.opt.yaml` as input. .. code-block:: console. $ llvm-opt-report foo.opt.yaml -o foo.lst. foo.lst is the generated optimization report. .. code-block::. < foo.c; 1 | void bar();; 2 | void foo() { bar(); }; 3 |; 4 | void Test(int *res, int *c, int *d, int *p, int n) {; 5 | int i;; 6 |; 7 | #pragma clang loop vectorize(assume_safety); 8 V4,1 | for (i = 0; i < 1600; i++) {; 9 | res[i] = (p[i] == 0) ? res[i] : res[i] + d[i];; 10 | }; 11 |; 12 U16 | for (i = 0; i < 16; i++) {; 13 | res[i] = (p[i] == 0) ? res[i] : res[i] + d[i];; 14 | }; 15 |; 16 I | foo();; 17 |; 18 | foo(); bar(); foo();; I | ^; I | ^; 19 | }; 20 |. Symbols printed on the left side of the program indicate what kind of optimization was performed.; The meanings of the symbols are as follows:. - I: The function is inlined.; - U: The loop is unrolled. The following number indicates the unroll factor.; - V: The loop is vectorized. The following numbers indicate the vector length and the interleave factor. .. note:: . If a specific line of code is output twice, it means that the same optimization pass was applied to that ; line of code twice, and the pass was able to further optimize the code on the second iteration. OPTIONS; -------. If ``input`` is ""``-``"" or omitted, :program:`llvm-opt-report` reads from standard; input. Otherwise, it will read from the specified filename. If the :option:`-o` option is omitted, then :program:`llvm-opt-report` will send its output; to standard output. If the :option:`-o` option specifies ""``-``"", then the output will also; be sent to standard output. .. option:: --help. Display available options. .. option:: --version. Display the version of this p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst:1448,optimiz,optimization,1448,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-opt-report.rst,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"grammers to identify all possible secret data using; static source code annotations or code vulnerable to a variant #1 style; attack. Limitations of this approach:; * It requires re-compiling source code to insert hardening instruction; sequences. Only software compiled in this mode is protected.; * The performance is heavily dependent on a particular architecture's; implementation strategy. We outline a potential x86 implementation below and; characterize its performance.; * It does not defend against secret data already loaded from memory and; residing in registers or leaked through other side-channels in; non-speculative execution. Code dealing with this, e.g cryptographic; routines, already uses constant-time algorithms and code to prevent; side-channels. Such code should also scrub registers of secret data following; [these; guidelines](https://github.com/HACS-workshop/spectre-mitigations/blob/master/crypto_guidelines.md).; * To achieve reasonable performance, many loads may not be checked, such as; those with compile-time fixed addresses. This primarily consists of accesses; at compile-time constant offsets of global and local variables. Code which; needs this protection and intentionally stores secret data must ensure the; memory regions used for secret data are necessarily dynamic mappings or heap; allocations. This is an area which can be tuned to provide more comprehensive; protection at the cost of performance.; * [Hardened loads](#hardening-the-address-of-the-load) may still load data from; _valid_ addresses if not _attacker-controlled_ addresses. To prevent these; from reading secret data, the low 2gb of the address space and 2gb above and; below any executable pages should be protected. Credit:; * The core idea of tracing misspeculation through data and marking pointers to; block misspeculated loads was developed as part of a HACS 2018 discussion; between Chandler Carruth, Paul Kocher, Thomas Pornin, and several other; individuals.; * Core idea of maski",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:7199,perform,performance,7199,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,"['load', 'perform']","['loads', 'performance']"
Performance,"gramming model that we are aiming for? Maximum freedom or more easy checker development?. I think if we aim for maximum freedom, we do not need to worry about the; potential stress on checkers, and we can introduce abstractions to mitigate that; later on.; If we want to simplify the API, then maybe it makes more sense to move language; construct modeling to the engine when the checker API is not sufficient instead; of complicating the API. Right now I have no preference or objections between the alternatives but there; are some random thoughts:. * Maybe it would be great to have a guideline how to evolve the analyzer and; follow it, so it can help us to decide in similar situations. * I do care about performance in this case. The reason is that we have a; limited performance budget. And I think we should not expect most of the checker; writers to add modeling of language constructs. So, in my opinion, it is ok to; have less nice/more verbose API for language modeling if we can have better; performance this way, since it only needs to be done once, and is done by the; framework developers. **Artem:** These are some great questions, i guess it'd be better to discuss; them more openly. As a quick dump of my current mood:. * To me it seems obvious that we need to aim for a checker API that is both; simple and powerful. This can probably by keeping the API as powerful as; necessary while providing a layer of simple ready-made solutions on top of it.; Probably a few reusable components for assembling checkers. And this layer; should ideally be pleasant enough to work with, so that people would prefer to; extend it when something is lacking, instead of falling back to the complex; omnipotent API. I'm thinking of AST matchers vs. AST visitors as a roughly; similar situation: matchers are not omnipotent, but they're so nice. * Separation between core and checkers is usually quite strange. Once we have; shared state traits, i generally wouldn't mind having region store or ran",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst:5916,perform,performance,5916,interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/developer-docs/InitializerLists.rst,1,['perform'],['performance']
Performance,"gs set, it should be done within the target's InstrPostProcess class.; For an example, look at the `X86InstrPostProcess::postProcessInstruction` method; within `llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp`. A load/store barrier consumes one entry of the load/store queue. A load/store; barrier enforces ordering of loads/stores. A younger load cannot pass a load; barrier. Also, a younger store cannot pass a store barrier. A younger load; has to wait for the memory/load barrier to execute. A load/store barrier is; ""executed"" when it becomes the oldest entry in the load/store queue(s). That; also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are:. #. A store may not pass a previous store.; #. A store may not pass a previous load (regardless of ``-noalias``).; #. A store has to wait until an older store barrier is fully executed.; #. A load may pass a previous load.; #. A load may not pass a previous store unless ``-noalias`` is set.; #. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; In-order processors are modelled as a single ``InOrderIssueStage`` stage. It; bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as; soon as their operand registers are available and resource requirements are; met. Multiple instructions can be issued in one cycle according to the value of; the ``IssueWidth`` parameter in LLVM's scheduling model. Once issued, an instruction is moved to ``IssuedInst`` set until it is ready to; retire. :program:`llvm-mca` ensures that writes are committed in-order. However,; an instruction is allowed to commit writes and retire out-of-order if; ``RetireOOO`` property is true for at least one of its writes. Custom Behaviour; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Due to certain instructions not being expressed perfectly within their; scheduling model, :program:`llvm-mc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:42991,load,load,42991,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['load']
Performance,"gular calls of the THttpServer::ProcessRequests() method, like:. ```cpp; serv->ProcessRequests();; ```. In such case, one can fully disable the timer of the server:. ```cpp; serv->SetTimer(0, kTRUE);; ```. ## Data access from command shell. The big advantage of the http protocol is that it is not only supported in web browsers, but also in many other applications. One could use http requests to directly access ROOT objects and data members from any kind of scripts. If one starts a server and register an object like for example:. ```cpp; auto serv = new THttpServer(""http:8080"");; TNamed* n1 = new TNamed(""obj"", ""title"");; serv->Register(""subfolder"", n1);; ```. One could request a JSON representation of such object with the command:. ```bash; [shell] wget http://localhost:8080/Objects/subfolder/obj/root.json; ```. Then, its representation will look like:. ```json; {; ""_typename"" : ""TNamed"",; ""fUniqueID"" : 0,; ""fBits"" : 0,; ""fName"" : ""obj"",; ""fTitle"" : ""title""; }; ```. The following requests can be performed:. | Name | Description |; | :----------- | :---------------- |; | `root.bin` | binary data produced by object streaming with `TBufferFile` |; | `root.json` | ROOT JSON representation for object and objects members |; | `file.root` | Creates TMemFile with the only object, from ROOT 6.32 |; | `root.xml` | ROOT XML representation |; | `root.png` | PNG image (if object drawing implemented) |; | `root.gif` | GIF image |; | `root.jpeg` | JPEG image |; | `exe.json` | method execution in the object |; | `exe.bin` | method execution, return result in binary form |; | `cmd.json` | command execution |; | `item.json` | item (object) properties, specified on the server |; | `multi.json` | perform several requests at once |; | `multi.bin` | perform several requests at once, return result in binary form |. All data will be automatically zipped if '.gz' extension is appended. Like:. ```bash; [shell] wget http://localhost:8080/Objects/subfolder/obj/root.json.gz; ```. If the access t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md:15167,perform,performed,15167,documentation/HttpServer/HttpServer.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md,1,['perform'],['performed']
Performance,"gument is a; pointer to the object. Semantics:; """""""""""""""""""". This intrinsic indicates that the memory is mutable again. '``llvm.launder.invariant.group``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The memory object can belong to any address; space. The returned pointer must belong to the same address space as the; argument. ::. declare ptr @llvm.launder.invariant.group.p0(ptr <ptr>). Overview:; """""""""""""""""". The '``llvm.launder.invariant.group``' intrinsic can be used when an invariant; established by ``invariant.group`` metadata no longer holds, to obtain a new; pointer value that carries fresh invariant group information. It is an; experimental intrinsic, which means that its semantics might change in the; future. Arguments:; """""""""""""""""""". The ``llvm.launder.invariant.group`` takes only one argument, which is a pointer; to the memory. Semantics:; """""""""""""""""""". Returns another pointer that aliases its argument but which is considered different; for the purposes of ``load``/``store`` ``invariant.group`` metadata.; It does not read any accessible memory and the execution can be speculated. '``llvm.strip.invariant.group``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The memory object can belong to any address; space. The returned pointer must belong to the same address space as the; argument. ::. declare ptr @llvm.strip.invariant.group.p0(ptr <ptr>). Overview:; """""""""""""""""". The '``llvm.strip.invariant.group``' intrinsic can be used when an invariant; established by ``invariant.group`` metadata no longer holds, to obtain a new pointer; value that does not carry the invariant information. It is an experimental; intrinsic, which means that its semantics might change in the future. Arguments:; """""""""""""""""""". The ``llvm.strip.invariant.group`` takes only one argument, which is a pointer; to the memory. Semantics:; """""""""""""""""""". Returns another pointer that aliase",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:865979,load,load,865979,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"h branches for each of the sub-folders:. ``` {.cpp}; TTree folder_tree(""MyFolderTree"",""/MyFolder"");; ```. The second argument `""/MyFolder"" `is the top folder, and the ""/"" signals; the **`TTree`** constructor that this is a folder not just the title.; You fill the tree by placing the data into the folder structure and; calling `TTree::Fill`. ### Tree and TRef Objects. ``` {.cpp}; MyTree->BranchRef();; ```. This call requests the construction of an optional branch supporting; table of references (**`TRefTable`**). This branch (**`TBranchRef`**); will keep all the information needed to find the branches containing; referenced objects at each `Tree::Fill`, the branch numbers containing; the referenced objects are saved in the table of references. When the; Tree header is saved (via `TTree::Write` for example), the branch is; saved, keeping the information with the pointers to the branches having; referenced objects. Enabling this optional table, allow; `TTree::Draw` to automatically load the branches needed to; dereference a **`TRef`** (or **`TRefArray`**) object. ### Autosave. `Autosave` gives the option to save all branch buffers every `n` byte.; We recommend using `Autosave` for large acquisitions. If the acquisition; fails to complete, you can recover the file and all the contents since; the last `Autosave`. To set the number of bytes between `Autosave` you; can use the `TTree::SetAutosave()` method. You can also call; **`TTree::Autosave` in the acquisition loop every `n `entry.**. ### Trees with Circular Buffers. When a **`TTree`** is memory resident, you set it up so that it retains; retain only the last few entries. For example, this can be very useful; for monitoring purpose. ``` {.cpp}; void TTree::SetCircular(Long64_t maxEntries);; ```. where `maxEntries` is the maximum number of entries to be kept in the; buffers. When the number of entries exceeds this value, the first; entries in the **`Tree`** are deleted and the buffers used again. An; example of a script ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:13281,load,load,13281,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"h can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1258,optimiz,optimizer,1258,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"h can result in miscompiles in some cases.; - Fix crash on use of a variadic overloaded operator.; (`#42535 <https://github.com/llvm/llvm-project/issues/42535>`_); - Fix a hang on valid C code passing a function type as an argument to; ``typeof`` to form a function declaration.; (`#64713 <https://github.com/llvm/llvm-project/issues/64713>`_); - Clang now reports missing-field-initializers warning for missing designated; initializers in C++.; (`#56628 <https://github.com/llvm/llvm-project/issues/56628>`_); - Clang now respects ``-fwrapv`` and ``-ftrapv`` for ``__builtin_abs`` and; ``abs`` builtins.; (`#45129 <https://github.com/llvm/llvm-project/issues/45129>`_,; `#45794 <https://github.com/llvm/llvm-project/issues/45794>`_); - Fixed an issue where accesses to the local variables of a coroutine during; ``await_suspend`` could be misoptimized, including accesses to the awaiter; object itself.; (`#56301 <https://github.com/llvm/llvm-project/issues/56301>`_); The current solution may bring performance regressions if the awaiters have; non-static data members. See; `#64945 <https://github.com/llvm/llvm-project/issues/64945>`_ for details.; - Clang now prints unnamed members in diagnostic messages instead of giving an; empty ''. Fixes; (`#63759 <https://github.com/llvm/llvm-project/issues/63759>`_); - Fix crash in __builtin_strncmp and related builtins when the size value; exceeded the maximum value representable by int64_t. Fixes; (`#64876 <https://github.com/llvm/llvm-project/issues/64876>`_); - Fixed an assertion if a function has cleanups and fatal erors.; (`#48974 <https://github.com/llvm/llvm-project/issues/48974>`_); - Clang now emits an error if it is not possible to deduce array size for a; variable with incomplete array type.; (`#37257 <https://github.com/llvm/llvm-project/issues/37257>`_); - Clang's ``-Wunused-private-field`` no longer warns on fields whose type is; declared with ``[[maybe_unused]]``.; (`#61334 <https://github.com/llvm/llvm-project/issues/61334>",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst:34239,perform,performance,34239,interpreter/llvm-project/clang/docs/ReleaseNotes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst,1,['perform'],['performance']
Performance,"h case; ``%base`` is the first element of the vector induction variable (VIV) and; ``%n`` is the loop tripcount. Thus, these intrinsics perform an element-wise; less than comparison of VIV with the loop tripcount, producing a mask of; true/false values representing active/inactive vector lanes, except if the VIV; overflows in which case they return false in the lanes where the VIV overflows.; The arguments are scalar types to accommodate scalable vector types, for which; it is unknown what the type of the step vector needs to be that enumerate its; lanes without overflow. This mask ``%m`` can e.g. be used in masked load/store instructions. These; intrinsics provide a hint to the backend. I.e., for a vector loop, the; back-edge taken count of the original scalar loop is explicit as the second; argument. Examples:; """""""""""""""""". .. code-block:: llvm. %active.lane.mask = call <4 x i1> @llvm.get.active.lane.mask.v4i1.i64(i64 %elem0, i64 429); %wide.masked.load = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* %3, i32 4, <4 x i1> %active.lane.mask, <4 x i32> poison). .. _int_experimental_vp_splice:. '``llvm.experimental.vp.splice``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <2 x double> @llvm.experimental.vp.splice.v2f64(<2 x double> %vec1, <2 x double> %vec2, i32 %imm, <2 x i1> %mask, i32 %evl1, i32 %evl2); declare <vscale x 4 x i32> @llvm.experimental.vp.splice.nxv4i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2, i32 %imm, <vscale x 4 x i1> %mask, i32 %evl1, i32 %evl2). Overview:; """""""""""""""""". The '``llvm.experimental.vp.splice.*``' intrinsic is the vector length; predicated version of the '``llvm.experimental.vector.splice.*``' intrinsic. Arguments:; """""""""""""""""""". The result and the first two arguments ``vec1`` and ``vec2`` are vectors with; the same type. The third argument ``imm`` is an immediate signed integer that; indicates the offset index. The fourth argument ``mask`` i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:779169,load,load,779169,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"h gcc/g++ about empty structs/unions passing was fixed.; - ``_mcount`` was generated instead of ``mcount``. RISC-V Support; ^^^^^^^^^^^^^^; - Unaligned memory accesses can be toggled by ``-m[no-]unaligned-access`` or the; aliases ``-m[no-]strict-align``.; - CodeGen of RV32E/RV64E was supported experimentally.; - CodeGen of ilp32e/lp64e was supported experimentally. - Default ABI with F but without D was changed to ilp32f for RV32 and to lp64f; for RV64. - ``__attribute__((rvv_vector_bits(N)))`` is now supported for RVV vbool*_t types.; - ``-mtls-dialect=desc`` is now supported to enable TLS descriptors (TLSDESC). CUDA/HIP Language Changes; ^^^^^^^^^^^^^^^^^^^^^^^^^. CUDA Support; ^^^^^^^^^^^^. - Clang now supports CUDA SDK up to 12.3; - Added support for sm_90a. PowerPC Support; ^^^^^^^^^^^^^^^. - Added ``nmmintrin.h`` to intrinsics headers.; - Added ``__builtin_ppc_fence`` as barrier of code motion, and; ``__builtin_ppc_mffsl`` for corresponding instruction.; - Supported ``__attribute__((target(""tune=cpu"")))``.; - Emit ``float-abi`` module flag on 64-bit ELFv2 PowerPC targets if; ``long double`` type is used in current module. AIX Support; ^^^^^^^^^^^. - Introduced the ``-maix-small-local-exec-tls`` option to produce a faster; access sequence for local-exec TLS variables where the offset from the TLS; base is encoded as an immediate operand.; This access sequence is not used for TLS variables larger than 32KB, and is; currently only supported on 64-bit mode.; - Inline assembler supports VSR register in pure digits.; - Enabled ThinLTO support. Requires AIX 7.2 TL5 SP7 or newer, or AIX 7.3 TL2; or newer. Similar to the LTO support on AIX, ThinLTO is implemented with; the libLTO.so plugin. SystemZ Support; ^^^^^^^^^^^^^^^; - Properly support 16 byte atomic int/fp types and ops. Atomic __int128 (and; long double) variables are now aligned to 16 bytes by default (like gcc 14). WebAssembly Support; ^^^^^^^^^^^^^^^^^^^. AVR Support; ^^^^^^^^^^^. DWARF Support in Clang; --",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst:65075,tune,tune,65075,interpreter/llvm-project/clang/docs/ReleaseNotes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ReleaseNotes.rst,1,['tune'],['tune']
Performance,"h include paths given to the Cling.; Example::. >>> cppyy.include(""vector"") # equivalent to ""#include <vector>""; True; >>> . * ``c_include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Name mangling is an important difference between C and C++ code.; The use of ``c_include`` instead of ``include`` prevents mangling. * ``load_library``: load compiled C++ into the interpreter.; This function takes the name of a shared library and loads it into current; process, exposing all external symbols to Cling.; Libraries are located through load paths given to Cling, either through the; ""-L"" compiler flag or the dynamic search path environment variable (system; dependent).; Any method that brings symbols into the process (including normal linking,; e.g. when embedding Python in a C++ application) is suitable to expose; symbols.; An alternative for ``load_library`` is for example ``ctypes.CDLL``, but; that function does not respect dynamic load paths on all platforms. If a compilation error occurs during JITing of C++ code in any of the above; helpers, a Python ``SyntaxError`` exception is raised.; If a compilation warning occurs, a Python warning is issued. `Configuring Cling`; -------------------. It is often convenient to add additional search paths for Cling to find; headers and libraries when loading a module (Python does not have standard; locations to place headers and libraries, but their locations can usually; be inferred from the location of the module, i.e. it's ``__file__``; attribute).; cppyy provides the following two helpers:. * ``add_include_path``: add additional paths for Cling to look for headers. * ``add_library_path``: add additional paths for Cling to look for libraries. Both functions accept either a string (a single path) or a list (for adding; multiple paths).; Paths are allowed to be relative, but absolute paths are recommended. `C++ language`; --------------. Some C++ compilation-ti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:3191,load,load,3191,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['load']
Performance,"h is also very fast and uses; only three words for its state. ### gEnv. **`gEnv`** is the global variable (of type **`TEnv`**) with all the; environment settings for the current session. This variable is set by; reading the contents of a `.rootrc` file (or; `$ROOTSYS/etc/system.rootrc`) at the beginning of the root session.; See Environment Setup below for more information. ## Environment Setup. The behavior of a ROOT session can be tailored with the options in the; .`rootrc` file. At start-up, ROOT looks for a .`rootrc` file in the; following order:. - `./.rootrc` *`//local directory`*. - `$HOME/.rootrc ` *`//user directory`*. - `$ROOTSYS/etc/system.rootrc ` *`//global ROOT directory`*. If more than one `.rootrc` files are found in the search paths above,; the options are merged, with precedence local, user, global. While in; a session, to see current settings, you can do:. ``` {.cpp}; root[] gEnv->Print(); ```. The `rootrc` file typically looks like:. ```; # Path used by dynamic loader to find shared libraries; Unix.*.Root.DynamicPath: .:~/rootlibs:$(ROOTSYS)/lib; Unix.*.Root.MacroPath: .:~/rootmacros:$(ROOTSYS)/macros. # Path where to look for TrueType fonts; Unix.*.Root.UseTTFonts: true; Unix.*.Root.TTFontPath:; ...; # Activate memory statistics; Rint.Load: rootalias.C; Rint.Logon: rootlogon.C; Rint.Logoff: rootlogoff.C; ...; Rint.Canvas.MoveOpaque: false; Rint.Canvas.HighLightColor: 5; ```. The various options are explained in `$ROOTSYS/etc/system.rootrc`. The; `.rootrc` file contents are combined. For example, if the flag to use; true type fonts is set to true in the `system.rootrc` file, you have; to set explicitly it false in your local `.rootrc` file if you do not; want to use true type fonts. Removing the `UseTTFonts `statement in; the local `.rootrc` file will not disable true fonts. The value of the; environment variable `ROOTDEBUG` overrides the value in the `.rootrc`; file at startup. Its value is used to set ***`gDebug`*** and helps for; quick turn on ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:36556,load,loader,36556,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['load'],['loader']
Performance,"h is; zero, then the default dispatch width is used. .. option:: -register-file-size=<size>. Specify the size of the register file. When specified, this flag limits how; many physical registers are available for register renaming purposes. A value; of zero for this flag means ""unlimited number of physical registers"". .. option:: -iterations=<number of iterations>. Specify the number of iterations to run. If this flag is set to 0, then the; tool sets the number of iterations to a default value (i.e. 100). .. option:: -noalias=<bool>. If set, the tool assumes that loads and stores don't alias. This is the; default behavior. .. option:: -lqueue=<load queue size>. Specify the size of the load queue in the load/store unit emulated by the tool.; By default, the tool assumes an unbound number of entries in the load queue.; A value of zero for this flag is ignored, and the default load queue size is; used instead. .. option:: -squeue=<store queue size>. Specify the size of the store queue in the load/store unit emulated by the; tool. By default, the tool assumes an unbound number of entries in the store; queue. A value of zero for this flag is ignored, and the default store queue; size is used instead. .. option:: -timeline. Enable the timeline view. .. option:: -timeline-max-iterations=<iterations>. Limit the number of iterations to print in the timeline view. By default, the; timeline view prints information for up to 10 iterations. .. option:: -timeline-max-cycles=<cycles>. Limit the number of cycles in the timeline view, or use 0 for no limit. By; default, the number of cycles is set to 80. .. option:: -resource-pressure. Enable the resource pressure view. This is enabled by default. .. option:: -register-file-stats. Enable register file usage statistics. .. option:: -dispatch-stats. Enable extra dispatch statistics. This view collects and analyzes instruction; dispatch events, as well as static/dynamic dispatch stall events. This view; is disabled by default. .. option",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:4467,queue,queue,4467,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,"['load', 'queue']","['load', 'queue']"
Performance,"h mode evaluation is now the default when used within the `TMVA::Factory` class (i.e. when calling; `Factory::TestAllMethod()` or `Factory::EvaluateAllMethods()`; - Support splitting the overall training data in Train and Validation data. The train data is used for finding the optimal network weight and the validation data is used to monitor the validation; error. The weights which are giving a minimal validation error will be stored. For the splitting a new option, *ValidationSize* has been added to the global options for `MethodDL`.; The same option is also available in the `PyKeras` method of `PyMVA`; - The fast tanh implementation from VDT is now used as activation function when training the network on CPU.; - Using `Cblas` from the GSL library is supported for CPU training when no other Blas libraries are found. However, it is strongly recommended, to use an optimized Blas implementation such as `libopenblas`, that is; available in cvmfs.; - Add several performance optimizations for both CPU and GPU versions of `MethodDL`. . ### Other New TMVA Features. - Add a new option to the `DataLoader` to switch off computation of correlation matrix. The new option is called *CalcCorrelations* and it should be used when a large number of input variables are; provided, otherwise TMVA will spend a long time in setting up the data set before training. ; ; - Build configuration:; - Add new cmake flags, `tmva-cpu` and `tmva-gpu`, which can be used to swicth on/off the CPU and GPU (based on CUDA) implementations of the TMVA Deep Learning module. `tmva-cpu` is enabled by; default if a Blas or CBlas library is found in the system. `tmva-gpu` is enabled when the cmake flag `cuda` is enabled and a compatible Cuda library is found. ; enabled if the corre; - Add possibility to independently configure building of optional pymva part of tmva with flag `-Dpymva=ON|OFF`. - New Cross Validation features:; - Add stratified splitting for cross validation.; - New plotting option in cross val",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md:15044,perform,performance,15044,README/ReleaseNotes/v616/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"h not; always), and it can make computation significantly faster if the target lacks; direct hardware support for arithmetic in a particular type. However, it can; also undermine strict floating-point reproducibility. Under the standards, assignments and explicit casts force the operand to be; converted to its formal type, discarding any excess precision. Because data; can only flow between statements via an assignment, this means that the use; of excess precision arithmetic is a reliable local property of a single; statement, and results do not change based on optimization. However, when; excess precision arithmetic is in use, Clang does not guarantee strict; reproducibility, and future compiler releases may recognize more; opportunities to use excess precision arithmetic, e.g. with floating-point; builtins. Clang does not use excess precision arithmetic for most types or on most; targets. For example, even on pre-SSE X86 targets where ``float`` and; ``double`` computations must be performed in the 80-bit X87 format, Clang; rounds all intermediate results correctly for their type. Clang currently; uses excess precision arithmetic by default only for the following types and; targets:. * ``_Float16`` on X86 targets without ``AVX512-FP16``. The ``-fexcess-precision=<value>`` option can be used to control the use of; excess precision arithmetic. Valid values are:. * ``standard`` - The default. Allow the use of excess precision arithmetic; under the constraints of the C and C++ standards. Has no effect except on; the types and targets listed above.; * ``fast`` - Accepted for GCC compatibility, but currently treated as an; alias for ``standard``.; * ``16`` - Forces ``_Float16`` operations to be emitted without using excess; precision arithmetic. .. option:: -fcx-limited-range:. This option enables the naive mathematical formulas for complex division and; multiplication with no NaN checking of results. The default is; ``-fno-cx-limited-range``, but this option is enabled b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:67593,perform,performed,67593,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performed']
Performance,"h previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hardening | Mitigated Speedup |; | -------------------------------------- | -------: | -------------: | ----------------: |; | Google microbenchmark suite | -74.8% | -36.4% | **2.5x** |; | Large server QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a visualization of the microbenchmark suite results which helps show; the distribution of results that is somewhat lost in the summary. The y-axis is; a log-scale speedup ratio of load hardening relative to `lfence` (up -> faster; -> better). Each box-and-whiskers represents one microbenchmark which may have; many different metrics measured. The red line marks the median, the box marks; the first and third quartiles, and the whiskers mark the min and max. ![Microbenchmark result visualization](speculative_load_hardening_microbenchmarks.png). We don't yet have benchmark data on SPEC or th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:47551,load,load,47551,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"h prohibitive; performance cost.; 1. Loads may not target a general purpose register requiring extra instructions; to map the state value into the correct register class, and potentially more; expensive instructions to mask the value in some way.; 1. The flags registers on x86 are very likely to be live, and challenging to; preserve cheaply.; 1. There are many more values loaded than pointers & indices used for loads. As; a consequence, hardening the result of a load requires substantially more; instructions than hardening the address of the load (see below). Despite these challenges, hardening the result of the load critically allows; the load to proceed and thus has dramatically less impact on the total; speculative / out-of-order potential of the execution. There are also several; interesting techniques to try and mitigate these challenges and make hardening; the results of loads viable in at least some cases. However, we generally; expect to fall back when unprofitable from hardening the loaded value to the; next approach of hardening the address itself. ###### Loads folded into data-invariant operations can be hardened after the operation. The first key to making this feasible is to recognize that many operations on; x86 are ""data-invariant"". That is, they have no (known) observable behavior; differences due to the particular input data. These instructions are often used; when implementing cryptographic primitives dealing with private key data; because they are not believed to provide any side-channels. Similarly, we can; defer hardening until after them as they will not in-and-of-themselves; introduce a speculative execution side-channel. This results in code sequences; that look like:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; orl %eax, %edi; ```. While an addition happens to the loaded (potentially secret) value, that; doesn't leak any data and we then imm",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:24999,load,loaded,24999,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"h uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===----------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2028,optimiz,optimizations,2028,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,4,['optimiz'],['optimizations']
Performance,"h.probability`` on any integer bit width. ::. declare i1 @llvm.expect.with.probability.i1(i1 <val>, i1 <expected_val>, double <prob>); declare i32 @llvm.expect.with.probability.i32(i32 <val>, i32 <expected_val>, double <prob>); declare i64 @llvm.expect.with.probability.i64(i64 <val>, i64 <expected_val>, double <prob>). Overview:; """""""""""""""""". The ``llvm.expect.with.probability`` intrinsic provides information about; expected value of ``val`` with probability(or confidence) ``prob``, which can; be used by optimizers. Arguments:; """""""""""""""""""". The ``llvm.expect.with.probability`` intrinsic takes three arguments. The first; argument is a value. The second argument is an expected value. The third; argument is a probability. Semantics:; """""""""""""""""""". This intrinsic is lowered to the ``val``. .. _int_assume:. '``llvm.assume``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.assume(i1 %cond). Overview:; """""""""""""""""". The ``llvm.assume`` allows the optimizer to assume that the provided; condition is true. This information can then be used in simplifying other parts; of the code. More complex assumptions can be encoded as; :ref:`assume operand bundles <assume_opbundles>`. Arguments:; """""""""""""""""""". The argument of the call is the condition which the optimizer may assume is; always true. Semantics:; """""""""""""""""""". The intrinsic allows the optimizer to assume that the provided condition is; always true whenever the control flow reaches the intrinsic call. No code is; generated for this intrinsic, and instructions that contribute only to the; provided condition are not used for code generation. If the condition is; violated during execution, the behavior is undefined. Note that the optimizer might limit the transformations performed on values; used by the ``llvm.assume`` intrinsic in order to preserve the instructions; only used to form the intrinsic's input argument. This might prove undesirable; if the extra information provided by the ``llvm.assume`` intr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:935209,optimiz,optimizer,935209,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"h264ref 928.70; ...; baseline; count 506.000000; mean 20.563098; std 111.423325; min 0.003400; 25% 0.011200; 50% 0.339450; 75% 4.067200; max 1222.896800; ```. - Show compile_time or text segment size metrics:. ```bash; % test-suite/utils/compare.py -m compile_time baseline.json; % test-suite/utils/compare.py -m size.__text baseline.json; ```. - Compare two result files and filter short running tests:. ```bash; % test-suite/utils/compare.py --filter-short baseline.json experiment.json; ...; Program baseline experiment diff. SingleSour.../Benchmarks/Linpack/linpack-pc 5.16 4.30 -16.5%; MultiSourc...erolling-dbl/LoopRerolling-dbl 7.01 7.86 12.2%; SingleSour...UnitTests/Vectorizer/gcc-loops 3.89 3.54 -9.0%; ...; ```. - Merge multiple baseline and experiment result files by taking the minimum; runtime each:. ```bash; % test-suite/utils/compare.py base0.json base1.json base2.json vs exp0.json exp1.json exp2.json; ```. ### Continuous Tracking with LNT. LNT is a set of client and server tools for continuously monitoring; performance. You can find more information at; [https://llvm.org/docs/lnt](https://llvm.org/docs/lnt). The official LNT instance; of the LLVM project is hosted at [http://lnt.llvm.org](http://lnt.llvm.org). External Suites; ---------------. External suites such as SPEC can be enabled by either. - placing (or linking) them into the `test-suite/test-suite-externals/xxx` directory (example: `test-suite/test-suite-externals/speccpu2000`); - using a configuration option such as `-D TEST_SUITE_SPEC2000_ROOT=path/to/speccpu2000`. You can find further information in the respective README files such as; `test-suite/External/SPEC/README`. For the SPEC benchmarks you can switch between the `test`, `train` and; `ref` input datasets via the `TEST_SUITE_RUN_TYPE` configuration option.; The `train` dataset is used by default. Custom Suites; -------------. You can build custom suites using the test-suite infrastructure. A custom suite; has a `CMakeLists.txt` file at the to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:8708,perform,performance,8708,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['perform'],['performance']
Performance,"h; file cannot be read correctly.; * Significantly improved the scaling of hadd tear-down/cleanup-phase in the presence; of large number histograms and in the presence of large number of directories.; * TMemFile: Apply customization of minimal block size also to the first block.; * Add renaming rule for instances of the math classes from `genvector` and `smatrix` to; instance for one floating point type (`float`, `double`, `Double32_t`, `Float16_t`) to; instances for any other floating point type.; * Corrected the application of `I/O customization rules` when the target classes contained; typedefs (in particular `Double32_t`); * Prevent splitting of objects when a `Streamer Function` was explicitly attached to their; `TClass`.; * In hadd fix verbose level arg parsing; * Allow user to change the type of the content of a TClonesArray.; * Avoid deleted memory access in `MakeProject` and in handling of; `I/O customization rules`. ## TTree Libraries. * Prevent a situation in `TTreeFormula` when stale cached information was re-used.; * Prevent a noticeable memory leak when reading uncompressed TTree. ## Histogram Libraries. * Allow reading v5 TF1 that were stored memberwise in a TClonesArray. ## Math Libraries. ## RooFit Libraries. ## 2D Graphics Libraries. * Provide support of NDC coordinates for TArrow.; * Fix interactive movement of TLine/TArrow objects when NDC coordinates are used; * Provide TGraph::MovePoints() method; * New options `RX`and `RY` for TMultiGraph in order to draw reverse axis along X and Y.; * Combined with the option ""Z"" the option ""CJUST"" allows to draw the color palette; with axis labels justified on the color boundaries (implemented by Otto Schaile).; * The `TCanvas` Event Status Bar now displays the date and time when the mouse cursor; is moved over a time axis (implemented by Otto Schaile).; * Negative values were not painted with option ""TEXT"" for TH2Poly. ## 3D Graphics Libraries. ## Geometry Libraries. ## Database Libraries. ## Networking Libr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v620/index.md:4816,cache,cached,4816,README/ReleaseNotes/v620/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v620/index.md,1,['cache'],['cached']
Performance,"h; has V# 64-bit address support), flat instructions (GFX7-GFX11), or global; instructions (GFX9-GFX11). If buffer operations are used, then the compiler can generate a V# with the; following properties:. * base address of 0; * no swizzle; * ATC: 1 if IOMMU present (such as APU); * ptr64: 1; * MTYPE set to support memory coherence that matches the runtime (such as CC for; APU and NC for dGPU). .. _amdgpu-amdhsa-kernarg-preload:. Preloaded Kernel Arguments; ++++++++++++++++++++++++++. On hardware that supports this feature, kernel arguments can be preloaded into; User SGPRs, up to the maximum number of User SGPRs available. The allocation of; Preload SGPRs occurs directly after the last enabled non-kernarg preload User; SGPR. (See :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). The data preloaded is copied from the kernarg segment, the amount of data is; determined by the value specified in the kernarg_preload_spec_length field of; the kernel descriptor. This data is then loaded into consecutive User SGPRs. The; number of SGPRs receiving preloaded kernarg data corresponds with the value; given by kernarg_preload_spec_length. The preloading starts at the dword offset; within the kernarg segment, which is specified by the; kernarg_preload_spec_offset field. If the kernarg_preload_spec_length is non-zero, the CP firmware will append an; additional 256 bytes to the kernel_code_entry_byte_offset. This addition; facilitates the incorporation of a prologue to the kernel entry to handle cases; where code designed for kernarg preloading is executed on hardware equipped with; incompatible firmware. If hardware has compatible firmware the 256 bytes at the; start of the kernel entry will be skipped. .. _amdgpu-amdhsa-kernel-prolog:. Kernel Prolog; ~~~~~~~~~~~~~. The compiler performs initialization in the kernel prologue depending on the; target and information about things like stack usage in the kernel and called; functions. Some of this initialization requires the compi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:191078,load,loaded,191078,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"h; the dictionaries are to be created. In addition, it allows to specify; properties of classes or data members, without the need to add comments in; the source code. This is of primary importance when dictionaries must be; created for classes residing in code which cannot be modified.; For a complete description of the structure of the *selection XML files*; and the way in which attributes can be set, refer to the `genreflex --help`; command. It is important to observe that *selection XML files* can be used in presence; of `rootcling` invocations instead of `LinkDef` files. ### The `ROOT::Meta::Selection` namespace. Not only `LinkDef` and `selection` files allow to select the classes for which; the dictionaries must be created: a third method is available. This is; represented by the `ROOT::Meta::Selection` namespace. The idea behind this; technique is that all the classes which are located in this special namespace; are automatically selected for dictionary generation. All the properties and; annotations allowed by `LinkDef` and `selection XML` files are possible.; For a detailed documentation of the features of the `ROOT::Meta::Selection`; namespace, refer to its online documentation. ## Adding a Class with ACLiC. \index{adding a class!ACLiC}; **Step 1:** Define your class. ``` {.cpp}; #include ""TObject.h"". // define the ABC class and make it inherit from TObject so that; // we can write ABC to a ROOT file; class ABC : public TObject {. public:; Float_t a, b, c, p;; ABC() : a(0), b(0), c(0), p(0){};. // Define the class for the dictionary; ClassDef (ABC,1); };. // Call the ClassImp macro to give the ABC class RTTI and; // full I/O capabilities. #if !defined(__CLING__); ClassImp(ABC);; #endif; ```. **Step 2:** Load the ABC class in the script. ``` {.cpp}; // Check if ABC is already loaded; if (!TClass::GetDict(""ABC"")) {; gROOT->ProcessLine("".L ABCClass.C++"");; }. // Use the Class; ABC *v = new ABC;; v->p = (sqrt((v->a * v->a)+ (v->b * v->b)+(v->c * v->c)));; ```; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/AddingaClass.md:36475,load,loaded,36475,documentation/users-guide/AddingaClass.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/AddingaClass.md,1,['load'],['loaded']
Performance,"hadow call stack' in the function prolog in; non-leaf functions and loading the return address from the shadow call stack; in the function epilog. The return address is also stored on the regular stack; for compatibility with unwinders, but is otherwise unused. The aarch64 implementation is considered production ready, and; an `implementation of the runtime`_ has been added to Android's libc; (bionic). An x86_64 implementation was evaluated using Chromium and was found; to have critical performance and security deficiencies--it was removed in; LLVM 9.0. Details on the x86_64 implementation can be found in the; `Clang 7.0.1 documentation`_. .. _`implementation of the runtime`: https://android.googlesource.com/platform/bionic/+/808d176e7e0dd727c7f929622ec017f6e065c582/libc/bionic/pthread_create.cpp#128; .. _`Clang 7.0.1 documentation`: https://releases.llvm.org/7.0.1/tools/clang/docs/ShadowCallStack.html. Comparison; ----------. To optimize for memory consumption and cache locality, the shadow call; stack stores only an array of return addresses. This is in contrast to other; schemes, like :doc:`SafeStack`, that mirror the entire stack and trade-off; consuming more memory for shorter function prologs and epilogs with fewer; memory accesses. `Return Flow Guard`_ is a pure software implementation of shadow call stacks; on x86_64. Like the previous implementation of ShadowCallStack on x86_64, it is; inherently racy due to the architecture's use of the stack for calls and; returns. Intel `Control-flow Enforcement Technology`_ (CET) is a proposed hardware; extension that would add native support to use a shadow stack to store/check; return addresses at call/return time. Being a hardware implementation, it; would not suffer from race conditions and would not incur the overhead of; function instrumentation, but it does require operating system support. .. _`Return Flow Guard`: https://xlab.tencent.com/en/2016/11/02/return-flow-guard/; .. _`Control-flow Enforcement Technology",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ShadowCallStack.rst:1297,optimiz,optimize,1297,interpreter/llvm-project/clang/docs/ShadowCallStack.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ShadowCallStack.rst,2,"['cache', 'optimiz']","['cache', 'optimize']"
Performance,"han `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ###### Protect large, load-heavy blocks with a single lfence. It may be worth using a single `lfence` instruction at the start of a block; which begins with a (very) large number of loads that require ind",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35417,load,loaded,35417,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"han other alias; analyses have. Alias analysis driven transformations; -------------------------------------. LLVM includes several alias-analysis driven transformations which can be used; with any of the implementations above. The ``-adce`` pass; ^^^^^^^^^^^^^^^^^^. The ``-adce`` pass, which implements Aggressive Dead Code Elimination uses the; ``AliasAnalysis`` interface to delete calls to functions that do not have; side-effects and are not used. The ``-licm`` pass; ^^^^^^^^^^^^^^^^^^. The ``-licm`` pass implements various Loop Invariant Code Motion related; transformations. It uses the ``AliasAnalysis`` interface for several different; transformations:. * It uses mod/ref information to hoist or sink load instructions out of loops if; there are no instructions in the loop that modifies the memory loaded. * It uses mod/ref information to hoist function calls out of loops that do not; write to memory and are loop-invariant. * It uses alias information to promote memory objects that are loaded and stored; to in loops to live in a register instead. It can do this if there are no may; aliases to the loaded/stored memory location. The ``-argpromotion`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-argpromotion`` pass promotes by-reference arguments to be passed in; by-value instead. In particular, if pointer arguments are only loaded from it; passes in the value loaded instead of the address to the function. This pass; uses alias information to make sure that the value loaded from the argument; pointer is not modified between the entry of the function and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis information to reason about loads and stores. .. _the clients:. Clients for debugging and evaluation of implementations; -------------------------------------------------------. These passes are useful for evaluating the various alias analysis; implementations. You ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:28061,load,loaded,28061,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['load'],['loaded']
Performance,"han; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_wa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:280298,load,load,280298,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:279522,load,load,279522,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
